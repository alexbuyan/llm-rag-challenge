import os

os.environ["TOKENIZERS_PARALLELISM"] = "false"

from typing import Dict, Any, List
import logging

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


class InterviewRAGService:
    def __init__(self, index, config: Dict):
        self.index = index
        self.config = config
        self.query_engine = None
        self._initialize_query_engine()

    def _initialize_query_engine(self):
        """Ð˜Ð½Ð¸Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ Ð´Ð²Ð¸Ð¶ÐºÐ° Ð·Ð°Ð¿Ñ€Ð¾ÑÐ¾Ð²"""
        try:
            self.query_engine = self.index.as_query_engine(
                similarity_top_k=self.config.get("similarity_top_k", 4),
                response_mode="tree_summarize",
                temperature=0.1,
                verbose=True
            )
            logger.info("Query engine initialized")
        except Exception as e:
            logger.error(f"Error initializing query engine: {e}")

    def get_study_context(self, query: str) -> Dict[str, Any]:
        """ÐŸÐ¾Ð»ÑƒÑ‡ÐµÐ½Ð¸Ðµ Ñ€ÐµÐ»ÐµÐ²Ð°Ð½Ñ‚Ð½Ð¾Ð³Ð¾ ÐºÐ¾Ð½Ñ‚ÐµÐºÑÑ‚Ð° Ð´Ð»Ñ Ð¸Ð·ÑƒÑ‡ÐµÐ½Ð¸Ñ"""
        if not self.query_engine:
            return {"error": "Query engine not initialized"}

        try:
            # ÐŸÐ¾Ð»ÑƒÑ‡ÐµÐ½Ð¸Ðµ Ñ€ÐµÐ»ÐµÐ²Ð°Ð½Ñ‚Ð½Ñ‹Ñ… Ñ‡Ð°Ð½ÐºÐ¾Ð²
            retriever = self.index.as_retriever(
                similarity_top_k=self.config.get("similarity_top_k", 4)
            )

            nodes = retriever.retrieve(query)

            # Ð¡Ñ‚Ñ€ÑƒÐºÑ‚ÑƒÑ€Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ ÐºÐ¾Ð½Ñ‚ÐµÐºÑÑ‚Ð°
            context = {
                "query": query,
                "total_chunks": len(nodes),
                "chunks": [],
                "topics": set(),
                "sources": set()
            }

            for i, node in enumerate(nodes, 1):
                score = node.score or 0.0
                content = node.node.get_content().strip()
                metadata = node.node.metadata

                chunk_info = {
                    "id": i,
                    "score": float(score),
                    "content": content[:500] + "..." if len(content) > 500 else content,
                    "full_content": content,
                    "metadata": {
                        "title": metadata.get('title', 'Unknown'),
                        "source": metadata.get('source', 'Unknown'),
                        "url": metadata.get('url', ''),
                        "topic": metadata.get('topic', '')
                    }
                }

                context["chunks"].append(chunk_info)
                if metadata.get('topic'):
                    context["topics"].add(metadata['topic'])
                if metadata.get('source'):
                    context["sources"].add(metadata['source'])

            context["topics"] = list(context["topics"])
            context["sources"] = list(context["sources"])

            return context

        except Exception as e:
            logger.error(f"Error getting study context: {e}")
            return {"error": str(e)}

    def generate_study_guide(self, query: str, context: Dict[str, Any]) -> str:
        """Ð“ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ñ ÑÑ‚Ñ€ÑƒÐºÑ‚ÑƒÑ€Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ð¾Ð³Ð¾ Ñ€ÑƒÐºÐ¾Ð²Ð¾Ð´ÑÑ‚Ð²Ð° Ð´Ð»Ñ Ð¸Ð·ÑƒÑ‡ÐµÐ½Ð¸Ñ"""
        if "error" in context:
            return f"Error: {context['error']}"

        guide_parts = []

        # Ð—Ð°Ð³Ð¾Ð»Ð¾Ð²Ð¾Ðº
        guide_parts.append("=" * 80)
        guide_parts.append(f"INTERVIEW PREPARATION GUIDE: {query.upper()}")
        guide_parts.append("=" * 80)

        # ÐžÐ±Ð·Ð¾Ñ€
        guide_parts.append("\nðŸ“‹ OVERVIEW")
        guide_parts.append("-" * 40)
        guide_parts.append(f"â€¢ Found {context['total_chunks']} relevant information chunks")
        guide_parts.append(f"â€¢ Sources: {', '.join(context['sources'])}")
        guide_parts.append(f"â€¢ Topics covered: {', '.join(context['topics'])}")

        # ÐšÐ»ÑŽÑ‡ÐµÐ²Ñ‹Ðµ Ð²Ð¾Ð¿Ñ€Ð¾ÑÑ‹ Ð´Ð»Ñ Ð¸Ð·ÑƒÑ‡ÐµÐ½Ð¸Ñ
        guide_parts.append("\nâ“ KEY QUESTIONS TO PREPARE")
        guide_parts.append("-" * 40)

        # Ð˜Ð·Ð²Ð»ÐµÑ‡ÐµÐ½Ð¸Ðµ Ð¿Ð¾Ñ‚ÐµÐ½Ñ†Ð¸Ð°Ð»ÑŒÐ½Ñ‹Ñ… Ð²Ð¾Ð¿Ñ€Ð¾ÑÐ¾Ð² Ð¸Ð· ÐºÐ¾Ð½Ñ‚ÐµÐºÑÑ‚Ð°
        key_questions = self._extract_key_questions(context)
        for i, question in enumerate(key_questions, 1):
            guide_parts.append(f"{i}. {question}")

        # ÐšÐ¾Ð½Ñ†ÐµÐ¿Ñ†Ð¸Ð¸ Ð´Ð»Ñ Ð¿Ð¾Ð½Ð¸Ð¼Ð°Ð½Ð¸Ñ
        guide_parts.append("\nðŸ’¡ KEY CONCEPTS TO UNDERSTAND")
        guide_parts.append("-" * 40)

        concepts = self._extract_key_concepts(context)
        for concept in concepts:
            guide_parts.append(f"â€¢ {concept}")

        # Ð ÐµÐ»ÐµÐ²Ð°Ð½Ñ‚Ð½Ñ‹Ðµ Ð¼Ð°Ñ‚ÐµÑ€Ð¸Ð°Ð»Ñ‹
        guide_parts.append("\nðŸ“š RELEVANT STUDY MATERIALS")
        guide_parts.append("-" * 40)

        for chunk in context["chunks"]:
            guide_parts.append(f"\n[Source: {chunk['metadata']['source']}]")
            guide_parts.append(f"Title: {chunk['metadata']['title']}")
            if chunk['metadata']['url']:
                guide_parts.append(f"URL: {chunk['metadata']['url']}")
            guide_parts.append(f"Relevance score: {chunk['score']:.3f}")
            guide_parts.append(f"Content:\n{chunk['content']}\n")

        # ÐŸÑ€Ð°ÐºÑ‚Ð¸Ñ‡ÐµÑÐºÐ¸Ðµ ÑÐ¾Ð²ÐµÑ‚Ñ‹
        guide_parts.append("\nðŸŽ¯ PRACTICAL TIPS")
        guide_parts.append("-" * 40)
        guide_parts.append("1. Practice explaining each concept out loud")
        guide_parts.append("2. Create flashcards for key terms")
        guide_parts.append("3. Solve related coding problems")
        guide_parts.append("4. Prepare real-world examples")
        guide_parts.append("5. Review system design trade-offs")

        # Ð¡Ð»ÐµÐ´ÑƒÑŽÑ‰Ð¸Ðµ ÑˆÐ°Ð³Ð¸
        guide_parts.append("\nðŸš€ NEXT STEPS")
        guide_parts.append("-" * 40)
        guide_parts.append("1. Review the materials above")
        guide_parts.append("2. Practice with mock interviews")
        guide_parts.append("3. Update your knowledge gaps")
        guide_parts.append("4. Prepare your success stories")

        return "\n".join(guide_parts)

    def _extract_key_questions(self, context: Dict) -> List[str]:
        """Ð˜Ð·Ð²Ð»ÐµÑ‡ÐµÐ½Ð¸Ðµ ÐºÐ»ÑŽÑ‡ÐµÐ²Ñ‹Ñ… Ð²Ð¾Ð¿Ñ€Ð¾ÑÐ¾Ð² Ð¸Ð· ÐºÐ¾Ð½Ñ‚ÐµÐºÑÑ‚Ð°"""
        questions = []

        # ÐžÐ±Ñ‰Ð¸Ðµ Ð²Ð¾Ð¿Ñ€Ð¾ÑÑ‹ Ð´Ð»Ñ ÑÐ¾Ð±ÐµÑÐµÐ´Ð¾Ð²Ð°Ð½Ð¸Ð¹
        base_questions = [
            f"What is {context['query']} and how does it work?",
            f"Explain the main components of {context['query']}",
            f"What are the advantages and disadvantages of {context['query']}?",
            f"How would you implement {context['query']} in a real system?",
            f"What are common use cases for {context['query']}?"
        ]

        # Ð”Ð¾Ð±Ð°Ð²Ð»ÐµÐ½Ð¸Ðµ Ð²Ð¾Ð¿Ñ€Ð¾ÑÐ¾Ð² Ð¸Ð· ÐºÐ¾Ð½Ñ‚ÐµÐºÑÑ‚Ð°
        for chunk in context["chunks"]:
            content = chunk["full_content"].lower()

            # ÐŸÐ¾Ð¸ÑÐº Ð²Ð¾Ð¿Ñ€Ð¾ÑÐ¾Ð² Ð² Ñ‚ÐµÐºÑÑ‚Ðµ
            if '?' in content:
                sentences = content.split('.')
                for sentence in sentences:
                    if '?' in sentence and len(sentence.split()) > 5:
                        question = sentence.strip()
                        if question not in questions and len(questions) < 10:
                            questions.append(question[:200])  # ÐžÐ³Ñ€Ð°Ð½Ð¸Ñ‡ÐµÐ½Ð¸Ðµ Ð´Ð»Ð¸Ð½Ñ‹

        # Ð•ÑÐ»Ð¸ Ð½Ðµ Ð½Ð°ÑˆÐ»Ð¸ Ð²Ð¾Ð¿Ñ€Ð¾ÑÐ¾Ð² Ð² ÐºÐ¾Ð½Ñ‚ÐµÐºÑÑ‚Ðµ, Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼ Ð±Ð°Ð·Ð¾Ð²Ñ‹Ðµ
        if not questions:
            questions = base_questions[:5]

        return questions[:7]  # ÐžÐ³Ñ€Ð°Ð½Ð¸Ñ‡Ð¸Ð²Ð°ÐµÐ¼ ÐºÐ¾Ð»Ð¸Ñ‡ÐµÑÑ‚Ð²Ð¾

    def _extract_key_concepts(self, context: Dict) -> List[str]:
        """Ð˜Ð·Ð²Ð»ÐµÑ‡ÐµÐ½Ð¸Ðµ ÐºÐ»ÑŽÑ‡ÐµÐ²Ñ‹Ñ… ÐºÐ¾Ð½Ñ†ÐµÐ¿Ñ†Ð¸Ð¹ Ð¸Ð· ÐºÐ¾Ð½Ñ‚ÐµÐºÑÑ‚Ð°"""
        concepts = set()

        # ÐžÐ±Ñ‰Ð¸Ðµ ÐºÐ¾Ð½Ñ†ÐµÐ¿Ñ†Ð¸Ð¸ Ð´Ð»Ñ ÑÐ¾Ð±ÐµÑÐµÐ´Ð¾Ð²Ð°Ð½Ð¸Ð¹
        base_concepts = [
            "Time and Space Complexity",
            "System Architecture",
            "Design Patterns",
            "Best Practices",
            "Trade-offs and Optimization",
            "Scalability Considerations",
            "Security Implications"
        ]

        # Ð”Ð¾Ð±Ð°Ð²Ð»ÐµÐ½Ð¸Ðµ Ð¸Ð· ÐºÐ¾Ð½Ñ‚ÐµÐºÑÑ‚Ð°
        for chunk in context["chunks"]:
            metadata = chunk["metadata"]
            if metadata.get('topic'):
                concepts.add(metadata['topic'].title())

        # Ð”Ð¾Ð±Ð°Ð²Ð»ÐµÐ½Ð¸Ðµ Ð±Ð°Ð·Ð¾Ð²Ñ‹Ñ… ÐºÐ¾Ð½Ñ†ÐµÐ¿Ñ†Ð¸Ð¹
        for concept in base_concepts:
            concepts.add(concept)

        return list(concepts)[:10]  # ÐžÐ³Ñ€Ð°Ð½Ð¸Ñ‡Ð¸Ð²Ð°ÐµÐ¼ ÐºÐ¾Ð»Ð¸Ñ‡ÐµÑÑ‚Ð²Ð¾


# ÐšÐ»Ð°ÑÑ Ð´Ð»Ñ Ð¾Ð±Ñ€Ð°Ñ‚Ð½Ð¾Ð¹ ÑÐ¾Ð²Ð¼ÐµÑÑ‚Ð¸Ð¼Ð¾ÑÑ‚Ð¸
class RAGService:
    def __init__(self, persist_dir: str = "data/processed", use_openai_embeddings: bool = True):
        self.persist_dir = persist_dir
        self.use_openai_embeddings = use_openai_embeddings
        self.rag_service = None

    def initialize(self):
        from src.processing.index import load_existing_index
        index = load_existing_index(self.persist_dir, self.use_openai_embeddings)
        if not index:
            return False

        config = {"similarity_top_k": 4}
        self.rag_service = InterviewRAGService(index, config)
        return True

    def query(self, question: str) -> str:
        if not self.rag_service:
            if not self.initialize():
                return "Error: Failed to initialize RAG service"

        context = self.rag_service.get_study_context(question)
        return self.rag_service.generate_study_guide(question, context)