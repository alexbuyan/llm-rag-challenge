{"docstore/metadata": {"ac7499e0-371d-4829-a4b8-563891562236": {"doc_hash": "577064dc812471e5287ab89a37efadfe0f6b591ebe15856a420867b22dfd79c3"}, "0f9c0d06-8922-4dbe-a246-08baf420899b": {"doc_hash": "0f6d54af25310afcc43f78565f9edd303db2b47b54360a4918caf553ef41f18b"}, "66b7b794-a9d2-4d13-a6e5-9dd1837e9ee3": {"doc_hash": "3bf0849c3235faca5849cf0f906b9ff880744c18d6e033691e763686b78ae2bf"}, "6317a77f-e5be-4269-8575-3ee8813a2132": {"doc_hash": "df0de7f9529bca2dd355582f81bc893759c107a909c5ce6d421c47fedc431ddb"}, "04f8d0a1-308f-40f8-92fe-9384ad9d72fb": {"doc_hash": "26c2033f5b6bcd2e0d018b2b3a4177ec2ea9d0f87d6e6c1c8144bc6d3be1403e"}, "4b1d74f3-f204-45e7-af3d-fe1d5c283494": {"doc_hash": "133ad218bd9d1a81a1a54732dd429e263c99e08195eb2c567cec92ab544163b6"}, "0a3a9eab-cd91-40b5-a082-99f565c8aa9d": {"doc_hash": "40fc1c121908f5b9413d1076a3ce8cc676980f83021b36a58472e2485b0e9ddf"}, "958cd863-8c06-4326-b6b7-9b02d03b9cc9": {"doc_hash": "481a8d7c663f11c98eb6d2233e8fcc419564977fb61f6ea4993611949a2f2a4e"}, "51ab4d3a-f32a-48a6-b92c-ffb52d2f78db": {"doc_hash": "3a92265c2753374eaf9833c324aaefecbfa704f276946f47a85cbd378fb63e25"}, "d150f798-78fb-4979-94e1-a803e1e43a06": {"doc_hash": "10a61caf3302a3ff46ebfd1769d6bf5e32a4c1a811cebc904cb7d51b5f18b7e0"}, "adc017cc-ac1a-4f0e-ac74-76d2445ee2bc": {"doc_hash": "5f4a247bf0865aff1d93e73498e3f1b6302a00414c6b3d87bea118c3df2999a5"}, "0f1c73c9-b684-4671-aa71-47e6eb8ac488": {"doc_hash": "585632617bd840d020eab73f1835620130ddd1e4fc00d3fb599c87c94f769de2"}, "3a6e1496-8dd5-4de9-9768-3f24e646a775": {"doc_hash": "ae6ca83e09b0144f3edc4367838ba897b0a217f5a72f8df8fcdbf2b9734bb323"}, "010eda0c-8a04-4332-9be4-a79e76b5d00a": {"doc_hash": "2a91bd164a2655b49df072de5f99342666150f24959b63186681850bf6fe40a0"}, "0591e821-5b81-42e4-838f-6f6de18f7fc3": {"doc_hash": "ab0ddb6d9d28ee1064e8c2cbce8c46dc14220907a0e32f2468d362f5d6069a7e"}, "b92297f3-bcb0-4c62-97fb-f22f4c86396f": {"doc_hash": "1de291695f509e23f7a968ad5b0fb589366af63431ba7c2e85ec424e052ffed9"}, "d251dafc-c95d-4067-9111-eca2d7dd67ed": {"doc_hash": "2775f2c6259e67678afe2022670a9f39cfdbd97fccee40e5c8f85c7812ed58dd"}, "ac764fbb-d2ed-4832-ab34-50ce58d65ed6": {"doc_hash": "0277067319e8189aaf9c8ddf280436887890038ed8c015c361acce74ff96fe5e"}, "c518dd2c-1601-4e39-bd26-680434916d24": {"doc_hash": "a159dc290c0c293f7d81e2fa0441f7dcd7cd26bc99811fecca8e4ce1f468c621"}, "6900c6ce-0e26-4bf5-be6b-d349443630af": {"doc_hash": "5143c12d79f9826daf9d895b8b4602db2a07f76b159dfffb5e903e1dcac30447"}, "23995e31-dcbf-46a9-941e-599c6e4c9b5a": {"doc_hash": "186fac0d0f66dde8296a1d124a5017ab7818a3708dd56fad126ca172093d0a9c"}, "e7692e97-75d4-452d-8eda-eecdfa47a4fc": {"doc_hash": "488e7794bd8ba9bdb78a101aab3a9ab4cd1690dae3f68e2bd46ebe3726c22c10"}, "126e363a-3782-48c3-ac37-112382f9bffb": {"doc_hash": "8467afd73dcd4d8157cb28a627e53c3d94112e80f44eb051a446d04ff0631c15"}, "599f904d-f72e-4f5f-be6d-e9c5d25483ce": {"doc_hash": "6d856465026edbb93dd7205950a70b03696996777c780e53f9bdc9d3ca712726"}, "c0673f5c-d45b-41c3-9220-951c9694195f": {"doc_hash": "34e1b60b756c1935083620dc991c54ad28e70fb6bc4a9a5d554fb499937e7724"}, "513abfd1-07ca-4784-bd5e-41c0c3f8dd0f": {"doc_hash": "e0a1f1de902c2937a4eca4e0035bb90f6405d912cd0cfdc9446ba8d5f756cb31"}, "f39e0524-039c-4e15-9417-39925ee5588e": {"doc_hash": "7d8657c776abebaaf406e0332a70119c4590221934ca4cd588910a9f5592a3bb"}, "1b74cad1-591c-4e89-81c0-0fd0159d645a": {"doc_hash": "89306f5f6d11f9f69594821258392cd19f4dc75c996c366dda1c5a8bc883a2e0"}, "1a304a07-df7f-4b7f-8c85-ed37974052af": {"doc_hash": "83e35233197ea1182512605c58a47977e8b711ecf705d5a57562f8066e64e374"}, "285e0372-1d38-4d5c-a3a0-5bc305c26aa1": {"doc_hash": "fc559e373b81480d5b1407bc6db2b70175912c85639986266e802791090fa33c"}, "b1c1759b-075a-4b9d-8632-3d6c8b8757e4": {"doc_hash": "ff02da890b0b52878346a70e6cd1a55bae952b8f9df05261865073ec4820ba5c"}, "db4b926c-c2fa-48b9-b971-d445cbb2a32d": {"doc_hash": "a7cdee1c928411ab19eff5cb842a86ad8b517179b1d7380511c2e9e151229f90"}, "bfd03b52-815b-4501-a310-d4bcefc0cae7": {"doc_hash": "2663eb4044736c92e687e10fa2b6d337b99bb5bec1fdb002ceb7165396aed3cc"}, "e5d7d61a-3801-4a38-8bc1-dd5064fb1c22": {"doc_hash": "19b09248e881212245acbc6ec7b8542f49ee0037cab09b88309b9b1fe21a8541"}, "54c0018c-5d06-4f2d-b667-40a030539f52": {"doc_hash": "fcbaa3c9a6b110b571da04f14547c498f901183b81db69968d41796ead5b5793"}, "9b912e35-b125-42e8-be12-70fdb75056ad": {"doc_hash": "c5b418cf98be147b154006bd4f5f655bd755794dbbd6a8c65a321b1ab579d765"}, "93fad366-2ad9-4df2-8f76-fe61187b2bce": {"doc_hash": "6e198e5b85c6b0d30d723eae98f1e9ca738a5709bf5f367a746056fa7a0f8b30"}, "e75ddac4-8bcb-43a5-ac3a-a61218f561c3": {"doc_hash": "b5397efe52f7d773e12723c55fdac600c1aca8753ba94425e9c66fb3bbba0082"}, "f6a7c0a4-f9d2-4aa8-9a41-e76cddf15c5f": {"doc_hash": "bcda864a5646bfd77b549e5a45834eb959896d6a1cd6fca9bdd58664133d1980"}, "9d2c7fb5-b6e0-4a53-8fa4-af7c73ed831a": {"doc_hash": "460687a00e7a1e6827ca77944994f1764047b4748734008db08fd510fe9d97f5"}, "cb641677-5bfb-48b8-9e3c-20035e771c2f": {"doc_hash": "bfbdabd52933c7c8d165a350aba1a162e24876c0854bd7d5321fe5ae88f78c9c"}, "47d7f859-a471-4a50-b2bb-c9ed011d899b": {"doc_hash": "2a3bfb94d0316479cf3888e67a01395701106bc725b0843ebaa0c55412a34c84"}, "3fd785be-8bb9-4078-aefb-20303d24b412": {"doc_hash": "628638e8ef873a29b220608393e68758596aecd46f2ee6e08fceecacd6ce932e"}, "fef459fe-9ea0-4559-a73f-25461bf42691": {"doc_hash": "7b1f407e5e0b75c8789b7b500ed89d9a155d65babb6df6839787f9f8d0b95f3f"}, "1d654ea2-8161-40e8-b3a6-c024089c2824": {"doc_hash": "cf7c1bf1c145f8babb2cf0c30fd0e931dae99ea53b0f3aed30b7ec5ce5f6d3cc"}, "e2d4fbf6-11e7-49fd-b4ac-62173546f7a9": {"doc_hash": "2fe55a75ed6aec34c233d84167ba714a14c2ccbcf88378bb58f92464c722ee33"}, "5312f52d-471a-4c62-89b7-e62c0bb9c629": {"doc_hash": "b532a0c7e1df13b55719b36e12c32efb443b17bff772304dd71dbbec0312a5b6"}, "3066ea3c-0d8a-4182-8bd3-c2df08edc055": {"doc_hash": "33aa895932d3b1e6d60b38a9f1a1b3e49af0bff449e7105980353047adca7f30"}, "66cb1223-55f7-46b6-948e-64b7c60f34e0": {"doc_hash": "39c977b6944e9c3da6377725efd510ba100a2ecd2373288e9fe5041a2cbde9d4"}, "977d7711-45c1-4695-b982-1da44768bb07": {"doc_hash": "b6d0b142202a3dbaf71df18068f84e9754c077dea392b63176a2dcbeb39da061"}, "c02d6c69-294b-470a-942d-4b12350a0dc1": {"doc_hash": "b9c00f78624f44327f3a618d851ac6585266c516264b8aa929bcaafa8fcb3a19"}, "421542e0-89b9-4ac0-be2f-3f46dd755936": {"doc_hash": "70a1e36966b7ffc15c75ce0824fbb156b6c99fc94a13121216a2f849784c44ad"}, "74817cbc-3a70-4f7e-afcf-319e94e9405c": {"doc_hash": "9bc9c22e732d10759f8974f0e3fdd7d9469da3a4d532b5acb378e303a6f2b989"}, "c9639cdd-73e5-4d6e-b377-ce6712cb0b81": {"doc_hash": "1ec5c1fe25b62ea7ddf9f78846acca25cbf827198a1c7cb466724774c4ced6bb"}, "172d5992-a73d-4be5-a1e6-fb1f1ef24036": {"doc_hash": "9f0cfb891bbb603d205abedb5dca9ef845c9534c2801078942fa6748739aa211"}, "4b5a143f-9773-4431-a175-8cacbb178d4d": {"doc_hash": "8c8bbfca9418d55b436c28d10a3e3a878fb6b84298932f3e269193684fd06e12"}, "92aad19a-bea7-4108-a444-0579e31347d6": {"doc_hash": "0253affd518bdb51c2cdef370bacd21524e4da225ad0a49484a150c00a190875"}, "bda5dfbd-7903-40b5-ba83-8e3df737970f": {"doc_hash": "753d1abb38d8e310c9ca54607a74fc226f30f2d1e9115dabdb5f59a53a2d5a6b"}, "1d5bff59-7709-4441-a877-da551aa6e560": {"doc_hash": "590f824815b0fcc755e6076ed50b9f06087270fdd30aab22aaae60c7e3949a19"}, "b380c5f2-b67f-477b-9c58-c85eb4cd8879": {"doc_hash": "fff786c0cd729be09621dd1dc9e5386b81051cae6ae93813e6f41980ca077d1f"}, "52030ca6-5ac9-426c-86af-f1b4f687e3da": {"doc_hash": "027c49f5cb51424169ec0b138a2b5dd36f1d2ec0f5263426fb72bf6d93db79f9"}, "be9bc060-3ff2-4dd7-8069-31c76fb3da0b": {"doc_hash": "cf6fa4affaa369336257984e9d12701c84ff9439de3d293ed880b607262fdd2f"}, "b2678f8e-18c8-4e4a-ab66-90bffda3334a": {"doc_hash": "02fbe89170501df1d583988364445e7d2c53297e7ff8db748a38a949e9189d7d"}, "0db00f16-1ab3-44b3-864f-239a8b6c0c83": {"doc_hash": "52f34ac20b9965f01ddbe35e44a9f9665a387fc03c685b45385742909c831a77"}, "75ac6d6d-352b-4f98-a0bd-368c6ed4130d": {"doc_hash": "2c13d87dade1bb2a47f4ff7db26400baa731f4d5916c8f4a3647497d83dfb2d4"}, "e7c3358f-0526-4599-8e79-510d33e5ccb0": {"doc_hash": "175dc6f9f07cab6c94406620c23f833387f44b94dccd1a9b11142aa682431382"}, "1202ddf4-35be-44e0-a50b-828eb2214f51": {"doc_hash": "65f548656df65f51831fe0527bc9239bdec5077f586bb33cc1540e2c067aed1f"}, "40ab73bf-551a-4c7a-b187-2ab6de80c003": {"doc_hash": "67c56123971eedff24af4affdd3a7840acc09363d3e29bc0e9da024bbb7fbcd9"}, "2cbed6e4-0a7c-4751-a651-52749f1a6c5b": {"doc_hash": "cab0e6ba1c08d55394d62e142ffaa7049ab00bf000c98e7f801c91b9243ae3bf"}, "719b18b2-98fd-42d5-851f-b599fda485c3": {"doc_hash": "f3428cfca611d591eea35dee7fc468f1d9d0be1d6e939adaf6d5808fcbaf709f"}, "23f78ec7-dd81-4ceb-abe6-8758531c22b3": {"doc_hash": "99edf2ebbca946aeff4a710657ef4670f20de038960422532292bf63450f7f32"}, "ab8ffe0a-1b6e-4183-b065-c154033730a8": {"doc_hash": "b525c0463485504af21d488d4870ea00c7b933c35a0218728afbec36811e9ae6"}, "cd73c69f-1842-4df7-badf-dcd8a335ce67": {"doc_hash": "4989cc0c9701acad07f9d15575d64bb258686454d1cdafc33355631b9d1776ad"}, "785f800a-a126-42a7-9386-bcedeec6db60": {"doc_hash": "f9f9d40132dcb04e09cf11790f00c866a8dfa9745c44bb60f894b9f578276ae5"}, "7464b228-9029-41fd-8004-b8efec212ad5": {"doc_hash": "3ece0eda22840eb32f7bc935d82e0ab0a5376cd6554f7f37cb0f38cf3e33b2ee"}, "63e2cf92-f789-4614-a348-79e8023ec8f9": {"doc_hash": "21632bb990287575e123969668038629823b8ad057393c2926ec11fa45ee0666"}, "3c43b511-d047-477b-9ed0-767b9e119d32": {"doc_hash": "93254a95b4760a369cf152139700dbd87e14bdcd39143efac679d33c7a858caa"}, "9569d4d3-8096-4a1f-bff2-a0dbcf3f27fb": {"doc_hash": "98e0d7c98b14adb62bc810536c6e13da99346ee98a1c27c38329c536083808f4"}, "c0be8d2d-0c6b-483a-802f-cb32f8777943": {"doc_hash": "a6f8fbfdd030bcacc6267eddf1222191798fa832c14830df23a6de00cd332f23"}, "e664eb65-e022-4f27-a078-8b36382578f3": {"doc_hash": "aa476ea20312a01028acc5d25dbc0d92fef3e823ab80d059d50d12c7fbb7c099"}, "e2b81dee-580f-459c-93d1-8675ef524f9c": {"doc_hash": "bdd45160a4b9375562b72d746cdbc3f4178276cfd55ad41733458310d5acbade"}, "0915ef3f-e0e8-434c-aaed-6f76249a9327": {"doc_hash": "87e203125fb92eaae4f7b0f8285339a10a7b011fa380f000740ab0934e605fcf"}, "7fe2e34a-f0dc-4292-a292-0859f7f1af33": {"doc_hash": "72ff3bc422ffed25fca706cd8d4cc8e36b1e0ad499ae3404f80165b8d0c33c3e"}, "78e70001-7d64-4d27-96d3-282a8683115f": {"doc_hash": "769d150c93f6d553d2fb7fe1f94b82e61955b8c7ba29cf50dfcc07ee3b76dcbc"}, "aa4f36f2-41f5-4dba-8939-ca44ceb388fd": {"doc_hash": "4d6856fe86da81c2d42cd87e678d871dda1ce7b139fe0e65c53305599d8174fe"}, "d5581b74-0a29-461b-bf09-713c5c91b2fa": {"doc_hash": "914b707a8509aa797d11b30e9b4cd39205371e53e8eea0204a2910688d9203ce"}, "9fd6f04d-b140-46e8-995e-5cecd157e86f": {"doc_hash": "8333c9038e20fe358e14effeea599b0706c7f982c7fe6d89e10e43f04d619d3a"}, "acf5c0ed-ee63-4cb0-8cfa-6cc1f117ce51": {"doc_hash": "d9a25087cc9bf1e8ad8e8a0699a8b192e73a6c7c7d15b76de5f99d8e4a7e0a84"}, "f5a327cb-be6e-40af-9f97-03cf8d03f531": {"doc_hash": "b3f1c5df0c47f63669fc6deb46e7c603f702d7590b3ca70471386ebd63d7ed4e"}, "e5956047-3129-4ab4-957d-3b78d6caa8fd": {"doc_hash": "aed9c451cb2d6633bcee70fd8d5cd7d3e59594170df2c3abdce721d0889f39c3"}, "d4ffc73a-eef6-4665-af8c-3dcaaa015fa9": {"doc_hash": "5d673a8b52c88b8b5af6d559bd0bceffb686bed37fe3d05663a3b4c79eb550a2"}, "00d68343-4b5d-4cf2-9fdf-009d363b8f77": {"doc_hash": "a7191460b8e58b2cc19a740cbbf508c8c5a1a554292e3446afe1527d38c743cd"}, "42ea8556-166d-4f2b-9fbf-1ad18a2a669e": {"doc_hash": "69ccc5980fca7747dda6eb6d773e23eae93cc6b5676d0b0f48651c12f2bd8e9e"}, "e578675a-57f3-4346-a9cf-63d534aeaf60": {"doc_hash": "254b534d133afa0bd32e43232fc0d8a44702398b6c5a84c22a750fe186fc7ed1"}, "4cf34b23-5a92-4095-96d3-94605545a6aa": {"doc_hash": "55841b6733bff6d4e053aa799bf7e93cf734e231c04454e9e776bb5f221184b3"}, "3024380c-80b7-4d35-8d81-71c242fc3441": {"doc_hash": "3d722a965bd67e32a16113f617b8a29c9e0508e70ccf457e3e3ab6aa6a48453d"}, "d74fee4b-74a0-40f0-8eeb-f876277c7fca": {"doc_hash": "0b5a02750d5a19d812ee3f7c9b8abaa01a7495f842a633cf4ff5c92b1b354242"}, "20b3b4bc-a2d9-469d-b6eb-3e4440044a36": {"doc_hash": "4434a62c57bfd46b68b25af2385cd5a2f87cf7a05917eeca408004ac3a66fdc5"}, "2324ca46-bbb4-4a98-9c89-299fb48e765d": {"doc_hash": "27c6362f362215deeb4449f8986db7475e3b3807b1ba0eddc4a5e5e1384f60fd"}, "54d5c5c8-3556-48a1-9e79-2e3cb7c99224": {"doc_hash": "69c11401f33a8d9f453058bbb6d964cc95958dc154149d5030e1eed37377615b"}, "6e777095-602f-4dbb-8308-a3de47f1f1ca": {"doc_hash": "9ffec42c177ac7beeef1e9cd26a6311e88841307897cad45f1cb0ceedbe5c74c"}, "4f70a28c-6500-4f4d-a9e9-62ac94100ecf": {"doc_hash": "2aaed818b57c9cfe3c0c31c34e433186b0f12b66ab9ab24c1704e00414ec1300"}, "90663b36-1b16-4f13-9ceb-7f26ae82fb9e": {"doc_hash": "e323bd567d8f006ca8567cba1e1e6c7d748b96ea84b76cd2a05e2b7e13fc4cdd"}, "6cbb9f05-e75a-471d-addd-56f8c12cae6c": {"doc_hash": "1a7d7a8b9cc55f415eac86a6d7593f05cb7fb8969f03a82851fe8c1dd7e4b72a"}, "218daabb-d00f-47ce-b417-5953b7da3bdf": {"doc_hash": "01769b712a5c64ce17ccdc153ba38d6b4f4fe25a4b62d478b6bcc8efa60ac159"}, "c7426651-9112-4859-bd12-e48f074a35c8": {"doc_hash": "083b3e0bfe45f6ad637f62259969313c0efd70506a6eb03f550d406ecf34b9f9"}, "d3c910f2-7973-491a-93b7-66c2551f9c71": {"doc_hash": "276e3233b3ced9b0b2ccc60ea33e75172d2cded4531cb59fad73fd6c6eefae65"}, "ab6e80be-7b67-4943-ad37-ae28fcef98f4": {"doc_hash": "62555ae63a013f82f1a2d2edb68bdcfea29348a78459b39edb426779fd909f99"}, "d8b67d49-0086-4fb6-8c74-d1f947b98013": {"doc_hash": "08d418811c5886f3bc3f680cceda7e7a0a29d83bac0b45ce4539b31cfc9073d4"}, "7b547050-93f2-4e56-87a1-d88f740d3255": {"doc_hash": "5d91c843ba01f934ae809f887558d59b175611741bf886f3439eaee43636d813"}, "39681515-8c85-4abb-ab62-c4d0f0977426": {"doc_hash": "2d91b0114ec7dd6f9042b946aedd07c0e70a6da4c4a4648a9e01345aef857308"}, "86094565-3380-4e17-9a7a-493cbe5da686": {"doc_hash": "f6adf67971b79fc82346ad6afd9e3ec83d92d9aa9f22fd50eeaefd1cf187f460"}, "7447810b-a508-49de-b11a-06cce427985d": {"doc_hash": "8dc1bad59e0da325f8fb225d6e87ecfcdc2a1d7fdbe51f1243e056c65a22e56d"}, "63eea7af-fb3c-447e-a615-96c7fa6bd98e": {"doc_hash": "119897b3a600771a9f0da6638d3026f3f00d327859812b3c234214f41b7780aa"}, "12e1ca77-8426-4d3d-ab10-3879b17971e5": {"doc_hash": "e3ed043eda8378b90f8f3c14bb9228b12c43872b9c34251106304e07afbe5abe"}, "0d854286-079e-4779-bef3-d571491f4ce4": {"doc_hash": "c0661303fb6383fb2588abe98a957e43edff15fdb5a16198f6663357fb1986e8"}, "d8e6f09a-1c6f-4d08-a08e-608525b982f1": {"doc_hash": "578576b4ec68c3caabcdebcbdf505f2049d51c748cd10a2f7b30700df521d605"}, "1e08d2fe-8ac0-4f64-a56b-0decdb99d71c": {"doc_hash": "1709d83a3bb97eda58162e9ca31ab2f588123a3c53a07e60c43fa8241c159d15"}, "a63198f0-3d82-4093-9c75-253d3fa4b043": {"doc_hash": "5e4c952cf82d9d284d3a1e3c570e9b0a84e89ed480a3771a38e30e498657ee6e"}, "102c4efc-eeb7-41c8-b875-39ba595955a3": {"doc_hash": "8e46ba9dc85c2adf0df7314a3b2152ab70e3e636b152e7e7fdc7ea7ee491e106"}, "36f33a13-8462-4ceb-8ee2-4c23704a09ff": {"doc_hash": "99683f125e386de9dea4bdcb7412d0e52ad25afa0be7ff59a7ce6cacda4cf5ea"}, "f450d412-a1d5-4f30-a705-4d92ca30de83": {"doc_hash": "9d8d40a96d7e21662352da5dd92ec554dcc704b3c2299cdc71e6919dd01eac67"}, "7db9e367-ee68-4ea4-82f9-d99b87cef3c1": {"doc_hash": "d53e239866f0885d45ded546fab80230070fa397e2f21390f000627033d9b809"}, "923e5c9b-accb-4571-8cd8-11d062542bd3": {"doc_hash": "8b0f24c5bb3505a7c0cd8a6aaad625ec050268e8a122aad695cd4990fc39390f"}, "dc0734d4-8eb5-4574-a651-0aede047b15a": {"doc_hash": "290fff0a42ed70315ebaa53572f0d21151fec6d88970d5ed5fb0435fb1f08bfa"}, "9bb82254-c236-482c-94b0-1bd5f94184c2": {"doc_hash": "4b6b467f3ba96676a7c3b151788d624f259f9c6ca66f77b456739e689299cbdf"}, "8b0e73f7-027b-4e4e-92b7-20031e68e0cb": {"doc_hash": "cb6c20100cb95fc3393cca81121278a3b07dac52de4fd12e18f7fa5397f61d13"}, "fd325f97-52db-439d-8df0-2598c173c248": {"doc_hash": "5e0cdeb86ef84f2b62cb51097e8ddb2e24994d9087d384d261784969eb5cc15d"}, "00e6d59c-b9ce-4256-a537-4c7c3657ef7d": {"doc_hash": "a2bc7d27bdb1d29e0953bfd39d50f9bce9ed03409f488536b52d94f07194a21e"}, "cdff2e2b-da73-41cf-bbb7-68fb83ce9f53": {"doc_hash": "6afbb34906eb45d216a286cc1144999860d36546bebc7de5076c36d0a3cc1b6d"}, "c4355eb6-0952-4918-b144-08ff2a7ea3a9": {"doc_hash": "c013745e67774de237b6cdc965e07216290111e88185738c31b12d1abd8a6822"}, "ef9e6651-081a-419e-86b0-c3051cce4eb3": {"doc_hash": "22ff91e84678ea2b8aabdb2b404a4270dde7d3c84fd755f8af149e4b8409e29d"}, "7f699359-86b4-4892-944d-e1fda009bbaa": {"doc_hash": "542b4f962a0dff056ebe0b522f2db387587a057c70358aeb5274b1e22864fd14"}, "62d28a5d-4e62-41fa-9a37-8bc1fc70066c": {"doc_hash": "d0575a415d28bf715291c9a8e8000b0e4f0956c0fc9b19040a0d529c5497344d"}, "54337252-5c91-4a4a-9966-d6d892be9a9f": {"doc_hash": "eb3e34d46be940b481c583f916d2d72d9a1a92f944cba6e2aa75a4ef7cd70910"}, "325ed61c-1ee0-4c24-bde5-695dbbf99d96": {"doc_hash": "67a6f04dfe5f950ea989f70214245fd6944d01415c862015ef58755f70eadbd1"}, "8254e844-bd0f-47b6-90a8-e361f55c579f": {"doc_hash": "b09cf00d582c5a43dc4346cc4324a5b76fcae201f4c9183aeb00d3f982448842"}, "94fb958a-6867-40bb-a62f-e156eb0c0119": {"doc_hash": "87ed08fbf5bc438026a3907e42be92d4c503684dab753a385ab6ece6929c05e5"}, "cf56fe60-877b-4154-9fd0-a6bd4f1231da": {"doc_hash": "3de2d9ba1596c5c067660792a42a7ce437161372e436eb0bf08d550f3dd1643d"}, "ebafa02d-1bdc-4ff3-82eb-e033d21a3137": {"doc_hash": "86890ee8d144de2ac277b234568dce27fe15c7491ba24f2e202b2430012a4380"}, "3db175be-3dcf-416a-afa8-a347105a3aff": {"doc_hash": "94d6bd0668cefdbb2442f9979d5f6cfd65613758ed3574b14e49a6f3ef2bae57"}, "d5ace620-0092-4fe9-8466-619627db533f": {"doc_hash": "09fddb7d33589170af91a7b77a29b284f77c60067473efb4224b593866ed9d7c"}, "9f0dcd29-a46d-4b5d-8484-03029a0dc288": {"doc_hash": "8097dc97c3b9b187d7f5696964c89a6d1dbd90c70796c82e7479038bbcc98537"}, "f1c3f67d-9095-40f1-883b-5ae1a91d501f": {"doc_hash": "ebf9448590a721f68b6c6c5410a6020bedb1cd6469260773984231a338ea69b6"}, "861cc119-4f77-4d3b-97e8-4ce7edcbc432": {"doc_hash": "58a887f1ae646c20dcfbed349e66647b19a987694d258bd5178c1d4539996816"}, "569d4a4b-d97b-4a12-9c5d-a21b5c157e37": {"doc_hash": "6aa3d590c7c3cc6fba160944254281ac3c3b1d39f4b7782aaa0bf6dcaa34d093"}, "3c742491-f432-4f86-89e9-016bc1550d37": {"doc_hash": "821dcbb4d6e3accf9e43e4c768ca2059435760dc158c7ef9cdbd6ac40e907dba"}, "1bb1a9bd-d279-4390-94dd-f0097b72bc27": {"doc_hash": "a9ab23990aca70cb93b0a599400d1dcae8974c4d95fef865f70d43320a989e7a"}, "ec2c7748-1085-4cb4-820d-ce6271e84277": {"doc_hash": "f53288cd9d0778d1ebe3322dc8b2ac42811f6e6a6251a93f956456908d78bf13"}, "b535cfb4-12b0-4197-9250-377620fae3e4": {"doc_hash": "361a8fc88435d688368758617bad2b369db744ecfb8de2286fda72d8d7eb3baf"}, "9156c4f1-b290-44e0-8d1e-24d2e26909ed": {"doc_hash": "9f6acb453fa97fc055a3053d3eff451559dc5e9ab759f68798a0df5e4c1667fd"}, "18ff8af6-4eb7-4aa0-a944-3ea2d01da36e": {"doc_hash": "f5626f2c6cb107c6e3610cd78fd10102f85f984d4dfc38aa185eb0b5fd85559a"}, "4fdb394b-7948-454d-b9a2-8909e7f62ca6": {"doc_hash": "7cf5c2b744b880aab21670909d13c6459b8dd5fef45497335c76a8169445637e"}, "d0bf7c98-55e5-4c65-a3e7-758fbd108e46": {"doc_hash": "f7005659cd27cd00497dca801966df2cb44121546c26767048446b702dc961e2"}, "e2d4d540-8b29-4b34-b448-408686daa67e": {"doc_hash": "b000c4ed80fa33fdf65dff9defc8cfbafc273184b07037a2177c9c0a6ba6c95f"}, "54351abb-e44c-4f09-b0b2-bd52fe977ba1": {"doc_hash": "7d85114c64c2f13c8edc9cd131a169051c283f655867d867b3dd7e18567445f6"}, "15c15b76-a7b1-43fa-a92d-9cf86445052d": {"doc_hash": "75b87056e7dadc544e29eb96c52cc2e58566ca998088e23bac5406d46ef81cf8"}, "e7fd7b48-74f2-4cf9-89ff-f8dd86e55b9e": {"doc_hash": "a0e24726b4eff4593dfa808c3f45dd47347ab5abaf51033b53bade288ea4b56b"}, "1cdf99cf-eb30-423e-a995-98433423add4": {"doc_hash": "c4e71834671d100999c1445d6afb419a426feb7de8f453773ef7366bfc8fbf66"}, "69f216c0-09b8-4677-ba8c-ab93527c0eea": {"doc_hash": "2e74a1fc48bc59133ec36d28ee54f0af52efd345f4b790b2b8cd5ded46ecccce"}, "c57292c7-148f-44be-be23-551f81f6a484": {"doc_hash": "3a40b1b4e71186994f89348d723ad2c681edcbc3d8401f3e50594c4185a624d7"}, "9b8101e8-9e2c-4d3b-838c-4a9e624a3c80": {"doc_hash": "cfe5ec47093e994c9c10c1f0523418eafb941c50c3e2af2544ee7de487e627cb"}, "335b341f-1b61-47d9-ad56-aabd3f0e18ac": {"doc_hash": "c00f8890584c21f55cdef12f8e3de38d9b196354da42007f353fd043723c7f05"}, "69a1b0b1-754a-47da-b8f1-688d44bf219e": {"doc_hash": "5043c3aa481aa17c0c147083e58bf6df410a1b7c9390de68f7617a59236bbe34"}, "becc8b28-a7cd-43fd-9e3e-bed107ead8e3": {"doc_hash": "e3a8da3558729ccb9a664008b7aac2f24f848449c0f702c96201db76533a539a"}, "381593af-638e-45b3-8a3c-bf260d76b812": {"doc_hash": "5976a7ce296a2245d714cb7f60819822934bc78916dedf0af4e9628cc4fcb85f"}, "fff1f62d-f8d2-4ffc-bd7d-bc745aa724e5": {"doc_hash": "35ecf74f71221b15a8e3b29e27505ab481e2c835ffa3ababfed5affa91a2f170"}, "b0f7085f-6fab-48ac-b13d-84caeb9733b8": {"doc_hash": "4a5b8604c54e84c75128a2536add1d718061b03af2190e6fd5a303739cc59a03"}, "edafc58b-3767-4a84-8a55-46c512ba63f6": {"doc_hash": "821e572d2f395b08b681ba0155d96b4e639c563366c00c40936bcc599c9d7cb0"}, "015de721-0ca6-435e-a2e8-19ac57134a83": {"doc_hash": "875088fc895332c340fc5f9a9e57b8fa68fbe4503a04674f66559440f8f72884"}, "d3ae84b9-c424-4056-92d8-ca476d79952c": {"doc_hash": "15e9dd9160f96e3c417f42c61c88e3cce0ce8d06bf3445a8d0820806ab00fd7a"}, "9a2abde5-e80a-4a3e-8de7-8319bf297e4e": {"doc_hash": "369aff3911a805d7ac7f9c605ac9fe8c9506cd8182a2b0a2a9c5d175a75c4995"}, "ce09470b-27bf-486e-9cc6-50afc19c9344": {"doc_hash": "49985c85be42ad2b1f1ea5ff0f66bacd708295894fe9a921c8de700d72aa850a"}, "1ac9952c-8e56-4f42-bc03-89499d61f274": {"doc_hash": "8f805ac49911adcc4cc78c2fdc1edd04e3e8133e9ac465acb444466be1794e7f"}, "dc5d61f6-8449-4666-8803-ec6b05e4624c": {"doc_hash": "fd4d78a935d382f7367646dd2d26f1a6319ab7848eb4d3576240bf14c17c65ad"}, "5004794f-8fad-4cfa-873c-afee88c05512": {"doc_hash": "34169702c6217f555052f4f971468d5f515348f35ce9b547d48f56eeb091539e"}, "fa380b50-86e2-4706-b4df-51e92f1485e3": {"doc_hash": "88fa7f83a467c3564dcf8a29cd8b14922f1cb8b06da8d9be5e714c11006a1fe8"}, "a8e6f383-662c-432d-bee7-b76c229710bb": {"doc_hash": "7908e72df36a69a49d72f4db5f59c5853e076aaefe331b4bc5b9fe787e1a58ea"}, "a20ad3c1-4e85-4a6b-8f98-0120e65fcbaa": {"doc_hash": "d536dddc8b87bdfc2a92d97c057713f43e91b3023c6d063e5d9dae4b6c0a5502"}, "826dc389-ac19-4651-a617-723e560d5274": {"doc_hash": "b17094572a8983ea3616442f56d23f3c5589930ccebdede1a250c90c4b707524"}, "13946762-11ea-4dfa-8b54-aa0c1d1182d6": {"doc_hash": "12ba8834b7ce1fb86c8f26661a7b91ae3c49dc655148095b15db3fc1579119df"}, "05d16aed-5cf9-47c5-a726-0078f2338d97": {"doc_hash": "2b64c10caac797b2630772d24f64c2f16830b21f2554e770fda00e40fb3a530d"}, "64a04ac2-ba78-4ded-af50-1cc8ac85cae4": {"doc_hash": "7078a1913610fff3aff735d36c9d691f1dc57929b454c324a655f741e4bf85af"}, "00447671-771d-4dd4-a593-08d6c6461507": {"doc_hash": "69274e21eef45c39ce5b558bd10bcf40bab13ec5aa2ff5340395c5ac35035dfe"}, "69e12c8d-b042-4093-b98a-68474f1fa878": {"doc_hash": "ab6ebcd4d0539753aab29963616e05a6f4ddf9ee38ef3cc92d159067faaed350"}, "da936525-a75c-4d6f-8a38-dda6c858d471": {"doc_hash": "63020c2edbefe95635251fef3fb1f5fd32fce9cda11ab3a7d642db605d998014"}, "88fd3487-c9a5-4186-8451-f12151bc2d5d": {"doc_hash": "5bffd276bf0f473a2e1527705f9ff64f14850b23b41ea6a6d6b185ad16c0b4e5"}, "cc7f1185-7492-4cfe-9125-de2d18d7c034": {"doc_hash": "fdecc283399fbf43d668f329045da3209dcf92606005acc33cb8c79dacef3142"}, "b9d45763-1d10-42ab-bfa2-6de099eeb82b": {"doc_hash": "d50bc17b33bb39687a3cdda129eb595fee563ee14d57f750380e5cd56645c3fc"}, "b4fd648e-8a8d-485d-9c52-882d80a446f6": {"doc_hash": "866031743aabdfe2d4396f5ccd49ab7be368193049b8b0fcae0e0e03f5320164"}, "00ad8487-36d0-4826-bcbd-6b03fb8dfebc": {"doc_hash": "a14883ba0ae94856b73b641b7086ebdeeb9ed20a25d103a10811bf47983f17ac"}, "3d8f54e0-fa0b-4c5f-8438-a58023c3b962": {"doc_hash": "c8a289a6123cfb79cfd3efe70a276c2dd737c405d97929700a9d9fc0f0745977"}, "7a756b8f-0693-4e36-973f-219388407f63": {"doc_hash": "ae0a1ca0bbe54d0f766cd067df330a77f543a260caea4b570ce13d632a537c48"}, "67691e55-6477-4441-9f2d-3624e6feaeda": {"doc_hash": "3d060d74a37691f1e7585f48e328d132b021fabd8a0ad8981b0372cf012d09c3"}, "9187776a-cf65-4167-813c-c2e4f2f04fcb": {"doc_hash": "09ae7f30b7161f27d665df7efa25233383eeee7e5c45ba35e5d7806988325fc9"}, "f7101bac-4f92-4921-9f37-b74d5f74b7b8": {"doc_hash": "87edd03e708af1b582cc35bdde2e96e69e4fbbbd902d26e9c072526d93d084c0"}, "2f03bb22-cfe0-4c92-8542-b26b0ff63df7": {"doc_hash": "61f0738735c51f1a2ee6732a966b32da3f4a133436fd189f2bfc7e4b942a4398"}, "e483e7bb-9482-43cf-8a6e-9fb7ecf842b1": {"doc_hash": "6eede1919fd13fedfddb75a3331191161a2de77aaaeffd23e62e2dde56f06230"}, "a701a624-44d3-4760-b045-f85ac38bcdfb": {"doc_hash": "b92d81b56c8ee70df89367e41a5f9a94270257711bd76cfe71bc3488ab9dbb5a"}, "6cfb97aa-3b95-434f-933e-3f6d7b96ef4d": {"doc_hash": "bf95306b6d3d53baa14c58c0835e6fedc7a8e6f44f1bc963f2547489b3636806"}, "fc9ca98c-5219-4d03-84aa-68573f75be36": {"doc_hash": "fbc7aaf14ca6e251f9f3a6e0c3c3eb7cc0c48bed4ee26b20742b760d05b9b049"}, "b8d680df-8f79-42be-a9e0-b6d1ac8d377e": {"doc_hash": "d48b4cd438505595319935144bf02f1dfc0584586c4f7efdda12fb5b72720461"}, "6d377f55-0a40-4441-bea7-d3d008dc73de": {"doc_hash": "4c911c53f7fb06052a90e74dd0fd7fe567ba1195350735fe088d0ceb279348b7"}, "4ca49f59-d30e-44d1-88ba-56e61c5fec91": {"doc_hash": "1cfdd2a956badcb013231738dd72596e1bbef23dbddad86b4973116f2c88e6d7"}, "b12c88d4-4ca3-409c-ab0f-a2311e307575": {"doc_hash": "1d4bcc8bebab336365cb9800eda157ea9126333f7d39e1701182159b55271a4d"}, "6449787e-09fe-4a04-b4d7-928c95cc64fd": {"doc_hash": "f45085afffc6b1046fedab884d347340cc562e7f838692f2dc14571b900d8906"}, "ca4cf3c9-fdcb-4553-96d9-17729835c1d8": {"doc_hash": "dba8616aaaa68deb1bd9d73ec087af0253defa4700bf309490e9d4c1ec78df39"}, "525bdc5d-41de-4064-85e1-f65b2263582e": {"doc_hash": "7e4553664fd728c8353ab09278d132126a2a97d7ce0626bcd0157ea84d9d5cde"}, "a2f2ca14-c42c-4e98-8b29-c24c33642c77": {"doc_hash": "77d9202cb7a335a3599942cf5b37dd04a0a28b6e78bfdb623e92f23a91bb1978"}, "83b52b8f-4a88-4463-8c40-c6fe192e7996": {"doc_hash": "96ed720f8e87036da95b5726646653dd74621fd3d0f128dfdeda2fc01f166355"}, "b2f66f50-9bf8-461f-859d-e07db1006098": {"doc_hash": "f4359b034fc9557a38f33e81d15de499418cc00c3a066d334e3402928c8f3e40"}, "72723e0a-6f18-4d27-99b7-de3cc6ae58af": {"doc_hash": "6bba17ab5bec9cfe45680b1e236f066811f443f85239f83a2d8c00d7d60c8222"}, "df290570-5fc0-4389-a1eb-7897e876af3d": {"doc_hash": "3c5f68c4fa35b64127e12cb19496cbf175468dfb9193b9472cf2da609440e986"}, "380d8a71-ad8a-46db-9653-7526828b4e56": {"doc_hash": "c6d794c1492bf8ed85e07df10aabaea6f4305edefba350213a0e332b6810961f"}, "6df5af8d-7975-48ad-af80-1e1836876ef9": {"doc_hash": "ca92b8d5f281da1cb4a228a88306dc6ea5c96db5a3729578496174c6257aa89c"}, "329477dc-805e-4977-af4d-a763da688782": {"doc_hash": "05d692a65263ccddb1cf90c7292289adcc14acead672773ff86e848e29471a8c"}, "42718cef-5d44-42a7-82b5-7f4fe3085b17": {"doc_hash": "38d016a56e53a1de47404e649076631af478f935872bae756ad181f4d1656e95"}, "813bf178-50a6-4a4a-9fd5-86b059566ab4": {"doc_hash": "a458b37c459bd9bf074d08daa9bc08a5775540537ae4d146e57b7f8d00a7bb3d"}, "669fa008-2820-464d-a69a-ae0af5251564": {"doc_hash": "86d74b765a2136f3c2245df11f9cdf58998685b3947a21a55ea029f4aed873d1"}, "b5d9ed9f-db03-4238-955e-7dd3e87b2f6a": {"doc_hash": "ddc99db33ca45c66fc20c89d33f7cb7c0422f03caef5861d77afec4260a63c52"}, "5f66b373-401d-44cf-b190-2d29d945b673": {"doc_hash": "aa7130c1b1c367b69b8c62801b38154b6bbeac397fbc4a9b4286724e9527d61e"}, "a87cca8c-519c-4df0-92d9-65e171968f32": {"doc_hash": "b9d2d1214a2f020c6387c76b7c7c22f9e73b618338d144e2f5667dcdaa090452"}, "3e106a48-5437-4ade-a8cd-5d91545a38be": {"doc_hash": "5c8a61f31bf1c2d52734a4313ede3081e4837b0481ead2165edeb3cefcdd0368"}, "b03bb92d-f069-4964-976d-f515fe1a2f0a": {"doc_hash": "3d0125aefc5d9cea1cb40bb5965b16b4cf5faa0d29dab90510000aad3da0b3c5"}, "9fa45690-f2cc-450e-9f1a-d4de1a061488": {"doc_hash": "215eceed10712d0f186cb6baa0fd90af71feedb8b03bb9249e29fda8eb39b3b2"}, "9f4973cb-4178-4156-b9aa-ecade5f50bd0": {"doc_hash": "27010afb16786ecc5df0a7b6ad4a74a2f02849949b27f0feb97353b764fe69fa"}, "dd634a05-fdfc-411f-ab90-5ec2396cb6b8": {"doc_hash": "deecae2776de9fdbc78c63a01ae191e6b70d0d5eda01d23105d03c656f114813"}, "dffb8c57-0d5c-4fa8-8cb7-7640c078dc1c": {"doc_hash": "e91c6aef4137e90240a11301eca40a9701073fefe09840b1491a926f5b23598c"}, "46104237-96f5-4c1b-aa16-7bc7349188b8": {"doc_hash": "56611869454e66fb258d074d371453577831ff9b0d4bb53152b8bd63ae55120c"}, "2fbeedce-cd28-4e24-991d-9223580ff5b3": {"doc_hash": "ec13e3768c413b42636672886cb10c4d55f31d956fc0ed1ec5839b70465bfb61"}, "a0b6ff7f-2024-45e0-98f1-1e9e02537530": {"doc_hash": "758bba1995e55a5d202ffda5494dafdbede9c8196b08838ba85b705e04b1da01"}, "c4f5e536-9a83-4c5f-8176-3303f5eaf92a": {"doc_hash": "b3735f88a63cf35c7b2352c7d37c50ab1f702366bb1dba1df569688c9de9f8dc"}, "af6125f2-2c55-40d5-aeb1-bf13c2fce76e": {"doc_hash": "e1e019a8d26e4f49d155090897a0013d97f11edcfd0be902cbebf2069370f97e"}, "5381dcf4-c1b0-499b-a167-54df327ca060": {"doc_hash": "f5cbf5206c8574cdbc926ba99533e7f243efac565895dcf8d0768d13c358ae67"}, "5821c050-5ce0-47f9-adc9-b45e73016fe8": {"doc_hash": "0429ba003a6b63164ddd550597d5de44978030e159095e3167837ddf619debf5"}, "6edf2408-9b59-4f48-9b3a-ca94afa1c417": {"doc_hash": "3b3a19adde44f22742e7d98173413c3cb8ce466d7d4528ad39fc445ac9e55390"}, "5b0dbe2e-512b-4045-940d-f1960e5272d4": {"doc_hash": "dd82492eb2cb1cf8148b456ebe69518dc623e34e48d8788e4311186824e1a2eb"}, "8692766b-4ae5-4305-9005-2f1ab011afd7": {"doc_hash": "e80d057f9181ea88f4e3f92980c8dd638422731b1f3a6dd025f373a6a047fba6"}, "b8757888-f049-404c-8c99-07bc1e9bfb24": {"doc_hash": "44e360069f89df9ae88fd204d3c9e7c72684dab5df4f6ebb1e79d118ec174b1d"}, "91966957-7747-45bd-a03f-d0c43453dedb": {"doc_hash": "608c108d59507acac8c3f72afdf987cb7815571634e3f6daef1a50675693a004"}, "74eb1b8c-9d9e-4397-be86-3f3612bedf83": {"doc_hash": "c0516a0e99ee7e456e125a18cd8e64138002effdf84840feba7dfa31dcc6e080"}, "80855d06-ef81-4500-9c04-40b832e4221d": {"doc_hash": "823e3db69d2c9a77235f99789810d2343256d3ae4de01a4f3aeb0ebc8b8e26ea"}, "38693a26-8c55-4b11-b7eb-b602c5d8efdf": {"doc_hash": "81ee9166a4e3c4fc2a4ea7abd4737a213020d01ccd5555801c86e03010d4fdae"}, "20c73727-d29e-4c9d-8765-e5290761bd01": {"doc_hash": "8861b12dedb6a8768153f3fbfc97258b02506e391e7db186f788472b6e243a9f"}, "ff9a8e44-a3e4-40c6-917b-f9dd535848f3": {"doc_hash": "e8417f3eb396aac84e5c9bc44761295a455aa6fc1b5bd496ca42a40ac2e81b56"}, "c2e60a7a-b400-4a65-81eb-ee0a30798d78": {"doc_hash": "7e9652efc63bfb1a1912728dbcce812b3bf31f8ff77d177efcdde6361c6473ff"}, "7f5aa553-dfdd-4bab-ba88-6025e3760d92": {"doc_hash": "2154857215658be4a2d9a7a2659b77f0b4f25c1ff7715d37692156500205a949"}, "f62d972e-7b41-47b2-9bc9-855b46565dfa": {"doc_hash": "6350f9ed15736240627534c58a3110ce3543804d37a319f57ff30357e8459cd7"}, "47ba212c-fa32-4730-86ea-70f7b8ef0ea1": {"doc_hash": "061b767ad6486d5d307f2324f28234ff5f65ec9203fb2007c06f7886dd1bb9a2"}, "a2efa05f-e582-4af3-9798-c351506768bf": {"doc_hash": "c07fe9d93ec61d678005d7db9642e9c1359ec705aff9d5375d5b95f9399bc18c"}, "c5f0cfe2-8c16-4f64-b558-2ab8fdf4cafd": {"doc_hash": "2430a9c8dd69502e5f9dadfe6c4edeaa22837974d92797d4cd507e2f898350ef"}, "0defce02-ea5d-4345-994e-e84e6570e0b4": {"doc_hash": "efececee8aafb9b019b0c09d33b8cc40cebce686f51b4f00df844792bf57a6f3"}, "a363b883-8b6e-45e6-a6e4-78d54bc51cf8": {"doc_hash": "a0bd5092cd5e4cf3cbe18dfa79ce2b8816ed5e463e31305abb5451522b528099"}, "a07860db-4f84-4673-840e-80adb4b7c7f2": {"doc_hash": "1ab39b8582db469ede79123e650c5d95ff2a3c62568bca3238a22c88c9732881"}, "99f38eb9-9953-44e4-9411-21ad8234c490": {"doc_hash": "ffad56d6e3fcca1aba6abe2dd0369288ade5255cd894044bdd51a52d0aa8e7e2"}, "6f7c45c8-4b1f-41a7-9fb8-4e2d716dd634": {"doc_hash": "129efa80864c2d1044813111427a349b6d66ebf54331d8c09a8198b57b3b2730"}, "d5140a05-729e-4628-918f-a044b9bbdeda": {"doc_hash": "c2546fcdd67d56ef8dc7733150ddbc12ac3df99da01d359fde59a1cb8341ac30"}, "f7bc83d6-6855-4ec3-a5bb-a4135cdfffdc": {"doc_hash": "9c5d22c6827c6d1deeddfd0274091148f2ee54a0f8f9c494bfb674d3e17eb095"}, "14ebd053-462b-4700-a31c-03c944a0ce29": {"doc_hash": "4915dfbe3d57d707b4f7dec8bfe8a4af340c970d5a20ee0e776da7cd1590598a"}, "a1ed0bfb-8953-4137-a7e4-0371e6bde0a6": {"doc_hash": "1c511ffa2c8a203d537093b9ec017f9621b535625d538e274c46b2ad185fad13"}, "af012466-de7c-4201-a840-90f780af132d": {"doc_hash": "84a5652fc08bd2ed86ee0c164ad482104aefab3f9a79ca76243c70aef7864cc2"}, "37cd8caf-a241-4665-b4a6-7a01a8677890": {"doc_hash": "9ddda92ff736ac3b5fed6d71bfb2f25937e0caa1c23b73cb421f0bec457fcc50"}, "389302f2-e074-40fd-a6c2-f9b3c69461f0": {"doc_hash": "1fa7de9459b2f190e80e30ee46a6d57930f67ef1c58a475fcf095911aafc7540"}, "bc381e92-8ba4-45f7-8be6-58a06ddb0100": {"doc_hash": "33007f0de8f0f56f3c382b1383b2ec66a137fb9436033ca5bb42ca8f0bae9bab"}, "8c6ecd77-87d5-401e-a1fb-7581149cbc58": {"doc_hash": "9c146484b4f73822def862eb0bd34772a049e8a75f297ab3450a90c554805220"}, "1abd0ab0-cac6-42dd-900d-db62e73afad0": {"doc_hash": "c694ea37548166fbafb81f6cdcffc898b631f5c6b24e0e1c88e7d8be87285a0e"}, "570eb31e-738d-4493-b7e2-4ac63f688606": {"doc_hash": "861bafc2b61aab6f86d5f814197afd4ab2690b7a02a4da33c064530a444eebee"}, "6ffb189f-fd69-4a30-8d40-b3753854c569": {"doc_hash": "c6a7ee505f3fec04d0fbf4755f10d8cfa137f6e45062ef96dc10de4a26093f1c"}, "f8fc1b27-3fc1-4ae7-96a5-6c388c6de267": {"doc_hash": "dcfec4b99d54fd6d7130bbd1a927c034dda7b223c1ea5a75eaf2655a38236a56"}, "27f4c239-a832-4016-91e0-385fc534d943": {"doc_hash": "5eea5208e90753fb3d0191faf6994072f6f38f0f298d8043a753ecf436df8e04"}, "76222c74-1a7a-429b-88f2-a4ed6fa37c0f": {"doc_hash": "a8bd99e2cdba6e85900f72da8a39d481a7c548f24d32918a504f90819bfa6e2a"}, "239162ef-3dd3-4a8f-80ab-b2273614b495": {"doc_hash": "89b71e5c8fc04bd7b7e70f1a854496d36b1e3cd654661a8433d4ad01fb4281ec"}, "a6588327-9da3-4981-8f90-f859ca2934f2": {"doc_hash": "9d9e7865ff2b6538153635dd6aee950a0390077bb9376de4fdf015061a6d2638"}, "7d342485-4866-499e-bca6-1a2f3178ea06": {"doc_hash": "d4b68fc6ec192150ab43622a81c623bd39ca3403d151c82dd61c30e407f863cb"}, "5450fee1-0f33-4a6a-9e18-ef6f25d6fede": {"doc_hash": "6246ad4df211d4cada1d1b8aac87943613efb8748606d7753522a41e7b138801"}, "0c266220-80bd-43df-b799-10278a3c2009": {"doc_hash": "59a3bc299fb2bae28df6e75c86594723cd286953971768201e4bbaa43587aee3"}, "dbc09027-59e7-45b2-babc-9a46a8d66665": {"doc_hash": "73dff53e56e3a74d112bfc65198ecc3c017d86c898bbfb83def73df918fe8f88"}, "a51a9d5b-8d3d-4c4a-86f8-d1cd040ef289": {"doc_hash": "74d3f09dbc6d77762e0113ee8cbe09be20dd686e43cff8dd4ac95ea4b81a5c36"}, "122a356a-560b-4765-b4a3-49c3566409a6": {"doc_hash": "ac244666836410d720224d8d21af9d57890b249c9efa12e559805cbbdcb49bc0"}, "cb4135a6-e312-46b3-b1a3-cff024aa7263": {"doc_hash": "7a9fe5607cdf0ebe4a2d7c37e03afe5aa6c3c0951092add593ff005bce0f4ed1"}, "fb69018b-7e55-40cf-a486-dda2ebb7e8ce": {"doc_hash": "b7da5a3ef1dcc7520c56047ada272e1271b9996ebf33a9dd2c3a5795fcfdff2f"}, "da908028-a38c-419a-a992-d070d12cf349": {"doc_hash": "796be24385fc27b67f0f11680e26a5ea28a821478d378ef1e5b3cbfc6303ebfb"}, "20b8d52b-d00a-448b-bd9e-2fbd198a83dc": {"doc_hash": "d8fae46af7721d9545f41eb6d839f04cf667d1c0139aa88ab1bef14978a12989"}, "f92eebd4-284e-4e4d-8e4f-6143deed8c60": {"doc_hash": "1f313f770d52ce241eeb7ae236284a8b7bceb362a8c0ea745d6cffee22152cf7"}, "330da648-2bd7-419a-adb7-9d4831526ebd": {"doc_hash": "30eb55b89184ff7fd1d9d9ee68f9fec8eed03ab928e3882c846f8c3158f8f487"}, "447bf0b4-af11-4369-84b8-9eaea9c8bab9": {"doc_hash": "1214e9b7983ec46acb89073977f1012bcb265cc1beb17fa42ef9822a40bd6e28"}, "0b652de9-e188-4b14-913d-74336aa59bbc": {"doc_hash": "f85babca168a0c20810b510feb0cf951953d9595136043a84652942aa896e402"}, "c4de1146-dcb5-4171-ae01-d6a4582b9142": {"doc_hash": "fb2ac5b422e5ab8c2f16399b7adc8377342189173f341b31046b135aa79fadef"}, "38105673-de91-4a2d-971a-90ea9d6486b9": {"doc_hash": "6239c96cd988c477663e7e295d3cb5fb658edb39295b965ce95a7c7a41e9589e"}, "38db0674-39e1-4922-9a0f-f14bcb4b57e1": {"doc_hash": "1dfb95ecbf882895dec19e158cd6be3e4f082d039148a31ebc5ad2f6384da22a"}, "7caeaab5-80fd-4d57-b502-1d62c9fb45d3": {"doc_hash": "6027cf2a7f7f4df25a01155118d62ecbd2217cf383fcd1c82d0b0f33acadb941"}, "d35c9166-27f9-406f-b1d6-a1ed2f8edd52": {"doc_hash": "b1a1b4fe83f282a68f2a6b40b7d692fe5a0592789d768e793a36f446716f3c01"}, "4f91bdee-157e-4746-a894-9e2da47ef02c": {"doc_hash": "0643e77bbae148dd5423bd3c334d4736d3c09629161ba4c298fcabe0149d9cfc"}, "9fced9de-97fc-4686-baef-00f0d9f8ca6c": {"doc_hash": "24a90b02437e23e0fef8c205b8bb96bb76126c9d824a27403aa29243794a204a"}, "5cae1afd-9dee-456f-b93d-b3b04aea1e19": {"doc_hash": "a3b56829df3d59bcef725416b0d181f1ba3004650148a93e9fee8a8dfe68718f"}, "6d64437e-60b9-4d40-9c27-bb4e2c24c4dd": {"doc_hash": "4cef9d4b707e9b5277090e58ddb6bf6bbd538b84c569f60feb372aac42cd4364"}, "66760d97-fffb-49ff-8621-3a9deefaae1d": {"doc_hash": "0529e2500f273d2ee70fdf0b0753abfd4a7227edf807a9e5f3c780842111f4c8"}, "e11a913e-de48-41ec-9589-55b00054df69": {"doc_hash": "7ee4732ccf4a979a975b5d5bdfce0ab5add5e2a6a7da88c4df670b64bcec03ca"}, "617fbb52-97af-4a49-bf31-5e77a2fcafab": {"doc_hash": "352bd2e9c656c0de8e1edf7c84fad4bb61706fae0e504149b569399985d7bc8d"}, "48802810-28be-4173-b838-94521469f095": {"doc_hash": "e67992486fff57bc0ea31cdd5699ba86fb393873a15ea146c0e20dc19e76aacb"}, "71e24d16-7899-4464-a3d7-237b4f4e19af": {"doc_hash": "3a2ee9a3fa9c7330f033beaa0b7a7c86acd8ef8af4cfb052f63841be2845e13a"}, "5cce9f87-4830-4ffd-95f8-8168b8e69f9d": {"doc_hash": "6f2c123ad309e531467c732104c8afdaf0824a8c6563960c79ffc8a213579ba1"}, "531524fe-39e7-446c-bfd5-721e713f81cf": {"doc_hash": "265151f8212b02c2ac8bfd8047beb2494798237f147f88918e8073f7d2ef2b86"}, "66ad9bbf-b68e-4fb3-a2a0-1d0afa59cddc": {"doc_hash": "2ee11d505b43e1d49dd01b8fb38c0629e16f9dbfcbec4921a131279fb34d1796"}, "8182cd31-5d48-4aa4-888a-538a8d6ce947": {"doc_hash": "374ac25a017cc334f10cf22053a12bced8dbdc816e95b1f8fb085e65bfea8c62"}, "cd4215e9-931c-40f0-99c8-604f0ad0264b": {"doc_hash": "35a94709eefb1564bfda4cb1d0fafdf4defb8f9de3817e40b2a56aff1819e6f0"}, "6d8f42cc-45e9-46b6-9cfc-9a8761acf7af": {"doc_hash": "e7d4f29a1bd5aca469af9949fe439d14e6b9e18897396fe3e5b31dfba19478a0"}, "f0161f95-883f-4fa1-8f51-6513d40aaef8": {"doc_hash": "21b993e8d55b175258fd10f6e84d1c7bb890463ef2795e8d6e03213f689f507c"}, "789cbb66-582a-443b-bccc-898e26215823": {"doc_hash": "7be383d4b390b714620350f8939809d75732510aafa4e51b382842890e897202"}, "eeeeda32-5bf5-4134-9c36-0c0782e9b23a": {"doc_hash": "1b6a07084ead13e38702ab3b9f44547057c158883effb6420f0a86d17763e83b"}, "ab2030b5-fe75-450b-b26b-1a466cf0cf84": {"doc_hash": "13a70f270ff405eaea4eb2b11a7e0864162b895a04fe2e7e4a0628fd46f6a0c0"}, "18c1b152-077b-4f0e-b1fd-cee76fc657aa": {"doc_hash": "ba011275d2251bfed897f98f6ff4c23a4f73324e1fdae52fd19f40b454acd3f8"}, "04e3ebd1-b8db-4062-a283-53bc6832b97d": {"doc_hash": "0ca256041fe2d73932e403d72c6caa99607ab07dbb17b248a5b2d6159dfe8885"}, "cb8766eb-2995-4d1c-b4d4-9ca879952fd6": {"doc_hash": "8c448f0b325c45ad944a4183da6e372a91be166bf79de92275819759b1c3a4b7"}, "7e81fd11-d143-4fdf-8f67-17f94973dbea": {"doc_hash": "2dae01ee3f1a948ec18f03291ca40923a683fb283676ba8e4007ccc98a875452"}, "a6bb627e-c456-49bc-b3d5-448447e87f86": {"doc_hash": "08fdf63e79f1c85be7e12eb4c3a03d0dce97004f944ce3331b9f4f73ca0f8c0e"}, "ac0e6c14-6143-4286-b40f-b346270e5489": {"doc_hash": "c481ff4d460df823a3b25ed32b4d97dae70bfa0542ed7fa48dac09d6055f6141"}, "e97167ce-af33-40d1-bb9f-9481ff89bcae": {"doc_hash": "cd954d7bfa9a2bf4c2936a62a30f2074a2238ea50fab0ddfaeb93f0bb50c5b3b"}, "5b4a4a81-0e34-4844-a957-88f42954e54b": {"doc_hash": "2365876e02a86d542df73eff7d1207280ae97d7538f0cf58d1d2143b3f162789"}, "f3ecccd2-e8f9-4111-919c-6f53f8e48d41": {"doc_hash": "900bcc577fc355215da51048205755032bc3a5119682bf205d9bfef933e6fc48"}, "388eea62-d4e4-484d-a6f2-b4ec38768125": {"doc_hash": "0220e62a6d0b355c645fb4adbd96d05e942257c82261a3270263dd95dc3f6adc"}, "7f2d16ed-f192-4be3-bea8-7a06fe3cc8d1": {"doc_hash": "577ff413c3d55202c696876569daf0d920d5c7d2f4bc1352de3366755ed8dbc8"}, "4308e195-72ea-4bd0-add0-e28d8f12bcd9": {"doc_hash": "86fdcb9347ec6b8e42e3402c3ad2d307a0096c74b1db8059dc8db1d961af8989"}, "1ab1fd64-9043-46f8-860e-493e2d3a8e9a": {"doc_hash": "22935d96f46e9b69f9d5e9ba5b599999d92a81770b3fab59b573c8b3ea749111"}, "be85231e-714a-4143-b2d6-228d2a25c524": {"doc_hash": "490885db27f1bfe31cb88c64e09be845ccc3089a6f63b55152be89d2483bfdc2"}, "1903e3f4-bc07-4500-98fb-15f8177ff7c3": {"doc_hash": "ae65fc4feedd40c12ff6149a866799c6c2c84e78c9c61e3b98e4176bd12e8894"}, "8d44095a-c65f-4834-8c8d-9bedc6da9743": {"doc_hash": "50db44d0692542417e8722bcf67736d94f10e2986866ec1227fc1e3085c4071a"}, "61083f84-5a31-421f-9306-68b76f191817": {"doc_hash": "10698b826437bb4ac3e3fe230542be5fce81bca6d11379bbf7972785707f75bf"}, "79c5fdb3-d643-4015-9c66-b2e61aaab2b9": {"doc_hash": "73b244e894c64047bf5d9f085fa2e4a2bfb4e489eae27d1ac2b3b15f6dcf26f7"}, "945e5d8f-1672-4ab5-a945-d8c32d647d34": {"doc_hash": "cda4e83071bb2612f2ef65590199651cb4f23f2ae3f4de0e1b24af6737c67295"}, "54a51921-8b13-47cc-8eae-46d96b665531": {"doc_hash": "fbcb7810bb46d54ed976daf4ffb9b29a96c3a8bc494f2e553f0007b1e60496f0"}, "2d5dd2f9-9fa2-456d-b60a-2170cbe91df3": {"doc_hash": "d42631a35a72437708a1667de5c12d0c477273e5580160aef25fd01255b19204"}, "46bf62b4-5558-43c6-8d65-4f3b62dae319": {"doc_hash": "bef0a0cb779d9b34239cf606df268a8d5ea5847e27ffeef4e41cc6da37573908"}, "6810f935-b315-4872-9ab1-276a40e46e44": {"doc_hash": "44cae6ed45ce29086100682ad10de908919c2fa4fc7fa4400c8976fe17cc529d"}, "ff3f6703-3601-4572-b67c-23ebbd05d5dd": {"doc_hash": "126594a105e827d511d8b83bcdae60767ec177130c6004880ddbedd639d0c1a2"}, "3bff6b00-6fcb-4b8d-b1aa-6a615fdaec51": {"doc_hash": "08bfba1ba9562ca7ee3a54a37f79fd1ccf2104c389e3b2ff65b6614d8dbe29fd"}, "d1e8ccb1-1fb3-4536-ba94-30657d180b19": {"doc_hash": "d17128c69b9abc887f96c9812941cfb0d16c746739c833c330b34ef3c627b986"}, "ef883407-1e48-4ec1-b2cf-37f0b283ce69": {"doc_hash": "9ac8559ec361cd0cca5396cf12a874435be21725c5d0e55053a6683926559a41"}, "7f20761a-e0eb-4a36-a29f-3d481c8bb256": {"doc_hash": "b8ea3e601425e96d097e28ca2cb75eec031e0748f1ec3177a3ed38dfa8e56f3f"}, "2d50e2be-5c8a-41a2-b2c4-7ae1942dc685": {"doc_hash": "51a6fc2b82e8556af24d8b9aa35b288e357fb10de59dc1602a7004ae8a6be684"}, "c99a54fc-b8fb-4b9e-b121-2beeb24440f4": {"doc_hash": "bd23fce0ed933309b04ac74bee8ff18818a79c1f2138623a7cd122e1ff8e64f4"}, "179937f3-5518-4faf-b3e9-7f03a18a6a30": {"doc_hash": "abff43c84ab0e8c563f6eb0c7405aa5660fb215eac2f3c1ae4ffdf3e9dbe5b87"}, "264887f3-d4b6-4c45-82e5-479e1ee7ef04": {"doc_hash": "4ac03be12c9362dd11b3c4eb6d1a65e9fa1d2043891c3eee7d1078b90bfa3e60"}, "38234d53-0ca9-4146-9ec4-2ade5b7c8b67": {"doc_hash": "5f44a303d4282cfa0b6482607dead985cb1b8a437ee15542b4fe9dbb277ffd81"}, "afe91798-85ed-4db9-89bd-1d14beba03b8": {"doc_hash": "e45048117b5d60ba344f3860878cd42bfaeb807d67049ff6cc31b5e52c75e047"}, "7fdcc2cd-5535-4b78-9fdc-fa6a5c90efce": {"doc_hash": "ab482e9f1d017867c93bda3935a1cf72ef59158bbc5699e92c34a811c7148b34"}, "a13ee79e-1f7c-4c64-b3ac-fe5ba9b1b7dc": {"doc_hash": "93659235244012e31648f944420dc94f4c2237a650ce2f154c5eadfb748f93a2"}, "c9e28ad3-7513-4b4e-a179-18849ad39c35": {"doc_hash": "17507228a2fed61aaf071c570984091bc918b0f502f75407046c213ce26a7789"}, "03c6cd1f-be62-42b7-b509-9bbdaabc3213": {"doc_hash": "2ffb910af0961c1263b4399bd4035595daf514a119fda0eb3d7a23f79ebce39e"}, "7d6a84cb-5208-424b-a403-25580b000ae7": {"doc_hash": "9a85a00898217f441aff82af0a418e74403a1db1375508bf0371d4e5e53c4454"}, "c90c7d5a-067a-4c5d-9cdc-9f8129fd09cc": {"doc_hash": "e6eb5abf7bff466cdad3c4e796ad9bc9c8defd8520a9dfee5db825be0ab6dc27"}, "82e61a9d-2eef-49e6-9f5d-41562c995589": {"doc_hash": "3dbd5fd12d7a4a87dfaa15bf87bc02738a1a1666ce46490331fccd01732198b7"}, "6391cb5d-b101-4644-908b-40bdfd0e913f": {"doc_hash": "4080fb71f0dc5c0100a5005df48873247678c7cf4dd495af9b6dd14325e07fa4"}, "2f22b994-8dbb-4d88-8352-5a9fdb868e06": {"doc_hash": "178bcb255ba4d31d983fa5a9caf6e6b4cbf3eb7154ebfa3b7ee53e09f3e5d891"}, "fbaf96c8-70f8-4731-bbdb-85d321f0991b": {"doc_hash": "e24d2bd38fd1496b975f5fa4237f458121e1228c39b37768e7a6b7a5bfefda9f"}, "e7a885ea-3f65-4915-83cf-564f18e50bc9": {"doc_hash": "006bb082cc124a1352a6ce257fda2648d35cf4d94dc8c318a310fd52e9585e8b"}, "733191a2-af86-45fa-b525-856ed65adfe5": {"doc_hash": "0d9656bca86313a29a58e96aec961734a4d0f69626651a7db27c0df625a0c8dc"}, "dc0ebc3a-0a26-400d-bc38-e3f109a5fe0a": {"doc_hash": "5c663a2998ce76c62d597fd5df8cb447449ad0ba8a768fcde63b006c0edff98f"}, "7fc6007a-767c-4390-9164-62179c873c77": {"doc_hash": "8d185582124c53e3f7ce6565ce5ad2797a33cfb5f493ff74d71556109b67051d"}, "c185ac8c-06be-416b-9fdc-19087684730e": {"doc_hash": "97ec145b276410ee4946fffd15bcd9827ab9530faa41cb472ca03c1e8fe69b4b"}, "76ea6bf7-ce7f-42d0-b254-73e1c37fea3d": {"doc_hash": "dd0c9d2e435e13b54ccad455ae40bb1baa988555d5449e24f2eb8636eb127c59"}, "3ad6f4b3-100a-44df-b1f8-d40424f52a0f": {"doc_hash": "5f0c3c3e7467d4073b1aa9f29f5a4686d15c0202b14d96865b860438a6ca0a26"}, "a23afbef-faa8-42eb-bb0b-d4af01c2b1af": {"doc_hash": "399b834db0edb80b219ee4dadb7b5b8c29e1664f95aa05f6992d8cec067d2045"}, "0cb8eb46-a71d-4228-be2c-3f20cb31025a": {"doc_hash": "5c48f15f02421040488bcb86329335f2006a3d20812c084e6cdf514b2fb4757c"}, "92238105-858e-4f90-b6ed-c7c7dbfa0e08": {"doc_hash": "e29b191bd7fad3979962b7d5d26b303164d236c3a53e837d9b93913fb5ff15dc"}, "3f830bbd-7bf1-4acc-9193-84dd47a2e212": {"doc_hash": "997ca88bf9fe707ae463a72842b88ed7190149b17d2517fb6151da5ea5c088cc"}, "3ecb5304-c784-4118-922a-f7b6eeac23f1": {"doc_hash": "b43dc61f8fa1eea320fd287966d9c35fc85cfa4189f61b8cd50655c084634f5b"}, "bcb391ad-c487-4189-bad3-e742a75aa340": {"doc_hash": "a5c28e43d8e9bca4ac33013b48161172b5b28ce8e0e650bc119e2d26f5266253"}, "7b70200b-1c4f-4615-86e3-8382f9b8d2de": {"doc_hash": "040f53adcc7c5a08282937f5d0894c73eb5da1f567dd75e66977764cd473721f"}, "f235f253-ab1e-43be-8cbd-800eab485f88": {"doc_hash": "e6f219bff11aaa144b032946ae549604dfc9c6c5d1d7c7b081c8204681355532"}, "b25d40ae-ab86-4c14-9eab-038ab0b41760": {"doc_hash": "68f457b5501bb987d0e19367805db436d3e9d995fbc0b6cfcef9893384d23475"}, "b9cb15a3-f949-45b7-9e01-bdc0bc178945": {"doc_hash": "679c6eff755658ee7a7f550ff8698f5cb627bbe5aabe0fb67fcfbee67fbe8f11"}, "5a8d2dcf-bbd5-471f-8a65-1f2a555cb229": {"doc_hash": "05963e9dcfef74f6dcfc01d7d81814f2a3edef70c0860cd28d28131879d9406b"}, "21e929ce-3e11-4f56-b6ee-16f41a72cdfc": {"doc_hash": "69a6107d3e07f5618d92c1c685f2bb185a4880cb83250488bb6348e72d1650d2"}, "8a45578d-5044-4d5f-9bf5-aef629d70917": {"doc_hash": "94ae50c3ac5b5812639b29233d7a42d7ee1a69ebf11f02bfbec69240d0bdc239"}, "1a5fc65b-7546-48c3-ba29-614b6df88236": {"doc_hash": "d424a63e583d156c8d8c92950451bf838961fc40504a2d5cda5f4567f8c2da9c"}, "40643ffa-8800-43b2-8e36-59f56a8b69c2": {"doc_hash": "79e3155b2f703d2bf842bfed7af262a2293b53a21426bf1f8232168088ec7531"}, "5835ceb4-48bc-4ca8-a95e-4b1e4e120339": {"doc_hash": "5e40df3c25206ee38ace277a487d0911c02162c7624c36619d692f306bfb834a"}, "2539089e-3c01-4b47-9494-0965758b3da6": {"doc_hash": "593873d095efcab4a86c907a4840184b0eceffaf55e5846d05cbb3b96dfbbbae"}, "b52beb20-a889-48d0-bd38-048e254e362f": {"doc_hash": "277d4a69886b66c8bb6fde25ea28cc5ccefcf0ee5287d4c4c96ca7d0ea4cadf4"}, "deb3df48-8344-41db-abd5-930649e70bb2": {"doc_hash": "b420680c970cfab2fa66fbfad3c6aa8cff01b01230de57bd731ec6a1c798312e"}, "77469f62-cb8e-4cac-8417-5abb9faad643": {"doc_hash": "a2cfae4222049737fe692c74e904bcab2cce44f232e1b8f528f58e1c4c10af7c"}, "54a03e67-6b84-4e5b-9b3a-a09db6c0ab87": {"doc_hash": "e0373dca9c93e635616e2c917fa062f88e520bc52821d4c67c0e89b91ae3c5e8"}, "616bfc8b-252f-4638-a3a8-6f2afa12e662": {"doc_hash": "4ffa37f2bc4bbc263b471ab20e485911610a6bf0ecb63c3eb94f98bb1c07ada5"}, "6367feb9-ef1e-478d-83bb-83c04c32be85": {"doc_hash": "9c826de5edaaae747ffa8cedba392cf6d8fdb8421796242babc95c22d4864ea2"}, "de08b289-6536-4ff0-98e4-a3bb8e363727": {"doc_hash": "0f5dff02ea31faeaf4a453b1e4dd12a56be26f2d9f2d082d2b5f9dd6c28aa5cf"}, "17b71dfa-ed2e-4ed9-a506-129875e96aa1": {"doc_hash": "ededac9df06f7cf9de563de2d0d9d6a0014e3dffc717f486dc45ba1c58426c1c"}, "c0f50496-e9b3-4987-9ee1-20cd9906df30": {"doc_hash": "36b303d1fadb12dcecc5d5fdc1d9ae95f50c3c3dd6f8b0c445e4c750a4a263be"}, "3f100fe1-c9e6-475e-ae8f-fffdea4fb8be": {"doc_hash": "ac887bd488984b036bfa8e433c09e300b2fb36dc4a5958b21fabacd3ff60df7c"}, "e9a9f5ea-7412-45f7-9142-30b634592df7": {"doc_hash": "ad911c3496ff0adecb4a2c00fe5c5423043842ad6a30641f8b84e1da0afa2316"}, "a1d13aff-daac-421d-b502-7622dc524df1": {"doc_hash": "70a76e8c8e5fb9d7234b286ff6124fa84ac7835a153f36e4b470232668559542"}, "46eaf67c-3fe1-4685-b76e-3415d54d0f4e": {"doc_hash": "34e47fd5588f03e24109dd5df44508d7a9c821be2c245070b53ebe14c106616b"}, "36a208b8-0096-4368-8887-c275ad86fc53": {"doc_hash": "715c6c7eedd0c99372b5ab6abb2844bd1f2f098394caabd0c1988ecee2a51156"}, "a7ae0c48-21a8-40a9-8579-82ce516fd829": {"doc_hash": "65b576fed4e0e2115ce31a17fb77f50bb13b40947042a46dc142761f5381991f"}, "c55f44e6-a7e1-4d5c-a03b-18b5bc439a42": {"doc_hash": "baebe1eaf3ad30b86aca3df415981f053b82908103fb2c0c8b60915cfd239e51"}, "96b1ffb7-aa8d-4c60-a33b-e77c45258faa": {"doc_hash": "ae1666c8902af7176137264b8e4dbc5adea3b364c4ae5a581e3c492b127db4da"}, "081d9993-3af6-4d06-9ab2-16c8e4d0a73a": {"doc_hash": "a6955475ca0961d1cb2f7d3a20fc196b11a777829aaaf14e8cd574a3bea76a18"}, "19ba5d76-21bf-459b-a64c-92901a5cd433": {"doc_hash": "69de8994eddcfb85b43144b93019e8ad6994c9cf06c65855116aaa2023c561a4"}, "a99da12d-8348-47c5-802b-c6af765971a5": {"doc_hash": "0b38598462a5aaf543107bde7e3641648e6370856ee5aea25b230436e865f7fa"}, "a48b8dd5-8f64-4fe7-8ab0-41590a053fc9": {"doc_hash": "ff5b9de42a7a8348dc70fc88a52764d6e9b2ecc5799480ef512536cdcd7f3e49"}, "62b36143-da7c-47cb-8e02-54983dfb2ddf": {"doc_hash": "b1e0f6645f83528bfc812ad0719f801a666e6be72092393d8c386aa50978aaa6"}, "ea16b9e1-0038-4912-b08d-c05d79e992f1": {"doc_hash": "0cb4cb124ee8399f52d5620f87331bce446c48bdd82fb60dc53e33e02cf25486"}, "822f4bed-8f8a-4572-b635-7b64a7802223": {"doc_hash": "52716557faa62fe9cecd8130ef743e8009c29f9e8b61ff2008f31c9356f8c67e"}, "4b7ea0ca-7298-4661-8773-916cf7e69911": {"doc_hash": "da653db62379c22b7cd648147f2935767508fae894517d0f40ed5784e9a8fb5e"}, "6d98a4fd-261a-4226-bab2-9d69678cc51d": {"doc_hash": "1e04dbd184e6f7844a79585266ae4ceeee47675f8f5eb9232c628a465a07f7b6"}, "c11ba10e-ddae-4306-9fb4-974daf734cd8": {"doc_hash": "b33881efe627c2daf04359c1c304130ff1258272ca36dfc24ccbc4dfcd9fc34c"}, "321e862d-d329-4921-8663-5779de794cb1": {"doc_hash": "052bdb7d3079d681f9b40e7a25b50794c84c855f9fa75c504b0fa86dcf00fc67"}, "a457ed4e-d4bf-4b08-bed1-47cb18a5307e": {"doc_hash": "9b0285c8d9817169544e4cd6b4992d9d4a1dff1d4a42b54e2e2c35412218d264"}, "d8ea723f-7cda-4fa1-b639-a93942fffda8": {"doc_hash": "0b5a44e56ccd6cbfde2faf781b911095134133292f16a635d9e0571d4adcc8c4", "ref_doc_id": "ac7499e0-371d-4829-a4b8-563891562236"}, "2a68eef7-628b-4aea-8489-45beb414897d": {"doc_hash": "f1075e6e11b8d1beb493a93964b54edbc012d316df055713b2c2d2bbf446124d", "ref_doc_id": "0f9c0d06-8922-4dbe-a246-08baf420899b"}, "c91746db-492d-42fb-9b4c-1152fc1c4db4": {"doc_hash": "d7a165ce4fc86d1cec9133df6b27784c1cad0c322fe6942e9a19ca428623115e", "ref_doc_id": "66b7b794-a9d2-4d13-a6e5-9dd1837e9ee3"}, "d28f6583-9704-4b9b-9c46-2fba7bc72f19": {"doc_hash": "5f30ceea3fc9922303d8b14501f0e6d8d1c3f34f85d4425174e8b1cd68bc05f9", "ref_doc_id": "66b7b794-a9d2-4d13-a6e5-9dd1837e9ee3"}, "662cae6e-3041-4439-bc35-d7b071c62124": {"doc_hash": "86f083b8e2b86a8854f8f335759f382fccec26eab329498191d0f605c44adeb9", "ref_doc_id": "6317a77f-e5be-4269-8575-3ee8813a2132"}, "07481227-7921-4b5e-a7f6-f10e343516eb": {"doc_hash": "4bd3246fcf7a52a7fa9354049529d907c79c577b2830d4c5cdbcfd21f750b52a", "ref_doc_id": "6317a77f-e5be-4269-8575-3ee8813a2132"}, "4a0da9a4-05ee-491a-a6d6-5a3a274bd365": {"doc_hash": "77aa8c2c665c5d8b30fce99e75dbe2ebe90e47166870ed9f6460be0fd90e7305", "ref_doc_id": "04f8d0a1-308f-40f8-92fe-9384ad9d72fb"}, "39a8b576-95ee-4f2b-bf50-b61a12715802": {"doc_hash": "4ec85dc516314b0f40defffdc3b4b35cad71111b101c6d1cbd12be82539d280a", "ref_doc_id": "4b1d74f3-f204-45e7-af3d-fe1d5c283494"}, "fcf0646c-36eb-4a5c-91c5-931b603b93c1": {"doc_hash": "ffdd3770fe1a7d561d322c39e0b9ecb88965768ede5cdf4c9b833eccab34ae32", "ref_doc_id": "0a3a9eab-cd91-40b5-a082-99f565c8aa9d"}, "d28ffa8e-4319-4369-85be-ed2907ea7a51": {"doc_hash": "291863998375f102cded856d5f41c4f253da975e8c9bce88128f66757f74673c", "ref_doc_id": "0a3a9eab-cd91-40b5-a082-99f565c8aa9d"}, "362e4686-5809-4165-b2c3-31a3998beeee": {"doc_hash": "73db0010e75524558c52f7f923df356597ad36c76f63d0609b87c6c60dde0a7e", "ref_doc_id": "958cd863-8c06-4326-b6b7-9b02d03b9cc9"}, "3941b510-279e-4797-84a1-1839af8fc475": {"doc_hash": "d3844a0dec7713230f5f4fc319171485a8ab76f1d59945523a8e578dd5652d83", "ref_doc_id": "51ab4d3a-f32a-48a6-b92c-ffb52d2f78db"}, "7ceba8cf-fd94-4296-b757-ee6f59baa14e": {"doc_hash": "b11133b711ab32a79f66637ba71ea4cf1132a7521d47098d08cdb951d9387171", "ref_doc_id": "d150f798-78fb-4979-94e1-a803e1e43a06"}, "39b60357-2add-471c-abb5-6c3ca1d9fa21": {"doc_hash": "f18f60e670f4f01e7c32a174c10e32c4e71518dee27dd6ffa6beece137db6f80", "ref_doc_id": "d150f798-78fb-4979-94e1-a803e1e43a06"}, "974db033-555a-4661-979c-0618c0541dea": {"doc_hash": "271daa8ca57d441fd5df1548d81330de1bf66db18a0f65076f16219fda823ccf", "ref_doc_id": "adc017cc-ac1a-4f0e-ac74-76d2445ee2bc"}, "7ef21053-d125-42f8-9ccb-7b613ecae9b1": {"doc_hash": "633dbfee8a0808326607ed3b5d4a47a47442ed6d6ad23b8bc228bb7b6173f0b5", "ref_doc_id": "0f1c73c9-b684-4671-aa71-47e6eb8ac488"}, "b3b37b44-10e0-4975-8265-9ce4ee96e7f2": {"doc_hash": "03a3dc9ccb202f528d4e47ab15c6833298f6923d3b759a9659da54efd80e24d5", "ref_doc_id": "0f1c73c9-b684-4671-aa71-47e6eb8ac488"}, "ce69075e-e4fe-4ec6-99ef-a35dd5ac653f": {"doc_hash": "38f06692d0b67de00529ea535f32732d14aa2b0c4ac5e412effe70c0453aa6cc", "ref_doc_id": "3a6e1496-8dd5-4de9-9768-3f24e646a775"}, "8893edb3-58b0-43ee-813c-164af9d5d5b4": {"doc_hash": "91ef8883aebe6f1105b188dff828d29c6a1f72a3986a3651136cb9e7398646f9", "ref_doc_id": "010eda0c-8a04-4332-9be4-a79e76b5d00a"}, "9c6b14b4-8b8f-48f7-9c7c-293adbe5d950": {"doc_hash": "7991f79dcdd17999e6ca0b531fc9efeadf4790cd8e0cec193d08b51f9595ee23", "ref_doc_id": "010eda0c-8a04-4332-9be4-a79e76b5d00a"}, "527b6e2d-0492-42a5-bf0e-fc2fe031d79a": {"doc_hash": "e99256e14e063c9be81ea69fe71bb5d24b05cec49cb50ba78c97228a265d4fae", "ref_doc_id": "0591e821-5b81-42e4-838f-6f6de18f7fc3"}, "20891a2a-bc5a-4515-ac40-c3fbd7f6bc7d": {"doc_hash": "4f67a62e570232efec5bb3f0d7c3b53f7645c98fd6e79af909a78e46cba0a0d4", "ref_doc_id": "b92297f3-bcb0-4c62-97fb-f22f4c86396f"}, "30d2bdda-48ac-4131-b09d-dec60ba55975": {"doc_hash": "029349bf10313c5cc342700fe8108a66b3aceb839d9f53915c939efd7ba75a54", "ref_doc_id": "b92297f3-bcb0-4c62-97fb-f22f4c86396f"}, "fd834def-22e7-43dc-aebf-e385bddce180": {"doc_hash": "0b3342640c6d29537466518293e026944b79507f0160c743ec9b365aa9e68868", "ref_doc_id": "d251dafc-c95d-4067-9111-eca2d7dd67ed"}, "196f2e54-008c-49fa-a236-ce7f97720913": {"doc_hash": "0388d3662d1705475abe19ad3b7cd936cf370046681807c336739b033b10eac7", "ref_doc_id": "ac764fbb-d2ed-4832-ab34-50ce58d65ed6"}, "04c6973a-b190-4aba-b52d-4dc378388e8a": {"doc_hash": "118263da12e7af1c0047f2e7ea3364cc42603a6714b5924b89340e474cc47c7f", "ref_doc_id": "c518dd2c-1601-4e39-bd26-680434916d24"}, "2c6e4549-324f-4d63-93df-94c0ddebf0a8": {"doc_hash": "9a89319f1f58027fb93c0f334daac2c536afc2de12c0bb8d95042981d27c7377", "ref_doc_id": "c518dd2c-1601-4e39-bd26-680434916d24"}, "cae362ed-f7cf-4fbd-93cd-502f12ca4484": {"doc_hash": "0db68b807580d314ca8358176c05549433138cbbc1ec4dea15dd4dacdf8d40b1", "ref_doc_id": "6900c6ce-0e26-4bf5-be6b-d349443630af"}, "b0343ff0-b755-45a6-a6f2-1232907b0b35": {"doc_hash": "a67d89ab78d87e544e9d86fdf38644920cdccc9dd74960d6fba3e0d82e009096", "ref_doc_id": "6900c6ce-0e26-4bf5-be6b-d349443630af"}, "99ce0cb6-1223-4a61-a443-f179ecbd059c": {"doc_hash": "26409a0c88a436f37f01dafa8ef0bc4632461b0360a69221d6b98c0163377c93", "ref_doc_id": "23995e31-dcbf-46a9-941e-599c6e4c9b5a"}, "5e402284-a5a9-4a38-b50e-9d18b637bf0b": {"doc_hash": "bee64f3dadaba78b1c5702f7a4104455cb3e02d66a1491262ec0dfe9e4352cd2", "ref_doc_id": "23995e31-dcbf-46a9-941e-599c6e4c9b5a"}, "c9f59cfb-a61b-49d5-ba37-9cf4ff88f3cb": {"doc_hash": "65c4841b85500a248ecfa12826f086ac1144b28e0b4e166f1cc81a9ac2d33e39", "ref_doc_id": "e7692e97-75d4-452d-8eda-eecdfa47a4fc"}, "ddeae525-791f-4ad5-9795-8eab22c13f61": {"doc_hash": "f65e20147b26955cdfa27d7515ff509e373d0cfc3b7478cd9f510c6d735c0025", "ref_doc_id": "e7692e97-75d4-452d-8eda-eecdfa47a4fc"}, "a3d1a7a6-d4f6-45c7-b9b0-0181d4e6140b": {"doc_hash": "de5c81e81dcd45296863da9bb931e91263f70302323799ce34e8f97e29195c72", "ref_doc_id": "126e363a-3782-48c3-ac37-112382f9bffb"}, "f5ee87f7-db19-4c24-9b54-4ddd1f4f5079": {"doc_hash": "8f0747d5d3dc9aeec5900d84681fa51b96a6461145fdc7d471745cbc9b148617", "ref_doc_id": "599f904d-f72e-4f5f-be6d-e9c5d25483ce"}, "1b54ab41-3a7d-4272-9b71-f9f5a9dabd14": {"doc_hash": "60941d6f98bd45e057ae74a4bb5a83b062d1629aacba0bae1b8927bc26b78b82", "ref_doc_id": "c0673f5c-d45b-41c3-9220-951c9694195f"}, "3caa9e0e-1b46-4b01-84c0-8c4b45ec31e1": {"doc_hash": "48912b5d907ac4cf6918e509b871fa8eaf81cd503e7780de01bd5b7a9b6c394a", "ref_doc_id": "c0673f5c-d45b-41c3-9220-951c9694195f"}, "82e90a79-617c-4aa9-a794-dc0d054e5694": {"doc_hash": "420b65cc39c69b520652a173bb58a69d76ff361bd4a88dfbc2fd01ed52c8f7df", "ref_doc_id": "513abfd1-07ca-4784-bd5e-41c0c3f8dd0f"}, "ef63d4b0-c211-4e84-8711-c84ea2c36260": {"doc_hash": "805afbfdd083828cca41d0a28086b2c49c1233d44a4d9db1d2fe1d0328147812", "ref_doc_id": "513abfd1-07ca-4784-bd5e-41c0c3f8dd0f"}, "e15480fd-fdc3-4b54-9af9-92b563abac6e": {"doc_hash": "095e12c0cfd4698f269933060a6db7b5541f706c4bdf8eece1dffb4960a4e418", "ref_doc_id": "f39e0524-039c-4e15-9417-39925ee5588e"}, "62db4191-b1c1-4c79-8fe9-2816ce69c04f": {"doc_hash": "1dafd28a3bbe4181e1efa8c72c3a49238e56fe545d01f9b7215687bc2736985c", "ref_doc_id": "f39e0524-039c-4e15-9417-39925ee5588e"}, "2c16ae91-bd75-4f5c-95c4-e41637e0dba4": {"doc_hash": "3c4d1016e004d48d40fdf2964d99f9958e50c0ce2e5b1596b0c854c906466f38", "ref_doc_id": "1b74cad1-591c-4e89-81c0-0fd0159d645a"}, "be19f4f1-817d-48d3-a3e0-b43fb59a1afe": {"doc_hash": "9577ee3538144998ed1637a2bec6b25a75f29c7abed4527cc4b95eaf7796a182", "ref_doc_id": "1b74cad1-591c-4e89-81c0-0fd0159d645a"}, "8857792d-dd67-4049-8a81-4868c4eec805": {"doc_hash": "29c391fbb604dd5782c42e29fa7a240ac34349f9b086a17204dea92424483e8c", "ref_doc_id": "1a304a07-df7f-4b7f-8c85-ed37974052af"}, "f63d9ba7-9141-4a28-9f2b-d5298112a4f8": {"doc_hash": "558dcbf030ccb5bf3dad20948c3af1fc2d9a123aa1c083d24d37f5ee76bb6556", "ref_doc_id": "1a304a07-df7f-4b7f-8c85-ed37974052af"}, "d730d98c-69ee-40a3-8d27-3d6ae993ac53": {"doc_hash": "b94bc95166447d5f1479089a5ca6f56ffbc6a01c725dc7433198c1e8857eeb93", "ref_doc_id": "285e0372-1d38-4d5c-a3a0-5bc305c26aa1"}, "57ac611c-2374-43f1-b2ec-1d68d7f69ecf": {"doc_hash": "d7dab4c9b11c558af5348c8b1e0d7cc9b5c11c3cdf0a6c1ddac92eccad5be294", "ref_doc_id": "285e0372-1d38-4d5c-a3a0-5bc305c26aa1"}, "faf46dba-5b94-4561-9b1d-6257c8e2f895": {"doc_hash": "3c2e268054bb184919d9ffbb4d458744d0f432255b033baf2969113ddbf212b1", "ref_doc_id": "b1c1759b-075a-4b9d-8632-3d6c8b8757e4"}, "e48d354d-824e-4e24-bdd6-61f14429cfb8": {"doc_hash": "b0ddee62824849a1227de8ad3407fd66296cfa09145fe4e62894093793f9c4a5", "ref_doc_id": "b1c1759b-075a-4b9d-8632-3d6c8b8757e4"}, "81aff390-b5e4-4853-b83f-5bda8c5237d1": {"doc_hash": "a70e68d721da41464166c6659ec723a632570eed067cd56b034313d73efdd121", "ref_doc_id": "db4b926c-c2fa-48b9-b971-d445cbb2a32d"}, "ff7157e1-2771-4895-8631-9df2d75eaa83": {"doc_hash": "7843501154760fa1bd4c34c8b7fdb83981d852373cab795452f47a7a1c34d8fc", "ref_doc_id": "bfd03b52-815b-4501-a310-d4bcefc0cae7"}, "7e029afa-a2a8-4bf9-810e-a45009ec1bd3": {"doc_hash": "54b6cad623abf37df84a4607ea2e0eb491179fab88ba914931101c4c5817ecab", "ref_doc_id": "e5d7d61a-3801-4a38-8bc1-dd5064fb1c22"}, "d22e6934-8eda-46b6-8517-caf4972fee3c": {"doc_hash": "4c63ac143dc473aaa1f3f7f1dc4d3dd7a392911e48cba4dba1f9f202ca242578", "ref_doc_id": "54c0018c-5d06-4f2d-b667-40a030539f52"}, "6b2df7ad-c417-479b-b13a-a3574c0e5432": {"doc_hash": "a41a6a8d33115e037de35e8455fdd2c8cc81f90584a4a65999c210a1eb2cb4f8", "ref_doc_id": "9b912e35-b125-42e8-be12-70fdb75056ad"}, "aecc4bee-8cc7-4d4d-8567-5dadbd103570": {"doc_hash": "8eea59661d522272cd1151a0107d5bacea358c3b0daaa86e491679974f5682e0", "ref_doc_id": "9b912e35-b125-42e8-be12-70fdb75056ad"}, "636a652b-b1c7-45a3-aa59-9fc0b81a0806": {"doc_hash": "d9de52d3db47dd51b83321fd0d61a20719d9539e9a8a5687672277c66106cbad", "ref_doc_id": "93fad366-2ad9-4df2-8f76-fe61187b2bce"}, "19ff54e0-8e86-4fb5-8024-95be13cb6234": {"doc_hash": "cb58141b4d946cd628eab17381ed4eaf25e75a3b5fe89385b6f6ca0af69b899e", "ref_doc_id": "93fad366-2ad9-4df2-8f76-fe61187b2bce"}, "1f512c41-97df-4a6c-9194-f7dcb170b248": {"doc_hash": "db8ae4b6ae2bf3f0ff6d1b4556ed35e0dc316f95204002cdc55e5196a2b18f9e", "ref_doc_id": "e75ddac4-8bcb-43a5-ac3a-a61218f561c3"}, "d32ec64c-1971-4029-a7a7-47186f07483d": {"doc_hash": "11337bd60c057f96b81e64dacd67e08c0b0bb069225f8a2e7638d9f87919c98d", "ref_doc_id": "f6a7c0a4-f9d2-4aa8-9a41-e76cddf15c5f"}, "c8930588-0bdf-45a0-9568-f2b3a59d1b1a": {"doc_hash": "74faebf7faafd4a7b7af25d3fdc81354e373de28452539028af335c36f0f26e7", "ref_doc_id": "f6a7c0a4-f9d2-4aa8-9a41-e76cddf15c5f"}, "f579576f-3dbc-4dbd-8a80-318cebc52c5f": {"doc_hash": "318af0a5a800d2c8193bd6b4cbaf17d1b8f64a1a9e2833b33245d275e3e426b8", "ref_doc_id": "9d2c7fb5-b6e0-4a53-8fa4-af7c73ed831a"}, "2be3c88a-7d34-46fc-ba01-99ad7ac78bca": {"doc_hash": "660d5bf902e7d1ae857c7e80bac3f8360d55f34ba9f4b0b4208fb9f1a7601239", "ref_doc_id": "cb641677-5bfb-48b8-9e3c-20035e771c2f"}, "5323d89a-2e66-4d57-b5ad-2fbeb11ece0e": {"doc_hash": "3612531c29708b197564edce9375e2357a205c982bf2834778ba3d08248c57d7", "ref_doc_id": "47d7f859-a471-4a50-b2bb-c9ed011d899b"}, "c4dba48c-e801-451d-8aef-0a56bb377c3e": {"doc_hash": "db6d8266fb66362501569e27b7a134147027de5756bf04dd3cef7a4373f40045", "ref_doc_id": "3fd785be-8bb9-4078-aefb-20303d24b412"}, "26a79565-3381-45fe-8fc9-cc3634f4a7aa": {"doc_hash": "7e1e1660337b9dc1d62ea7f7b77eba20787e570a46ef8898e5d84eacfe5cc59c", "ref_doc_id": "3fd785be-8bb9-4078-aefb-20303d24b412"}, "47bb971b-a435-479c-b074-10081c2cdff4": {"doc_hash": "f6fde3ee38a5ae7775226fc4e865dc225c012203c2cf20f96f23f1f10a07ae86", "ref_doc_id": "fef459fe-9ea0-4559-a73f-25461bf42691"}, "59a16964-71c1-4933-aadb-d6a80803bed8": {"doc_hash": "e21cb0e7d69b3c454bd4944ec8cfad90044d8f1dc8a2f73e5d6b4813a6788e8a", "ref_doc_id": "fef459fe-9ea0-4559-a73f-25461bf42691"}, "64c05e30-4a51-4f25-90eb-ffd528fc6f67": {"doc_hash": "44bc682b6e7004986486bd7bd44244236c6214ba722603083ee6c3b9ca7be1bd", "ref_doc_id": "1d654ea2-8161-40e8-b3a6-c024089c2824"}, "ee81acce-1b2d-41ff-afe1-96786e9b4fd2": {"doc_hash": "a0565376f83055dd91dfb720ad623988b2c2302051cf21a94dd18831b737cfac", "ref_doc_id": "1d654ea2-8161-40e8-b3a6-c024089c2824"}, "fe902f5a-e6b8-4f09-9021-59f9eda26eea": {"doc_hash": "7d825b54e088a0bf8a0502b065db6d866177b850e1274d65da27e26efc87b56b", "ref_doc_id": "e2d4fbf6-11e7-49fd-b4ac-62173546f7a9"}, "b489ef1c-b4b7-498e-a170-290fb373da18": {"doc_hash": "8eeec6b74bd9696a2e93eb8e115c2a1ac80f78bbc9aef436c4aa755885004105", "ref_doc_id": "e2d4fbf6-11e7-49fd-b4ac-62173546f7a9"}, "f9514b9c-0be6-4e9e-9c02-2670b3a84890": {"doc_hash": "1bf922db4ac7cd01fac0c38ee9ae319268be15fd51e0acb79dddedcefdf81cab", "ref_doc_id": "5312f52d-471a-4c62-89b7-e62c0bb9c629"}, "2e80c844-f3f3-4c57-9d71-530fd2c70210": {"doc_hash": "479db89c6c26cdddc4c04093cbd10a1b80ac9e7a6bf790d2d98a88f5183831bb", "ref_doc_id": "5312f52d-471a-4c62-89b7-e62c0bb9c629"}, "20b619fb-704f-4b06-97df-07a6417faf4c": {"doc_hash": "434c5589a3c68653b20dfa5e2ffa146a35cb4863dcd6694d3b8e3f15db8eb8fd", "ref_doc_id": "3066ea3c-0d8a-4182-8bd3-c2df08edc055"}, "1121b124-b685-45d2-869e-aa87e1d805cf": {"doc_hash": "1180da7c42ceeff9ff58c599d89d845a499564dba01319ce1e996d375c92551a", "ref_doc_id": "66cb1223-55f7-46b6-948e-64b7c60f34e0"}, "6ecf0ce4-36a4-4950-8b27-ba2925a00db5": {"doc_hash": "8750de280704eeb82ac7f74314b9e37acc82334d836050449eb0e53571243e4c", "ref_doc_id": "977d7711-45c1-4695-b982-1da44768bb07"}, "d7975a85-b489-45c5-9464-6d178db2937f": {"doc_hash": "8fe57efc60a62885410c0770a695ea4e7c0201b304cfba252901102f0304b5e3", "ref_doc_id": "c02d6c69-294b-470a-942d-4b12350a0dc1"}, "f0c14dff-579d-4aa6-b557-609646e3d187": {"doc_hash": "8e10d56ddd2eaef28d9a05bee39dd7750dd731c9a80c46bb9e413e436742e3b1", "ref_doc_id": "421542e0-89b9-4ac0-be2f-3f46dd755936"}, "ca9b4ec0-af45-4100-8415-e91b88997757": {"doc_hash": "fe27211c3d4cfa798aef80825e4e021d3d7678448a0e7490c398af9aef650ca3", "ref_doc_id": "74817cbc-3a70-4f7e-afcf-319e94e9405c"}, "095659a5-e835-49c9-88a5-6d805e252209": {"doc_hash": "33e7dd729322ed8d6eb53cf4a6d1549cc00d1638e7cf0d8150bd757cd4f31447", "ref_doc_id": "c9639cdd-73e5-4d6e-b377-ce6712cb0b81"}, "038aa9a8-eda7-4a76-a321-7a3111fa3843": {"doc_hash": "e8b75b2913253ae002b19e1c580cdc27d653952f1f1a29b2d5b2d90e2bee776e", "ref_doc_id": "172d5992-a73d-4be5-a1e6-fb1f1ef24036"}, "c8537cde-2dc5-4bf0-bd57-d729dc28a90c": {"doc_hash": "ee3510ad8c2cd907c3f71415cdb742d7c2b9e35eb1ccba7b1110ab91d1fe6ad8", "ref_doc_id": "4b5a143f-9773-4431-a175-8cacbb178d4d"}, "13384d8f-78b1-4a39-a226-92d45f6438f1": {"doc_hash": "3c7575049d53a46172e6f73ae54bd26c15507171e9955afdb37498533eb636c8", "ref_doc_id": "92aad19a-bea7-4108-a444-0579e31347d6"}, "aa9fe60e-26f8-4409-ad57-21771ca2f3b0": {"doc_hash": "e243eead7330fc74b7f8052964257e68606e966346232e43287b93bfd821e2ce", "ref_doc_id": "bda5dfbd-7903-40b5-ba83-8e3df737970f"}, "de41b2a1-bb42-4f87-aa67-a19c14d90a55": {"doc_hash": "3784092e1f9d4cc44e8c0f53db09484d1a58b6bc161243c8092d8e7a9d5d1ab5", "ref_doc_id": "1d5bff59-7709-4441-a877-da551aa6e560"}, "aca76b28-4d08-4817-9461-a646fb9c2d47": {"doc_hash": "5b724fea6961f87829d3bb67c1ecb6f0cbc5bc0aa7dc8602fa89aa89d2605c79", "ref_doc_id": "b380c5f2-b67f-477b-9c58-c85eb4cd8879"}, "2c43f437-63d2-4def-aac7-0fb718e620b4": {"doc_hash": "a8076bd0e130ba4fb3a871eb89011b6f6969941bd81fc791c0b255f925f4407b", "ref_doc_id": "52030ca6-5ac9-426c-86af-f1b4f687e3da"}, "1f2dec87-169e-4b85-ab8e-25d65f598eb7": {"doc_hash": "1281bfc04a1b636730bea286a45887712ddd3478fdd441a8648f7cd6b22a126b", "ref_doc_id": "be9bc060-3ff2-4dd7-8069-31c76fb3da0b"}, "9e34941c-a110-4bbc-be25-be12f0693b0c": {"doc_hash": "446c5c952c8741aeafc5a7062b8de1855c3e48e8e5157f2e388325f3fa68c36e", "ref_doc_id": "b2678f8e-18c8-4e4a-ab66-90bffda3334a"}, "905dfce7-a1e7-4f3c-af86-f8e98f1f38d3": {"doc_hash": "a34918cfbff3c23f208a9fb9d5f1d8e16f366da0a063fd8d89db754c5dbd3d81", "ref_doc_id": "0db00f16-1ab3-44b3-864f-239a8b6c0c83"}, "f7d4e4db-c9f0-4f00-83c8-a60cac82696d": {"doc_hash": "7045d836a4bbdffede015d633f1aef8300f20848e3c94b1476e8218e66205b02", "ref_doc_id": "75ac6d6d-352b-4f98-a0bd-368c6ed4130d"}, "4c3d9a1d-e919-4e2f-a20d-3e36318e751c": {"doc_hash": "b8f4a62df5b37e723ec97038fc1a864aceadaff3a47d6b9f7f8b43e1770fae7a", "ref_doc_id": "e7c3358f-0526-4599-8e79-510d33e5ccb0"}, "47be159d-1115-48f6-9e8f-e694ce5475a2": {"doc_hash": "32940d133a45bd58485648c6ed7d47af22947404654cebb69ab7032e58e5ab17", "ref_doc_id": "1202ddf4-35be-44e0-a50b-828eb2214f51"}, "ec754534-c0cd-477c-9935-9e9072ae23ee": {"doc_hash": "6a0fa50c35772af3a63d1d9b86841567f4bc6436218bf569a032daf19a82a8ea", "ref_doc_id": "40ab73bf-551a-4c7a-b187-2ab6de80c003"}, "cc9edfa4-b41c-4f8a-9dce-ef190a745511": {"doc_hash": "41f655d6357b02d48a43d80104c58cf86963f3d735997ff5e93952d7bb944c0b", "ref_doc_id": "2cbed6e4-0a7c-4751-a651-52749f1a6c5b"}, "9c24ac73-4ba8-4e98-a3c5-c0ab98e13e40": {"doc_hash": "88c11c22185d1b4de3c4997aff5d1750ac284549cfb12f6bedbc65d0429127e5", "ref_doc_id": "719b18b2-98fd-42d5-851f-b599fda485c3"}, "26bd3f70-0b07-46d6-be7b-5a3d198382f6": {"doc_hash": "9cf3830324accebe300fdeffaa1efeece93450c7ed1bad9de5d2f893d19fb6ac", "ref_doc_id": "23f78ec7-dd81-4ceb-abe6-8758531c22b3"}, "8c0f5768-51d1-49a0-959d-24137b11d742": {"doc_hash": "e961a74b6adb2ab8aa4cfa79b8eeb3410e16a87cb0efbdcce7e5465d3f741bab", "ref_doc_id": "ab8ffe0a-1b6e-4183-b065-c154033730a8"}, "97c8df9d-f9fb-46f4-a96c-99fede51fc28": {"doc_hash": "efe05fc9b7c30843bea526613c563ce6b72f7fa52c22a76d06922758d53a3b25", "ref_doc_id": "cd73c69f-1842-4df7-badf-dcd8a335ce67"}, "82cf48be-d09d-43ea-b837-cfe29b706ca6": {"doc_hash": "a7e0a6705b6a2284595bd7b1755b24fcaa18cec87d06a9a40c9ec97fab9e7d6b", "ref_doc_id": "785f800a-a126-42a7-9386-bcedeec6db60"}, "b8117518-d915-4af7-8db3-328ecc5e68b3": {"doc_hash": "7e96ffba58516f2af7e5d958ffb50356c16135497838fbb3d79e3ee66529ddd7", "ref_doc_id": "7464b228-9029-41fd-8004-b8efec212ad5"}, "95815446-9a5c-4022-a768-b3cbc4581d16": {"doc_hash": "1b00cb2a7bc9b0d21886c85c9e54debf56cb6af3bd82ad276187a3fa4050302a", "ref_doc_id": "63e2cf92-f789-4614-a348-79e8023ec8f9"}, "98459118-1db2-4db3-b6b0-a4b7fe998470": {"doc_hash": "ee6ffcfe1baf4efbeb635a4d533fac84d14a94c8ec1739520684b32f6433af1a", "ref_doc_id": "3c43b511-d047-477b-9ed0-767b9e119d32"}, "3c85efd1-b1fd-4827-aee1-b6388a5bd45a": {"doc_hash": "48fbf47215b845c166d0ce0b57e928a12633390a6485c32c7313fb05b84720cb", "ref_doc_id": "9569d4d3-8096-4a1f-bff2-a0dbcf3f27fb"}, "d87f3985-8d37-433a-9e3b-478f9d9918cd": {"doc_hash": "5834eb2dd75bb838eb4814a9a468ac1f9f2e69703f2e3203868f720902190f9d", "ref_doc_id": "c0be8d2d-0c6b-483a-802f-cb32f8777943"}, "a0142fbc-da85-4fc1-b4ae-5e8f0a176d7a": {"doc_hash": "8a490c81042a3d04012e9f78a25b5bdcc646b05830d480795e8d3b5872080370", "ref_doc_id": "e664eb65-e022-4f27-a078-8b36382578f3"}, "c7ff93f1-8506-4778-b8ac-ec6bb60716f3": {"doc_hash": "ae11e8b44feaaef8d4fd236ab4589d6bb1f973e30875e2dc95ac163c12d59f9e", "ref_doc_id": "e2b81dee-580f-459c-93d1-8675ef524f9c"}, "638cc97e-3205-43c7-9318-35d21326b496": {"doc_hash": "6f10267535ed70b766f80b33058c8fe57418881156690b7e9a8e9be64ea23921", "ref_doc_id": "0915ef3f-e0e8-434c-aaed-6f76249a9327"}, "d09c51d2-a16f-4467-8e52-87f38c5c8a19": {"doc_hash": "df5715b6fd9077d47063c3cb21fbdad4b4477fcac240f86b32af51251df8b851", "ref_doc_id": "7fe2e34a-f0dc-4292-a292-0859f7f1af33"}, "0ccfc0a3-f0ed-4b5c-8a2c-220beac42363": {"doc_hash": "9cd29753614a6fe4e96dee119bf152c963278134112af6cefcb70053c174e0e3", "ref_doc_id": "78e70001-7d64-4d27-96d3-282a8683115f"}, "2d258e3b-4a67-4755-990a-80f4922545ad": {"doc_hash": "cc25a918c3ea2bc63c36d08d14e4de685f17292ce307be2180e03d0ea78145bf", "ref_doc_id": "aa4f36f2-41f5-4dba-8939-ca44ceb388fd"}, "93a1b5d1-ca47-4add-ae2e-7125d83c0346": {"doc_hash": "91ae9119333a0f67bed1f18c1e9176ad2e6c9cb61e37880a2a1300704bbe9d9f", "ref_doc_id": "d5581b74-0a29-461b-bf09-713c5c91b2fa"}, "a4d2e71b-c680-4609-bc9b-41fe242ea5c9": {"doc_hash": "7168fe277f0f72dc0a3e208cb7b1abbdbe68c2aa792e96459074beac75b0c264", "ref_doc_id": "9fd6f04d-b140-46e8-995e-5cecd157e86f"}, "5700acd5-cb6b-4fd6-988a-ebaf5cb8f7d8": {"doc_hash": "ed501dbe94ea907f1facee75ba073cb36ac1cb505a5454c44d87d2a657b7daf9", "ref_doc_id": "acf5c0ed-ee63-4cb0-8cfa-6cc1f117ce51"}, "0aac1b45-9757-4fc1-991f-1de25e023c02": {"doc_hash": "8274ba0ae69dca6844b7e55e21124dfe17cdc72b9a7ccde9ae92e2cec0b38173", "ref_doc_id": "f5a327cb-be6e-40af-9f97-03cf8d03f531"}, "840afaa9-710e-4d94-832e-fd8112d90102": {"doc_hash": "92824277a329c9b54d658bdf1961e5f4bc3e3056e1b3f1cc3cbf03b34a5a2cd5", "ref_doc_id": "e5956047-3129-4ab4-957d-3b78d6caa8fd"}, "33e73d7f-8b91-45c8-9f38-1c676279eb82": {"doc_hash": "284d3b0c00ff8b1487fb722aed76847a00312f6f813c34df3effed5c68d2cf11", "ref_doc_id": "d4ffc73a-eef6-4665-af8c-3dcaaa015fa9"}, "e0f8c644-ade6-4e71-b0af-6df46df74bcc": {"doc_hash": "34928e1a24d87726f1304a8f56b045c3ecc1f972ff9cb9a74c67a4cb22bb0b60", "ref_doc_id": "00d68343-4b5d-4cf2-9fdf-009d363b8f77"}, "4dd8fdc8-c9ba-4673-868c-62536cd5ae35": {"doc_hash": "3c940b41264069940f83281a1a72673684c644c9cf084f637d27f3fc2f7fd139", "ref_doc_id": "42ea8556-166d-4f2b-9fbf-1ad18a2a669e"}, "8497417e-65b9-439c-8c6e-e223e194a739": {"doc_hash": "39ded6dd0ee7314fb2d3a70c7478cf1d390ed940b614739dabe611546c29db33", "ref_doc_id": "42ea8556-166d-4f2b-9fbf-1ad18a2a669e"}, "304af189-e79f-41c3-ba62-c9005b3f9a55": {"doc_hash": "3f77d2c41a2e9868790e69411d047707f30d26359c219cbc2e1211d7e89e8117", "ref_doc_id": "e578675a-57f3-4346-a9cf-63d534aeaf60"}, "361a193c-0f63-40e2-9ea9-7dc219c5cbc4": {"doc_hash": "07b6bfd9e03fa71696be9d32e2f068a4460d7d3e4574b1840acc608f35505d85", "ref_doc_id": "4cf34b23-5a92-4095-96d3-94605545a6aa"}, "5752b15b-86c9-4a3f-8997-312b7afdc63c": {"doc_hash": "999d2448604354cf525a0c64629ba557382eb73b8b5940603130cc2d3554a790", "ref_doc_id": "3024380c-80b7-4d35-8d81-71c242fc3441"}, "0b726b32-b5fb-48fa-953d-1f386693c4d8": {"doc_hash": "6377bc9a5d034fa493351da98b5863366f02a95e3e13e8df24ea4b4a49b5f444", "ref_doc_id": "d74fee4b-74a0-40f0-8eeb-f876277c7fca"}, "8d0560b4-4290-4e19-8566-4c23d608c96b": {"doc_hash": "3e9806fa8a0f94b6fbc513224b7524159ce242be1e9f06bd2c0992dc4e3fec07", "ref_doc_id": "20b3b4bc-a2d9-469d-b6eb-3e4440044a36"}, "88c42e7f-a908-4cb7-8026-0356b60bd717": {"doc_hash": "ef64aee7e3df91fb4bf1cfdd10d326c38a2995253576c2faf34ae04116f911a6", "ref_doc_id": "2324ca46-bbb4-4a98-9c89-299fb48e765d"}, "f82c81f7-a44e-431a-93fc-0f4b7493d9f1": {"doc_hash": "e6193963e78208e55e2287439b5a8001a37be01df74d1d157000ddd57053c5ec", "ref_doc_id": "54d5c5c8-3556-48a1-9e79-2e3cb7c99224"}, "369136cc-11f6-42b5-9138-2393f21e36cd": {"doc_hash": "7e691d83ef05a2e6d138e83d0437b3c15ba7b0ce8862bca9541797afa2f61fd1", "ref_doc_id": "6e777095-602f-4dbb-8308-a3de47f1f1ca"}, "73fbaaef-fe9a-437a-9ba4-0f917b38ce34": {"doc_hash": "eee0ca150058ba55b0cf4fc67ad33d96c10f3a4cf32d979c224d03024ee2ee35", "ref_doc_id": "4f70a28c-6500-4f4d-a9e9-62ac94100ecf"}, "5994d7d9-bd15-4874-ab95-8b93f9702b4a": {"doc_hash": "c10010dd10bc99dd96a4a8852ace1b8452ea7251f7915a39f8f242affa23f2a4", "ref_doc_id": "90663b36-1b16-4f13-9ceb-7f26ae82fb9e"}, "fafd2570-3a4d-4983-9123-022aa5d415ef": {"doc_hash": "ec32c96e7d9e1b6b8628db3752679df370004272a800cbb8a337d9ede9ad4d5e", "ref_doc_id": "6cbb9f05-e75a-471d-addd-56f8c12cae6c"}, "fdf80ab7-8d16-474d-8843-06066360b0ab": {"doc_hash": "ab0cc73ba6b24bb86cdf568450963ac16b03fbe23e1ec8b19dd592e078b8e0be", "ref_doc_id": "218daabb-d00f-47ce-b417-5953b7da3bdf"}, "a3dd4d5d-f172-46e6-b86b-7ad9d5abd1df": {"doc_hash": "8279b3977f9cef4b2431d3e1608b9a8f8975d724d3a6e5d045b305eebaf52b33", "ref_doc_id": "c7426651-9112-4859-bd12-e48f074a35c8"}, "11e154ef-0913-409f-af3a-b0740c10a48e": {"doc_hash": "e40c55b5064c048624ebd130fba36b682991829166d37df365293c9cb38b91bb", "ref_doc_id": "d3c910f2-7973-491a-93b7-66c2551f9c71"}, "2a791035-2164-44e6-a8ac-63f959bab642": {"doc_hash": "2e4c89ddcfa96e4a16fb307897b5db2a5e3605af1eb3b5bb48991c20851a6f0c", "ref_doc_id": "ab6e80be-7b67-4943-ad37-ae28fcef98f4"}, "536f8afd-cc56-496e-a997-f1eccd13d1eb": {"doc_hash": "10853eab443211187999d690fcf72bfc3b1f09537dcc105a7bbfca0141be5b7a", "ref_doc_id": "d8b67d49-0086-4fb6-8c74-d1f947b98013"}, "bd4a7088-2165-4501-8e78-bb39ed23b385": {"doc_hash": "767ce41c1f9e0d2c03b34aff8cd2c95aaad85f98f6a6f7327274724465175b97", "ref_doc_id": "d8b67d49-0086-4fb6-8c74-d1f947b98013"}, "8ebc271f-4e93-404e-a65f-827cbebd235b": {"doc_hash": "3d1b4ef8887396cb689a267a0b5b5bff57f26d85935c6b632fa27861e3498948", "ref_doc_id": "d8b67d49-0086-4fb6-8c74-d1f947b98013"}, "25bfcfa7-6c79-419e-9d1b-41c88f5723b6": {"doc_hash": "81701dbdb5c433f41159ad0533f189e5c149fe58bd31f42bec635983524bcf51", "ref_doc_id": "7b547050-93f2-4e56-87a1-d88f740d3255"}, "8216b001-23d0-494c-a426-3dcbe88cc54f": {"doc_hash": "9a0eb32b6668b9aafd50bdf83ced1b9b7e38e3f83545b6ccf7440160d6176096", "ref_doc_id": "7b547050-93f2-4e56-87a1-d88f740d3255"}, "e9e45378-07d2-41da-bfcb-4a5d282c4ebf": {"doc_hash": "7e89968d242cb176af7990f26cd0d730c95c80634acbf0697da636d56d74ac58", "ref_doc_id": "7b547050-93f2-4e56-87a1-d88f740d3255"}, "361c2b8e-de61-4f90-8a25-83c6ebe308c3": {"doc_hash": "be849540b211fb2b89301bac0d5816ac87bc4749ffb7225244a23e5accf90813", "ref_doc_id": "7b547050-93f2-4e56-87a1-d88f740d3255"}, "ae8c764a-8be7-4380-883c-3e8105993751": {"doc_hash": "f2a0540322376787f147c3f26f6b9a2edc4c315509966cb63e19b96d5a85112a", "ref_doc_id": "39681515-8c85-4abb-ab62-c4d0f0977426"}, "d7e9081e-e29d-441f-b2a9-aa6509a8db9a": {"doc_hash": "747620277e42014259320345373a17ed40f651fbe043fa44538369004f3a2900", "ref_doc_id": "86094565-3380-4e17-9a7a-493cbe5da686"}, "2a3d6128-83ba-4206-b97b-0596ab3d51b3": {"doc_hash": "6fc27de34c4178a10972c84b7816fb3cd388e140d4b392d7c48c79151e4f1e85", "ref_doc_id": "7447810b-a508-49de-b11a-06cce427985d"}, "4e7a4ac9-8f75-4056-897e-35cfc16e073a": {"doc_hash": "91d2ba0d341e1b57a6591afe1315f0f13360f5c38d3f178ed2e8b320d6cf583c", "ref_doc_id": "63eea7af-fb3c-447e-a615-96c7fa6bd98e"}, "100c7829-edde-4cb0-bbd2-03324a0100c6": {"doc_hash": "a7a16de30a6bb6cf249ed21df4c816457be9ca02af473df21fbf9d3d4555530e", "ref_doc_id": "12e1ca77-8426-4d3d-ab10-3879b17971e5"}, "c637d880-3c53-44a8-87f3-ab72b6c2d5be": {"doc_hash": "009d6ab3bfcf5c196fb60033132885bcf1eaa8e8597e756c55ed75f7db49f567", "ref_doc_id": "0d854286-079e-4779-bef3-d571491f4ce4"}, "1ddcc79c-d999-4448-b698-49fd13bf9bbb": {"doc_hash": "95f750311264cd4146fd6b0045f278645a5c3f8f92fabd82ca14394bb119fa34", "ref_doc_id": "d8e6f09a-1c6f-4d08-a08e-608525b982f1"}, "f27f5465-af2e-447d-a6a8-bea50657e0b6": {"doc_hash": "122c481d6799dbaec9974e3c870c1dff9a326a6961f9a2b5819757a2e18db658", "ref_doc_id": "1e08d2fe-8ac0-4f64-a56b-0decdb99d71c"}, "9645e3c7-3ed6-46dc-b693-c114f5ddad7a": {"doc_hash": "f466bf3a4995715442f9f5cc054dccd86f7fc6cd107d434b69935310f277c929", "ref_doc_id": "a63198f0-3d82-4093-9c75-253d3fa4b043"}, "19b0c44c-2fc0-409c-a866-743222f5eea9": {"doc_hash": "410ff742429d3f0a77a29e648996120888c68167609192121b3ac825978cf171", "ref_doc_id": "102c4efc-eeb7-41c8-b875-39ba595955a3"}, "bb111975-0519-46e5-bb2a-d56d98ba2bf7": {"doc_hash": "36098cc7de4a36cb3b1c4458656dc4e96e96e29067f097cad87438772e6f0471", "ref_doc_id": "36f33a13-8462-4ceb-8ee2-4c23704a09ff"}, "f0789c36-bc8b-4ffa-8f2b-da3c70ab7a1b": {"doc_hash": "1316ec0d0b0fef351d639124185d4f413f7e8fa303ab3a8b879ac841c90e0912", "ref_doc_id": "f450d412-a1d5-4f30-a705-4d92ca30de83"}, "80235b9c-74bb-40af-a3b7-33f3c6327b85": {"doc_hash": "b2b95d3ddaa9c31c419a7491b5b2034f46b1d2a8a8194f962681bd9d58d51666", "ref_doc_id": "7db9e367-ee68-4ea4-82f9-d99b87cef3c1"}, "a48afe87-044f-4bba-926a-1f523e338f27": {"doc_hash": "d42e2ef3f3745a87574c9769e34a46c49851c97f3c0a4aaf5862937000e1e363", "ref_doc_id": "923e5c9b-accb-4571-8cd8-11d062542bd3"}, "34502f2c-174f-4b24-9295-7467d177b6cd": {"doc_hash": "7620c7af82fe69caae75099915a25da9b8528e7aee93b51d1a7452febfba05b1", "ref_doc_id": "dc0734d4-8eb5-4574-a651-0aede047b15a"}, "5bec7abe-352f-4107-8b8f-ff3c752f2370": {"doc_hash": "8acd6cfd5a0ae3e04b3cbf1e7e47a7fdb525032bbbaddb9c0ae65704d054916d", "ref_doc_id": "9bb82254-c236-482c-94b0-1bd5f94184c2"}, "831b9a68-e85f-4bd8-9fd7-8bbfa093bb43": {"doc_hash": "cdf15c96f8b61b1c02e55405901eb7f48fa28deca2dd4bee6f29958b54e20b8d", "ref_doc_id": "8b0e73f7-027b-4e4e-92b7-20031e68e0cb"}, "482fe824-8664-48ec-8fca-96ea7735c5eb": {"doc_hash": "25409b491d82f1c74eaeef7bd9ecbfa1c7b568b6fcc5883308a2bf18e5de9439", "ref_doc_id": "fd325f97-52db-439d-8df0-2598c173c248"}, "abcd1762-d6e1-4d55-a86c-34290d7f1b8c": {"doc_hash": "f8c92a6332bca7412d72e804dd20842aedce1b4dd952041e7bd0216e5f72ae5b", "ref_doc_id": "00e6d59c-b9ce-4256-a537-4c7c3657ef7d"}, "a8ae107b-a548-4de3-a049-7ead7caa8ed7": {"doc_hash": "0091f6f6fe23830a5b215608bc5694b5905895868027723a27051648fc2133d9", "ref_doc_id": "cdff2e2b-da73-41cf-bbb7-68fb83ce9f53"}, "d6bfd81a-c863-4b8e-af80-40c16879f183": {"doc_hash": "46ac03ce31e0d61839db5e8ae7077451e5681289c56c862a92f41eb87a74a552", "ref_doc_id": "c4355eb6-0952-4918-b144-08ff2a7ea3a9"}, "022752de-e320-470b-a41b-0dc4b8cf0b97": {"doc_hash": "3ee22d7e0149a031db7a8e52b2dd861fb6ba7a537dbfee30e588be56c1630f5d", "ref_doc_id": "ef9e6651-081a-419e-86b0-c3051cce4eb3"}, "2b1781e4-210a-4576-ad01-36c934cc55b7": {"doc_hash": "c92e24323e80ace396dc89698886c420436f6121d858f9122a2625868cfab712", "ref_doc_id": "ef9e6651-081a-419e-86b0-c3051cce4eb3"}, "c777173e-4af4-4e3a-87da-c28860e2d567": {"doc_hash": "d61537371d88b2033c8f19e359eb2d6b4f8c0353682173c0b5411f5dfe65ed12", "ref_doc_id": "7f699359-86b4-4892-944d-e1fda009bbaa"}, "55618c4c-92da-41a1-9d8e-54120568517f": {"doc_hash": "977c24e1dce06fd4841848439273c3ff030ff809f70beea8b1b50eda6a0e3cee", "ref_doc_id": "62d28a5d-4e62-41fa-9a37-8bc1fc70066c"}, "1c4569f2-fff6-489f-900b-dc84675ade87": {"doc_hash": "b4b2e2d423d4acb7299faaeb8d972a52e2f8bbd074428f6c95c2da580e670dab", "ref_doc_id": "54337252-5c91-4a4a-9966-d6d892be9a9f"}, "bb5f773b-29f0-4fc1-a00b-4661f850c476": {"doc_hash": "28d07d16d8f0a7a72c6535b3ebe1daab2903c33d75975feab4edd11a9a564c17", "ref_doc_id": "325ed61c-1ee0-4c24-bde5-695dbbf99d96"}, "ae9994b3-ac8a-4c3f-8751-51346607356c": {"doc_hash": "8aae0fb8a516a03d2e4bfe3b79f8340495a59ded42c1f24e45db07040f3735ba", "ref_doc_id": "8254e844-bd0f-47b6-90a8-e361f55c579f"}, "aca267f2-a652-45d4-80aa-79cae64b1695": {"doc_hash": "1a3ae1d1a01bb0e937ab660ae47f5b0004190035ef5729854638825a208ea4c2", "ref_doc_id": "94fb958a-6867-40bb-a62f-e156eb0c0119"}, "1eff1206-0600-4273-9acd-fbcb5c7e2be9": {"doc_hash": "7ece2d2237adf2cfcdbfabefd094649fab626890c0b5ee734991242bfdbd57bf", "ref_doc_id": "cf56fe60-877b-4154-9fd0-a6bd4f1231da"}, "3239efe0-81cb-4380-868b-752a7b399e2e": {"doc_hash": "ea72b8302b84549c6724600a38c290860e60cd3446809398ac9cf6572bb7684b", "ref_doc_id": "ebafa02d-1bdc-4ff3-82eb-e033d21a3137"}, "34694811-b364-4f27-b3c7-205b46b8f85c": {"doc_hash": "2de745bc6763a86c06981cc2114c226ab6a3041ab7ad1255a04df2bb000a79c2", "ref_doc_id": "3db175be-3dcf-416a-afa8-a347105a3aff"}, "13902d1f-614e-4535-8fc3-4642c28b0ff9": {"doc_hash": "aa547ef83f522adcded61a3dd1a689552ac87a2eac1e45fc8aa6db5b30e70249", "ref_doc_id": "d5ace620-0092-4fe9-8466-619627db533f"}, "28a429c9-dfaf-4d34-b3fe-9c19b7a0172d": {"doc_hash": "41a82f2ae4f5309bbf9e0d7d228f49fb702d7014991cb480871029dcdbc04ece", "ref_doc_id": "9f0dcd29-a46d-4b5d-8484-03029a0dc288"}, "32c18a82-0324-44cc-a9a2-aa23bb0c7e6d": {"doc_hash": "699c872f42d1529ed08fbb2cf60365d8e05f515bc719333729154f14d72815e1", "ref_doc_id": "f1c3f67d-9095-40f1-883b-5ae1a91d501f"}, "9b24daae-d643-4a73-87ee-0e1aa7117ef2": {"doc_hash": "9a7efbe7ed75086609a8153b809664dda296e838b85f4bb1464a5ee677a1e497", "ref_doc_id": "861cc119-4f77-4d3b-97e8-4ce7edcbc432"}, "cccd916d-b521-40b0-916d-ae9a24877e8b": {"doc_hash": "26a34930e423ebabfe26183601d3650b020da344969132b39e18ec9b4bb19953", "ref_doc_id": "569d4a4b-d97b-4a12-9c5d-a21b5c157e37"}, "918b2ba1-0584-406f-b3b4-381b8fc62a99": {"doc_hash": "a0564542c9ffbc50107d2bdf592f728c85bfa52619b98ad542afad329e0d881b", "ref_doc_id": "3c742491-f432-4f86-89e9-016bc1550d37"}, "8b19bb8a-cc74-48a1-a1d6-c5648cb11db6": {"doc_hash": "ae150aad4af50c89b9070ec0d8b54fdb23ed399c006cca26190e06757ea9b639", "ref_doc_id": "3c742491-f432-4f86-89e9-016bc1550d37"}, "199b9f94-0f7d-4c1b-af7d-546b9729cc3b": {"doc_hash": "feade21a61378c21990fbd5568768b3b84569dfaaef569b5bb2e2e6654bced7a", "ref_doc_id": "1bb1a9bd-d279-4390-94dd-f0097b72bc27"}, "3348b843-0260-4877-9b39-5e692cc0586c": {"doc_hash": "1afed5565fddc8c3789403ae02dde8967d167fb52b20c647246996c4731dc4fa", "ref_doc_id": "ec2c7748-1085-4cb4-820d-ce6271e84277"}, "974222bc-6c9d-46b2-b80e-e533482200a2": {"doc_hash": "e88774ba44f43acab84e298075b29d37d3faf4444b7bb2ddd0abc9f68899ebf7", "ref_doc_id": "b535cfb4-12b0-4197-9250-377620fae3e4"}, "085d1224-3e86-45e0-a95f-dc4ab6e24987": {"doc_hash": "1d8c426a3919c56416b5369d3a939b42256a6052db2686dca94eea9cc4299dc0", "ref_doc_id": "b535cfb4-12b0-4197-9250-377620fae3e4"}, "23696645-d29f-47e8-b676-cdd0e90e7674": {"doc_hash": "6c386cf4a74bf34cc366d055640318ce93a8e93a613733b1ba6d672a38064b23", "ref_doc_id": "9156c4f1-b290-44e0-8d1e-24d2e26909ed"}, "b724ab26-df5e-49f1-9267-3a737c5e117c": {"doc_hash": "311c100d2ba7acd064470bef2ddd8be8a5829f6a30c6052e24c1f9fdee0f3954", "ref_doc_id": "18ff8af6-4eb7-4aa0-a944-3ea2d01da36e"}, "4bf2f767-6be2-4db0-85e4-2098f6478a3f": {"doc_hash": "d75e9a606f329a252c98babd6956713f12498de2bca03b205d27bf5f6ccd60bf", "ref_doc_id": "4fdb394b-7948-454d-b9a2-8909e7f62ca6"}, "72a1c232-1cd6-453a-a576-eb6b956b7a3f": {"doc_hash": "1171bf0c2aa156d7c9ed42cda78166e3a4514b6ed4736c1a2b29d2f014773aec", "ref_doc_id": "d0bf7c98-55e5-4c65-a3e7-758fbd108e46"}, "6b613688-b1d8-4ea4-b159-dea680b50df9": {"doc_hash": "023da6f736cd5c41667c23d6ffd28f84a57b8640b2943514d87fad84dc240ac5", "ref_doc_id": "e2d4d540-8b29-4b34-b448-408686daa67e"}, "6e432994-c9a8-4467-ae64-8cf787f41453": {"doc_hash": "c80e110a12a48b1befa1d9cb099d4728fe9ab38dc781dec642f4aa5bfab421cc", "ref_doc_id": "54351abb-e44c-4f09-b0b2-bd52fe977ba1"}, "5dd5afb7-c496-4dad-8131-86b0d8edf44b": {"doc_hash": "d2a25b6a22427da6ef8f707f1c24f19250379d15a163bf1e6bc9f514f91cae51", "ref_doc_id": "15c15b76-a7b1-43fa-a92d-9cf86445052d"}, "ee936b87-dd41-44c6-b38c-b557bb052e45": {"doc_hash": "009c32d8449a7042d5370d860e3788dea85574de2934d2f4d2af7b5a31deea42", "ref_doc_id": "e7fd7b48-74f2-4cf9-89ff-f8dd86e55b9e"}, "eb09f623-cf9a-4f24-a46e-8b093ac0ba52": {"doc_hash": "931d6db616aaec2eb23b1a6d387404250e87430435fba04bf8e0a1558876db38", "ref_doc_id": "1cdf99cf-eb30-423e-a995-98433423add4"}, "83d1ff0c-c425-46a2-9bcb-abe89cef003d": {"doc_hash": "c779784541cb52bbd9d0072ea2da452cc9461d57e68709959ed9096082625750", "ref_doc_id": "69f216c0-09b8-4677-ba8c-ab93527c0eea"}, "670c5bea-a3ab-4db6-843f-3198dd2518c3": {"doc_hash": "4235c1f2b176afb32aea5ab1dd89d9224d768447acf7af0353d2d47a4092d391", "ref_doc_id": "c57292c7-148f-44be-be23-551f81f6a484"}, "f3284bd5-4742-49bc-b322-fab3b81f223a": {"doc_hash": "9e84ef1f33b0e9c5a6d83fc2ccaa61c9bb5852beaf06f7b126cfc2faf48a2f17", "ref_doc_id": "9b8101e8-9e2c-4d3b-838c-4a9e624a3c80"}, "a335367e-9c89-4ca5-965a-2db86a918a83": {"doc_hash": "bba0db0b7149d017d070add9b1eb51d94d5eacbe12de07b78cc33e0486c87417", "ref_doc_id": "335b341f-1b61-47d9-ad56-aabd3f0e18ac"}, "85086b2c-3cdd-4e68-8a99-5803d67abfc6": {"doc_hash": "8b1f74130f2eef12210771d2993367741cc6d189e996f504cd5fe10a5e628c74", "ref_doc_id": "69a1b0b1-754a-47da-b8f1-688d44bf219e"}, "fa38f147-4f8c-425e-a520-9c1e959c1cf7": {"doc_hash": "f038f70b7e9ebf5cf852a4295c3e32da04d25dbf073f6fa82f25058016bb354a", "ref_doc_id": "becc8b28-a7cd-43fd-9e3e-bed107ead8e3"}, "9486bcc0-8330-4407-a7f8-effa09ab135c": {"doc_hash": "9c5dce11e99f52d54e7839ecaf74f35f82095a8ec0602da7e600efedfab3deeb", "ref_doc_id": "381593af-638e-45b3-8a3c-bf260d76b812"}, "9e57ffa4-8266-4c97-aaa3-39948246437e": {"doc_hash": "961c76f917cb46f603544909ce011b43470ccad13938ff3534bea163228ef097", "ref_doc_id": "fff1f62d-f8d2-4ffc-bd7d-bc745aa724e5"}, "f3a0c624-0b0e-41f4-bafd-03bd91b5a3c7": {"doc_hash": "90702d0cf5c3d394b263aac0c14d8a237cc59a289efcf2f470c5ac9546322144", "ref_doc_id": "b0f7085f-6fab-48ac-b13d-84caeb9733b8"}, "d7c75882-374e-4608-9b47-e6b37590f4b3": {"doc_hash": "809ed5672bfe259c8a5417336f79d391e541be3201634a41e080eccc2b70c4f2", "ref_doc_id": "edafc58b-3767-4a84-8a55-46c512ba63f6"}, "ff2d0a97-48d4-480f-b8e2-ae19b94f4278": {"doc_hash": "ffaa22b5557eddb5025d70c2c22bd51c15d33ee224390ca1dabe7d1b7fc05214", "ref_doc_id": "015de721-0ca6-435e-a2e8-19ac57134a83"}, "85aab95b-e950-4ade-b8a9-810611a92c55": {"doc_hash": "fbab0a1f6f0c8233d03a5a7d796527590a1c3fedd18f663e51d53fab77d3e038", "ref_doc_id": "d3ae84b9-c424-4056-92d8-ca476d79952c"}, "3b8cac5b-6ffa-4707-9ae6-9f54417057ce": {"doc_hash": "312130502bbfb79d73f96f5eff4ad3369a4060a0d2f2df76dc0bc9aeb483208b", "ref_doc_id": "9a2abde5-e80a-4a3e-8de7-8319bf297e4e"}, "0bcb1d91-96c7-483c-852b-4fede3a3a011": {"doc_hash": "151d6526cee5aa74ec8a16312ad4df1e1a47753df754f25ae7b86eb166981ebd", "ref_doc_id": "ce09470b-27bf-486e-9cc6-50afc19c9344"}, "77737234-7603-4fd3-9f55-efd32080cd45": {"doc_hash": "ba416243640af93ce595bf4856b05dd71f602f57d698cb85157e95a0e0862b8b", "ref_doc_id": "1ac9952c-8e56-4f42-bc03-89499d61f274"}, "0e3cdeea-6308-41d6-89c8-b07d1766002c": {"doc_hash": "ba61557abc862e9dc61ff33f90370bf06e99f05dc6f36b4008bc7576fd811911", "ref_doc_id": "dc5d61f6-8449-4666-8803-ec6b05e4624c"}, "630c7b20-7592-4575-966f-bf66e5679b26": {"doc_hash": "8ad8b2f96a69bade09366419d086218ee4564f7b91280b25869c00b8ffaa9779", "ref_doc_id": "5004794f-8fad-4cfa-873c-afee88c05512"}, "717fdeb9-3400-458b-9f3f-c6dd08e4d794": {"doc_hash": "bf0d9f7a062cdf902371f303b272f5491755bf6c5be2b54412c4aacc79ef696e", "ref_doc_id": "5004794f-8fad-4cfa-873c-afee88c05512"}, "20fa629d-cbf9-4446-8dc1-6360b64098cb": {"doc_hash": "89ebc98034972ba3750d483d90bdd6f803a669486c087feb4632284c3187c1dd", "ref_doc_id": "fa380b50-86e2-4706-b4df-51e92f1485e3"}, "c1df10ca-9475-48ae-96bc-a843073e20c2": {"doc_hash": "1063a4d9a34de70363d85b9d3aa0639e12f6943d3e72e09543452e6b86ba986b", "ref_doc_id": "a8e6f383-662c-432d-bee7-b76c229710bb"}, "424ddae3-fcfe-47ea-b4fc-e85206c08df8": {"doc_hash": "71d6c4b07a959685b19136bb189ea162efaca7cce24bf5377e9d8d6fbbab1ebe", "ref_doc_id": "a20ad3c1-4e85-4a6b-8f98-0120e65fcbaa"}, "2bb98602-e045-4794-ac3b-121321276418": {"doc_hash": "1eb61e2d7104c9b23b8417b93265a819bc084b48860f95c154d0915fc8428d68", "ref_doc_id": "826dc389-ac19-4651-a617-723e560d5274"}, "43f4798f-f176-4d13-b629-e1c81f922ecf": {"doc_hash": "8574768e8717e664228d72374341be80ef8ce79cadf506f66ed20fc0f4ac3699", "ref_doc_id": "13946762-11ea-4dfa-8b54-aa0c1d1182d6"}, "d81d0f14-6eb4-49b0-9d8d-a3e5478ecfff": {"doc_hash": "5e620de5c2d9b39a7edd63fc4052a0a3efbc0c92b379863609c214361abf8181", "ref_doc_id": "05d16aed-5cf9-47c5-a726-0078f2338d97"}, "2371ed6e-074a-444b-97ba-bc07ced68a98": {"doc_hash": "db5ef35e26c7f69b21d5b2ae967c61cdcbfdd789d41f5bf938fd9efbdb947ed8", "ref_doc_id": "64a04ac2-ba78-4ded-af50-1cc8ac85cae4"}, "003c077d-1796-4738-a3c3-55f88ba0db54": {"doc_hash": "4abdb2fd148fcf0e2802ac05a0a294c163329929f89fce40718d93dbd9ef3419", "ref_doc_id": "00447671-771d-4dd4-a593-08d6c6461507"}, "c6c6f28c-af97-49a7-bb44-22a87a15232c": {"doc_hash": "376ab6c351fc3bfdde5ca7c23d6506902efdc2ed4f4a647aba38db1915b2b58f", "ref_doc_id": "69e12c8d-b042-4093-b98a-68474f1fa878"}, "94a79259-faa5-4eba-a8f8-954f86b7f338": {"doc_hash": "2eb5c90c030b5fb1b2d1c9b1c8b3c19ecd3efad54289356cd8ff7466abcbcf20", "ref_doc_id": "da936525-a75c-4d6f-8a38-dda6c858d471"}, "fa5c73c6-1a08-485c-8677-e20dc1e13cc3": {"doc_hash": "e5ba6fbed5ac71299900db2846d4284c46d082267effda88f392361658f41afa", "ref_doc_id": "88fd3487-c9a5-4186-8451-f12151bc2d5d"}, "16143dfc-e6f4-48ba-b8fc-07b9f23b4ae1": {"doc_hash": "7ea00c3782ed3e99a204956d5ead48080ea5d850a9b9fc3b3ebbf6b7c59a2a05", "ref_doc_id": "cc7f1185-7492-4cfe-9125-de2d18d7c034"}, "af72a553-f066-4d2a-bf94-60ca58d8be9d": {"doc_hash": "78bc6871e491cb5c7fc6b33df76af0e0e803b5e7415ef14d338847af235aa52a", "ref_doc_id": "b9d45763-1d10-42ab-bfa2-6de099eeb82b"}, "36913ad0-643d-438e-8656-f4882cb3e7cb": {"doc_hash": "85b4feaf1a9651940c802b46e4e9e8d983b46e9b7ae63d306673b7cff8e63f06", "ref_doc_id": "b4fd648e-8a8d-485d-9c52-882d80a446f6"}, "bc1c2bc5-8655-4af5-9e9d-e9019dfb7b60": {"doc_hash": "5b8af6411008d3a84ba57924e368f23aa5d1224e8982c97dbac65da232815bee", "ref_doc_id": "00ad8487-36d0-4826-bcbd-6b03fb8dfebc"}, "aa0c9a56-eff2-49dd-85e0-07a8a93069aa": {"doc_hash": "9712f4d52249e1264a00ed2d76b108ad3a96148bf2fa3b76c7a795f594eabbe5", "ref_doc_id": "3d8f54e0-fa0b-4c5f-8438-a58023c3b962"}, "144253ef-9821-4751-9f5f-0dde630deaba": {"doc_hash": "b5bc4a655ca80ce77725d6c2af2bc0508403c1bf3c375653f26cc33ea7dd61f9", "ref_doc_id": "7a756b8f-0693-4e36-973f-219388407f63"}, "5d55c68c-e41a-4adb-85d2-80b3cfecc8ac": {"doc_hash": "708dd84adfc8cbc8803837f36356af776cd79596aa00a9eb6dd2dc72a26fbaf1", "ref_doc_id": "67691e55-6477-4441-9f2d-3624e6feaeda"}, "c76a81d4-d386-4d3a-a588-9d076ea25bf7": {"doc_hash": "e3b2703a9470e1bbb51c2e3d3fdb2d9d07ea242070645241cbb9856578096240", "ref_doc_id": "9187776a-cf65-4167-813c-c2e4f2f04fcb"}, "6ed1605a-a802-4fc5-9e1c-15e594141d40": {"doc_hash": "d28a580624f92b630bb7b7555945ecb9aa238e18f928605df24ad25eb7e6126a", "ref_doc_id": "f7101bac-4f92-4921-9f37-b74d5f74b7b8"}, "b533a28a-3ab8-421e-ba9d-2f1998af6b59": {"doc_hash": "51de3ce1abcdedb3fb8805daa315eb10fb3e1a86bb0ad78a61268ade2a4967ea", "ref_doc_id": "f7101bac-4f92-4921-9f37-b74d5f74b7b8"}, "d85c658f-63b2-455d-b5d7-cf55ab85312f": {"doc_hash": "a6d0e24a4761297680a68ac139fc5620833867679e54bd23bc3a6af5a0d8d3d0", "ref_doc_id": "2f03bb22-cfe0-4c92-8542-b26b0ff63df7"}, "3563d0fa-8486-45d4-ab88-a5dec22571d9": {"doc_hash": "19fd445cfc8fb4abce771cb1afc7b0007a546d9d004bc36c0572da3e205123c4", "ref_doc_id": "2f03bb22-cfe0-4c92-8542-b26b0ff63df7"}, "88d1cc86-e414-4d69-a36c-97779c8c1936": {"doc_hash": "ada85bdbb22f3fc4543f5a3292283015df5156104ddef34a8c5da3e8929ffad8", "ref_doc_id": "e483e7bb-9482-43cf-8a6e-9fb7ecf842b1"}, "40ef311c-1225-444e-af32-41caf868b7e5": {"doc_hash": "82ad1b44044fdef653ec4114718feaa176dd6b4882d865beb34c87da7dae1d8c", "ref_doc_id": "e483e7bb-9482-43cf-8a6e-9fb7ecf842b1"}, "978b2435-7c45-4a6d-881a-b1ccc8752393": {"doc_hash": "8a711302721f2bf61cd2cae93bdceb6d51737398a66849e88f3e5a84b54b46dd", "ref_doc_id": "a701a624-44d3-4760-b045-f85ac38bcdfb"}, "53620188-954c-4559-a6bb-716c35af91ce": {"doc_hash": "0b1c231e73dcd443578b81d12a1eb0b9eda8c97f297fa61c222708af0379b88d", "ref_doc_id": "a701a624-44d3-4760-b045-f85ac38bcdfb"}, "e59a67ec-9086-4f9e-be0a-17939446ca48": {"doc_hash": "35d364c51ee4dbc901308ec92edcfc481f860fcad0599b0335c56ee309083141", "ref_doc_id": "6cfb97aa-3b95-434f-933e-3f6d7b96ef4d"}, "2ee96738-d045-4805-99e1-a6f65be8a03f": {"doc_hash": "08199fa93369584728c5dd53d29f1e8d4a696a447184fab70f11271ffdb78e91", "ref_doc_id": "6cfb97aa-3b95-434f-933e-3f6d7b96ef4d"}, "42c7a770-8911-44c9-ba64-ff61dbffc0f5": {"doc_hash": "adfa65c47be86493592b0c2d04c716470de8732cd3b665386ed42561b0544912", "ref_doc_id": "fc9ca98c-5219-4d03-84aa-68573f75be36"}, "578b9fb6-e009-4823-a130-8b367d3fb4f4": {"doc_hash": "eb533877575ecde8069bf8648453a27c717b34c82726e99c57877be8f865dc48", "ref_doc_id": "fc9ca98c-5219-4d03-84aa-68573f75be36"}, "76e50d05-b188-4184-af94-23eca4412709": {"doc_hash": "8a35db54ea49fd7ed0fe8febc6364f6c419947edd886e14b3b0bb0bd4483d90c", "ref_doc_id": "b8d680df-8f79-42be-a9e0-b6d1ac8d377e"}, "3979d82e-e9db-4cca-9437-9d23c8ef8ecb": {"doc_hash": "ced46726258d652f6528cd676ea6be9ae5be2da24f2038e10c6d9951e38600f2", "ref_doc_id": "6d377f55-0a40-4441-bea7-d3d008dc73de"}, "3b7475b6-c6eb-49fd-ae2f-cdc66965437e": {"doc_hash": "94c72d5770e8a4b6c8fe9c6ba3ee7ca1661c7a4d3ae14447f414408b20511b67", "ref_doc_id": "6d377f55-0a40-4441-bea7-d3d008dc73de"}, "99ef0f17-ec63-4e3f-b57b-2092c34a6cee": {"doc_hash": "b3a339713f491a2852de08b1dbb650757a344f80a9271248032019ca188bf59e", "ref_doc_id": "4ca49f59-d30e-44d1-88ba-56e61c5fec91"}, "1f6b1282-3467-4b3d-ab05-b18b90cd6d94": {"doc_hash": "066e0103d40305fc9f8ff1edd7b89558199360c56c29e3e3935863566e5eb4f2", "ref_doc_id": "b12c88d4-4ca3-409c-ab0f-a2311e307575"}, "0b544a7c-a71e-4705-887f-420c244dc31c": {"doc_hash": "190fd64727c8e8eaccb3563cb4162b9f4e14c04f49c91c631ce8f8944abf9b57", "ref_doc_id": "6449787e-09fe-4a04-b4d7-928c95cc64fd"}, "590058da-4478-4de4-968b-718fd3e0e07f": {"doc_hash": "d2c65404133bb083d3783412d2d365ed5910970f807930ca8e6360b810ac868b", "ref_doc_id": "ca4cf3c9-fdcb-4553-96d9-17729835c1d8"}, "c5a905af-93ca-43db-98aa-d2babbc59a5f": {"doc_hash": "49bd507a6b39b03f9fc109f5817d41bfe52200fc8e4d18807de783b91fdbe3bc", "ref_doc_id": "525bdc5d-41de-4064-85e1-f65b2263582e"}, "18047a63-286a-4b95-87da-318c0594ff66": {"doc_hash": "e91fe61682f95add6157b9f31bf5efcf54dddf72be8137b02077e9d4b414bc37", "ref_doc_id": "a2f2ca14-c42c-4e98-8b29-c24c33642c77"}, "c7d15724-c13a-4d04-b039-8fd07a79fb2a": {"doc_hash": "52ae502214ce237e96e105d5096755ba31dfacc406eb1f234645966bcf54f15d", "ref_doc_id": "83b52b8f-4a88-4463-8c40-c6fe192e7996"}, "51996d81-b287-48e2-8b83-3ba979fb3a17": {"doc_hash": "043c3daee87827b3928c98ddd5c31f2a411042e7b67e81694273b9fa1f5409ad", "ref_doc_id": "b2f66f50-9bf8-461f-859d-e07db1006098"}, "2d2ce17f-39f4-4596-ae29-7085bbd3b4af": {"doc_hash": "51e79d01130048ff85911ab6cdecb9f771ecc8fb7973b4d6599be2b4e66c36a9", "ref_doc_id": "72723e0a-6f18-4d27-99b7-de3cc6ae58af"}, "44b285b6-89c7-41b3-9ecf-ffb64dad2419": {"doc_hash": "e1578578e0980f5df76c03b55909178c9f00796960de1eb057fa754de55a208b", "ref_doc_id": "df290570-5fc0-4389-a1eb-7897e876af3d"}, "41996106-4dd8-49b7-ac40-2a3375ed63d0": {"doc_hash": "6f45b2c329ed7ed39c67a52a03ca017b5c2317b8641766bfcc97fc12e908f011", "ref_doc_id": "380d8a71-ad8a-46db-9653-7526828b4e56"}, "fe6ec668-fdc8-4808-9ea4-7956f6096185": {"doc_hash": "1d4ae7eeb83f3e38d7602494ea2f2695ce71d9e10ce68cb2fa368d46b9930314", "ref_doc_id": "6df5af8d-7975-48ad-af80-1e1836876ef9"}, "0bf60834-6a4e-40b8-8f7d-d55185dc9c71": {"doc_hash": "49349ab6b225e275eb5d7b8db29aed01adb5eda94c10b60c5abe7742ea39330c", "ref_doc_id": "329477dc-805e-4977-af4d-a763da688782"}, "151c90ef-8368-4361-bf03-1a8da75f2f5d": {"doc_hash": "7cba80b2331f961ce6007e392a8924a3dbd4ed3f81860deca06d96f16d37b855", "ref_doc_id": "42718cef-5d44-42a7-82b5-7f4fe3085b17"}, "7818ea45-b4ad-4e9b-89d6-26084a8d09fe": {"doc_hash": "457c82eafd7c4686f874f7f29e5cb1d45a33c277f30e2a9a3eaf629f4bc29e75", "ref_doc_id": "813bf178-50a6-4a4a-9fd5-86b059566ab4"}, "0661582b-9d37-4292-959c-b4ef5d494e34": {"doc_hash": "81c107f592c09266e34e760fc98e2c7a5f387d0c95bf9a51873406b30b3fe2ce", "ref_doc_id": "669fa008-2820-464d-a69a-ae0af5251564"}, "f611d476-7408-4084-8294-504b5344f102": {"doc_hash": "67a2ec514ee705c5fc2dd3223ff27fa8c49029304e74b85f75ca606029dde5bc", "ref_doc_id": "b5d9ed9f-db03-4238-955e-7dd3e87b2f6a"}, "c5b7aa51-5701-4f22-b03e-81657e369dcb": {"doc_hash": "74c0f5485d4c0a83fe5869cbc15a505540a95d99609f0bfc1588ef2ba9953e78", "ref_doc_id": "5f66b373-401d-44cf-b190-2d29d945b673"}, "ffdb55d8-66c7-4305-8217-160b4ecf2093": {"doc_hash": "9411203e51cbdbaf5c384abcf6d895ab5d12b3a2174b275633c18ac1309d6440", "ref_doc_id": "a87cca8c-519c-4df0-92d9-65e171968f32"}, "45485170-99f4-4d02-a6a9-94ba57f4e6af": {"doc_hash": "3bb359e19ebfe14d73c3bc3f7de4b120533faf10612ac6329fc0645f898cf964", "ref_doc_id": "3e106a48-5437-4ade-a8cd-5d91545a38be"}, "8dca77ab-e30c-4d4b-b1ee-a4cf14f9c978": {"doc_hash": "eadd4229297cf572148acc4d6f021f6f7d36af749fa61fbd1a3ce17866364176", "ref_doc_id": "b03bb92d-f069-4964-976d-f515fe1a2f0a"}, "19652b9b-5aa7-4627-ba76-cc4c071ce05c": {"doc_hash": "e001c78cb7b0e51e1ccb31272331893a1c5d3b1566dca54e8f274963db0c3724", "ref_doc_id": "9fa45690-f2cc-450e-9f1a-d4de1a061488"}, "76b8dba6-f235-4056-8d3b-9f078718c2ad": {"doc_hash": "18a200f0652e49b6f432c3bccba81ff11cbedc920d53382b6099f709f8e36ec2", "ref_doc_id": "9f4973cb-4178-4156-b9aa-ecade5f50bd0"}, "068b3422-3149-46d2-8d67-a40a1246fdb9": {"doc_hash": "d9419497fdff87f43786d904df7484a586217a1ba943bcb1ec86502dd97520c2", "ref_doc_id": "dd634a05-fdfc-411f-ab90-5ec2396cb6b8"}, "b1130f0c-ba1d-43da-84ea-50a98b198609": {"doc_hash": "b7821145d1800c273d24dae1a5dec3a09bef225d72f994fc1108a9a1f8be92c9", "ref_doc_id": "dffb8c57-0d5c-4fa8-8cb7-7640c078dc1c"}, "52e5970d-a7bd-412c-9aa5-e3f468121357": {"doc_hash": "7ba0772564199529e27213c6d51b84f86c77174a803592a51c3caa5684456398", "ref_doc_id": "46104237-96f5-4c1b-aa16-7bc7349188b8"}, "c95842e3-fde7-41ce-8713-890b29df36b0": {"doc_hash": "fb2866e901bb940ea5da3e7ddaf39a77fe5aedaf829bbb54ff4552619640d143", "ref_doc_id": "2fbeedce-cd28-4e24-991d-9223580ff5b3"}, "39e23b5f-9fbd-463d-b1dc-f1f2fcf12038": {"doc_hash": "5e13122ac3265f7cf1bb0013a9917c21fff9705ce8e4a77dd638d57c28cd1e76", "ref_doc_id": "a0b6ff7f-2024-45e0-98f1-1e9e02537530"}, "0a7e44d4-c747-4d1a-b3fc-a6573bf9c2a6": {"doc_hash": "ad932653dc684c97514ad1bad748e9d7139cbf718cf811530e707383de9fa272", "ref_doc_id": "c4f5e536-9a83-4c5f-8176-3303f5eaf92a"}, "ce1a5097-643e-4633-8e68-e8ec7c8eece6": {"doc_hash": "454a2bb9dbfe9b2b9de138429c784e3fa7c8078e46d00a0e78f80338073b2470", "ref_doc_id": "af6125f2-2c55-40d5-aeb1-bf13c2fce76e"}, "56aab57c-0ba3-40ee-8bb8-225965c8e425": {"doc_hash": "26e617c4093c29bdae22a173174e74ed0f1e62c4bb7427b30b4777cd387fc0c5", "ref_doc_id": "5381dcf4-c1b0-499b-a167-54df327ca060"}, "31fc9742-1f40-4124-b401-9b8aa91dd598": {"doc_hash": "becf46f9865f076aafa865bbdcea91aae1607295233a84335e40ebc16cf40031", "ref_doc_id": "5821c050-5ce0-47f9-adc9-b45e73016fe8"}, "5abca84e-f967-4f7c-b65e-a3d7987552a9": {"doc_hash": "1d633d26dc5d340b69e5a7569ec4fa9aba0cf3a8c1c2e652d4c478e4c1df9f15", "ref_doc_id": "6edf2408-9b59-4f48-9b3a-ca94afa1c417"}, "7abe67c2-2431-495d-8fb4-cd8b18963119": {"doc_hash": "e8054c59d81a975b4333e03c494f5cba543df2655dbd92ffeae15ad760910fdd", "ref_doc_id": "5b0dbe2e-512b-4045-940d-f1960e5272d4"}, "5a1da780-6669-4049-8874-ed80e232a40c": {"doc_hash": "2b828f51645a9b328cc8607ea25ad2d27caa21ed8720a96e7027c12f0f688e51", "ref_doc_id": "8692766b-4ae5-4305-9005-2f1ab011afd7"}, "320ee3c1-0dc6-43d2-84d8-f6ca288b1f0a": {"doc_hash": "931c1c398d64402a3e4b6aa42e95e2702431e53040eb21f6cc74177feb3d34ca", "ref_doc_id": "b8757888-f049-404c-8c99-07bc1e9bfb24"}, "27415eb5-3f0a-4bba-96f3-d8faa6592550": {"doc_hash": "a682da09e1c419872714499d34d36b941871ea5c4b42ebea4151893f0dd1d53c", "ref_doc_id": "91966957-7747-45bd-a03f-d0c43453dedb"}, "0a59d981-25f3-4ae7-aece-52b512e55688": {"doc_hash": "9f8d235c7e2585607f4e38a77e7e492c6b145757aeb7b3d9b1f816cdb3b50b6b", "ref_doc_id": "91966957-7747-45bd-a03f-d0c43453dedb"}, "b8ff471f-e757-4b86-b535-6c994edcfcdb": {"doc_hash": "d0d0d5077bab42a50ab1ca50b0e4984cd473eb3151faf0b7d34f690a4831e4d4", "ref_doc_id": "74eb1b8c-9d9e-4397-be86-3f3612bedf83"}, "3c5315f1-3832-44df-924b-85f89aa9ea0d": {"doc_hash": "278d55b8a48723bbfeb19549450bdf6efea31fd3a4b705254dad7c286c5c8f74", "ref_doc_id": "74eb1b8c-9d9e-4397-be86-3f3612bedf83"}, "c51b41cf-f6e8-4090-b421-6968114f6eb5": {"doc_hash": "9a9d6ec13e760662f33972ff3c5523c3b1d07d201d04ccb149f115ee45caac18", "ref_doc_id": "80855d06-ef81-4500-9c04-40b832e4221d"}, "d57973d9-67ab-4e57-8c6a-e75a8fbe5858": {"doc_hash": "72d2bfacb5c7e40db1296d590c5308ff080dca5900aace0b3dd41ba7b454f1e9", "ref_doc_id": "80855d06-ef81-4500-9c04-40b832e4221d"}, "0b17b6fc-5626-41cf-a0b7-318620e50530": {"doc_hash": "f3f2527b79b98ea89fa4691acad42c00f40c291ea6051e91fe5e2fd5b51cdb25", "ref_doc_id": "38693a26-8c55-4b11-b7eb-b602c5d8efdf"}, "a4ad48a1-d891-4906-9039-e1a37dce7374": {"doc_hash": "d74271bb7a9a06af799b8bb367a1b08c970bdea034a3853628e3f8b4e7724e28", "ref_doc_id": "38693a26-8c55-4b11-b7eb-b602c5d8efdf"}, "dda88c9e-c0b5-4658-a64c-faa42052def6": {"doc_hash": "c85891606840b515053a98c2a1344cbe57a005142f64fbf64068982fccab9671", "ref_doc_id": "20c73727-d29e-4c9d-8765-e5290761bd01"}, "34d749ef-6f2d-41a6-8c9f-6a9f6451e240": {"doc_hash": "f30930d5f1f66cae221edea1b2f45f105d05b1108db6b1460a6d240b9073b964", "ref_doc_id": "20c73727-d29e-4c9d-8765-e5290761bd01"}, "8965e4d8-d7d7-4017-b5a0-cc7483909ef0": {"doc_hash": "9a0a99235c809aa7635a60a53e04016cabaae14ede5dd0de31a242ec70d3ce55", "ref_doc_id": "ff9a8e44-a3e4-40c6-917b-f9dd535848f3"}, "76c2ca8b-cf8d-47ed-b983-7341d75cf193": {"doc_hash": "2f6318a6adfe31343a96cd61d491fe820b2cbec5802eb71fbba0abef1b0f3e96", "ref_doc_id": "c2e60a7a-b400-4a65-81eb-ee0a30798d78"}, "0fe8908c-bd22-4eda-90c0-2c04921a5f48": {"doc_hash": "c4277a4afcfe98a529b411d90cf9ae8dabf8ebfca37b22388c3b6dc4370d7875", "ref_doc_id": "c2e60a7a-b400-4a65-81eb-ee0a30798d78"}, "f1e1df9f-1c98-4e8b-ab3d-e0d5ae88a74e": {"doc_hash": "fce4dce329e531278aac1152c235b7ec8e61824d650d224cb9ba835f46517ca4", "ref_doc_id": "7f5aa553-dfdd-4bab-ba88-6025e3760d92"}, "f04ecaa4-ac88-4509-80d2-2661c1ed02c8": {"doc_hash": "82ccd1fbe31c8391150fc3bd0fe63f32ef31076cef4ee406b6ba88d758f0d7d4", "ref_doc_id": "7f5aa553-dfdd-4bab-ba88-6025e3760d92"}, "11fe23aa-ab54-420e-8591-9985e2416897": {"doc_hash": "96a89f79dd77409530c510a65649f11938b08dbec21a00c6ccc7de2c3eef8179", "ref_doc_id": "f62d972e-7b41-47b2-9bc9-855b46565dfa"}, "486e479c-e64c-4266-855a-eb4bc57b34c6": {"doc_hash": "1b090c84ab7b141d9d4501d86e92f6f6737192fb90071f836bdebaa3907ac0d8", "ref_doc_id": "47ba212c-fa32-4730-86ea-70f7b8ef0ea1"}, "093a4658-f460-40d5-a1f2-b14bcd70336b": {"doc_hash": "e6b5f1b19c0b1bc4f70efd06501b5aff514f01793da5886fe5dc4bee403464e2", "ref_doc_id": "47ba212c-fa32-4730-86ea-70f7b8ef0ea1"}, "137e5b84-358b-4e54-bcf1-bb0c3d427592": {"doc_hash": "cb819ef1731746c83efb597fb7bb2c9ad2b56fd3ed09f4bc35f6784897495f5d", "ref_doc_id": "a2efa05f-e582-4af3-9798-c351506768bf"}, "5fb5511c-6f36-4dc6-96ca-f6aa8b423f1e": {"doc_hash": "ad421a0fe8186263a741bf04e68b7d125b50f4eb097ee22b617443ae9e09ad4f", "ref_doc_id": "a2efa05f-e582-4af3-9798-c351506768bf"}, "1ce29805-188b-417d-a30f-af9c2955d2b5": {"doc_hash": "d36cd4989922afa8e977f4d35d810ca94662c9c6ab10303921164377a4f7fbb3", "ref_doc_id": "c5f0cfe2-8c16-4f64-b558-2ab8fdf4cafd"}, "bdeedcd9-20e6-4219-b236-da2b8b56d9e0": {"doc_hash": "ab01e52f66f4db285acd0cb7b4bd3a1d15a253dbfa83edd22970fcdbeaa35a84", "ref_doc_id": "c5f0cfe2-8c16-4f64-b558-2ab8fdf4cafd"}, "97be7611-a131-490c-888c-b98cd2aba5ac": {"doc_hash": "ef8f56a1ef6563d41b1a5e7845594fe893abfe042d96d1323596cb12fb6da301", "ref_doc_id": "0defce02-ea5d-4345-994e-e84e6570e0b4"}, "24341575-0552-4162-92a7-4b04ea961302": {"doc_hash": "4f93b0bb3b0c1a0b80ead37d6b2260641cb5700bc74ebb566575c790c4b33a5c", "ref_doc_id": "0defce02-ea5d-4345-994e-e84e6570e0b4"}, "6e85332c-a9ef-4f4d-b1b3-6a84a2163910": {"doc_hash": "1f5222ea111d96adf024d83ed7748ee01c3588a30522a8ff0ae67ee2f2bb8e77", "ref_doc_id": "a363b883-8b6e-45e6-a6e4-78d54bc51cf8"}, "dc84dba2-5377-460c-b83c-a7d56cd34395": {"doc_hash": "fecf4bbaf3fa5e12b268a2867be4c73f143b1f45a1fa65b43be4075583346c12", "ref_doc_id": "a363b883-8b6e-45e6-a6e4-78d54bc51cf8"}, "05c5cb43-8417-45d9-b07c-3a11d7a9f595": {"doc_hash": "e44b28cf8b723d2b994670ad9d38d0dc2aa99447975d3d471cb318fdd7fbe71e", "ref_doc_id": "a07860db-4f84-4673-840e-80adb4b7c7f2"}, "6ab425c6-f964-41a0-8cfe-31a3a69c86cf": {"doc_hash": "e933bb164139e08cefa8471c9809814e79855d95177af8d044ea4f27d93ac95e", "ref_doc_id": "99f38eb9-9953-44e4-9411-21ad8234c490"}, "55ce15a4-c761-484b-9fae-db2c52ce3cb5": {"doc_hash": "333e485883bb5d12c115a9c1a33e44dc16a24721c04f36ce7c16e828fb733eef", "ref_doc_id": "6f7c45c8-4b1f-41a7-9fb8-4e2d716dd634"}, "bdff2aad-1647-45f0-8a03-6e853f0c95d7": {"doc_hash": "1881f1a1b38f3d3ed20d33bb60b607274326bb5885e772e3552d2ccee1871069", "ref_doc_id": "d5140a05-729e-4628-918f-a044b9bbdeda"}, "3759c985-34e2-4ab9-adcb-525409e4ecea": {"doc_hash": "8d6f25c3c93ea5aefa29c310595ee8dbdd91090bacd39d0c4aed5a155bd7c8ee", "ref_doc_id": "f7bc83d6-6855-4ec3-a5bb-a4135cdfffdc"}, "fc2a617d-2126-4026-b1ac-903d5b5a556a": {"doc_hash": "a2bd7f32bb49e76082c4413f0dd933be9f2ec30b21df02f549a4c388e6a6a0e2", "ref_doc_id": "f7bc83d6-6855-4ec3-a5bb-a4135cdfffdc"}, "90566177-f0cc-49ac-82c9-959e5bdfc12a": {"doc_hash": "46cc6513d70bae6f49c63953a62567eb72814f87ee0cc5ac69592e3b25f03b09", "ref_doc_id": "14ebd053-462b-4700-a31c-03c944a0ce29"}, "d24febc5-aad5-4be1-af9e-073ce6adc763": {"doc_hash": "04eb975f04b8e27e3b9831da5c97e90f236a33ca96410a71fdd99221b6b38340", "ref_doc_id": "a1ed0bfb-8953-4137-a7e4-0371e6bde0a6"}, "cd3ceaaa-3ca9-4ce5-8839-959ef939dcb2": {"doc_hash": "3a8a0276668f3477431906cb8f017e061ecadd4906ca87af2bb1bb3703ba021f", "ref_doc_id": "af012466-de7c-4201-a840-90f780af132d"}, "3bfd9e46-65cd-4004-96f4-6d644c333188": {"doc_hash": "8a348165ee7a66de6343bfbf2d4c098784edacb95a6034e704b014ceb3f1b083", "ref_doc_id": "af012466-de7c-4201-a840-90f780af132d"}, "c4603e68-71fd-4782-9e69-e79eac54d26d": {"doc_hash": "b5be9b2dc9c1bfa91335ac9937a2e0aad2d4889d5e85fdbd75775f8af67aa63c", "ref_doc_id": "37cd8caf-a241-4665-b4a6-7a01a8677890"}, "105119c7-5759-49bc-a8bb-cfb2f6681588": {"doc_hash": "790ada6b16f39ba3ac912c3c07c585684c23156bfddc67ead9fb7222f1506a3d", "ref_doc_id": "37cd8caf-a241-4665-b4a6-7a01a8677890"}, "845c2370-1994-47c2-841e-2a84d4213591": {"doc_hash": "6e8eb03b0af95f8dab6069b093f508422da2fa50a4b3909be055f03a5bc3db9d", "ref_doc_id": "389302f2-e074-40fd-a6c2-f9b3c69461f0"}, "22452d97-2010-4532-8f33-9a0d61c55216": {"doc_hash": "f7f21f4ad983085cf287e91b5176ef7188bc13bd1ebced6872ac5bf1edd6ba91", "ref_doc_id": "389302f2-e074-40fd-a6c2-f9b3c69461f0"}, "ea9fa585-d3e1-4804-8530-e88e6ffefd9e": {"doc_hash": "be2b92472c82a3b227a76f85f75dbad04dcfefc118542749a3a456df7a10f296", "ref_doc_id": "bc381e92-8ba4-45f7-8be6-58a06ddb0100"}, "daa5f973-e246-4d32-b013-da8b527e3250": {"doc_hash": "7e883bf7fc04efcb3b5b21c4ee2822f5039b9ce9c088fb17da96d1f1cfbc4539", "ref_doc_id": "bc381e92-8ba4-45f7-8be6-58a06ddb0100"}, "83762c53-d02e-412e-b133-69bc9c97b8dd": {"doc_hash": "816c6b171a2af23e67bd64db9a135102a6c5cb7668236eac7c5e10cd43599912", "ref_doc_id": "8c6ecd77-87d5-401e-a1fb-7581149cbc58"}, "ddf3a6fd-2501-44b0-81b5-fa5a5aa59ad5": {"doc_hash": "1eab230547188a9b0c42554916349941bd4c765b4f2b9ee393ced2111daf5a41", "ref_doc_id": "8c6ecd77-87d5-401e-a1fb-7581149cbc58"}, "aba461e4-1c6b-44a2-aba5-379dfa0ffddc": {"doc_hash": "e16c342e822583e43ca1799602ad231f6eed57a03d7f7768560c391924e7510e", "ref_doc_id": "8c6ecd77-87d5-401e-a1fb-7581149cbc58"}, "d73280f5-7e6f-4306-ae87-e3678b933ac1": {"doc_hash": "e0eb5eb80cef8e0878f99cbeb71b3be169741d1e384872f09368e6fc36f6573c", "ref_doc_id": "1abd0ab0-cac6-42dd-900d-db62e73afad0"}, "84b77102-ad65-49af-a68e-e8b98324a8fe": {"doc_hash": "8649379e46620225429e7615a00787a898f48a684240647202b2b862799c9448", "ref_doc_id": "1abd0ab0-cac6-42dd-900d-db62e73afad0"}, "8ecc17c4-4d16-4cd5-b72c-ca609dab826a": {"doc_hash": "e7acdf9092e66e0a88081ab9f52681709970cac31f44a290097faf29802544c8", "ref_doc_id": "1abd0ab0-cac6-42dd-900d-db62e73afad0"}, "29de92db-3dcd-4a0a-b487-fe16b81d8c2c": {"doc_hash": "ba48bca6e613960d12802be06cbb210be0dd2b7e21cd3a05d5047b3ca675926d", "ref_doc_id": "570eb31e-738d-4493-b7e2-4ac63f688606"}, "08dcc7d2-d8a0-4f4e-8b5d-8103cd163907": {"doc_hash": "a920acab548b34793b682229bd41c635eed6b11f0df2003219687cc42e2628aa", "ref_doc_id": "570eb31e-738d-4493-b7e2-4ac63f688606"}, "dc286893-8803-4e33-a246-13cd50c5c268": {"doc_hash": "c2e23105ddbc8195de0c78ba28f7b9be76e98e0c24662f010377e2e1917ac34a", "ref_doc_id": "570eb31e-738d-4493-b7e2-4ac63f688606"}, "16fae7b4-07e7-4137-a637-ccfbcf72d8e4": {"doc_hash": "0d6452b31b1378ab9a6ff51398ceb81a264e1bba17954c7b9dff5127a2057bc5", "ref_doc_id": "570eb31e-738d-4493-b7e2-4ac63f688606"}, "87216d92-ae2d-4147-b70d-bf07c6074206": {"doc_hash": "ab2e69941a77a312b3f7a9cefbaf71fc0e8a3cb306d2b8d89c2050eb03c63fae", "ref_doc_id": "6ffb189f-fd69-4a30-8d40-b3753854c569"}, "cb5664aa-ecde-4a8a-9e90-1769bba41b27": {"doc_hash": "a0c74e528d80601ea3e1b1ae9f7acc3a0b348b803a226438d20a680e20ffa0eb", "ref_doc_id": "6ffb189f-fd69-4a30-8d40-b3753854c569"}, "f66e05d1-3e84-49fa-afbc-a375602fd48d": {"doc_hash": "ba21d0bd43e2c2ca4bcea24185f09dc584970a74b48011e4abe75897f2cf2333", "ref_doc_id": "6ffb189f-fd69-4a30-8d40-b3753854c569"}, "ddd73be3-0c6a-489b-bb7d-2220debca08c": {"doc_hash": "77c7501258d339daa10da923d2d1d6a1501f8fb5eee4b494dcb0fe0c420764ce", "ref_doc_id": "6ffb189f-fd69-4a30-8d40-b3753854c569"}, "35a2357b-cc80-4c87-929d-1899d8eecae9": {"doc_hash": "4a7465b03e7e4de9f4d8f4d66d03d43e33fbbed950f58614c66e26716db688b2", "ref_doc_id": "f8fc1b27-3fc1-4ae7-96a5-6c388c6de267"}, "c3009f7a-96cf-447e-b732-b3c665c865be": {"doc_hash": "9cbf0f5a1c73ffe48030cff4209c74dbffb41d389a01dedf1f48e66a51f68bf5", "ref_doc_id": "f8fc1b27-3fc1-4ae7-96a5-6c388c6de267"}, "6e3c1a8d-2671-42c9-9903-b2bf73c25f07": {"doc_hash": "829acfe34afe1791066fff64384ada060b888b25a735d9cb4975a5ae678d217e", "ref_doc_id": "27f4c239-a832-4016-91e0-385fc534d943"}, "af5f0b41-f9e7-4ffc-a321-f571af0304e1": {"doc_hash": "f7460d4c6aec761ac65e5b32be76f90be02492ae4d4bf3d5f6947e475c627180", "ref_doc_id": "27f4c239-a832-4016-91e0-385fc534d943"}, "3c295f1c-7959-4c82-baad-ceceae5185c0": {"doc_hash": "43eae063086241bb9b3279f72dc54a59905bc997061a1bf06d33661e51206764", "ref_doc_id": "76222c74-1a7a-429b-88f2-a4ed6fa37c0f"}, "50ff1b79-77ed-49f5-bb6a-671c186fd066": {"doc_hash": "09420f466657cae25a85e355b00714a8a95fb890cd7309c30645da41f3e5d6c9", "ref_doc_id": "76222c74-1a7a-429b-88f2-a4ed6fa37c0f"}, "4560d650-9763-44a3-a62f-302b6204a058": {"doc_hash": "52ad48e8aa3f4b1d491448a3427d06becf5466da5d4e716fca07139f8e95524a", "ref_doc_id": "76222c74-1a7a-429b-88f2-a4ed6fa37c0f"}, "5e88a86a-0c21-4113-88b3-59da15d2006d": {"doc_hash": "bf8d0dea913d9e547ada0f4c09f87550b1bc82ca011fb5982d9abe27350a5d6e", "ref_doc_id": "76222c74-1a7a-429b-88f2-a4ed6fa37c0f"}, "43686da1-60d8-45fa-b7ed-b5c9a5f0b7d3": {"doc_hash": "2d587bd19a0c2b4c95ceb505298bc3fbd4ddd0eb6fb1281d749a8e5a0332759d", "ref_doc_id": "239162ef-3dd3-4a8f-80ab-b2273614b495"}, "55708ffe-04f4-43f1-87eb-c649e796341e": {"doc_hash": "478bd8b77d5d273ec01f12f831805823210161b5a2e875d5e1b666fe3878699e", "ref_doc_id": "239162ef-3dd3-4a8f-80ab-b2273614b495"}, "330173e5-3244-4333-8eb1-99887241be2d": {"doc_hash": "33b25d7e3d7929ca87969a34db817aeb8456e9473bb2196ab7878c8823159e58", "ref_doc_id": "a6588327-9da3-4981-8f90-f859ca2934f2"}, "cedc07a4-2f9a-4da2-b70e-0234162b6c3e": {"doc_hash": "467c5fe6423d366f176d295a90ed06f54c5fad8a5cb9e07ed77c0a7a05e3e1cf", "ref_doc_id": "a6588327-9da3-4981-8f90-f859ca2934f2"}, "2d3dcf54-241e-4ac6-8943-7d3b8f9b791e": {"doc_hash": "f9ca96ee65f733e241a9ba7ba822621670c5b08f9580de5ab203e32f1a45452f", "ref_doc_id": "7d342485-4866-499e-bca6-1a2f3178ea06"}, "e060e362-ebc9-4553-a4ea-a891d1fad74d": {"doc_hash": "3dc44d2a74fb3806bed7a025a9354435add388b7acab6f6555c71dc9be184442", "ref_doc_id": "7d342485-4866-499e-bca6-1a2f3178ea06"}, "6da607c4-d2d7-4855-aa2f-f521964c41e0": {"doc_hash": "251da0319160ae30aa5fd5473215bae708b949d65e91e55bf26baa94a6acbb53", "ref_doc_id": "5450fee1-0f33-4a6a-9e18-ef6f25d6fede"}, "31ebd248-ec81-4731-8134-09cbe74020c7": {"doc_hash": "0490e00a75666d4177ee5b5302eed12c14bc49d4a28ad7e88c2e6d09dffae009", "ref_doc_id": "5450fee1-0f33-4a6a-9e18-ef6f25d6fede"}, "5b63552a-f65c-479f-b39f-ef5e9cdc8267": {"doc_hash": "7988389abbbdb34f74031e64e4e6ab502307794e27d6c04dd5299fdfaef1a719", "ref_doc_id": "0c266220-80bd-43df-b799-10278a3c2009"}, "e946cd83-0855-4a5e-a3a1-6dd6f28b29b9": {"doc_hash": "6bf835699aac8a8cde377ce42bd571afa272dfd1bd60e6740d05af3d255896c6", "ref_doc_id": "0c266220-80bd-43df-b799-10278a3c2009"}, "fb33ae35-b68b-4043-80fc-0dc3793943a0": {"doc_hash": "7a1fc35e26c7809a48b4ae7bef2ac1c5a8a6917e56e25231675e140b40e0cae1", "ref_doc_id": "dbc09027-59e7-45b2-babc-9a46a8d66665"}, "cf4950db-ce5b-489e-b6ba-af95d9ded45d": {"doc_hash": "cb9207680e48173b4fca21313c199ec6636d9f04361c07ead947d23a7834faa8", "ref_doc_id": "dbc09027-59e7-45b2-babc-9a46a8d66665"}, "a23bb4b8-8bce-4d31-8faf-1d29ebd4b53b": {"doc_hash": "5712c8ac872a27ea784d69332914047a030e4461936191dcad10aff1f87e7d3e", "ref_doc_id": "a51a9d5b-8d3d-4c4a-86f8-d1cd040ef289"}, "c3d2c8d2-4610-4844-b880-5a5e0e1619b1": {"doc_hash": "dcf362648f9f780842658502a9f25c06f648bed44db11a284faff44b57751505", "ref_doc_id": "122a356a-560b-4765-b4a3-49c3566409a6"}, "a2fc65a4-c794-47ec-a6d5-8f5605fcbb01": {"doc_hash": "83fb4e2f4b8c7a06c4ce714f1bf115cd1f7edf6014d83d70d97cbb7ddaac461b", "ref_doc_id": "122a356a-560b-4765-b4a3-49c3566409a6"}, "1c45901e-04c8-4b71-8002-4591c7fe6579": {"doc_hash": "8d810b32dc7a2e44445c727cdfedfaf9f83a70aef657ca4d411fbdf6186d2d0b", "ref_doc_id": "cb4135a6-e312-46b3-b1a3-cff024aa7263"}, "8148cce7-bad0-41d0-a147-c2daeb37fb08": {"doc_hash": "ec533cb1c49f5f57aa27a990afcd87e960f883134aa6d5d7423ab6c0db8dc2c4", "ref_doc_id": "cb4135a6-e312-46b3-b1a3-cff024aa7263"}, "7ea043a2-7d33-46ef-9cde-d1204f2caf9f": {"doc_hash": "f23f416c0f32721eb477af9500cbb37566990345504c91b20e1927157c9506e3", "ref_doc_id": "fb69018b-7e55-40cf-a486-dda2ebb7e8ce"}, "c69362e8-cd2d-4de7-bb1f-8fa6435e5671": {"doc_hash": "006ca3551ad72d8651295837bd211c149988ae913436467be6eb322f581b06ee", "ref_doc_id": "fb69018b-7e55-40cf-a486-dda2ebb7e8ce"}, "a09f5385-a3ea-4ea1-afb4-8b5bbd833531": {"doc_hash": "5789b2f3e3b18ccf99111f8904f9c45f9dc07333184fb3bb7f83290f5301136e", "ref_doc_id": "da908028-a38c-419a-a992-d070d12cf349"}, "8e0684d3-82e0-4238-bb63-2de33702345a": {"doc_hash": "81b11bb615790372d5f03d3065340a268046858eace9a125a1215b1962caa409", "ref_doc_id": "20b8d52b-d00a-448b-bd9e-2fbd198a83dc"}, "9710975d-22ac-432a-ab3a-973c2f3c9b1e": {"doc_hash": "a3156e2cfae1ac8e528cf7026396681b9cda5f789dfdc1ce52e522b914111db5", "ref_doc_id": "f92eebd4-284e-4e4d-8e4f-6143deed8c60"}, "f58f799b-f10d-4bd1-940b-3e69e87e8819": {"doc_hash": "cdafea793a0245ab74f482be9de055f207b2f10b93fb15f7fd5fe5829ca71296", "ref_doc_id": "330da648-2bd7-419a-adb7-9d4831526ebd"}, "908b4d61-a868-4aa2-9042-fa69fddc3cba": {"doc_hash": "09d1d38d199e64ba4afccecc4b0a11d57796e3db7d17657822f326b6836b8ec6", "ref_doc_id": "330da648-2bd7-419a-adb7-9d4831526ebd"}, "ac12a5f9-b0c3-4e99-89a3-d5a6bd176038": {"doc_hash": "3341f789222519cebd677d9c51a1d3fe27c0f17c01d458d06cfd31f4c715ce36", "ref_doc_id": "447bf0b4-af11-4369-84b8-9eaea9c8bab9"}, "5eccfe52-5aac-4acb-9d7b-742966373750": {"doc_hash": "5d9bc0855c9dbaea14f321852f9642d2865fca82c94bc75cb738f5a9547ac196", "ref_doc_id": "0b652de9-e188-4b14-913d-74336aa59bbc"}, "18acc2cf-6af5-457e-bd1d-328d55dcea96": {"doc_hash": "84b61c7f2d749d85d593daa415bb35e2a0fa2fa52c672983d809c4c141bbdf61", "ref_doc_id": "0b652de9-e188-4b14-913d-74336aa59bbc"}, "1e350745-3144-4ee9-8bba-511dd6041377": {"doc_hash": "3c597e21ce2855e87d0ac8178be52b7b9af74a0adf2a84b344c1ad15fda9dac9", "ref_doc_id": "c4de1146-dcb5-4171-ae01-d6a4582b9142"}, "a8aa00a7-78b7-4473-ab84-9a0578b06179": {"doc_hash": "1cc6dbdc5b78cac6a413e085fe02f001ba7397939feafe7cf2a54ac801a19daf", "ref_doc_id": "38105673-de91-4a2d-971a-90ea9d6486b9"}, "fcb6ab74-30fc-4a18-b291-a4f85e9481b0": {"doc_hash": "60afcd62d0d7620eb4877cdf099a220abbea008b24d99ecf4aa5b5bb80d0ef2d", "ref_doc_id": "38105673-de91-4a2d-971a-90ea9d6486b9"}, "bb62b73e-655e-4323-80db-c2c1f1036634": {"doc_hash": "ed510dff520a3e2070db3d1364c9b460f924f16569bd7b443c282b75db1ae79a", "ref_doc_id": "38db0674-39e1-4922-9a0f-f14bcb4b57e1"}, "32e163df-c232-4caf-9175-bd3366f96b11": {"doc_hash": "1dc2716a6a68fb0b13b746129e13cdcac6bb1c13f3c471947feca2d48db65963", "ref_doc_id": "7caeaab5-80fd-4d57-b502-1d62c9fb45d3"}, "22040c12-6938-4add-997e-e8a4f7f5e731": {"doc_hash": "50da205134ae4ab06eea77df8528369315b35da24c2c10e2e28af965969c905e", "ref_doc_id": "7caeaab5-80fd-4d57-b502-1d62c9fb45d3"}, "6a8bc2a1-98b6-4717-8768-a52595020e6a": {"doc_hash": "32da3c46c2c7c82bcfb1cfff9bfb7185ed31565c553e2d07630776795af98165", "ref_doc_id": "d35c9166-27f9-406f-b1d6-a1ed2f8edd52"}, "1dbb7934-4c9e-4ee0-94d5-78591806796d": {"doc_hash": "8ab6b3b7129f1ce417920c021ff8e17c909169bdd408e258cc2dd96800427896", "ref_doc_id": "4f91bdee-157e-4746-a894-9e2da47ef02c"}, "3c9467f1-00e2-452f-9473-75346f75bd61": {"doc_hash": "efde85f044b27eba694a0d07e5bffcbfa747630718331882a8a9f46733f18eaa", "ref_doc_id": "4f91bdee-157e-4746-a894-9e2da47ef02c"}, "0f6f1f46-be29-4317-b45e-0d29ff727ba9": {"doc_hash": "25fc038dfdaeeaa93272ccc4fa618e956d4b570bdce4faf14b650d6d0b31feb5", "ref_doc_id": "9fced9de-97fc-4686-baef-00f0d9f8ca6c"}, "455a2576-c663-4b27-8838-caf8820141c4": {"doc_hash": "1414c2f76fb3e9c031e40a8e24e3d4a43c794826b45cad0d34c617ea25be57c3", "ref_doc_id": "5cae1afd-9dee-456f-b93d-b3b04aea1e19"}, "9e11c56c-2de9-4505-92df-1de286bd95bb": {"doc_hash": "70c22c679148eea81607aa4d85cca3d121cf77bf7aa5c10863648f4a9736c9c8", "ref_doc_id": "6d64437e-60b9-4d40-9c27-bb4e2c24c4dd"}, "87d5b2f7-52bc-4c27-b792-f5be2adef879": {"doc_hash": "639de4dd537f96a70811064b90699d5d2bda984eea12d25232d0d1399c938297", "ref_doc_id": "66760d97-fffb-49ff-8621-3a9deefaae1d"}, "23226c63-ec27-44bf-8223-0840f10fea8b": {"doc_hash": "14a1c70c354b548f591fd716f6629ca249aa5b494d5599d82c801cb88bdba857", "ref_doc_id": "66760d97-fffb-49ff-8621-3a9deefaae1d"}, "5b0a4418-152b-4a25-8f7d-a522223bd9c4": {"doc_hash": "75293a68ce2548503a6664b5b3d9ae45184770db71264cd7c8269e826a15150b", "ref_doc_id": "e11a913e-de48-41ec-9589-55b00054df69"}, "2c555464-e4f6-448a-a485-9bbb1c3ac317": {"doc_hash": "f1b9fd8e31897368eeefa9f1754bb3486ec5fd6023d777d114827d496f4ff058", "ref_doc_id": "617fbb52-97af-4a49-bf31-5e77a2fcafab"}, "b5411b1a-2e62-4f1c-85e6-b4b8b282d59f": {"doc_hash": "ce177b491d91fdd08a564ecd58040217d4599902f54b9c0efcf1646cd4fc2451", "ref_doc_id": "48802810-28be-4173-b838-94521469f095"}, "2fb71b5f-0d95-440c-a8a4-6eb286cdc201": {"doc_hash": "07022b182b81f3c57924f7f42119aba3675d7ad9505cf36c1a80a9c9cf337ff6", "ref_doc_id": "48802810-28be-4173-b838-94521469f095"}, "0f63aba0-4019-4921-8059-9a6d2ca20b05": {"doc_hash": "e50ce6e7b0390cc499a9446ed0a5c3b32405b33d851be71c03f5dea2231a1c2a", "ref_doc_id": "71e24d16-7899-4464-a3d7-237b4f4e19af"}, "23092e61-fd1b-4aac-aac9-b521ceaafdef": {"doc_hash": "32d072ef0352a22b41a04b95f0a49c1241fcc8f20a013f627c69570a5e0fcb2f", "ref_doc_id": "71e24d16-7899-4464-a3d7-237b4f4e19af"}, "147ed8be-4e6e-45de-9c68-ba7f42ac940c": {"doc_hash": "1646ae02724a59fb313b1194062e43a136df16d001bb79a9bed490d37f050a33", "ref_doc_id": "5cce9f87-4830-4ffd-95f8-8168b8e69f9d"}, "9fac8ce4-588d-4df0-aaf4-ef00824eda4a": {"doc_hash": "67bdc8193082c6b4877e9d03f45bae764f64d30b4f40940d394e21c3b74c17d4", "ref_doc_id": "5cce9f87-4830-4ffd-95f8-8168b8e69f9d"}, "5e24c8ca-db47-47f3-b2f2-3c255a164ded": {"doc_hash": "bcf44180ac8b3b76b60075ea5ce0c8ff41547bd5bff56a34ed35dcd801d61272", "ref_doc_id": "531524fe-39e7-446c-bfd5-721e713f81cf"}, "e65e59dd-775c-4c22-89f6-6c8a319ebf5b": {"doc_hash": "b1152e8ead450a0bcaffad2ac9a2db05a8c4c227b368cbfa5658e64438ff024c", "ref_doc_id": "66ad9bbf-b68e-4fb3-a2a0-1d0afa59cddc"}, "76ffa9d1-cf25-404f-af7e-a73cbebaaa61": {"doc_hash": "9079eabcc8c44c6b833cbed7370d089ea8fe1d1669aba58ff995cc4888df71ba", "ref_doc_id": "66ad9bbf-b68e-4fb3-a2a0-1d0afa59cddc"}, "83808c69-31e8-42ae-b76e-515722df651f": {"doc_hash": "1b92543f7f8bb5e76e61b377293e61dc7ba0a1cf9859459aecf9a1008e1bdb92", "ref_doc_id": "66ad9bbf-b68e-4fb3-a2a0-1d0afa59cddc"}, "ab2920af-d9d7-4d26-9b9c-a5d6fd8b9f6d": {"doc_hash": "030813e5e988a1a04ac125f61d159d0583222112ce25d292fda98a4b12eb98e7", "ref_doc_id": "8182cd31-5d48-4aa4-888a-538a8d6ce947"}, "0843cce8-40e1-4bae-bddc-1f32f86051af": {"doc_hash": "5cddde2c543ddc2ee3743c57680a4bbb9d877c3bc22149295651b17c9d06d8e3", "ref_doc_id": "cd4215e9-931c-40f0-99c8-604f0ad0264b"}, "22cf4a36-db05-457c-b52e-e3cb380b5ee0": {"doc_hash": "1512c79f22619038fe5d9d51790ad4a3308020651a0440fb9c5cc4ad810e3367", "ref_doc_id": "cd4215e9-931c-40f0-99c8-604f0ad0264b"}, "3a1776cb-ca46-4ff5-9c05-03d2b3908de0": {"doc_hash": "495089965b9fd3fcb7330cfe01218e87ef3a6a35404a454bd4441c2c43f4ac37", "ref_doc_id": "6d8f42cc-45e9-46b6-9cfc-9a8761acf7af"}, "f83fb6d7-1087-43f2-aaec-55c1fe3a608c": {"doc_hash": "0eb4ca3d873b55343199bb33517e0071e893df4fe8c8d028970149393d0716bb", "ref_doc_id": "f0161f95-883f-4fa1-8f51-6513d40aaef8"}, "8ae4d42e-d28a-4e7d-88e6-6dfaacf53606": {"doc_hash": "4f7693190a76c026da38c436cf7cb13b1593e96dda260882c7f2c829b4d16fd6", "ref_doc_id": "789cbb66-582a-443b-bccc-898e26215823"}, "64633193-e263-48fb-85b0-c124a391e00e": {"doc_hash": "59a454d0c0a6625216475f1f5f01c36cc7f04080328f7d40a527209426d50054", "ref_doc_id": "eeeeda32-5bf5-4134-9c36-0c0782e9b23a"}, "e8101beb-8a1d-4dfb-9144-1c48bf6aada3": {"doc_hash": "5c9484a44949b84fdc2600c051f3906543553398ca4f69e28b90ee23f583a379", "ref_doc_id": "ab2030b5-fe75-450b-b26b-1a466cf0cf84"}, "152f4ed6-59ee-424f-8a64-85ac3478cc15": {"doc_hash": "3a003429a5fbaa9d781391470bc942a6d6d5dec633189261655e7a07c9f6f4fa", "ref_doc_id": "18c1b152-077b-4f0e-b1fd-cee76fc657aa"}, "676f9d3e-cd3f-4de9-92b9-0d9afa417d36": {"doc_hash": "61cb643c000ff18a88d5a61d43b215c7dd5e5b25424741f5677edf604b8a5e52", "ref_doc_id": "18c1b152-077b-4f0e-b1fd-cee76fc657aa"}, "2ccc01b0-c9f3-401e-851a-6483c5ca1f6a": {"doc_hash": "bdbc8331898a5c57ed3dab713dde5c0f7cb4128e3bb10e0049377466fcead917", "ref_doc_id": "04e3ebd1-b8db-4062-a283-53bc6832b97d"}, "665978c3-7758-423d-8713-292bcc7d611a": {"doc_hash": "91d0ca9611bcddeaee65a38ab306bf3cac46f4e4cec19a98f660bcbfcdbc999f", "ref_doc_id": "cb8766eb-2995-4d1c-b4d4-9ca879952fd6"}, "351519a5-7135-4f82-b2a6-6485cd44c96f": {"doc_hash": "9dc1defb28cbc23c963d3105c82168141dd42fba415044fa121bbca9e45bbbee", "ref_doc_id": "7e81fd11-d143-4fdf-8f67-17f94973dbea"}, "d34fda78-ab08-4f1a-a10f-21a3de4a98ad": {"doc_hash": "03b31257298f12ee30538ccbc642caf82872594c3d1564475e769c0609fff98b", "ref_doc_id": "7e81fd11-d143-4fdf-8f67-17f94973dbea"}, "a77be512-9fa5-413c-9568-9306fd9c4154": {"doc_hash": "9f320cccf759083a15ac5f714887936a06396fc10e165d5047e1593a82612e1f", "ref_doc_id": "a6bb627e-c456-49bc-b3d5-448447e87f86"}, "00cd395b-3217-496d-8ef3-ad7c5d06a1d1": {"doc_hash": "0940801dd17a736d32b6134741e259e150047b640325f74a6956c4df62ccf915", "ref_doc_id": "a6bb627e-c456-49bc-b3d5-448447e87f86"}, "65b1a335-4cf2-4d48-a9ea-0d1256d4ca67": {"doc_hash": "c43c0c494cd1366556c25665d1a638b9700c35c70b0801aae21c0b7e6e02a120", "ref_doc_id": "ac0e6c14-6143-4286-b40f-b346270e5489"}, "9885ecfe-7182-4d5a-a1e3-d82f0a498a37": {"doc_hash": "f738f595253d9ea650f675c19c35a34ec66a54c07ffb97ac8553fb012e7134c3", "ref_doc_id": "ac0e6c14-6143-4286-b40f-b346270e5489"}, "0554e0da-b43b-430a-8015-9dd91b97a5ea": {"doc_hash": "bd739a3e4a728bb1488fed41944f707ff3727d04c527c74e6bec48ec71d35099", "ref_doc_id": "e97167ce-af33-40d1-bb9f-9481ff89bcae"}, "78c6a24a-c20d-4e04-9024-8ff527e8895d": {"doc_hash": "76b7a215859ca9aa6f29de13e4b3f719e18a8e8c63a86f249d4815c2feeaae4f", "ref_doc_id": "e97167ce-af33-40d1-bb9f-9481ff89bcae"}, "4f325d4d-1645-4253-b138-e1c1c324450c": {"doc_hash": "521d205613032a5c0ad834329a0001575e0aa71e000bbffaf8faab01846d0e47", "ref_doc_id": "5b4a4a81-0e34-4844-a957-88f42954e54b"}, "308a579c-6a8b-47b9-ba18-4a4d0607c64a": {"doc_hash": "ecc81057c4e3eb25f7454c72dc2a2356ad1deb20dfc12060127ef54fbc2898d9", "ref_doc_id": "5b4a4a81-0e34-4844-a957-88f42954e54b"}, "01addac1-c7d8-4f50-b927-f971ed791063": {"doc_hash": "4f5809b0e0c2fd41d649d92fd2dec0c3da5acf4725e2f1db97f501b5240de2a3", "ref_doc_id": "f3ecccd2-e8f9-4111-919c-6f53f8e48d41"}, "c30eeaac-2854-44b6-8b37-f63350b6ade6": {"doc_hash": "76a61111160efa041216fd2957e9d1359c0f33b6a753fc73bd4012bde0fb4e4a", "ref_doc_id": "f3ecccd2-e8f9-4111-919c-6f53f8e48d41"}, "2b17367c-7442-433d-b535-4ead9a5da1f7": {"doc_hash": "d5bfd3e291e7b53d139d194bdddb05a4b3e202410dde5fe1ec5a26884fdbc0cd", "ref_doc_id": "388eea62-d4e4-484d-a6f2-b4ec38768125"}, "99f6d8ce-56e3-4e26-8fa4-060f61923764": {"doc_hash": "e82131df5b7349428eaa45022ac57c013136ccc4c2c141aa31a12d60b81ca46c", "ref_doc_id": "388eea62-d4e4-484d-a6f2-b4ec38768125"}, "e70446d2-ffda-46e4-80ec-cc49babc4579": {"doc_hash": "e92d452d3a2dc256c3e3c21d67781b71a91ea9511783bb396cfac3329e583f2f", "ref_doc_id": "7f2d16ed-f192-4be3-bea8-7a06fe3cc8d1"}, "82820804-bfdf-43bf-a481-a4e97a79b8cc": {"doc_hash": "784c69adf6b8c5ba9e58a860974a6672d6628f260791de1529857c7314113d02", "ref_doc_id": "4308e195-72ea-4bd0-add0-e28d8f12bcd9"}, "65d2d176-5594-4b32-bc6f-30675be72b49": {"doc_hash": "521ee566a6dee27804cd359d40fcb637f3c59fcb28c9752b9f4fef705e69e25f", "ref_doc_id": "1ab1fd64-9043-46f8-860e-493e2d3a8e9a"}, "d1a11195-06ae-4a82-9ebe-14e3d016c55d": {"doc_hash": "930e4b161adf3bccc8462c7b4af57e660cc71c072e915c70d10cce869fba3333", "ref_doc_id": "be85231e-714a-4143-b2d6-228d2a25c524"}, "899b6875-bc0f-4d42-a660-7006a800d1df": {"doc_hash": "1f5435c4b56a1d15db70cecc50daa9dd197bb083eb4a28f2eebcaf7a4f57d18f", "ref_doc_id": "1903e3f4-bc07-4500-98fb-15f8177ff7c3"}, "258d38d6-2b64-416b-a289-18e0c834aa18": {"doc_hash": "2c5529da6f1283b45df591feed340c3747f774d93c768e5171bd4bc34462e235", "ref_doc_id": "1903e3f4-bc07-4500-98fb-15f8177ff7c3"}, "40e5bb45-b79f-4960-9f58-278f36214f12": {"doc_hash": "ca6a7b2edc3859cdffa131d874ae8c2e9097eb8c7d1c42faf64a1c84440266fa", "ref_doc_id": "8d44095a-c65f-4834-8c8d-9bedc6da9743"}, "ce08e953-1108-406f-93f3-ef4ee4d15500": {"doc_hash": "8e0bc0f48b36cc230eac480fe64b55e8543af2eaaabfa088b90e312d1f75c6e6", "ref_doc_id": "61083f84-5a31-421f-9306-68b76f191817"}, "7633e7b8-650a-498c-a744-90f513d43d7d": {"doc_hash": "ccda86ecab315738802f01fa17e53f2a75d5394ec2a84d4ff8dc148ff27df395", "ref_doc_id": "61083f84-5a31-421f-9306-68b76f191817"}, "7b84ebcc-dadb-4d9f-b91c-e77d411a0a80": {"doc_hash": "d35b94b48302efa27bc07e08b71fed5213422684216125ed2940a57886fb1bf1", "ref_doc_id": "79c5fdb3-d643-4015-9c66-b2e61aaab2b9"}, "606ff04a-ec88-42b0-acf9-1049f88b5a39": {"doc_hash": "aa3751b0f5583ae8d90f33bcc6d7cde4b4888b4f90c6c8ff66167727714c3367", "ref_doc_id": "79c5fdb3-d643-4015-9c66-b2e61aaab2b9"}, "388dda35-36d1-470e-a6e2-e3c956808e9c": {"doc_hash": "ad36f3984a9bc1444804a11b82c95c0aa39ddbcf4063df24b316e166141c4be7", "ref_doc_id": "945e5d8f-1672-4ab5-a945-d8c32d647d34"}, "f42429c6-97b4-4348-abed-7a6349e36190": {"doc_hash": "67972839da8d3f8305e54c7540b4256c742367784db396e17d75f83502487039", "ref_doc_id": "945e5d8f-1672-4ab5-a945-d8c32d647d34"}, "0485283e-4100-42e5-817d-3024c8c43e0c": {"doc_hash": "bcb2208168027393987d3f3b483b3ebae8cae242dee630771bb0c08719cf9491", "ref_doc_id": "54a51921-8b13-47cc-8eae-46d96b665531"}, "85ef6169-8eeb-4f4f-adb9-42493246c37e": {"doc_hash": "a8935ef947ddee472930733dc9507313f085ad519bdac142dea6ba7cf02bf794", "ref_doc_id": "54a51921-8b13-47cc-8eae-46d96b665531"}, "0ab1e6f5-5cf4-444f-aa42-0cc0e8197617": {"doc_hash": "92b7cc67831b0ba93576419608c72d072e5d2a54812141db276a887a51380320", "ref_doc_id": "2d5dd2f9-9fa2-456d-b60a-2170cbe91df3"}, "6bc95d12-6caa-4147-b98c-d15237e4f197": {"doc_hash": "27ed90854c511a917fb23775ea3b25e2458de2579438a4e00f80be08195374d1", "ref_doc_id": "2d5dd2f9-9fa2-456d-b60a-2170cbe91df3"}, "ffe850f2-bd72-4464-9c2c-86375f2a5613": {"doc_hash": "ab4796155b62aa4c898a4d4be6cd52695722b3b990e65d515db8def83330561f", "ref_doc_id": "46bf62b4-5558-43c6-8d65-4f3b62dae319"}, "373929c7-7adb-456f-a25d-de8b9ba0de15": {"doc_hash": "35ebb94e0cf2c06330c7fd0fdff1acd9bebfc22f47a85fc25a2cefe78f09c6a0", "ref_doc_id": "46bf62b4-5558-43c6-8d65-4f3b62dae319"}, "b538336a-590a-467c-994e-2bf4294cf002": {"doc_hash": "7b7476beaa488a58d74d1fa1088005d13c1a42d6b657a9d9ad7be204b300de5f", "ref_doc_id": "6810f935-b315-4872-9ab1-276a40e46e44"}, "4ce76e13-7b10-4e8f-91dc-76419c43ba24": {"doc_hash": "2726448bb0033a6af122df3b1d123e72c6181dfdb4160dd4c285cd7ba265fc69", "ref_doc_id": "6810f935-b315-4872-9ab1-276a40e46e44"}, "6a937a97-dd88-45f8-b4b9-4033dff65df9": {"doc_hash": "a11606d28bf53411cbd709b4c3ef8adea868dec83291b105b9f21c614a861817", "ref_doc_id": "ff3f6703-3601-4572-b67c-23ebbd05d5dd"}, "08039658-2335-43fc-a53b-6ba055c7c852": {"doc_hash": "0d43516ef7e6682d3e0072c125398d767882803eafed31b88a30bd92b6fbc473", "ref_doc_id": "3bff6b00-6fcb-4b8d-b1aa-6a615fdaec51"}, "22bc9140-bfc2-4e55-bf78-e9467e99ae82": {"doc_hash": "4abe6760d06a644a23dacc194609eab9d42bd27425ed2fec4222540d37338c70", "ref_doc_id": "3bff6b00-6fcb-4b8d-b1aa-6a615fdaec51"}, "4081def5-86ef-463e-839a-9ea0a8912667": {"doc_hash": "865d2ad2cb47fc4e75ea2be2818c1e166fa9f1731f557c6512f2a714cc433e8c", "ref_doc_id": "d1e8ccb1-1fb3-4536-ba94-30657d180b19"}, "78c3d25a-919a-49a9-8cbc-f7a9e0073335": {"doc_hash": "a2a303255c92f7290f0ea9fe998fa73370972796add439e694873cf963d59d28", "ref_doc_id": "d1e8ccb1-1fb3-4536-ba94-30657d180b19"}, "4d5f0e60-2f52-4a96-b2c6-c6dbe84ab12e": {"doc_hash": "665aec01c128ae207d388973dbba78e63ebef99c49b5e1b8d3ea9f3067d3aad1", "ref_doc_id": "ef883407-1e48-4ec1-b2cf-37f0b283ce69"}, "c0d67455-c6c6-4fdc-821a-4a6517ef8e6d": {"doc_hash": "29d29cf255f883f8619542692eae8ac6f2539513143862d5fb8da2ab8c5ace96", "ref_doc_id": "ef883407-1e48-4ec1-b2cf-37f0b283ce69"}, "285c86b4-90ce-4894-8756-c9412336a579": {"doc_hash": "d16eaae33383fecf85b155a86df6f919ad399c5dc99c5e7bdbfc944861149ce2", "ref_doc_id": "7f20761a-e0eb-4a36-a29f-3d481c8bb256"}, "e598aaf2-00b1-4a0e-a252-b4e974b39cb9": {"doc_hash": "79eef3899556d672e739c530818ec3ecedea2864cde86ce196765f4857dc5f79", "ref_doc_id": "7f20761a-e0eb-4a36-a29f-3d481c8bb256"}, "c68a2840-4799-4755-8180-75f6c99dc3b6": {"doc_hash": "4a45664175e30e93eb2f5f3aae600cc3305f8c03848143abda8336af8cc58e71", "ref_doc_id": "2d50e2be-5c8a-41a2-b2c4-7ae1942dc685"}, "24877f06-16ec-4d78-9d87-2dec4f5827ea": {"doc_hash": "14cd51a4383d849efc6d3a83b0bfcfd3f84992538e6594498c7b863e3787a523", "ref_doc_id": "2d50e2be-5c8a-41a2-b2c4-7ae1942dc685"}, "f1d9b863-2fa5-4e9e-8aa1-f90217396e9c": {"doc_hash": "d226a9858152197e53d9fb063c7e441cfdc08133614bf59baf2e6dad2f208f8a", "ref_doc_id": "c99a54fc-b8fb-4b9e-b121-2beeb24440f4"}, "f8117bf1-30ce-4ce1-a324-df24aeab0f9a": {"doc_hash": "5a4aa5a55f8a5634597836c999a2dda24ee1c555d8525fb36360a8277490cead", "ref_doc_id": "179937f3-5518-4faf-b3e9-7f03a18a6a30"}, "dc671a17-8adf-4653-93a8-c28169f4dea2": {"doc_hash": "f9f6de9e7b1be421db22ac481badab7414c82e3ac3c7f093b7426d2799401d0a", "ref_doc_id": "179937f3-5518-4faf-b3e9-7f03a18a6a30"}, "6e323a8c-568d-48c5-9578-b9be33ebcc35": {"doc_hash": "d0ac3a2b4a3e4abc4e264587dadb595e5b0c005378c64e6131292ce30820ee28", "ref_doc_id": "264887f3-d4b6-4c45-82e5-479e1ee7ef04"}, "ea6610ca-6d7b-412f-8e9a-78670ff703ac": {"doc_hash": "2dc30c4ca3a206bdd2cc6e2710fb3a085dcb49cb575ccecab3938036ef05a487", "ref_doc_id": "38234d53-0ca9-4146-9ec4-2ade5b7c8b67"}, "fa142759-8335-47be-84a3-0b137ebeeed0": {"doc_hash": "63750477e90a8bf6feec791bfbc3861a4ea6ceea33e33438dc70a6baeb04ce86", "ref_doc_id": "38234d53-0ca9-4146-9ec4-2ade5b7c8b67"}, "92fc61de-7d9b-4c1a-b466-5f1a88e9e56b": {"doc_hash": "95f44c7f5b564afca01aaeccae55614258a36e1c93fd6ebb4d31f4a60bd8221f", "ref_doc_id": "afe91798-85ed-4db9-89bd-1d14beba03b8"}, "fe8354f0-2492-439e-baac-b334181bb272": {"doc_hash": "ff97ba111df74a58c03c99fec8610bc8f6bf42f0631df6cd25f3fe883f69b29b", "ref_doc_id": "afe91798-85ed-4db9-89bd-1d14beba03b8"}, "5a51e625-0d10-499b-8ef8-c9dfe3bd4d4b": {"doc_hash": "d31c06390768635d87c12518e198f3e4074e525216279aaee3243e97f34c763f", "ref_doc_id": "7fdcc2cd-5535-4b78-9fdc-fa6a5c90efce"}, "8f378dfb-fa87-48d0-a699-a4f8f3855126": {"doc_hash": "f0625bc4053ca58b3ad58a64f9572f5257bbe69d4d58694729bebe7c362818d5", "ref_doc_id": "7fdcc2cd-5535-4b78-9fdc-fa6a5c90efce"}, "33a24653-6f19-4a2b-94a3-8c6f20b75f22": {"doc_hash": "183f81ef05b15d09a14b354f456d8d65a386bdff3df9053a332fed3378b462e9", "ref_doc_id": "a13ee79e-1f7c-4c64-b3ac-fe5ba9b1b7dc"}, "24833518-abeb-4715-961b-a36fd0f89d82": {"doc_hash": "53948a7be43241a582f143d9a59c66a305ef7fb0490dc7da7a607c64daafa030", "ref_doc_id": "a13ee79e-1f7c-4c64-b3ac-fe5ba9b1b7dc"}, "23f1f6d1-c23d-4dbb-8384-29c71b880ae9": {"doc_hash": "6f7763ac334d29c9e4268f77e4d94f04d8228c4aea7111d39f6a49c8c03fc600", "ref_doc_id": "c9e28ad3-7513-4b4e-a179-18849ad39c35"}, "1578b625-b2f4-4741-844f-0084d2afd9c5": {"doc_hash": "580edae5a19ee47756d9f09d38414e0f72ae35c81ae540c9584f06ff409e4dc0", "ref_doc_id": "03c6cd1f-be62-42b7-b509-9bbdaabc3213"}, "10b8718a-0adb-493c-8cc0-fde9567f811b": {"doc_hash": "c9649d9323b84357ea830e0b815c382e32c1b5f71ffa3d565ffcc94a6f906781", "ref_doc_id": "7d6a84cb-5208-424b-a403-25580b000ae7"}, "d95f851d-6c05-4a99-95d7-a1a3ac1c369b": {"doc_hash": "c435f1686602e75cd3d02bd89a77943cd747784b5b90899379099013c8cb4121", "ref_doc_id": "7d6a84cb-5208-424b-a403-25580b000ae7"}, "c30ca5b7-c03a-420b-a587-04e1f1329118": {"doc_hash": "cc5eb9d8f388caa6eb2d18a439214fec0819cc983043e9ca9267016708bb14a8", "ref_doc_id": "c90c7d5a-067a-4c5d-9cdc-9f8129fd09cc"}, "533721ed-8bee-42be-aac0-68b084173b91": {"doc_hash": "db5bb4c7df867d4071f0f9cd07527ac647f6fdafb78df007fd1590f2a07ab552", "ref_doc_id": "c90c7d5a-067a-4c5d-9cdc-9f8129fd09cc"}, "e64259bf-9974-4ad0-bcbd-de8b5ddcbf5a": {"doc_hash": "b35f5a4a69207bb3e6108115443afc1a6b115ae1005a9368517b49594f0d10df", "ref_doc_id": "82e61a9d-2eef-49e6-9f5d-41562c995589"}, "282ccc27-83b2-4f25-9139-03d36952d0b6": {"doc_hash": "d362c62483e07ccebd2f5b986bd7dda56348fc407284be535c800ba4ac0c7442", "ref_doc_id": "82e61a9d-2eef-49e6-9f5d-41562c995589"}, "6b86910e-1423-470a-92f7-fae69e9edc93": {"doc_hash": "834deaeb3bbd9ec2cbc9c81b1b8f9c40c260412281f1992e78b7aa0f7aedfb19", "ref_doc_id": "6391cb5d-b101-4644-908b-40bdfd0e913f"}, "edd7f8aa-bb1f-40db-90ca-263652935f4e": {"doc_hash": "c8371f8b8981c7a71c6be93e80afd30ba380998acc6ac6719ca92aab2bf5c092", "ref_doc_id": "2f22b994-8dbb-4d88-8352-5a9fdb868e06"}, "5bee5618-8f26-487c-9665-d71cfad9648a": {"doc_hash": "cce96964cea98c9921c83ca78609d743afb85cc69b02513c12684b1f00be3d8b", "ref_doc_id": "fbaf96c8-70f8-4731-bbdb-85d321f0991b"}, "b1ba3042-b143-4869-9fb1-68dc374be716": {"doc_hash": "9fdf2b4bf5d186d0df6ab99ac85d9c0187c8ebe157711355b39c78c92dfef8ab", "ref_doc_id": "fbaf96c8-70f8-4731-bbdb-85d321f0991b"}, "b5b1f751-916b-4796-bfd3-3abf90bab973": {"doc_hash": "32efa4ea5306fac18c0572513800fb5fcd3971456e4f50cccf7d54a9d65c5388", "ref_doc_id": "e7a885ea-3f65-4915-83cf-564f18e50bc9"}, "79a75624-b355-4998-8258-d20e87aa68be": {"doc_hash": "3337ec9ab218828accecb6f85550c6ded1c253cdd4d25b71f77a5a1459707436", "ref_doc_id": "e7a885ea-3f65-4915-83cf-564f18e50bc9"}, "bad82dc9-395e-4c17-a51f-076ccb4582b2": {"doc_hash": "7f861fb6002e4ac74bd50b633fe3b83cecddd201b50bdd8718df56d8c1dc8d69", "ref_doc_id": "733191a2-af86-45fa-b525-856ed65adfe5"}, "0130df9e-d4c0-47ad-9115-86c7fc1406c7": {"doc_hash": "14449b1d2fa1cd455b6c3821d685f382eb3e6a69561b8fc8bcae6d8453c550e6", "ref_doc_id": "dc0ebc3a-0a26-400d-bc38-e3f109a5fe0a"}, "78aa6706-abfb-43a8-a4e1-d310f77cd127": {"doc_hash": "6022123ef072fac0ba6bd67668f9ce107edc7aaa24b08e1227c3473779316061", "ref_doc_id": "dc0ebc3a-0a26-400d-bc38-e3f109a5fe0a"}, "5e64c2bd-4e5b-4f41-a281-1c8ceb4a1d82": {"doc_hash": "f91e9111d3d35764f2d08ac6962762ed13de04758edd4d7c0466ff66da9e13b7", "ref_doc_id": "7fc6007a-767c-4390-9164-62179c873c77"}, "30934b34-7e35-49b4-93da-175808ded43b": {"doc_hash": "3bf35518238e7858ed076f79e14e4ef72316f6ded3d1b1d8d7777ef6cb729547", "ref_doc_id": "7fc6007a-767c-4390-9164-62179c873c77"}, "41feeed9-be6d-4556-a1b2-f8a307fa74fd": {"doc_hash": "6599a8bd674e7e6235cd906095bac9960a3b024c8072e6f5fd086ce8562efb87", "ref_doc_id": "c185ac8c-06be-416b-9fdc-19087684730e"}, "5edf799d-1457-484b-956e-d93d663b066e": {"doc_hash": "b3639a498d434c78c3a7c002535558af53a61e3c0c1f76d7a4e04775be9caf54", "ref_doc_id": "c185ac8c-06be-416b-9fdc-19087684730e"}, "15a937e0-2550-4b73-8e0c-1b64a2499e0c": {"doc_hash": "5977ac676c2b0e3becb4dbaa64ce71ac8d4daf330415bdd268cc1cd6a5fc1d56", "ref_doc_id": "76ea6bf7-ce7f-42d0-b254-73e1c37fea3d"}, "1560830c-1c3c-4fa2-bc50-f6ff4ed67562": {"doc_hash": "e902e5fa90ddc9dbb61cf97ecd93036c8096265b7cf91468f305c3a742d9fc04", "ref_doc_id": "3ad6f4b3-100a-44df-b1f8-d40424f52a0f"}, "803ddf7f-7017-41ce-a794-3b3ea592321b": {"doc_hash": "ab1fddfc7c3ea2656085e7c92905782a30f32c0f706cc7a4196996fab86f5f44", "ref_doc_id": "3ad6f4b3-100a-44df-b1f8-d40424f52a0f"}, "0f924c9f-34ee-44c2-bb38-507c517a6978": {"doc_hash": "2d00133af1dce31b4064cd3dd096002307c2d0fdad6ed918d0b22686c2a7af1d", "ref_doc_id": "a23afbef-faa8-42eb-bb0b-d4af01c2b1af"}, "6055b970-22d1-4fa6-a508-7022ddf32591": {"doc_hash": "3ebd3e934b0c4ede6afca37b24fc7f9b9dc9bcf3b33cd5757a062735acc75b69", "ref_doc_id": "0cb8eb46-a71d-4228-be2c-3f20cb31025a"}, "b1aff551-2170-47ff-8017-b18cf6cf58a9": {"doc_hash": "ff2dc48f846e2b2350e2fcbb90a12798ee29500f9f3ba223021ab1b86b654988", "ref_doc_id": "92238105-858e-4f90-b6ed-c7c7dbfa0e08"}, "d57c4995-c52a-41d2-a029-c28578759712": {"doc_hash": "5a3f2fc4b0beea8f419ce912d0673077b201dc93082f7721de163bec19d34787", "ref_doc_id": "3f830bbd-7bf1-4acc-9193-84dd47a2e212"}, "9708271e-0459-4801-8068-9e5e6f14ef88": {"doc_hash": "f3395f458eb6fff48d3c289cc8a4ea4982d1df26dbc98f9bf722fcdb706464ba", "ref_doc_id": "3f830bbd-7bf1-4acc-9193-84dd47a2e212"}, "8b6214a9-eda6-48b7-a92e-534c1aaa3235": {"doc_hash": "209f4b5d557c2abf953867ffb595c6a41133c61bd29c32c3ce7546fbbccbd983", "ref_doc_id": "3ecb5304-c784-4118-922a-f7b6eeac23f1"}, "629f4da1-2129-4201-ad23-3f75ed5d6f27": {"doc_hash": "f770496fd61705c10d130b8dc9a9e926e6f9b3d442fe8fdc24998f346e3bf3bc", "ref_doc_id": "bcb391ad-c487-4189-bad3-e742a75aa340"}, "1e4a7246-3d4a-4eda-98f7-70b2cc54eb7f": {"doc_hash": "62262ce97247253bcb4bef01eb7c89a2abceaf1e7e911de72dd7e5b1ba105f7f", "ref_doc_id": "bcb391ad-c487-4189-bad3-e742a75aa340"}, "5e405028-c4f0-43d1-b80a-a4a9c44ac8fe": {"doc_hash": "bada9161789cf0c70cafb4374892e058c444cc5569a7e64af64767aab38e6db6", "ref_doc_id": "7b70200b-1c4f-4615-86e3-8382f9b8d2de"}, "bf988a13-59e8-41d8-a608-5c5723a8b047": {"doc_hash": "aa7301bbb8286b5e770df9d85e0141901daadd7c4aa69ed0d13bb6d3e0043ddd", "ref_doc_id": "7b70200b-1c4f-4615-86e3-8382f9b8d2de"}, "4425c16b-56e9-4b99-9d78-b9c50b502c6f": {"doc_hash": "e2a64c394a770464b42511314d1a05ed858140c063d08702c24903eb7faa1273", "ref_doc_id": "f235f253-ab1e-43be-8cbd-800eab485f88"}, "0f447729-1c78-48d0-86bb-d606f1c45cf3": {"doc_hash": "c5e8ed73863802d609c69618542f2314c1ad0ff2ff1c7dabe18f045d49f2df6c", "ref_doc_id": "b25d40ae-ab86-4c14-9eab-038ab0b41760"}, "35e4a954-31d7-47d9-b34e-bd66ccbf1b08": {"doc_hash": "2f1332d055e7ec6e4c1db990d716ccc3fdc4389aebae6f025f5ed40d4be8e587", "ref_doc_id": "b9cb15a3-f949-45b7-9e01-bdc0bc178945"}, "b9ddd870-b29e-46c2-a979-5baf94fab778": {"doc_hash": "3a5ed2b21edcc908a4d0cd5cbd09bc06a2c139e85b1c670d1e3b2c84aea7c336", "ref_doc_id": "b9cb15a3-f949-45b7-9e01-bdc0bc178945"}, "d9d4f921-607d-4276-a6aa-7f73247e8cce": {"doc_hash": "8487f9bf39461c602b8c51fb48234af9a164656d955f4bd9c6a0ecfec47d80d9", "ref_doc_id": "5a8d2dcf-bbd5-471f-8a65-1f2a555cb229"}, "cb37f860-9f5e-4d7f-b527-ff6225663e83": {"doc_hash": "1c392c6754c32a24b8ce38a44e956bddb705cc596f7cb9ffa222235b0951a736", "ref_doc_id": "5a8d2dcf-bbd5-471f-8a65-1f2a555cb229"}, "4f7863fd-3cd1-464c-bde5-b55891d99915": {"doc_hash": "7bb66df619561457fec9c7a4fdd45061f181a1913c92024a6a5b8949aac11b2b", "ref_doc_id": "21e929ce-3e11-4f56-b6ee-16f41a72cdfc"}, "f79355ba-f756-4286-af95-b3985636e2d7": {"doc_hash": "2c3c6735d67bcc2f65b6f67de0c6c0a816e10d32d5f69769e71adf9766d19eb1", "ref_doc_id": "21e929ce-3e11-4f56-b6ee-16f41a72cdfc"}, "5401fa29-c9fb-4f3f-acc5-483f17e6799e": {"doc_hash": "6c6892cfcb4764d0aba704710036971aa3b6627c718ab1fe478cd90d70bfe629", "ref_doc_id": "8a45578d-5044-4d5f-9bf5-aef629d70917"}, "2930f660-dac4-4385-ab52-ad2be66041dd": {"doc_hash": "0d1f898bbd84d3910155414c467fda881af10beb38a81827f8cc65e64e426c5b", "ref_doc_id": "1a5fc65b-7546-48c3-ba29-614b6df88236"}, "b6d570f9-739c-4d9f-b7b8-e54a825b2a05": {"doc_hash": "c9173eb1976face56b9303abd5ffbf7a953492b3879e619862bcf5ddbfc117c7", "ref_doc_id": "40643ffa-8800-43b2-8e36-59f56a8b69c2"}, "7fad35e5-2200-43a8-b31a-bbe695837b6a": {"doc_hash": "4b4a5245e36fc15221abd11d522c8413fd5c2c6500eb7b943e106e8d24fd9ca3", "ref_doc_id": "5835ceb4-48bc-4ca8-a95e-4b1e4e120339"}, "ecdee644-f8bf-456a-98c4-68f67ae521e8": {"doc_hash": "05d0fa014fa8d1bf95ae039df90547b12c76e940de9e8c6de980d34d8c659b43", "ref_doc_id": "5835ceb4-48bc-4ca8-a95e-4b1e4e120339"}, "a3fe6483-a996-4ebf-a1b0-ed60566c7a04": {"doc_hash": "5f6de8cdde06a9fcc20a3b4795993421c749da1f222ef72395505155d25c3536", "ref_doc_id": "2539089e-3c01-4b47-9494-0965758b3da6"}, "3ac4883c-6d48-4d8c-ad80-72af3481f0ba": {"doc_hash": "7bda13e4bc153a9c8c2a100a2f42bc5e49963a7d446749bb8e76a0d8d8d259cd", "ref_doc_id": "b52beb20-a889-48d0-bd38-048e254e362f"}, "5a7d29a2-48b8-4f70-9439-7fb311d1d6f0": {"doc_hash": "9150d4c72b70a7c9a9707803669432a0222ef461d46f93cc9bddccb712c502d9", "ref_doc_id": "deb3df48-8344-41db-abd5-930649e70bb2"}, "71c3ece8-85d4-40f3-ad05-069a7bebc348": {"doc_hash": "f576fa6347df0ab0effa2157414aee0e3e343124df5342d9cf1a0fc4542acfe5", "ref_doc_id": "deb3df48-8344-41db-abd5-930649e70bb2"}, "658b06c4-f315-416c-8279-48d2f79cb6e9": {"doc_hash": "b08589bd78e8aec2bead0e24295f2cf730c3c759a741bb1827024a51668bb200", "ref_doc_id": "77469f62-cb8e-4cac-8417-5abb9faad643"}, "6d29a934-074d-4383-9454-32101b204afa": {"doc_hash": "7d80ac3d093d7bdd80dec3afaccb3d07432e475ba7fd48933c9e450ee0913ae3", "ref_doc_id": "54a03e67-6b84-4e5b-9b3a-a09db6c0ab87"}, "00f4e5c9-7f38-4231-9e12-276eb2cff2c7": {"doc_hash": "b3428a36f3e0ee559b0c63d8468cb4a7b654d20a80a9e245be610fce9d4a94da", "ref_doc_id": "616bfc8b-252f-4638-a3a8-6f2afa12e662"}, "0dee3d18-948c-4b7e-b11b-a0cc8b8c1bda": {"doc_hash": "73adec5e1f0ba11c1dcc130459200f924e9456da39945e1409d69ab6c1102a49", "ref_doc_id": "6367feb9-ef1e-478d-83bb-83c04c32be85"}, "ed812420-4b3e-4ebb-85f0-49455ce1a218": {"doc_hash": "b3cebc1ce5307d27ccbe271bfa9c3ab2fca82a9aa11d1339ce96d7a5adff4e39", "ref_doc_id": "de08b289-6536-4ff0-98e4-a3bb8e363727"}, "33f4e238-c13c-4c72-afdd-3de409192f7f": {"doc_hash": "b3ee8e9dc82bd71332ee9251794d3b9b598834123ce4473fd7b3c86de28be88e", "ref_doc_id": "17b71dfa-ed2e-4ed9-a506-129875e96aa1"}, "7a5f90f9-63ed-4f86-90bf-47fc920d42d6": {"doc_hash": "965f097e93d0b92418c662f23fd0547f16017b873902615d7feac40ffb2415ec", "ref_doc_id": "c0f50496-e9b3-4987-9ee1-20cd9906df30"}, "ae9e0db0-35b5-483d-a429-87ea38c0b745": {"doc_hash": "167ddfe1fc7846918b3ccb8b6a1e193bb983d154b84ea933cabade884ace9c34", "ref_doc_id": "c0f50496-e9b3-4987-9ee1-20cd9906df30"}, "2cf23693-a9f0-4f80-a2d2-3382c344a5c1": {"doc_hash": "731f310767895309bc7b3f6668510023d8385657cb8c41366e33a396da4279f7", "ref_doc_id": "3f100fe1-c9e6-475e-ae8f-fffdea4fb8be"}, "7985138a-fafb-4c38-827e-5f9873cb23e0": {"doc_hash": "0b9a9c5463dc587c83044b33498e3e142624c0b101e8a97c1dc095f08ed60ead", "ref_doc_id": "e9a9f5ea-7412-45f7-9142-30b634592df7"}, "d04d1808-31a3-4a6b-83fc-be775bf988ec": {"doc_hash": "2dd5b5f9ba76a5d7952fb902b6dd2b0399d16f79358fcc1ee8031cd921cef3e7", "ref_doc_id": "a1d13aff-daac-421d-b502-7622dc524df1"}, "b5507e3e-00d0-477a-bfaa-8004bc5b95d6": {"doc_hash": "85a4897e35d82278d537f3a47583f1f6902946a6c9ad37fb50f778162fe72ffc", "ref_doc_id": "46eaf67c-3fe1-4685-b76e-3415d54d0f4e"}, "8e7b3e37-4e53-4c83-bee6-cf4cc208d13c": {"doc_hash": "2703682be46c535f4626418ce4d93a01cb8eb629412b927e4a9ba182db93c540", "ref_doc_id": "36a208b8-0096-4368-8887-c275ad86fc53"}, "1c305eb2-7ee8-408e-b649-926e26a95290": {"doc_hash": "9312342b969521e17285209bce4803daadbbbff9a7e67778e8f5f80d8d17cb31", "ref_doc_id": "a7ae0c48-21a8-40a9-8579-82ce516fd829"}, "75e9230b-0fd4-416e-a386-f0a7a267fc57": {"doc_hash": "e227b5e7bb2f3977e8ccd2ea52cdd84cf596f669980c33cf2c01790fd9db38fb", "ref_doc_id": "c55f44e6-a7e1-4d5c-a03b-18b5bc439a42"}, "def97ab3-0457-48e7-857a-dfac6b7a9fcc": {"doc_hash": "080d46db81a415a69121e49e0e268b38a973e010df554a5bcaa1b2003d53a9fa", "ref_doc_id": "96b1ffb7-aa8d-4c60-a33b-e77c45258faa"}, "28d6c7d8-0110-4516-9ba2-0e900bd9d56b": {"doc_hash": "7c8ec0f9caab267ca0d121e6a1fdd2b0f227cf9de5666815a9ba4402655512b7", "ref_doc_id": "081d9993-3af6-4d06-9ab2-16c8e4d0a73a"}, "0ce6ba23-fa03-4bba-a83d-44dacb77a6c7": {"doc_hash": "95f521bf23f49ee8058963968d748a7d8fcc88a29d5947252ed501ea2a683b1b", "ref_doc_id": "19ba5d76-21bf-459b-a64c-92901a5cd433"}, "9696e663-ca63-4e83-8dd5-2e5b5a03c488": {"doc_hash": "0f395eb92226d491dac217cd3d53b08ab064328bf6fc4de23c2d50358095bea4", "ref_doc_id": "a99da12d-8348-47c5-802b-c6af765971a5"}, "1987c594-f40e-40b1-bb3b-0455045f87a3": {"doc_hash": "148094ba95e5c613c8f2ff270dece16ac17b34006f2ec3b6263579cef6ea1996", "ref_doc_id": "a48b8dd5-8f64-4fe7-8ab0-41590a053fc9"}, "b7260b9d-1811-42c5-9323-f2221a35bb18": {"doc_hash": "71533c11325a121cd29275e44cfe4efba9c4896dfff0049710e98327e552635e", "ref_doc_id": "62b36143-da7c-47cb-8e02-54983dfb2ddf"}, "694b693d-ae7e-4e11-b716-3c305cbe79be": {"doc_hash": "b3eb2d0d9ec41e948a6c53b7084186f9ee22803996e306da9e6c995af97ce70a", "ref_doc_id": "ea16b9e1-0038-4912-b08d-c05d79e992f1"}, "769c9b37-0fd4-499e-9c0a-4b78f25fea89": {"doc_hash": "d4f6b58d5b064a4792498acb03e0e6174d8da43dd9f49150a2ef3a84abe4572e", "ref_doc_id": "822f4bed-8f8a-4572-b635-7b64a7802223"}, "798385e4-1a16-46bf-b13c-2390a4c320fc": {"doc_hash": "ccb2ec2c9deb13e05d56966e864dab3781b04e196386f0a8721ca3038030b614", "ref_doc_id": "4b7ea0ca-7298-4661-8773-916cf7e69911"}, "4753d8fc-65fe-44d2-a0d7-e62ee70f1291": {"doc_hash": "b0730fb08c91cc60608844f2132ba33bb8e028c4ec4d2a3178a5a13bd75c9374", "ref_doc_id": "6d98a4fd-261a-4226-bab2-9d69678cc51d"}, "c3c32b5e-1242-40fd-8346-90d05bf95d4c": {"doc_hash": "6ee43c1952f016410c1a3dedc3cead921ecca2aa0bdf8b5814bfccc48685f718", "ref_doc_id": "c11ba10e-ddae-4306-9fb4-974daf734cd8"}, "ae77cfdb-e789-44e5-b28f-dd4706b16934": {"doc_hash": "419a3240a97c88f982aa2ae6a08665e1b7b695fa02da6c4c5fa57135bd92727e", "ref_doc_id": "c11ba10e-ddae-4306-9fb4-974daf734cd8"}, "6a15031b-2a99-41cc-9295-29120d30598a": {"doc_hash": "7cc073ebbaf50bb8379e1be47c6283de502da2a80d392101a3700e708a88e39b", "ref_doc_id": "321e862d-d329-4921-8663-5779de794cb1"}, "39285f5c-9723-4f9b-8eda-5295031cabd2": {"doc_hash": "8d707bac91f5089b5e0dc815c1285e83661a98f6e711abd4d7bd377058045218", "ref_doc_id": "321e862d-d329-4921-8663-5779de794cb1"}, "19db1f6d-e725-48ca-90fc-f08784195976": {"doc_hash": "135d8483213fe621db6312339a60c4db445a2f0a0b71922699fd8b23e80f1af3", "ref_doc_id": "a457ed4e-d4bf-4b08-bed1-47cb18a5307e"}}, "docstore/ref_doc_info": {"ac7499e0-371d-4829-a4b8-563891562236": {"node_ids": ["d8ea723f-7cda-4fa1-b639-a93942fffda8"], "metadata": {"page_label": "1", "file_name": "A cybersecurity AI agent selection and decision support framework.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\A cybersecurity AI agent selection and decision support framework.pdf", "file_type": "application/pdf", "file_size": 1115417, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "0f9c0d06-8922-4dbe-a246-08baf420899b": {"node_ids": ["2a68eef7-628b-4aea-8489-45beb414897d"], "metadata": {"page_label": "2", "file_name": "A cybersecurity AI agent selection and decision support framework.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\A cybersecurity AI agent selection and decision support framework.pdf", "file_type": "application/pdf", "file_size": 1115417, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "66b7b794-a9d2-4d13-a6e5-9dd1837e9ee3": {"node_ids": ["c91746db-492d-42fb-9b4c-1152fc1c4db4", "d28f6583-9704-4b9b-9c46-2fba7bc72f19"], "metadata": {"page_label": "3", "file_name": "A cybersecurity AI agent selection and decision support framework.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\A cybersecurity AI agent selection and decision support framework.pdf", "file_type": "application/pdf", "file_size": 1115417, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "6317a77f-e5be-4269-8575-3ee8813a2132": {"node_ids": ["662cae6e-3041-4439-bc35-d7b071c62124", "07481227-7921-4b5e-a7f6-f10e343516eb"], "metadata": {"page_label": "4", "file_name": "A cybersecurity AI agent selection and decision support framework.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\A cybersecurity AI agent selection and decision support framework.pdf", "file_type": "application/pdf", "file_size": 1115417, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "04f8d0a1-308f-40f8-92fe-9384ad9d72fb": {"node_ids": ["4a0da9a4-05ee-491a-a6d6-5a3a274bd365"], "metadata": {"page_label": "5", "file_name": "A cybersecurity AI agent selection and decision support framework.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\A cybersecurity AI agent selection and decision support framework.pdf", "file_type": "application/pdf", "file_size": 1115417, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "4b1d74f3-f204-45e7-af3d-fe1d5c283494": {"node_ids": ["39a8b576-95ee-4f2b-bf50-b61a12715802"], "metadata": {"page_label": "6", "file_name": "A cybersecurity AI agent selection and decision support framework.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\A cybersecurity AI agent selection and decision support framework.pdf", "file_type": "application/pdf", "file_size": 1115417, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "0a3a9eab-cd91-40b5-a082-99f565c8aa9d": {"node_ids": ["fcf0646c-36eb-4a5c-91c5-931b603b93c1", "d28ffa8e-4319-4369-85be-ed2907ea7a51"], "metadata": {"page_label": "7", "file_name": "A cybersecurity AI agent selection and decision support framework.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\A cybersecurity AI agent selection and decision support framework.pdf", "file_type": "application/pdf", "file_size": 1115417, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "958cd863-8c06-4326-b6b7-9b02d03b9cc9": {"node_ids": ["362e4686-5809-4165-b2c3-31a3998beeee"], "metadata": {"page_label": "8", "file_name": "A cybersecurity AI agent selection and decision support framework.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\A cybersecurity AI agent selection and decision support framework.pdf", "file_type": "application/pdf", "file_size": 1115417, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "51ab4d3a-f32a-48a6-b92c-ffb52d2f78db": {"node_ids": ["3941b510-279e-4797-84a1-1839af8fc475"], "metadata": {"page_label": "9", "file_name": "A cybersecurity AI agent selection and decision support framework.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\A cybersecurity AI agent selection and decision support framework.pdf", "file_type": "application/pdf", "file_size": 1115417, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "d150f798-78fb-4979-94e1-a803e1e43a06": {"node_ids": ["7ceba8cf-fd94-4296-b757-ee6f59baa14e", "39b60357-2add-471c-abb5-6c3ca1d9fa21"], "metadata": {"page_label": "10", "file_name": "A cybersecurity AI agent selection and decision support framework.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\A cybersecurity AI agent selection and decision support framework.pdf", "file_type": "application/pdf", "file_size": 1115417, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "adc017cc-ac1a-4f0e-ac74-76d2445ee2bc": {"node_ids": ["974db033-555a-4661-979c-0618c0541dea"], "metadata": {"page_label": "11", "file_name": "A cybersecurity AI agent selection and decision support framework.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\A cybersecurity AI agent selection and decision support framework.pdf", "file_type": "application/pdf", "file_size": 1115417, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "0f1c73c9-b684-4671-aa71-47e6eb8ac488": {"node_ids": ["7ef21053-d125-42f8-9ccb-7b613ecae9b1", "b3b37b44-10e0-4975-8265-9ce4ee96e7f2"], "metadata": {"page_label": "12", "file_name": "A cybersecurity AI agent selection and decision support framework.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\A cybersecurity AI agent selection and decision support framework.pdf", "file_type": "application/pdf", "file_size": 1115417, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "3a6e1496-8dd5-4de9-9768-3f24e646a775": {"node_ids": ["ce69075e-e4fe-4ec6-99ef-a35dd5ac653f"], "metadata": {"page_label": "13", "file_name": "A cybersecurity AI agent selection and decision support framework.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\A cybersecurity AI agent selection and decision support framework.pdf", "file_type": "application/pdf", "file_size": 1115417, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "010eda0c-8a04-4332-9be4-a79e76b5d00a": {"node_ids": ["8893edb3-58b0-43ee-813c-164af9d5d5b4", "9c6b14b4-8b8f-48f7-9c7c-293adbe5d950"], "metadata": {"page_label": "14", "file_name": "A cybersecurity AI agent selection and decision support framework.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\A cybersecurity AI agent selection and decision support framework.pdf", "file_type": "application/pdf", "file_size": 1115417, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "0591e821-5b81-42e4-838f-6f6de18f7fc3": {"node_ids": ["527b6e2d-0492-42a5-bf0e-fc2fe031d79a"], "metadata": {"page_label": "15", "file_name": "A cybersecurity AI agent selection and decision support framework.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\A cybersecurity AI agent selection and decision support framework.pdf", "file_type": "application/pdf", "file_size": 1115417, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "b92297f3-bcb0-4c62-97fb-f22f4c86396f": {"node_ids": ["20891a2a-bc5a-4515-ac40-c3fbd7f6bc7d", "30d2bdda-48ac-4131-b09d-dec60ba55975"], "metadata": {"page_label": "16", "file_name": "A cybersecurity AI agent selection and decision support framework.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\A cybersecurity AI agent selection and decision support framework.pdf", "file_type": "application/pdf", "file_size": 1115417, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "d251dafc-c95d-4067-9111-eca2d7dd67ed": {"node_ids": ["fd834def-22e7-43dc-aebf-e385bddce180"], "metadata": {"page_label": "17", "file_name": "A cybersecurity AI agent selection and decision support framework.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\A cybersecurity AI agent selection and decision support framework.pdf", "file_type": "application/pdf", "file_size": 1115417, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "ac764fbb-d2ed-4832-ab34-50ce58d65ed6": {"node_ids": ["196f2e54-008c-49fa-a236-ce7f97720913"], "metadata": {"page_label": "18", "file_name": "A cybersecurity AI agent selection and decision support framework.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\A cybersecurity AI agent selection and decision support framework.pdf", "file_type": "application/pdf", "file_size": 1115417, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "c518dd2c-1601-4e39-bd26-680434916d24": {"node_ids": ["04c6973a-b190-4aba-b52d-4dc378388e8a", "2c6e4549-324f-4d63-93df-94c0ddebf0a8"], "metadata": {"page_label": "19", "file_name": "A cybersecurity AI agent selection and decision support framework.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\A cybersecurity AI agent selection and decision support framework.pdf", "file_type": "application/pdf", "file_size": 1115417, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "6900c6ce-0e26-4bf5-be6b-d349443630af": {"node_ids": ["cae362ed-f7cf-4fbd-93cd-502f12ca4484", "b0343ff0-b755-45a6-a6f2-1232907b0b35"], "metadata": {"page_label": "20", "file_name": "A cybersecurity AI agent selection and decision support framework.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\A cybersecurity AI agent selection and decision support framework.pdf", "file_type": "application/pdf", "file_size": 1115417, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "23995e31-dcbf-46a9-941e-599c6e4c9b5a": {"node_ids": ["99ce0cb6-1223-4a61-a443-f179ecbd059c", "5e402284-a5a9-4a38-b50e-9d18b637bf0b"], "metadata": {"page_label": "21", "file_name": "A cybersecurity AI agent selection and decision support framework.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\A cybersecurity AI agent selection and decision support framework.pdf", "file_type": "application/pdf", "file_size": 1115417, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "e7692e97-75d4-452d-8eda-eecdfa47a4fc": {"node_ids": ["c9f59cfb-a61b-49d5-ba37-9cf4ff88f3cb", "ddeae525-791f-4ad5-9795-8eab22c13f61"], "metadata": {"page_label": "22", "file_name": "A cybersecurity AI agent selection and decision support framework.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\A cybersecurity AI agent selection and decision support framework.pdf", "file_type": "application/pdf", "file_size": 1115417, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "126e363a-3782-48c3-ac37-112382f9bffb": {"node_ids": ["a3d1a7a6-d4f6-45c7-b9b0-0181d4e6140b"], "metadata": {"page_label": "23", "file_name": "A cybersecurity AI agent selection and decision support framework.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\A cybersecurity AI agent selection and decision support framework.pdf", "file_type": "application/pdf", "file_size": 1115417, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "599f904d-f72e-4f5f-be6d-e9c5d25483ce": {"node_ids": ["f5ee87f7-db19-4c24-9b54-4ddd1f4f5079"], "metadata": {"page_label": "24", "file_name": "A cybersecurity AI agent selection and decision support framework.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\A cybersecurity AI agent selection and decision support framework.pdf", "file_type": "application/pdf", "file_size": 1115417, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "c0673f5c-d45b-41c3-9220-951c9694195f": {"node_ids": ["1b54ab41-3a7d-4272-9b71-f9f5a9dabd14", "3caa9e0e-1b46-4b01-84c0-8c4b45ec31e1"], "metadata": {"page_label": "25", "file_name": "A cybersecurity AI agent selection and decision support framework.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\A cybersecurity AI agent selection and decision support framework.pdf", "file_type": "application/pdf", "file_size": 1115417, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "513abfd1-07ca-4784-bd5e-41c0c3f8dd0f": {"node_ids": ["82e90a79-617c-4aa9-a794-dc0d054e5694", "ef63d4b0-c211-4e84-8711-c84ea2c36260"], "metadata": {"page_label": "26", "file_name": "A cybersecurity AI agent selection and decision support framework.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\A cybersecurity AI agent selection and decision support framework.pdf", "file_type": "application/pdf", "file_size": 1115417, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "f39e0524-039c-4e15-9417-39925ee5588e": {"node_ids": ["e15480fd-fdc3-4b54-9af9-92b563abac6e", "62db4191-b1c1-4c79-8fe9-2816ce69c04f"], "metadata": {"page_label": "27", "file_name": "A cybersecurity AI agent selection and decision support framework.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\A cybersecurity AI agent selection and decision support framework.pdf", "file_type": "application/pdf", "file_size": 1115417, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "1b74cad1-591c-4e89-81c0-0fd0159d645a": {"node_ids": ["2c16ae91-bd75-4f5c-95c4-e41637e0dba4", "be19f4f1-817d-48d3-a3e0-b43fb59a1afe"], "metadata": {"page_label": "28", "file_name": "A cybersecurity AI agent selection and decision support framework.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\A cybersecurity AI agent selection and decision support framework.pdf", "file_type": "application/pdf", "file_size": 1115417, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "1a304a07-df7f-4b7f-8c85-ed37974052af": {"node_ids": ["8857792d-dd67-4049-8a81-4868c4eec805", "f63d9ba7-9141-4a28-9f2b-d5298112a4f8"], "metadata": {"page_label": "29", "file_name": "A cybersecurity AI agent selection and decision support framework.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\A cybersecurity AI agent selection and decision support framework.pdf", "file_type": "application/pdf", "file_size": 1115417, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "285e0372-1d38-4d5c-a3a0-5bc305c26aa1": {"node_ids": ["d730d98c-69ee-40a3-8d27-3d6ae993ac53", "57ac611c-2374-43f1-b2ec-1d68d7f69ecf"], "metadata": {"page_label": "30", "file_name": "A cybersecurity AI agent selection and decision support framework.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\A cybersecurity AI agent selection and decision support framework.pdf", "file_type": "application/pdf", "file_size": 1115417, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "b1c1759b-075a-4b9d-8632-3d6c8b8757e4": {"node_ids": ["faf46dba-5b94-4561-9b1d-6257c8e2f895", "e48d354d-824e-4e24-bdd6-61f14429cfb8"], "metadata": {"page_label": "31", "file_name": "A cybersecurity AI agent selection and decision support framework.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\A cybersecurity AI agent selection and decision support framework.pdf", "file_type": "application/pdf", "file_size": 1115417, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "db4b926c-c2fa-48b9-b971-d445cbb2a32d": {"node_ids": ["81aff390-b5e4-4853-b83f-5bda8c5237d1"], "metadata": {"page_label": "1", "file_name": "A dissemination workshop for introducing young Italian students to NLP.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\A dissemination workshop for introducing young Italian students to NLP.pdf", "file_type": "application/pdf", "file_size": 3675277, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "bfd03b52-815b-4501-a310-d4bcefc0cae7": {"node_ids": ["ff7157e1-2771-4895-8631-9df2d75eaa83"], "metadata": {"page_label": "2", "file_name": "A dissemination workshop for introducing young Italian students to NLP.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\A dissemination workshop for introducing young Italian students to NLP.pdf", "file_type": "application/pdf", "file_size": 3675277, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "e5d7d61a-3801-4a38-8bc1-dd5064fb1c22": {"node_ids": ["7e029afa-a2a8-4bf9-810e-a45009ec1bd3"], "metadata": {"page_label": "3", "file_name": "A dissemination workshop for introducing young Italian students to NLP.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\A dissemination workshop for introducing young Italian students to NLP.pdf", "file_type": "application/pdf", "file_size": 3675277, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "54c0018c-5d06-4f2d-b667-40a030539f52": {"node_ids": ["d22e6934-8eda-46b6-8517-caf4972fee3c"], "metadata": {"page_label": "1", "file_name": "ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 18789005, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "9b912e35-b125-42e8-be12-70fdb75056ad": {"node_ids": ["6b2df7ad-c417-479b-b13a-a3574c0e5432", "aecc4bee-8cc7-4d4d-8567-5dadbd103570"], "metadata": {"page_label": "2", "file_name": "ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 18789005, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "93fad366-2ad9-4df2-8f76-fe61187b2bce": {"node_ids": ["636a652b-b1c7-45a3-aa59-9fc0b81a0806", "19ff54e0-8e86-4fb5-8024-95be13cb6234"], "metadata": {"page_label": "3", "file_name": "ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 18789005, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "e75ddac4-8bcb-43a5-ac3a-a61218f561c3": {"node_ids": ["1f512c41-97df-4a6c-9194-f7dcb170b248"], "metadata": {"page_label": "4", "file_name": "ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 18789005, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "f6a7c0a4-f9d2-4aa8-9a41-e76cddf15c5f": {"node_ids": ["d32ec64c-1971-4029-a7a7-47186f07483d", "c8930588-0bdf-45a0-9568-f2b3a59d1b1a"], "metadata": {"page_label": "5", "file_name": "ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 18789005, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "9d2c7fb5-b6e0-4a53-8fa4-af7c73ed831a": {"node_ids": ["f579576f-3dbc-4dbd-8a80-318cebc52c5f"], "metadata": {"page_label": "6", "file_name": "ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 18789005, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "cb641677-5bfb-48b8-9e3c-20035e771c2f": {"node_ids": ["2be3c88a-7d34-46fc-ba01-99ad7ac78bca"], "metadata": {"page_label": "7", "file_name": "ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 18789005, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "47d7f859-a471-4a50-b2bb-c9ed011d899b": {"node_ids": ["5323d89a-2e66-4d57-b5ad-2fbeb11ece0e"], "metadata": {"page_label": "8", "file_name": "ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 18789005, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "3fd785be-8bb9-4078-aefb-20303d24b412": {"node_ids": ["c4dba48c-e801-451d-8aef-0a56bb377c3e", "26a79565-3381-45fe-8fc9-cc3634f4a7aa"], "metadata": {"page_label": "9", "file_name": "ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 18789005, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "fef459fe-9ea0-4559-a73f-25461bf42691": {"node_ids": ["47bb971b-a435-479c-b074-10081c2cdff4", "59a16964-71c1-4933-aadb-d6a80803bed8"], "metadata": {"page_label": "10", "file_name": "ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 18789005, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "1d654ea2-8161-40e8-b3a6-c024089c2824": {"node_ids": ["64c05e30-4a51-4f25-90eb-ffd528fc6f67", "ee81acce-1b2d-41ff-afe1-96786e9b4fd2"], "metadata": {"page_label": "11", "file_name": "ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 18789005, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "e2d4fbf6-11e7-49fd-b4ac-62173546f7a9": {"node_ids": ["fe902f5a-e6b8-4f09-9021-59f9eda26eea", "b489ef1c-b4b7-498e-a170-290fb373da18"], "metadata": {"page_label": "12", "file_name": "ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 18789005, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "5312f52d-471a-4c62-89b7-e62c0bb9c629": {"node_ids": ["f9514b9c-0be6-4e9e-9c02-2670b3a84890", "2e80c844-f3f3-4c57-9d71-530fd2c70210"], "metadata": {"page_label": "13", "file_name": "ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 18789005, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "3066ea3c-0d8a-4182-8bd3-c2df08edc055": {"node_ids": ["20b619fb-704f-4b06-97df-07a6417faf4c"], "metadata": {"page_label": "14", "file_name": "ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 18789005, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "66cb1223-55f7-46b6-948e-64b7c60f34e0": {"node_ids": ["1121b124-b685-45d2-869e-aa87e1d805cf"], "metadata": {"page_label": "15", "file_name": "ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 18789005, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "977d7711-45c1-4695-b982-1da44768bb07": {"node_ids": ["6ecf0ce4-36a4-4950-8b27-ba2925a00db5"], "metadata": {"page_label": "16", "file_name": "ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 18789005, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "c02d6c69-294b-470a-942d-4b12350a0dc1": {"node_ids": ["d7975a85-b489-45c5-9464-6d178db2937f"], "metadata": {"page_label": "17", "file_name": "ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 18789005, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "421542e0-89b9-4ac0-be2f-3f46dd755936": {"node_ids": ["f0c14dff-579d-4aa6-b557-609646e3d187"], "metadata": {"page_label": "18", "file_name": "ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 18789005, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "74817cbc-3a70-4f7e-afcf-319e94e9405c": {"node_ids": ["ca9b4ec0-af45-4100-8415-e91b88997757"], "metadata": {"page_label": "19", "file_name": "ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 18789005, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "c9639cdd-73e5-4d6e-b377-ce6712cb0b81": {"node_ids": ["095659a5-e835-49c9-88a5-6d805e252209"], "metadata": {"page_label": "20", "file_name": "ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 18789005, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "172d5992-a73d-4be5-a1e6-fb1f1ef24036": {"node_ids": ["038aa9a8-eda7-4a76-a321-7a3111fa3843"], "metadata": {"page_label": "21", "file_name": "ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 18789005, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "4b5a143f-9773-4431-a175-8cacbb178d4d": {"node_ids": ["c8537cde-2dc5-4bf0-bd57-d729dc28a90c"], "metadata": {"page_label": "22", "file_name": "ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 18789005, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "92aad19a-bea7-4108-a444-0579e31347d6": {"node_ids": ["13384d8f-78b1-4a39-a226-92d45f6438f1"], "metadata": {"page_label": "23", "file_name": "ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 18789005, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "bda5dfbd-7903-40b5-ba83-8e3df737970f": {"node_ids": ["aa9fe60e-26f8-4409-ad57-21771ca2f3b0"], "metadata": {"page_label": "24", "file_name": "ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 18789005, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "1d5bff59-7709-4441-a877-da551aa6e560": {"node_ids": ["de41b2a1-bb42-4f87-aa67-a19c14d90a55"], "metadata": {"page_label": "25", "file_name": "ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 18789005, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "b380c5f2-b67f-477b-9c58-c85eb4cd8879": {"node_ids": ["aca76b28-4d08-4817-9461-a646fb9c2d47"], "metadata": {"page_label": "26", "file_name": "ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 18789005, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "52030ca6-5ac9-426c-86af-f1b4f687e3da": {"node_ids": ["2c43f437-63d2-4def-aac7-0fb718e620b4"], "metadata": {"page_label": "27", "file_name": "ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 18789005, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "be9bc060-3ff2-4dd7-8069-31c76fb3da0b": {"node_ids": ["1f2dec87-169e-4b85-ab8e-25d65f598eb7"], "metadata": {"page_label": "28", "file_name": "ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 18789005, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "b2678f8e-18c8-4e4a-ab66-90bffda3334a": {"node_ids": ["9e34941c-a110-4bbc-be25-be12f0693b0c"], "metadata": {"page_label": "29", "file_name": "ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 18789005, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "0db00f16-1ab3-44b3-864f-239a8b6c0c83": {"node_ids": ["905dfce7-a1e7-4f3c-af86-f8e98f1f38d3"], "metadata": {"page_label": "30", "file_name": "ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 18789005, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "75ac6d6d-352b-4f98-a0bd-368c6ed4130d": {"node_ids": ["f7d4e4db-c9f0-4f00-83c8-a60cac82696d"], "metadata": {"page_label": "31", "file_name": "ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 18789005, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "e7c3358f-0526-4599-8e79-510d33e5ccb0": {"node_ids": ["4c3d9a1d-e919-4e2f-a20d-3e36318e751c"], "metadata": {"page_label": "32", "file_name": "ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 18789005, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "1202ddf4-35be-44e0-a50b-828eb2214f51": {"node_ids": ["47be159d-1115-48f6-9e8f-e694ce5475a2"], "metadata": {"page_label": "33", "file_name": "ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 18789005, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "40ab73bf-551a-4c7a-b187-2ab6de80c003": {"node_ids": ["ec754534-c0cd-477c-9935-9e9072ae23ee"], "metadata": {"page_label": "34", "file_name": "ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 18789005, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "2cbed6e4-0a7c-4751-a651-52749f1a6c5b": {"node_ids": ["cc9edfa4-b41c-4f8a-9dce-ef190a745511"], "metadata": {"page_label": "35", "file_name": "ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 18789005, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "719b18b2-98fd-42d5-851f-b599fda485c3": {"node_ids": ["9c24ac73-4ba8-4e98-a3c5-c0ab98e13e40"], "metadata": {"page_label": "36", "file_name": "ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 18789005, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "23f78ec7-dd81-4ceb-abe6-8758531c22b3": {"node_ids": ["26bd3f70-0b07-46d6-be7b-5a3d198382f6"], "metadata": {"page_label": "37", "file_name": "ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 18789005, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "ab8ffe0a-1b6e-4183-b065-c154033730a8": {"node_ids": ["8c0f5768-51d1-49a0-959d-24137b11d742"], "metadata": {"page_label": "38", "file_name": "ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 18789005, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "cd73c69f-1842-4df7-badf-dcd8a335ce67": {"node_ids": ["97c8df9d-f9fb-46f4-a96c-99fede51fc28"], "metadata": {"page_label": "39", "file_name": "ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 18789005, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "785f800a-a126-42a7-9386-bcedeec6db60": {"node_ids": ["82cf48be-d09d-43ea-b837-cfe29b706ca6"], "metadata": {"page_label": "40", "file_name": "ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 18789005, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "7464b228-9029-41fd-8004-b8efec212ad5": {"node_ids": ["b8117518-d915-4af7-8db3-328ecc5e68b3"], "metadata": {"page_label": "41", "file_name": "ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 18789005, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "63e2cf92-f789-4614-a348-79e8023ec8f9": {"node_ids": ["95815446-9a5c-4022-a768-b3cbc4581d16"], "metadata": {"page_label": "42", "file_name": "ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 18789005, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "3c43b511-d047-477b-9ed0-767b9e119d32": {"node_ids": ["98459118-1db2-4db3-b6b0-a4b7fe998470"], "metadata": {"page_label": "43", "file_name": "ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 18789005, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "9569d4d3-8096-4a1f-bff2-a0dbcf3f27fb": {"node_ids": ["3c85efd1-b1fd-4827-aee1-b6388a5bd45a"], "metadata": {"page_label": "44", "file_name": "ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 18789005, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "c0be8d2d-0c6b-483a-802f-cb32f8777943": {"node_ids": ["d87f3985-8d37-433a-9e3b-478f9d9918cd"], "metadata": {"page_label": "45", "file_name": "ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 18789005, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "e664eb65-e022-4f27-a078-8b36382578f3": {"node_ids": ["a0142fbc-da85-4fc1-b4ae-5e8f0a176d7a"], "metadata": {"page_label": "46", "file_name": "ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 18789005, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "e2b81dee-580f-459c-93d1-8675ef524f9c": {"node_ids": ["c7ff93f1-8506-4778-b8ac-ec6bb60716f3"], "metadata": {"page_label": "47", "file_name": "ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 18789005, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "0915ef3f-e0e8-434c-aaed-6f76249a9327": {"node_ids": ["638cc97e-3205-43c7-9318-35d21326b496"], "metadata": {"page_label": "48", "file_name": "ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 18789005, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "7fe2e34a-f0dc-4292-a292-0859f7f1af33": {"node_ids": ["d09c51d2-a16f-4467-8e52-87f38c5c8a19"], "metadata": {"page_label": "49", "file_name": "ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 18789005, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "78e70001-7d64-4d27-96d3-282a8683115f": {"node_ids": ["0ccfc0a3-f0ed-4b5c-8a2c-220beac42363"], "metadata": {"page_label": "50", "file_name": "ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 18789005, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "aa4f36f2-41f5-4dba-8939-ca44ceb388fd": {"node_ids": ["2d258e3b-4a67-4755-990a-80f4922545ad"], "metadata": {"page_label": "51", "file_name": "ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 18789005, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "d5581b74-0a29-461b-bf09-713c5c91b2fa": {"node_ids": ["93a1b5d1-ca47-4add-ae2e-7125d83c0346"], "metadata": {"page_label": "52", "file_name": "ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 18789005, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "9fd6f04d-b140-46e8-995e-5cecd157e86f": {"node_ids": ["a4d2e71b-c680-4609-bc9b-41fe242ea5c9"], "metadata": {"page_label": "53", "file_name": "ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 18789005, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "acf5c0ed-ee63-4cb0-8cfa-6cc1f117ce51": {"node_ids": ["5700acd5-cb6b-4fd6-988a-ebaf5cb8f7d8"], "metadata": {"page_label": "54", "file_name": "ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 18789005, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "f5a327cb-be6e-40af-9f97-03cf8d03f531": {"node_ids": ["0aac1b45-9757-4fc1-991f-1de25e023c02"], "metadata": {"page_label": "55", "file_name": "ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 18789005, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "e5956047-3129-4ab4-957d-3b78d6caa8fd": {"node_ids": ["840afaa9-710e-4d94-832e-fd8112d90102"], "metadata": {"page_label": "1", "file_name": "Causal-Paced Deep Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Causal-Paced Deep Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 741276, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "d4ffc73a-eef6-4665-af8c-3dcaaa015fa9": {"node_ids": ["33e73d7f-8b91-45c8-9f38-1c676279eb82"], "metadata": {"page_label": "2", "file_name": "Causal-Paced Deep Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Causal-Paced Deep Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 741276, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "00d68343-4b5d-4cf2-9fdf-009d363b8f77": {"node_ids": ["e0f8c644-ade6-4e71-b0af-6df46df74bcc"], "metadata": {"page_label": "3", "file_name": "Causal-Paced Deep Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Causal-Paced Deep Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 741276, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "42ea8556-166d-4f2b-9fbf-1ad18a2a669e": {"node_ids": ["4dd8fdc8-c9ba-4673-868c-62536cd5ae35", "8497417e-65b9-439c-8c6e-e223e194a739"], "metadata": {"page_label": "4", "file_name": "Causal-Paced Deep Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Causal-Paced Deep Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 741276, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "e578675a-57f3-4346-a9cf-63d534aeaf60": {"node_ids": ["304af189-e79f-41c3-ba62-c9005b3f9a55"], "metadata": {"page_label": "5", "file_name": "Causal-Paced Deep Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Causal-Paced Deep Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 741276, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "4cf34b23-5a92-4095-96d3-94605545a6aa": {"node_ids": ["361a193c-0f63-40e2-9ea9-7dc219c5cbc4"], "metadata": {"page_label": "6", "file_name": "Causal-Paced Deep Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Causal-Paced Deep Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 741276, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "3024380c-80b7-4d35-8d81-71c242fc3441": {"node_ids": ["5752b15b-86c9-4a3f-8997-312b7afdc63c"], "metadata": {"page_label": "7", "file_name": "Causal-Paced Deep Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Causal-Paced Deep Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 741276, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "d74fee4b-74a0-40f0-8eeb-f876277c7fca": {"node_ids": ["0b726b32-b5fb-48fa-953d-1f386693c4d8"], "metadata": {"page_label": "8", "file_name": "Causal-Paced Deep Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Causal-Paced Deep Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 741276, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "20b3b4bc-a2d9-469d-b6eb-3e4440044a36": {"node_ids": ["8d0560b4-4290-4e19-8566-4c23d608c96b"], "metadata": {"page_label": "9", "file_name": "Causal-Paced Deep Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Causal-Paced Deep Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 741276, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "2324ca46-bbb4-4a98-9c89-299fb48e765d": {"node_ids": ["88c42e7f-a908-4cb7-8026-0356b60bd717"], "metadata": {"page_label": "10", "file_name": "Causal-Paced Deep Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Causal-Paced Deep Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 741276, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "54d5c5c8-3556-48a1-9e79-2e3cb7c99224": {"node_ids": ["f82c81f7-a44e-431a-93fc-0f4b7493d9f1"], "metadata": {"page_label": "11", "file_name": "Causal-Paced Deep Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Causal-Paced Deep Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 741276, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "6e777095-602f-4dbb-8308-a3de47f1f1ca": {"node_ids": ["369136cc-11f6-42b5-9138-2393f21e36cd"], "metadata": {"page_label": "12", "file_name": "Causal-Paced Deep Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Causal-Paced Deep Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 741276, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "4f70a28c-6500-4f4d-a9e9-62ac94100ecf": {"node_ids": ["73fbaaef-fe9a-437a-9ba4-0f917b38ce34"], "metadata": {"page_label": "13", "file_name": "Causal-Paced Deep Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Causal-Paced Deep Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 741276, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "90663b36-1b16-4f13-9ceb-7f26ae82fb9e": {"node_ids": ["5994d7d9-bd15-4874-ab95-8b93f9702b4a"], "metadata": {"page_label": "14", "file_name": "Causal-Paced Deep Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Causal-Paced Deep Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 741276, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "6cbb9f05-e75a-471d-addd-56f8c12cae6c": {"node_ids": ["fafd2570-3a4d-4983-9123-022aa5d415ef"], "metadata": {"page_label": "15", "file_name": "Causal-Paced Deep Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Causal-Paced Deep Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 741276, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "218daabb-d00f-47ce-b417-5953b7da3bdf": {"node_ids": ["fdf80ab7-8d16-474d-8843-06066360b0ab"], "metadata": {"page_label": "16", "file_name": "Causal-Paced Deep Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Causal-Paced Deep Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 741276, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "c7426651-9112-4859-bd12-e48f074a35c8": {"node_ids": ["a3dd4d5d-f172-46e6-b86b-7ad9d5abd1df"], "metadata": {"page_label": "17", "file_name": "Causal-Paced Deep Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Causal-Paced Deep Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 741276, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "d3c910f2-7973-491a-93b7-66c2551f9c71": {"node_ids": ["11e154ef-0913-409f-af3a-b0740c10a48e"], "metadata": {"page_label": "18", "file_name": "Causal-Paced Deep Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Causal-Paced Deep Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 741276, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "ab6e80be-7b67-4943-ad37-ae28fcef98f4": {"node_ids": ["2a791035-2164-44e6-a8ac-63f959bab642"], "metadata": {"page_label": "1", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "d8b67d49-0086-4fb6-8c74-d1f947b98013": {"node_ids": ["536f8afd-cc56-496e-a997-f1eccd13d1eb", "bd4a7088-2165-4501-8e78-bb39ed23b385", "8ebc271f-4e93-404e-a65f-827cbebd235b"], "metadata": {"page_label": "1", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "7b547050-93f2-4e56-87a1-d88f740d3255": {"node_ids": ["25bfcfa7-6c79-419e-9d1b-41c88f5723b6", "8216b001-23d0-494c-a426-3dcbe88cc54f", "e9e45378-07d2-41da-bfcb-4a5d282c4ebf", "361c2b8e-de61-4f90-8a25-83c6ebe308c3"], "metadata": {"page_label": "2", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "39681515-8c85-4abb-ab62-c4d0f0977426": {"node_ids": ["ae8c764a-8be7-4380-883c-3e8105993751"], "metadata": {"page_label": "3", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "86094565-3380-4e17-9a7a-493cbe5da686": {"node_ids": ["d7e9081e-e29d-441f-b2a9-aa6509a8db9a"], "metadata": {"page_label": "4", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "7447810b-a508-49de-b11a-06cce427985d": {"node_ids": ["2a3d6128-83ba-4206-b97b-0596ab3d51b3"], "metadata": {"page_label": "5", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "63eea7af-fb3c-447e-a615-96c7fa6bd98e": {"node_ids": ["4e7a4ac9-8f75-4056-897e-35cfc16e073a"], "metadata": {"page_label": "6", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "12e1ca77-8426-4d3d-ab10-3879b17971e5": {"node_ids": ["100c7829-edde-4cb0-bbd2-03324a0100c6"], "metadata": {"page_label": "7", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "0d854286-079e-4779-bef3-d571491f4ce4": {"node_ids": ["c637d880-3c53-44a8-87f3-ab72b6c2d5be"], "metadata": {"page_label": "8", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "d8e6f09a-1c6f-4d08-a08e-608525b982f1": {"node_ids": ["1ddcc79c-d999-4448-b698-49fd13bf9bbb"], "metadata": {"page_label": "9", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "1e08d2fe-8ac0-4f64-a56b-0decdb99d71c": {"node_ids": ["f27f5465-af2e-447d-a6a8-bea50657e0b6"], "metadata": {"page_label": "10", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "a63198f0-3d82-4093-9c75-253d3fa4b043": {"node_ids": ["9645e3c7-3ed6-46dc-b693-c114f5ddad7a"], "metadata": {"page_label": "11", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "102c4efc-eeb7-41c8-b875-39ba595955a3": {"node_ids": ["19b0c44c-2fc0-409c-a866-743222f5eea9"], "metadata": {"page_label": "12", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "36f33a13-8462-4ceb-8ee2-4c23704a09ff": {"node_ids": ["bb111975-0519-46e5-bb2a-d56d98ba2bf7"], "metadata": {"page_label": "13", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "f450d412-a1d5-4f30-a705-4d92ca30de83": {"node_ids": ["f0789c36-bc8b-4ffa-8f2b-da3c70ab7a1b"], "metadata": {"page_label": "14", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "7db9e367-ee68-4ea4-82f9-d99b87cef3c1": {"node_ids": ["80235b9c-74bb-40af-a3b7-33f3c6327b85"], "metadata": {"page_label": "15", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "923e5c9b-accb-4571-8cd8-11d062542bd3": {"node_ids": ["a48afe87-044f-4bba-926a-1f523e338f27"], "metadata": {"page_label": "16", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "dc0734d4-8eb5-4574-a651-0aede047b15a": {"node_ids": ["34502f2c-174f-4b24-9295-7467d177b6cd"], "metadata": {"page_label": "17", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "9bb82254-c236-482c-94b0-1bd5f94184c2": {"node_ids": ["5bec7abe-352f-4107-8b8f-ff3c752f2370"], "metadata": {"page_label": "18", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "8b0e73f7-027b-4e4e-92b7-20031e68e0cb": {"node_ids": ["831b9a68-e85f-4bd8-9fd7-8bbfa093bb43"], "metadata": {"page_label": "19", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "fd325f97-52db-439d-8df0-2598c173c248": {"node_ids": ["482fe824-8664-48ec-8fca-96ea7735c5eb"], "metadata": {"page_label": "20", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "00e6d59c-b9ce-4256-a537-4c7c3657ef7d": {"node_ids": ["abcd1762-d6e1-4d55-a86c-34290d7f1b8c"], "metadata": {"page_label": "21", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "cdff2e2b-da73-41cf-bbb7-68fb83ce9f53": {"node_ids": ["a8ae107b-a548-4de3-a049-7ead7caa8ed7"], "metadata": {"page_label": "22", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "c4355eb6-0952-4918-b144-08ff2a7ea3a9": {"node_ids": ["d6bfd81a-c863-4b8e-af80-40c16879f183"], "metadata": {"page_label": "23", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "ef9e6651-081a-419e-86b0-c3051cce4eb3": {"node_ids": ["022752de-e320-470b-a41b-0dc4b8cf0b97", "2b1781e4-210a-4576-ad01-36c934cc55b7"], "metadata": {"page_label": "24", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "7f699359-86b4-4892-944d-e1fda009bbaa": {"node_ids": ["c777173e-4af4-4e3a-87da-c28860e2d567"], "metadata": {"page_label": "25", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "62d28a5d-4e62-41fa-9a37-8bc1fc70066c": {"node_ids": ["55618c4c-92da-41a1-9d8e-54120568517f"], "metadata": {"page_label": "26", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "54337252-5c91-4a4a-9966-d6d892be9a9f": {"node_ids": ["1c4569f2-fff6-489f-900b-dc84675ade87"], "metadata": {"page_label": "27", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "325ed61c-1ee0-4c24-bde5-695dbbf99d96": {"node_ids": ["bb5f773b-29f0-4fc1-a00b-4661f850c476"], "metadata": {"page_label": "28", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "8254e844-bd0f-47b6-90a8-e361f55c579f": {"node_ids": ["ae9994b3-ac8a-4c3f-8751-51346607356c"], "metadata": {"page_label": "29", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "94fb958a-6867-40bb-a62f-e156eb0c0119": {"node_ids": ["aca267f2-a652-45d4-80aa-79cae64b1695"], "metadata": {"page_label": "30", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "cf56fe60-877b-4154-9fd0-a6bd4f1231da": {"node_ids": ["1eff1206-0600-4273-9acd-fbcb5c7e2be9"], "metadata": {"page_label": "31", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "ebafa02d-1bdc-4ff3-82eb-e033d21a3137": {"node_ids": ["3239efe0-81cb-4380-868b-752a7b399e2e"], "metadata": {"page_label": "32", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "3db175be-3dcf-416a-afa8-a347105a3aff": {"node_ids": ["34694811-b364-4f27-b3c7-205b46b8f85c"], "metadata": {"page_label": "33", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "d5ace620-0092-4fe9-8466-619627db533f": {"node_ids": ["13902d1f-614e-4535-8fc3-4642c28b0ff9"], "metadata": {"page_label": "34", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "9f0dcd29-a46d-4b5d-8484-03029a0dc288": {"node_ids": ["28a429c9-dfaf-4d34-b3fe-9c19b7a0172d"], "metadata": {"page_label": "35", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "f1c3f67d-9095-40f1-883b-5ae1a91d501f": {"node_ids": ["32c18a82-0324-44cc-a9a2-aa23bb0c7e6d"], "metadata": {"page_label": "36", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "861cc119-4f77-4d3b-97e8-4ce7edcbc432": {"node_ids": ["9b24daae-d643-4a73-87ee-0e1aa7117ef2"], "metadata": {"page_label": "37", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "569d4a4b-d97b-4a12-9c5d-a21b5c157e37": {"node_ids": ["cccd916d-b521-40b0-916d-ae9a24877e8b"], "metadata": {"page_label": "38", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "3c742491-f432-4f86-89e9-016bc1550d37": {"node_ids": ["918b2ba1-0584-406f-b3b4-381b8fc62a99", "8b19bb8a-cc74-48a1-a1d6-c5648cb11db6"], "metadata": {"page_label": "39", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "1bb1a9bd-d279-4390-94dd-f0097b72bc27": {"node_ids": ["199b9f94-0f7d-4c1b-af7d-546b9729cc3b"], "metadata": {"page_label": "40", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "ec2c7748-1085-4cb4-820d-ce6271e84277": {"node_ids": ["3348b843-0260-4877-9b39-5e692cc0586c"], "metadata": {"page_label": "41", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "b535cfb4-12b0-4197-9250-377620fae3e4": {"node_ids": ["974222bc-6c9d-46b2-b80e-e533482200a2", "085d1224-3e86-45e0-a95f-dc4ab6e24987"], "metadata": {"page_label": "42", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "9156c4f1-b290-44e0-8d1e-24d2e26909ed": {"node_ids": ["23696645-d29f-47e8-b676-cdd0e90e7674"], "metadata": {"page_label": "43", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "18ff8af6-4eb7-4aa0-a944-3ea2d01da36e": {"node_ids": ["b724ab26-df5e-49f1-9267-3a737c5e117c"], "metadata": {"page_label": "44", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "4fdb394b-7948-454d-b9a2-8909e7f62ca6": {"node_ids": ["4bf2f767-6be2-4db0-85e4-2098f6478a3f"], "metadata": {"page_label": "45", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "d0bf7c98-55e5-4c65-a3e7-758fbd108e46": {"node_ids": ["72a1c232-1cd6-453a-a576-eb6b956b7a3f"], "metadata": {"page_label": "46", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "e2d4d540-8b29-4b34-b448-408686daa67e": {"node_ids": ["6b613688-b1d8-4ea4-b159-dea680b50df9"], "metadata": {"page_label": "47", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "54351abb-e44c-4f09-b0b2-bd52fe977ba1": {"node_ids": ["6e432994-c9a8-4467-ae64-8cf787f41453"], "metadata": {"page_label": "48", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "15c15b76-a7b1-43fa-a92d-9cf86445052d": {"node_ids": ["5dd5afb7-c496-4dad-8131-86b0d8edf44b"], "metadata": {"page_label": "49", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "e7fd7b48-74f2-4cf9-89ff-f8dd86e55b9e": {"node_ids": ["ee936b87-dd41-44c6-b38c-b557bb052e45"], "metadata": {"page_label": "50", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "1cdf99cf-eb30-423e-a995-98433423add4": {"node_ids": ["eb09f623-cf9a-4f24-a46e-8b093ac0ba52"], "metadata": {"page_label": "51", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "69f216c0-09b8-4677-ba8c-ab93527c0eea": {"node_ids": ["83d1ff0c-c425-46a2-9bcb-abe89cef003d"], "metadata": {"page_label": "52", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "c57292c7-148f-44be-be23-551f81f6a484": {"node_ids": ["670c5bea-a3ab-4db6-843f-3198dd2518c3"], "metadata": {"page_label": "53", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "9b8101e8-9e2c-4d3b-838c-4a9e624a3c80": {"node_ids": ["f3284bd5-4742-49bc-b322-fab3b81f223a"], "metadata": {"page_label": "54", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "335b341f-1b61-47d9-ad56-aabd3f0e18ac": {"node_ids": ["a335367e-9c89-4ca5-965a-2db86a918a83"], "metadata": {"page_label": "55", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "69a1b0b1-754a-47da-b8f1-688d44bf219e": {"node_ids": ["85086b2c-3cdd-4e68-8a99-5803d67abfc6"], "metadata": {"page_label": "56", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "becc8b28-a7cd-43fd-9e3e-bed107ead8e3": {"node_ids": ["fa38f147-4f8c-425e-a520-9c1e959c1cf7"], "metadata": {"page_label": "57", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "381593af-638e-45b3-8a3c-bf260d76b812": {"node_ids": ["9486bcc0-8330-4407-a7f8-effa09ab135c"], "metadata": {"page_label": "58", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "fff1f62d-f8d2-4ffc-bd7d-bc745aa724e5": {"node_ids": ["9e57ffa4-8266-4c97-aaa3-39948246437e"], "metadata": {"page_label": "59", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "b0f7085f-6fab-48ac-b13d-84caeb9733b8": {"node_ids": ["f3a0c624-0b0e-41f4-bafd-03bd91b5a3c7"], "metadata": {"page_label": "60", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "edafc58b-3767-4a84-8a55-46c512ba63f6": {"node_ids": ["d7c75882-374e-4608-9b47-e6b37590f4b3"], "metadata": {"page_label": "61", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "015de721-0ca6-435e-a2e8-19ac57134a83": {"node_ids": ["ff2d0a97-48d4-480f-b8e2-ae19b94f4278"], "metadata": {"page_label": "62", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "d3ae84b9-c424-4056-92d8-ca476d79952c": {"node_ids": ["85aab95b-e950-4ade-b8a9-810611a92c55"], "metadata": {"page_label": "63", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "9a2abde5-e80a-4a3e-8de7-8319bf297e4e": {"node_ids": ["3b8cac5b-6ffa-4707-9ae6-9f54417057ce"], "metadata": {"page_label": "64", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "ce09470b-27bf-486e-9cc6-50afc19c9344": {"node_ids": ["0bcb1d91-96c7-483c-852b-4fede3a3a011"], "metadata": {"page_label": "65", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "1ac9952c-8e56-4f42-bc03-89499d61f274": {"node_ids": ["77737234-7603-4fd3-9f55-efd32080cd45"], "metadata": {"page_label": "66", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "dc5d61f6-8449-4666-8803-ec6b05e4624c": {"node_ids": ["0e3cdeea-6308-41d6-89c8-b07d1766002c"], "metadata": {"page_label": "67", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "5004794f-8fad-4cfa-873c-afee88c05512": {"node_ids": ["630c7b20-7592-4575-966f-bf66e5679b26", "717fdeb9-3400-458b-9f3f-c6dd08e4d794"], "metadata": {"page_label": "68", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "fa380b50-86e2-4706-b4df-51e92f1485e3": {"node_ids": ["20fa629d-cbf9-4446-8dc1-6360b64098cb"], "metadata": {"page_label": "69", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "a8e6f383-662c-432d-bee7-b76c229710bb": {"node_ids": ["c1df10ca-9475-48ae-96bc-a843073e20c2"], "metadata": {"page_label": "70", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "a20ad3c1-4e85-4a6b-8f98-0120e65fcbaa": {"node_ids": ["424ddae3-fcfe-47ea-b4fc-e85206c08df8"], "metadata": {"page_label": "71", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "826dc389-ac19-4651-a617-723e560d5274": {"node_ids": ["2bb98602-e045-4794-ac3b-121321276418"], "metadata": {"page_label": "72", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "13946762-11ea-4dfa-8b54-aa0c1d1182d6": {"node_ids": ["43f4798f-f176-4d13-b629-e1c81f922ecf"], "metadata": {"page_label": "73", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "05d16aed-5cf9-47c5-a726-0078f2338d97": {"node_ids": ["d81d0f14-6eb4-49b0-9d8d-a3e5478ecfff"], "metadata": {"page_label": "74", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "64a04ac2-ba78-4ded-af50-1cc8ac85cae4": {"node_ids": ["2371ed6e-074a-444b-97ba-bc07ced68a98"], "metadata": {"page_label": "75", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "00447671-771d-4dd4-a593-08d6c6461507": {"node_ids": ["003c077d-1796-4738-a3c3-55f88ba0db54"], "metadata": {"page_label": "76", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "69e12c8d-b042-4093-b98a-68474f1fa878": {"node_ids": ["c6c6f28c-af97-49a7-bb44-22a87a15232c"], "metadata": {"page_label": "77", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "da936525-a75c-4d6f-8a38-dda6c858d471": {"node_ids": ["94a79259-faa5-4eba-a8f8-954f86b7f338"], "metadata": {"page_label": "78", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "88fd3487-c9a5-4186-8451-f12151bc2d5d": {"node_ids": ["fa5c73c6-1a08-485c-8677-e20dc1e13cc3"], "metadata": {"page_label": "79", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "cc7f1185-7492-4cfe-9125-de2d18d7c034": {"node_ids": ["16143dfc-e6f4-48ba-b8fc-07b9f23b4ae1"], "metadata": {"page_label": "80", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "b9d45763-1d10-42ab-bfa2-6de099eeb82b": {"node_ids": ["af72a553-f066-4d2a-bf94-60ca58d8be9d"], "metadata": {"page_label": "81", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "b4fd648e-8a8d-485d-9c52-882d80a446f6": {"node_ids": ["36913ad0-643d-438e-8656-f4882cb3e7cb"], "metadata": {"page_label": "82", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "00ad8487-36d0-4826-bcbd-6b03fb8dfebc": {"node_ids": ["bc1c2bc5-8655-4af5-9e9d-e9019dfb7b60"], "metadata": {"page_label": "83", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "3d8f54e0-fa0b-4c5f-8438-a58023c3b962": {"node_ids": ["aa0c9a56-eff2-49dd-85e0-07a8a93069aa"], "metadata": {"page_label": "84", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "7a756b8f-0693-4e36-973f-219388407f63": {"node_ids": ["144253ef-9821-4751-9f5f-0dde630deaba"], "metadata": {"page_label": "85", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "67691e55-6477-4441-9f2d-3624e6feaeda": {"node_ids": ["5d55c68c-e41a-4adb-85d2-80b3cfecc8ac"], "metadata": {"page_label": "86", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "9187776a-cf65-4167-813c-c2e4f2f04fcb": {"node_ids": ["c76a81d4-d386-4d3a-a588-9d076ea25bf7"], "metadata": {"page_label": "87", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "f7101bac-4f92-4921-9f37-b74d5f74b7b8": {"node_ids": ["6ed1605a-a802-4fc5-9e1c-15e594141d40", "b533a28a-3ab8-421e-ba9d-2f1998af6b59"], "metadata": {"page_label": "1", "file_name": "Exploring Hierarchy-Aware Inverse Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Exploring Hierarchy-Aware Inverse Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 850491, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "2f03bb22-cfe0-4c92-8542-b26b0ff63df7": {"node_ids": ["d85c658f-63b2-455d-b5d7-cf55ab85312f", "3563d0fa-8486-45d4-ab88-a5dec22571d9"], "metadata": {"page_label": "2", "file_name": "Exploring Hierarchy-Aware Inverse Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Exploring Hierarchy-Aware Inverse Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 850491, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "e483e7bb-9482-43cf-8a6e-9fb7ecf842b1": {"node_ids": ["88d1cc86-e414-4d69-a36c-97779c8c1936", "40ef311c-1225-444e-af32-41caf868b7e5"], "metadata": {"page_label": "3", "file_name": "Exploring Hierarchy-Aware Inverse Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Exploring Hierarchy-Aware Inverse Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 850491, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "a701a624-44d3-4760-b045-f85ac38bcdfb": {"node_ids": ["978b2435-7c45-4a6d-881a-b1ccc8752393", "53620188-954c-4559-a6bb-716c35af91ce"], "metadata": {"page_label": "4", "file_name": "Exploring Hierarchy-Aware Inverse Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Exploring Hierarchy-Aware Inverse Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 850491, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "6cfb97aa-3b95-434f-933e-3f6d7b96ef4d": {"node_ids": ["e59a67ec-9086-4f9e-be0a-17939446ca48", "2ee96738-d045-4805-99e1-a6f65be8a03f"], "metadata": {"page_label": "5", "file_name": "Exploring Hierarchy-Aware Inverse Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Exploring Hierarchy-Aware Inverse Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 850491, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "fc9ca98c-5219-4d03-84aa-68573f75be36": {"node_ids": ["42c7a770-8911-44c9-ba64-ff61dbffc0f5", "578b9fb6-e009-4823-a130-8b367d3fb4f4"], "metadata": {"page_label": "6", "file_name": "Exploring Hierarchy-Aware Inverse Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Exploring Hierarchy-Aware Inverse Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 850491, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "b8d680df-8f79-42be-a9e0-b6d1ac8d377e": {"node_ids": ["76e50d05-b188-4184-af94-23eca4412709"], "metadata": {"page_label": "7", "file_name": "Exploring Hierarchy-Aware Inverse Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Exploring Hierarchy-Aware Inverse Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 850491, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "6d377f55-0a40-4441-bea7-d3d008dc73de": {"node_ids": ["3979d82e-e9db-4cca-9437-9d23c8ef8ecb", "3b7475b6-c6eb-49fd-ae2f-cdc66965437e"], "metadata": {"page_label": "8", "file_name": "Exploring Hierarchy-Aware Inverse Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Exploring Hierarchy-Aware Inverse Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 850491, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "4ca49f59-d30e-44d1-88ba-56e61c5fec91": {"node_ids": ["99ef0f17-ec63-4e3f-b57b-2092c34a6cee"], "metadata": {"page_label": "9", "file_name": "Exploring Hierarchy-Aware Inverse Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Exploring Hierarchy-Aware Inverse Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 850491, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "b12c88d4-4ca3-409c-ab0f-a2311e307575": {"node_ids": ["1f6b1282-3467-4b3d-ab05-b18b90cd6d94"], "metadata": {"page_label": "1", "file_name": "Foundations of GenIR.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Foundations of GenIR.pdf", "file_type": "application/pdf", "file_size": 699017, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "6449787e-09fe-4a04-b4d7-928c95cc64fd": {"node_ids": ["0b544a7c-a71e-4705-887f-420c244dc31c"], "metadata": {"page_label": "2", "file_name": "Foundations of GenIR.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Foundations of GenIR.pdf", "file_type": "application/pdf", "file_size": 699017, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "ca4cf3c9-fdcb-4553-96d9-17729835c1d8": {"node_ids": ["590058da-4478-4de4-968b-718fd3e0e07f"], "metadata": {"page_label": "3", "file_name": "Foundations of GenIR.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Foundations of GenIR.pdf", "file_type": "application/pdf", "file_size": 699017, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "525bdc5d-41de-4064-85e1-f65b2263582e": {"node_ids": ["c5a905af-93ca-43db-98aa-d2babbc59a5f"], "metadata": {"page_label": "4", "file_name": "Foundations of GenIR.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Foundations of GenIR.pdf", "file_type": "application/pdf", "file_size": 699017, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "a2f2ca14-c42c-4e98-8b29-c24c33642c77": {"node_ids": ["18047a63-286a-4b95-87da-318c0594ff66"], "metadata": {"page_label": "5", "file_name": "Foundations of GenIR.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Foundations of GenIR.pdf", "file_type": "application/pdf", "file_size": 699017, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "83b52b8f-4a88-4463-8c40-c6fe192e7996": {"node_ids": ["c7d15724-c13a-4d04-b039-8fd07a79fb2a"], "metadata": {"page_label": "6", "file_name": "Foundations of GenIR.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Foundations of GenIR.pdf", "file_type": "application/pdf", "file_size": 699017, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "b2f66f50-9bf8-461f-859d-e07db1006098": {"node_ids": ["51996d81-b287-48e2-8b83-3ba979fb3a17"], "metadata": {"page_label": "7", "file_name": "Foundations of GenIR.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Foundations of GenIR.pdf", "file_type": "application/pdf", "file_size": 699017, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "72723e0a-6f18-4d27-99b7-de3cc6ae58af": {"node_ids": ["2d2ce17f-39f4-4596-ae29-7085bbd3b4af"], "metadata": {"page_label": "8", "file_name": "Foundations of GenIR.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Foundations of GenIR.pdf", "file_type": "application/pdf", "file_size": 699017, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "df290570-5fc0-4389-a1eb-7897e876af3d": {"node_ids": ["44b285b6-89c7-41b3-9ecf-ffb64dad2419"], "metadata": {"page_label": "9", "file_name": "Foundations of GenIR.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Foundations of GenIR.pdf", "file_type": "application/pdf", "file_size": 699017, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "380d8a71-ad8a-46db-9653-7526828b4e56": {"node_ids": ["41996106-4dd8-49b7-ac40-2a3375ed63d0"], "metadata": {"page_label": "10", "file_name": "Foundations of GenIR.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Foundations of GenIR.pdf", "file_type": "application/pdf", "file_size": 699017, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "6df5af8d-7975-48ad-af80-1e1836876ef9": {"node_ids": ["fe6ec668-fdc8-4808-9ea4-7956f6096185"], "metadata": {"page_label": "11", "file_name": "Foundations of GenIR.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Foundations of GenIR.pdf", "file_type": "application/pdf", "file_size": 699017, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "329477dc-805e-4977-af4d-a763da688782": {"node_ids": ["0bf60834-6a4e-40b8-8f7d-d55185dc9c71"], "metadata": {"page_label": "12", "file_name": "Foundations of GenIR.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Foundations of GenIR.pdf", "file_type": "application/pdf", "file_size": 699017, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "42718cef-5d44-42a7-82b5-7f4fe3085b17": {"node_ids": ["151c90ef-8368-4361-bf03-1a8da75f2f5d"], "metadata": {"page_label": "13", "file_name": "Foundations of GenIR.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Foundations of GenIR.pdf", "file_type": "application/pdf", "file_size": 699017, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "813bf178-50a6-4a4a-9fd5-86b059566ab4": {"node_ids": ["7818ea45-b4ad-4e9b-89d6-26084a8d09fe"], "metadata": {"page_label": "14", "file_name": "Foundations of GenIR.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Foundations of GenIR.pdf", "file_type": "application/pdf", "file_size": 699017, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "669fa008-2820-464d-a69a-ae0af5251564": {"node_ids": ["0661582b-9d37-4292-959c-b4ef5d494e34"], "metadata": {"page_label": "15", "file_name": "Foundations of GenIR.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Foundations of GenIR.pdf", "file_type": "application/pdf", "file_size": 699017, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "b5d9ed9f-db03-4238-955e-7dd3e87b2f6a": {"node_ids": ["f611d476-7408-4084-8294-504b5344f102"], "metadata": {"page_label": "16", "file_name": "Foundations of GenIR.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Foundations of GenIR.pdf", "file_type": "application/pdf", "file_size": 699017, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "5f66b373-401d-44cf-b190-2d29d945b673": {"node_ids": ["c5b7aa51-5701-4f22-b03e-81657e369dcb"], "metadata": {"page_label": "17", "file_name": "Foundations of GenIR.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Foundations of GenIR.pdf", "file_type": "application/pdf", "file_size": 699017, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "a87cca8c-519c-4df0-92d9-65e171968f32": {"node_ids": ["ffdb55d8-66c7-4305-8217-160b4ecf2093"], "metadata": {"page_label": "18", "file_name": "Foundations of GenIR.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Foundations of GenIR.pdf", "file_type": "application/pdf", "file_size": 699017, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "3e106a48-5437-4ade-a8cd-5d91545a38be": {"node_ids": ["45485170-99f4-4d02-a6a9-94ba57f4e6af"], "metadata": {"page_label": "19", "file_name": "Foundations of GenIR.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Foundations of GenIR.pdf", "file_type": "application/pdf", "file_size": 699017, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "b03bb92d-f069-4964-976d-f515fe1a2f0a": {"node_ids": ["8dca77ab-e30c-4d4b-b1ee-a4cf14f9c978"], "metadata": {"page_label": "20", "file_name": "Foundations of GenIR.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Foundations of GenIR.pdf", "file_type": "application/pdf", "file_size": 699017, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "9fa45690-f2cc-450e-9f1a-d4de1a061488": {"node_ids": ["19652b9b-5aa7-4627-ba76-cc4c071ce05c"], "metadata": {"page_label": "21", "file_name": "Foundations of GenIR.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Foundations of GenIR.pdf", "file_type": "application/pdf", "file_size": 699017, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "9f4973cb-4178-4156-b9aa-ecade5f50bd0": {"node_ids": ["76b8dba6-f235-4056-8d3b-9f078718c2ad"], "metadata": {"page_label": "22", "file_name": "Foundations of GenIR.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Foundations of GenIR.pdf", "file_type": "application/pdf", "file_size": 699017, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "dd634a05-fdfc-411f-ab90-5ec2396cb6b8": {"node_ids": ["068b3422-3149-46d2-8d67-a40a1246fdb9"], "metadata": {"page_label": "23", "file_name": "Foundations of GenIR.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Foundations of GenIR.pdf", "file_type": "application/pdf", "file_size": 699017, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "dffb8c57-0d5c-4fa8-8cb7-7640c078dc1c": {"node_ids": ["b1130f0c-ba1d-43da-84ea-50a98b198609"], "metadata": {"page_label": "24", "file_name": "Foundations of GenIR.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Foundations of GenIR.pdf", "file_type": "application/pdf", "file_size": 699017, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "46104237-96f5-4c1b-aa16-7bc7349188b8": {"node_ids": ["52e5970d-a7bd-412c-9aa5-e3f468121357"], "metadata": {"page_label": "25", "file_name": "Foundations of GenIR.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Foundations of GenIR.pdf", "file_type": "application/pdf", "file_size": 699017, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "2fbeedce-cd28-4e24-991d-9223580ff5b3": {"node_ids": ["c95842e3-fde7-41ce-8713-890b29df36b0"], "metadata": {"page_label": "26", "file_name": "Foundations of GenIR.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Foundations of GenIR.pdf", "file_type": "application/pdf", "file_size": 699017, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "a0b6ff7f-2024-45e0-98f1-1e9e02537530": {"node_ids": ["39e23b5f-9fbd-463d-b1dc-f1f2fcf12038"], "metadata": {"page_label": "27", "file_name": "Foundations of GenIR.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Foundations of GenIR.pdf", "file_type": "application/pdf", "file_size": 699017, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "c4f5e536-9a83-4c5f-8176-3303f5eaf92a": {"node_ids": ["0a7e44d4-c747-4d1a-b3fc-a6573bf9c2a6"], "metadata": {"page_label": "28", "file_name": "Foundations of GenIR.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Foundations of GenIR.pdf", "file_type": "application/pdf", "file_size": 699017, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "af6125f2-2c55-40d5-aeb1-bf13c2fce76e": {"node_ids": ["ce1a5097-643e-4633-8e68-e8ec7c8eece6"], "metadata": {"page_label": "29", "file_name": "Foundations of GenIR.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Foundations of GenIR.pdf", "file_type": "application/pdf", "file_size": 699017, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "5381dcf4-c1b0-499b-a167-54df327ca060": {"node_ids": ["56aab57c-0ba3-40ee-8bb8-225965c8e425"], "metadata": {"page_label": "30", "file_name": "Foundations of GenIR.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Foundations of GenIR.pdf", "file_type": "application/pdf", "file_size": 699017, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "5821c050-5ce0-47f9-adc9-b45e73016fe8": {"node_ids": ["31fc9742-1f40-4124-b401-9b8aa91dd598"], "metadata": {"page_label": "31", "file_name": "Foundations of GenIR.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Foundations of GenIR.pdf", "file_type": "application/pdf", "file_size": 699017, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "6edf2408-9b59-4f48-9b3a-ca94afa1c417": {"node_ids": ["5abca84e-f967-4f7c-b65e-a3d7987552a9"], "metadata": {"page_label": "32", "file_name": "Foundations of GenIR.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Foundations of GenIR.pdf", "file_type": "application/pdf", "file_size": 699017, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "5b0dbe2e-512b-4045-940d-f1960e5272d4": {"node_ids": ["7abe67c2-2431-495d-8fb4-cd8b18963119"], "metadata": {"page_label": "33", "file_name": "Foundations of GenIR.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Foundations of GenIR.pdf", "file_type": "application/pdf", "file_size": 699017, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "8692766b-4ae5-4305-9005-2f1ab011afd7": {"node_ids": ["5a1da780-6669-4049-8874-ed80e232a40c"], "metadata": {"page_label": "34", "file_name": "Foundations of GenIR.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Foundations of GenIR.pdf", "file_type": "application/pdf", "file_size": 699017, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "b8757888-f049-404c-8c99-07bc1e9bfb24": {"node_ids": ["320ee3c1-0dc6-43d2-84d8-f6ca288b1f0a"], "metadata": {"page_label": "35", "file_name": "Foundations of GenIR.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Foundations of GenIR.pdf", "file_type": "application/pdf", "file_size": 699017, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "91966957-7747-45bd-a03f-d0c43453dedb": {"node_ids": ["27415eb5-3f0a-4bba-96f3-d8faa6592550", "0a59d981-25f3-4ae7-aece-52b512e55688"], "metadata": {"page_label": "1", "file_name": "GR-NLP-TOOLKIT An Open-Source NLP Toolkit for Modern Greek.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\GR-NLP-TOOLKIT An Open-Source NLP Toolkit for Modern Greek.pdf", "file_type": "application/pdf", "file_size": 556151, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "74eb1b8c-9d9e-4397-be86-3f3612bedf83": {"node_ids": ["b8ff471f-e757-4b86-b535-6c994edcfcdb", "3c5315f1-3832-44df-924b-85f89aa9ea0d"], "metadata": {"page_label": "2", "file_name": "GR-NLP-TOOLKIT An Open-Source NLP Toolkit for Modern Greek.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\GR-NLP-TOOLKIT An Open-Source NLP Toolkit for Modern Greek.pdf", "file_type": "application/pdf", "file_size": 556151, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "80855d06-ef81-4500-9c04-40b832e4221d": {"node_ids": ["c51b41cf-f6e8-4090-b421-6968114f6eb5", "d57973d9-67ab-4e57-8c6a-e75a8fbe5858"], "metadata": {"page_label": "3", "file_name": "GR-NLP-TOOLKIT An Open-Source NLP Toolkit for Modern Greek.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\GR-NLP-TOOLKIT An Open-Source NLP Toolkit for Modern Greek.pdf", "file_type": "application/pdf", "file_size": 556151, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "38693a26-8c55-4b11-b7eb-b602c5d8efdf": {"node_ids": ["0b17b6fc-5626-41cf-a0b7-318620e50530", "a4ad48a1-d891-4906-9039-e1a37dce7374"], "metadata": {"page_label": "4", "file_name": "GR-NLP-TOOLKIT An Open-Source NLP Toolkit for Modern Greek.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\GR-NLP-TOOLKIT An Open-Source NLP Toolkit for Modern Greek.pdf", "file_type": "application/pdf", "file_size": 556151, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "20c73727-d29e-4c9d-8765-e5290761bd01": {"node_ids": ["dda88c9e-c0b5-4658-a64c-faa42052def6", "34d749ef-6f2d-41a6-8c9f-6a9f6451e240"], "metadata": {"page_label": "5", "file_name": "GR-NLP-TOOLKIT An Open-Source NLP Toolkit for Modern Greek.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\GR-NLP-TOOLKIT An Open-Source NLP Toolkit for Modern Greek.pdf", "file_type": "application/pdf", "file_size": 556151, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "ff9a8e44-a3e4-40c6-917b-f9dd535848f3": {"node_ids": ["8965e4d8-d7d7-4017-b5a0-cc7483909ef0"], "metadata": {"page_label": "6", "file_name": "GR-NLP-TOOLKIT An Open-Source NLP Toolkit for Modern Greek.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\GR-NLP-TOOLKIT An Open-Source NLP Toolkit for Modern Greek.pdf", "file_type": "application/pdf", "file_size": 556151, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "c2e60a7a-b400-4a65-81eb-ee0a30798d78": {"node_ids": ["76c2ca8b-cf8d-47ed-b983-7341d75cf193", "0fe8908c-bd22-4eda-90c0-2c04921a5f48"], "metadata": {"page_label": "7", "file_name": "GR-NLP-TOOLKIT An Open-Source NLP Toolkit for Modern Greek.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\GR-NLP-TOOLKIT An Open-Source NLP Toolkit for Modern Greek.pdf", "file_type": "application/pdf", "file_size": 556151, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "7f5aa553-dfdd-4bab-ba88-6025e3760d92": {"node_ids": ["f1e1df9f-1c98-4e8b-ab3d-e0d5ae88a74e", "f04ecaa4-ac88-4509-80d2-2661c1ed02c8"], "metadata": {"page_label": "8", "file_name": "GR-NLP-TOOLKIT An Open-Source NLP Toolkit for Modern Greek.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\GR-NLP-TOOLKIT An Open-Source NLP Toolkit for Modern Greek.pdf", "file_type": "application/pdf", "file_size": 556151, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "f62d972e-7b41-47b2-9bc9-855b46565dfa": {"node_ids": ["11fe23aa-ab54-420e-8591-9985e2416897"], "metadata": {"page_label": "9", "file_name": "GR-NLP-TOOLKIT An Open-Source NLP Toolkit for Modern Greek.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\GR-NLP-TOOLKIT An Open-Source NLP Toolkit for Modern Greek.pdf", "file_type": "application/pdf", "file_size": 556151, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "47ba212c-fa32-4730-86ea-70f7b8ef0ea1": {"node_ids": ["486e479c-e64c-4266-855a-eb4bc57b34c6", "093a4658-f460-40d5-a1f2-b14bcd70336b"], "metadata": {"page_label": "1", "file_name": "Is Self-knowledge and Action Consistent or Not Investigating Large Language Models Personality.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Is Self-knowledge and Action Consistent or Not Investigating Large Language Models Personality.pdf", "file_type": "application/pdf", "file_size": 659650, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "a2efa05f-e582-4af3-9798-c351506768bf": {"node_ids": ["137e5b84-358b-4e54-bcf1-bb0c3d427592", "5fb5511c-6f36-4dc6-96ca-f6aa8b423f1e"], "metadata": {"page_label": "2", "file_name": "Is Self-knowledge and Action Consistent or Not Investigating Large Language Models Personality.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Is Self-knowledge and Action Consistent or Not Investigating Large Language Models Personality.pdf", "file_type": "application/pdf", "file_size": 659650, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "c5f0cfe2-8c16-4f64-b558-2ab8fdf4cafd": {"node_ids": ["1ce29805-188b-417d-a30f-af9c2955d2b5", "bdeedcd9-20e6-4219-b236-da2b8b56d9e0"], "metadata": {"page_label": "3", "file_name": "Is Self-knowledge and Action Consistent or Not Investigating Large Language Models Personality.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Is Self-knowledge and Action Consistent or Not Investigating Large Language Models Personality.pdf", "file_type": "application/pdf", "file_size": 659650, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "0defce02-ea5d-4345-994e-e84e6570e0b4": {"node_ids": ["97be7611-a131-490c-888c-b98cd2aba5ac", "24341575-0552-4162-92a7-4b04ea961302"], "metadata": {"page_label": "4", "file_name": "Is Self-knowledge and Action Consistent or Not Investigating Large Language Models Personality.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Is Self-knowledge and Action Consistent or Not Investigating Large Language Models Personality.pdf", "file_type": "application/pdf", "file_size": 659650, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "a363b883-8b6e-45e6-a6e4-78d54bc51cf8": {"node_ids": ["6e85332c-a9ef-4f4d-b1b3-6a84a2163910", "dc84dba2-5377-460c-b83c-a7d56cd34395"], "metadata": {"page_label": "5", "file_name": "Is Self-knowledge and Action Consistent or Not Investigating Large Language Models Personality.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Is Self-knowledge and Action Consistent or Not Investigating Large Language Models Personality.pdf", "file_type": "application/pdf", "file_size": 659650, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "a07860db-4f84-4673-840e-80adb4b7c7f2": {"node_ids": ["05c5cb43-8417-45d9-b07c-3a11d7a9f595"], "metadata": {"page_label": "6", "file_name": "Is Self-knowledge and Action Consistent or Not Investigating Large Language Models Personality.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Is Self-knowledge and Action Consistent or Not Investigating Large Language Models Personality.pdf", "file_type": "application/pdf", "file_size": 659650, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "99f38eb9-9953-44e4-9411-21ad8234c490": {"node_ids": ["6ab425c6-f964-41a0-8cfe-31a3a69c86cf"], "metadata": {"page_label": "7", "file_name": "Is Self-knowledge and Action Consistent or Not Investigating Large Language Models Personality.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Is Self-knowledge and Action Consistent or Not Investigating Large Language Models Personality.pdf", "file_type": "application/pdf", "file_size": 659650, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "6f7c45c8-4b1f-41a7-9fb8-4e2d716dd634": {"node_ids": ["55ce15a4-c761-484b-9fae-db2c52ce3cb5"], "metadata": {"page_label": "8", "file_name": "Is Self-knowledge and Action Consistent or Not Investigating Large Language Models Personality.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Is Self-knowledge and Action Consistent or Not Investigating Large Language Models Personality.pdf", "file_type": "application/pdf", "file_size": 659650, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "d5140a05-729e-4628-918f-a044b9bbdeda": {"node_ids": ["bdff2aad-1647-45f0-8a03-6e853f0c95d7"], "metadata": {"page_label": "9", "file_name": "Is Self-knowledge and Action Consistent or Not Investigating Large Language Models Personality.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Is Self-knowledge and Action Consistent or Not Investigating Large Language Models Personality.pdf", "file_type": "application/pdf", "file_size": 659650, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "f7bc83d6-6855-4ec3-a5bb-a4135cdfffdc": {"node_ids": ["3759c985-34e2-4ab9-adcb-525409e4ecea", "fc2a617d-2126-4026-b1ac-903d5b5a556a"], "metadata": {"page_label": "10", "file_name": "Is Self-knowledge and Action Consistent or Not Investigating Large Language Models Personality.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Is Self-knowledge and Action Consistent or Not Investigating Large Language Models Personality.pdf", "file_type": "application/pdf", "file_size": 659650, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "14ebd053-462b-4700-a31c-03c944a0ce29": {"node_ids": ["90566177-f0cc-49ac-82c9-959e5bdfc12a"], "metadata": {"page_label": "11", "file_name": "Is Self-knowledge and Action Consistent or Not Investigating Large Language Models Personality.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Is Self-knowledge and Action Consistent or Not Investigating Large Language Models Personality.pdf", "file_type": "application/pdf", "file_size": 659650, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "a1ed0bfb-8953-4137-a7e4-0371e6bde0a6": {"node_ids": ["d24febc5-aad5-4be1-af9e-073ce6adc763"], "metadata": {"page_label": "12", "file_name": "Is Self-knowledge and Action Consistent or Not Investigating Large Language Models Personality.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Is Self-knowledge and Action Consistent or Not Investigating Large Language Models Personality.pdf", "file_type": "application/pdf", "file_size": 659650, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "af012466-de7c-4201-a840-90f780af132d": {"node_ids": ["cd3ceaaa-3ca9-4ce5-8839-959ef939dcb2", "3bfd9e46-65cd-4004-96f4-6d644c333188"], "metadata": {"page_label": "1", "file_name": "Large Language Models Lack Understanding of Character Composition of Words.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Large Language Models Lack Understanding of Character Composition of Words.pdf", "file_type": "application/pdf", "file_size": 262705, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "37cd8caf-a241-4665-b4a6-7a01a8677890": {"node_ids": ["c4603e68-71fd-4782-9e69-e79eac54d26d", "105119c7-5759-49bc-a8bb-cfb2f6681588"], "metadata": {"page_label": "2", "file_name": "Large Language Models Lack Understanding of Character Composition of Words.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Large Language Models Lack Understanding of Character Composition of Words.pdf", "file_type": "application/pdf", "file_size": 262705, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "389302f2-e074-40fd-a6c2-f9b3c69461f0": {"node_ids": ["845c2370-1994-47c2-841e-2a84d4213591", "22452d97-2010-4532-8f33-9a0d61c55216"], "metadata": {"page_label": "3", "file_name": "Large Language Models Lack Understanding of Character Composition of Words.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Large Language Models Lack Understanding of Character Composition of Words.pdf", "file_type": "application/pdf", "file_size": 262705, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "bc381e92-8ba4-45f7-8be6-58a06ddb0100": {"node_ids": ["ea9fa585-d3e1-4804-8530-e88e6ffefd9e", "daa5f973-e246-4d32-b013-da8b527e3250"], "metadata": {"page_label": "4", "file_name": "Large Language Models Lack Understanding of Character Composition of Words.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Large Language Models Lack Understanding of Character Composition of Words.pdf", "file_type": "application/pdf", "file_size": 262705, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "8c6ecd77-87d5-401e-a1fb-7581149cbc58": {"node_ids": ["83762c53-d02e-412e-b133-69bc9c97b8dd", "ddf3a6fd-2501-44b0-81b5-fa5a5aa59ad5", "aba461e4-1c6b-44a2-aba5-379dfa0ffddc"], "metadata": {"page_label": "5", "file_name": "Large Language Models Lack Understanding of Character Composition of Words.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Large Language Models Lack Understanding of Character Composition of Words.pdf", "file_type": "application/pdf", "file_size": 262705, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "1abd0ab0-cac6-42dd-900d-db62e73afad0": {"node_ids": ["d73280f5-7e6f-4306-ae87-e3678b933ac1", "84b77102-ad65-49af-a68e-e8b98324a8fe", "8ecc17c4-4d16-4cd5-b72c-ca609dab826a"], "metadata": {"page_label": "6", "file_name": "Large Language Models Lack Understanding of Character Composition of Words.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Large Language Models Lack Understanding of Character Composition of Words.pdf", "file_type": "application/pdf", "file_size": 262705, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "570eb31e-738d-4493-b7e2-4ac63f688606": {"node_ids": ["29de92db-3dcd-4a0a-b487-fe16b81d8c2c", "08dcc7d2-d8a0-4f4e-8b5d-8103cd163907", "dc286893-8803-4e33-a246-13cd50c5c268", "16fae7b4-07e7-4137-a637-ccfbcf72d8e4"], "metadata": {"page_label": "7", "file_name": "Large Language Models Lack Understanding of Character Composition of Words.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Large Language Models Lack Understanding of Character Composition of Words.pdf", "file_type": "application/pdf", "file_size": 262705, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "6ffb189f-fd69-4a30-8d40-b3753854c569": {"node_ids": ["87216d92-ae2d-4147-b70d-bf07c6074206", "cb5664aa-ecde-4a8a-9e90-1769bba41b27", "f66e05d1-3e84-49fa-afbc-a375602fd48d", "ddd73be3-0c6a-489b-bb7d-2220debca08c"], "metadata": {"page_label": "8", "file_name": "Large Language Models Lack Understanding of Character Composition of Words.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Large Language Models Lack Understanding of Character Composition of Words.pdf", "file_type": "application/pdf", "file_size": 262705, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "f8fc1b27-3fc1-4ae7-96a5-6c388c6de267": {"node_ids": ["35a2357b-cc80-4c87-929d-1899d8eecae9", "c3009f7a-96cf-447e-b732-b3c665c865be"], "metadata": {"page_label": "9", "file_name": "Large Language Models Lack Understanding of Character Composition of Words.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Large Language Models Lack Understanding of Character Composition of Words.pdf", "file_type": "application/pdf", "file_size": 262705, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "27f4c239-a832-4016-91e0-385fc534d943": {"node_ids": ["6e3c1a8d-2671-42c9-9903-b2bf73c25f07", "af5f0b41-f9e7-4ffc-a321-f571af0304e1"], "metadata": {"page_label": "10", "file_name": "Large Language Models Lack Understanding of Character Composition of Words.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Large Language Models Lack Understanding of Character Composition of Words.pdf", "file_type": "application/pdf", "file_size": 262705, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "76222c74-1a7a-429b-88f2-a4ed6fa37c0f": {"node_ids": ["3c295f1c-7959-4c82-baad-ceceae5185c0", "50ff1b79-77ed-49f5-bb6a-671c186fd066", "4560d650-9763-44a3-a62f-302b6204a058", "5e88a86a-0c21-4113-88b3-59da15d2006d"], "metadata": {"page_label": "11", "file_name": "Large Language Models Lack Understanding of Character Composition of Words.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Large Language Models Lack Understanding of Character Composition of Words.pdf", "file_type": "application/pdf", "file_size": 262705, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "239162ef-3dd3-4a8f-80ab-b2273614b495": {"node_ids": ["43686da1-60d8-45fa-b7ed-b5c9a5f0b7d3", "55708ffe-04f4-43f1-87eb-c649e796341e"], "metadata": {"page_label": "1", "file_name": "Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_type": "application/pdf", "file_size": 2538334, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "a6588327-9da3-4981-8f90-f859ca2934f2": {"node_ids": ["330173e5-3244-4333-8eb1-99887241be2d", "cedc07a4-2f9a-4da2-b70e-0234162b6c3e"], "metadata": {"page_label": "2", "file_name": "Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_type": "application/pdf", "file_size": 2538334, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "7d342485-4866-499e-bca6-1a2f3178ea06": {"node_ids": ["2d3dcf54-241e-4ac6-8943-7d3b8f9b791e", "e060e362-ebc9-4553-a4ea-a891d1fad74d"], "metadata": {"page_label": "3", "file_name": "Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_type": "application/pdf", "file_size": 2538334, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "5450fee1-0f33-4a6a-9e18-ef6f25d6fede": {"node_ids": ["6da607c4-d2d7-4855-aa2f-f521964c41e0", "31ebd248-ec81-4731-8134-09cbe74020c7"], "metadata": {"page_label": "4", "file_name": "Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_type": "application/pdf", "file_size": 2538334, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "0c266220-80bd-43df-b799-10278a3c2009": {"node_ids": ["5b63552a-f65c-479f-b39f-ef5e9cdc8267", "e946cd83-0855-4a5e-a3a1-6dd6f28b29b9"], "metadata": {"page_label": "5", "file_name": "Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_type": "application/pdf", "file_size": 2538334, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "dbc09027-59e7-45b2-babc-9a46a8d66665": {"node_ids": ["fb33ae35-b68b-4043-80fc-0dc3793943a0", "cf4950db-ce5b-489e-b6ba-af95d9ded45d"], "metadata": {"page_label": "6", "file_name": "Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_type": "application/pdf", "file_size": 2538334, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "a51a9d5b-8d3d-4c4a-86f8-d1cd040ef289": {"node_ids": ["a23bb4b8-8bce-4d31-8faf-1d29ebd4b53b"], "metadata": {"page_label": "7", "file_name": "Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_type": "application/pdf", "file_size": 2538334, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "122a356a-560b-4765-b4a3-49c3566409a6": {"node_ids": ["c3d2c8d2-4610-4844-b880-5a5e0e1619b1", "a2fc65a4-c794-47ec-a6d5-8f5605fcbb01"], "metadata": {"page_label": "8", "file_name": "Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_type": "application/pdf", "file_size": 2538334, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "cb4135a6-e312-46b3-b1a3-cff024aa7263": {"node_ids": ["1c45901e-04c8-4b71-8002-4591c7fe6579", "8148cce7-bad0-41d0-a147-c2daeb37fb08"], "metadata": {"page_label": "9", "file_name": "Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_type": "application/pdf", "file_size": 2538334, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "fb69018b-7e55-40cf-a486-dda2ebb7e8ce": {"node_ids": ["7ea043a2-7d33-46ef-9cde-d1204f2caf9f", "c69362e8-cd2d-4de7-bb1f-8fa6435e5671"], "metadata": {"page_label": "10", "file_name": "Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_type": "application/pdf", "file_size": 2538334, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "da908028-a38c-419a-a992-d070d12cf349": {"node_ids": ["a09f5385-a3ea-4ea1-afb4-8b5bbd833531"], "metadata": {"page_label": "11", "file_name": "Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_type": "application/pdf", "file_size": 2538334, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "20b8d52b-d00a-448b-bd9e-2fbd198a83dc": {"node_ids": ["8e0684d3-82e0-4238-bb63-2de33702345a"], "metadata": {"page_label": "12", "file_name": "Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_type": "application/pdf", "file_size": 2538334, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "f92eebd4-284e-4e4d-8e4f-6143deed8c60": {"node_ids": ["9710975d-22ac-432a-ab3a-973c2f3c9b1e"], "metadata": {"page_label": "13", "file_name": "Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_type": "application/pdf", "file_size": 2538334, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "330da648-2bd7-419a-adb7-9d4831526ebd": {"node_ids": ["f58f799b-f10d-4bd1-940b-3e69e87e8819", "908b4d61-a868-4aa2-9042-fa69fddc3cba"], "metadata": {"page_label": "14", "file_name": "Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_type": "application/pdf", "file_size": 2538334, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "447bf0b4-af11-4369-84b8-9eaea9c8bab9": {"node_ids": ["ac12a5f9-b0c3-4e99-89a3-d5a6bd176038"], "metadata": {"page_label": "15", "file_name": "Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_type": "application/pdf", "file_size": 2538334, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "0b652de9-e188-4b14-913d-74336aa59bbc": {"node_ids": ["5eccfe52-5aac-4acb-9d7b-742966373750", "18acc2cf-6af5-457e-bd1d-328d55dcea96"], "metadata": {"page_label": "16", "file_name": "Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_type": "application/pdf", "file_size": 2538334, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "c4de1146-dcb5-4171-ae01-d6a4582b9142": {"node_ids": ["1e350745-3144-4ee9-8bba-511dd6041377"], "metadata": {"page_label": "17", "file_name": "Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_type": "application/pdf", "file_size": 2538334, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "38105673-de91-4a2d-971a-90ea9d6486b9": {"node_ids": ["a8aa00a7-78b7-4473-ab84-9a0578b06179", "fcb6ab74-30fc-4a18-b291-a4f85e9481b0"], "metadata": {"page_label": "18", "file_name": "Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_type": "application/pdf", "file_size": 2538334, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "38db0674-39e1-4922-9a0f-f14bcb4b57e1": {"node_ids": ["bb62b73e-655e-4323-80db-c2c1f1036634"], "metadata": {"page_label": "19", "file_name": "Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_type": "application/pdf", "file_size": 2538334, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "7caeaab5-80fd-4d57-b502-1d62c9fb45d3": {"node_ids": ["32e163df-c232-4caf-9175-bd3366f96b11", "22040c12-6938-4add-997e-e8a4f7f5e731"], "metadata": {"page_label": "20", "file_name": "Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_type": "application/pdf", "file_size": 2538334, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "d35c9166-27f9-406f-b1d6-a1ed2f8edd52": {"node_ids": ["6a8bc2a1-98b6-4717-8768-a52595020e6a"], "metadata": {"page_label": "21", "file_name": "Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_type": "application/pdf", "file_size": 2538334, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "4f91bdee-157e-4746-a894-9e2da47ef02c": {"node_ids": ["1dbb7934-4c9e-4ee0-94d5-78591806796d", "3c9467f1-00e2-452f-9473-75346f75bd61"], "metadata": {"page_label": "22", "file_name": "Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_type": "application/pdf", "file_size": 2538334, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "9fced9de-97fc-4686-baef-00f0d9f8ca6c": {"node_ids": ["0f6f1f46-be29-4317-b45e-0d29ff727ba9"], "metadata": {"page_label": "23", "file_name": "Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_type": "application/pdf", "file_size": 2538334, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "5cae1afd-9dee-456f-b93d-b3b04aea1e19": {"node_ids": ["455a2576-c663-4b27-8838-caf8820141c4"], "metadata": {"page_label": "24", "file_name": "Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_type": "application/pdf", "file_size": 2538334, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "6d64437e-60b9-4d40-9c27-bb4e2c24c4dd": {"node_ids": ["9e11c56c-2de9-4505-92df-1de286bd95bb"], "metadata": {"page_label": "25", "file_name": "Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_type": "application/pdf", "file_size": 2538334, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "66760d97-fffb-49ff-8621-3a9deefaae1d": {"node_ids": ["87d5b2f7-52bc-4c27-b792-f5be2adef879", "23226c63-ec27-44bf-8223-0840f10fea8b"], "metadata": {"page_label": "26", "file_name": "Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_type": "application/pdf", "file_size": 2538334, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "e11a913e-de48-41ec-9589-55b00054df69": {"node_ids": ["5b0a4418-152b-4a25-8f7d-a522223bd9c4"], "metadata": {"page_label": "27", "file_name": "Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_type": "application/pdf", "file_size": 2538334, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "617fbb52-97af-4a49-bf31-5e77a2fcafab": {"node_ids": ["2c555464-e4f6-448a-a485-9bbb1c3ac317"], "metadata": {"file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\README.md", "file_name": "README.md", "file_type": "text/markdown", "file_size": 629, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "48802810-28be-4173-b838-94521469f095": {"node_ids": ["b5411b1a-2e62-4f1c-85e6-b4b8b282d59f", "2fb71b5f-0d95-440c-a8a4-6eb286cdc201"], "metadata": {"page_label": "1", "file_name": "Safe Untrusted Proof-Carrying AI Agents toward the agentic lakehouse.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Safe Untrusted Proof-Carrying AI Agents toward the agentic lakehouse.pdf", "file_type": "application/pdf", "file_size": 752899, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "71e24d16-7899-4464-a3d7-237b4f4e19af": {"node_ids": ["0f63aba0-4019-4921-8059-9a6d2ca20b05", "23092e61-fd1b-4aac-aac9-b521ceaafdef"], "metadata": {"page_label": "2", "file_name": "Safe Untrusted Proof-Carrying AI Agents toward the agentic lakehouse.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Safe Untrusted Proof-Carrying AI Agents toward the agentic lakehouse.pdf", "file_type": "application/pdf", "file_size": 752899, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "5cce9f87-4830-4ffd-95f8-8168b8e69f9d": {"node_ids": ["147ed8be-4e6e-45de-9c68-ba7f42ac940c", "9fac8ce4-588d-4df0-aaf4-ef00824eda4a"], "metadata": {"page_label": "3", "file_name": "Safe Untrusted Proof-Carrying AI Agents toward the agentic lakehouse.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Safe Untrusted Proof-Carrying AI Agents toward the agentic lakehouse.pdf", "file_type": "application/pdf", "file_size": 752899, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "531524fe-39e7-446c-bfd5-721e713f81cf": {"node_ids": ["5e24c8ca-db47-47f3-b2f2-3c255a164ded"], "metadata": {"page_label": "4", "file_name": "Safe Untrusted Proof-Carrying AI Agents toward the agentic lakehouse.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Safe Untrusted Proof-Carrying AI Agents toward the agentic lakehouse.pdf", "file_type": "application/pdf", "file_size": 752899, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "66ad9bbf-b68e-4fb3-a2a0-1d0afa59cddc": {"node_ids": ["e65e59dd-775c-4c22-89f6-6c8a319ebf5b", "76ffa9d1-cf25-404f-af7e-a73cbebaaa61", "83808c69-31e8-42ae-b76e-515722df651f"], "metadata": {"page_label": "5", "file_name": "Safe Untrusted Proof-Carrying AI Agents toward the agentic lakehouse.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Safe Untrusted Proof-Carrying AI Agents toward the agentic lakehouse.pdf", "file_type": "application/pdf", "file_size": 752899, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "8182cd31-5d48-4aa4-888a-538a8d6ce947": {"node_ids": ["ab2920af-d9d7-4d26-9b9c-a5d6fd8b9f6d"], "metadata": {"page_label": "1", "file_name": "Teaching NLP with Bracelets and Restaurant Menus An Interactive Workshop for Italian Students.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Teaching NLP with Bracelets and Restaurant Menus An Interactive Workshop for Italian Students.pdf", "file_type": "application/pdf", "file_size": 3286173, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "cd4215e9-931c-40f0-99c8-604f0ad0264b": {"node_ids": ["0843cce8-40e1-4bae-bddc-1f32f86051af", "22cf4a36-db05-457c-b52e-e3cb380b5ee0"], "metadata": {"page_label": "2", "file_name": "Teaching NLP with Bracelets and Restaurant Menus An Interactive Workshop for Italian Students.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Teaching NLP with Bracelets and Restaurant Menus An Interactive Workshop for Italian Students.pdf", "file_type": "application/pdf", "file_size": 3286173, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "6d8f42cc-45e9-46b6-9cfc-9a8761acf7af": {"node_ids": ["3a1776cb-ca46-4ff5-9c05-03d2b3908de0"], "metadata": {"page_label": "3", "file_name": "Teaching NLP with Bracelets and Restaurant Menus An Interactive Workshop for Italian Students.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Teaching NLP with Bracelets and Restaurant Menus An Interactive Workshop for Italian Students.pdf", "file_type": "application/pdf", "file_size": 3286173, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "f0161f95-883f-4fa1-8f51-6513d40aaef8": {"node_ids": ["f83fb6d7-1087-43f2-aaec-55c1fe3a608c"], "metadata": {"page_label": "4", "file_name": "Teaching NLP with Bracelets and Restaurant Menus An Interactive Workshop for Italian Students.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Teaching NLP with Bracelets and Restaurant Menus An Interactive Workshop for Italian Students.pdf", "file_type": "application/pdf", "file_size": 3286173, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "789cbb66-582a-443b-bccc-898e26215823": {"node_ids": ["8ae4d42e-d28a-4e7d-88e6-6dfaacf53606"], "metadata": {"page_label": "5", "file_name": "Teaching NLP with Bracelets and Restaurant Menus An Interactive Workshop for Italian Students.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Teaching NLP with Bracelets and Restaurant Menus An Interactive Workshop for Italian Students.pdf", "file_type": "application/pdf", "file_size": 3286173, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "eeeeda32-5bf5-4134-9c36-0c0782e9b23a": {"node_ids": ["64633193-e263-48fb-85b0-c124a391e00e"], "metadata": {"page_label": "6", "file_name": "Teaching NLP with Bracelets and Restaurant Menus An Interactive Workshop for Italian Students.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Teaching NLP with Bracelets and Restaurant Menus An Interactive Workshop for Italian Students.pdf", "file_type": "application/pdf", "file_size": 3286173, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "ab2030b5-fe75-450b-b26b-1a466cf0cf84": {"node_ids": ["e8101beb-8a1d-4dfb-9144-1c48bf6aada3"], "metadata": {"page_label": "7", "file_name": "Teaching NLP with Bracelets and Restaurant Menus An Interactive Workshop for Italian Students.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Teaching NLP with Bracelets and Restaurant Menus An Interactive Workshop for Italian Students.pdf", "file_type": "application/pdf", "file_size": 3286173, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "18c1b152-077b-4f0e-b1fd-cee76fc657aa": {"node_ids": ["152f4ed6-59ee-424f-8a64-85ac3478cc15", "676f9d3e-cd3f-4de9-92b9-0d9afa417d36"], "metadata": {"page_label": "8", "file_name": "Teaching NLP with Bracelets and Restaurant Menus An Interactive Workshop for Italian Students.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Teaching NLP with Bracelets and Restaurant Menus An Interactive Workshop for Italian Students.pdf", "file_type": "application/pdf", "file_size": 3286173, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "04e3ebd1-b8db-4062-a283-53bc6832b97d": {"node_ids": ["2ccc01b0-c9f3-401e-851a-6483c5ca1f6a"], "metadata": {"page_label": "9", "file_name": "Teaching NLP with Bracelets and Restaurant Menus An Interactive Workshop for Italian Students.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Teaching NLP with Bracelets and Restaurant Menus An Interactive Workshop for Italian Students.pdf", "file_type": "application/pdf", "file_size": 3286173, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "cb8766eb-2995-4d1c-b4d4-9ca879952fd6": {"node_ids": ["665978c3-7758-423d-8713-292bcc7d611a"], "metadata": {"page_label": "10", "file_name": "Teaching NLP with Bracelets and Restaurant Menus An Interactive Workshop for Italian Students.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Teaching NLP with Bracelets and Restaurant Menus An Interactive Workshop for Italian Students.pdf", "file_type": "application/pdf", "file_size": 3286173, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "7e81fd11-d143-4fdf-8f67-17f94973dbea": {"node_ids": ["351519a5-7135-4f82-b2a6-6485cd44c96f", "d34fda78-ab08-4f1a-a10f-21a3de4a98ad"], "metadata": {"page_label": "11", "file_name": "Teaching NLP with Bracelets and Restaurant Menus An Interactive Workshop for Italian Students.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Teaching NLP with Bracelets and Restaurant Menus An Interactive Workshop for Italian Students.pdf", "file_type": "application/pdf", "file_size": 3286173, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "a6bb627e-c456-49bc-b3d5-448447e87f86": {"node_ids": ["a77be512-9fa5-413c-9568-9306fd9c4154", "00cd395b-3217-496d-8ef3-ad7c5d06a1d1"], "metadata": {"page_label": "1", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "ac0e6c14-6143-4286-b40f-b346270e5489": {"node_ids": ["65b1a335-4cf2-4d48-a9ea-0d1256d4ca67", "9885ecfe-7182-4d5a-a1e3-d82f0a498a37"], "metadata": {"page_label": "2", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "e97167ce-af33-40d1-bb9f-9481ff89bcae": {"node_ids": ["0554e0da-b43b-430a-8015-9dd91b97a5ea", "78c6a24a-c20d-4e04-9024-8ff527e8895d"], "metadata": {"page_label": "3", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "5b4a4a81-0e34-4844-a957-88f42954e54b": {"node_ids": ["4f325d4d-1645-4253-b138-e1c1c324450c", "308a579c-6a8b-47b9-ba18-4a4d0607c64a"], "metadata": {"page_label": "4", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "f3ecccd2-e8f9-4111-919c-6f53f8e48d41": {"node_ids": ["01addac1-c7d8-4f50-b927-f971ed791063", "c30eeaac-2854-44b6-8b37-f63350b6ade6"], "metadata": {"page_label": "5", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "388eea62-d4e4-484d-a6f2-b4ec38768125": {"node_ids": ["2b17367c-7442-433d-b535-4ead9a5da1f7", "99f6d8ce-56e3-4e26-8fa4-060f61923764"], "metadata": {"page_label": "6", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "7f2d16ed-f192-4be3-bea8-7a06fe3cc8d1": {"node_ids": ["e70446d2-ffda-46e4-80ec-cc49babc4579"], "metadata": {"page_label": "7", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "4308e195-72ea-4bd0-add0-e28d8f12bcd9": {"node_ids": ["82820804-bfdf-43bf-a481-a4e97a79b8cc"], "metadata": {"page_label": "8", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "1ab1fd64-9043-46f8-860e-493e2d3a8e9a": {"node_ids": ["65d2d176-5594-4b32-bc6f-30675be72b49"], "metadata": {"page_label": "9", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "be85231e-714a-4143-b2d6-228d2a25c524": {"node_ids": ["d1a11195-06ae-4a82-9ebe-14e3d016c55d"], "metadata": {"page_label": "10", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "1903e3f4-bc07-4500-98fb-15f8177ff7c3": {"node_ids": ["899b6875-bc0f-4d42-a660-7006a800d1df", "258d38d6-2b64-416b-a289-18e0c834aa18"], "metadata": {"page_label": "11", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "8d44095a-c65f-4834-8c8d-9bedc6da9743": {"node_ids": ["40e5bb45-b79f-4960-9f58-278f36214f12"], "metadata": {"page_label": "12", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "61083f84-5a31-421f-9306-68b76f191817": {"node_ids": ["ce08e953-1108-406f-93f3-ef4ee4d15500", "7633e7b8-650a-498c-a744-90f513d43d7d"], "metadata": {"page_label": "13", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "79c5fdb3-d643-4015-9c66-b2e61aaab2b9": {"node_ids": ["7b84ebcc-dadb-4d9f-b91c-e77d411a0a80", "606ff04a-ec88-42b0-acf9-1049f88b5a39"], "metadata": {"page_label": "14", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "945e5d8f-1672-4ab5-a945-d8c32d647d34": {"node_ids": ["388dda35-36d1-470e-a6e2-e3c956808e9c", "f42429c6-97b4-4348-abed-7a6349e36190"], "metadata": {"page_label": "15", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "54a51921-8b13-47cc-8eae-46d96b665531": {"node_ids": ["0485283e-4100-42e5-817d-3024c8c43e0c", "85ef6169-8eeb-4f4f-adb9-42493246c37e"], "metadata": {"page_label": "16", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "2d5dd2f9-9fa2-456d-b60a-2170cbe91df3": {"node_ids": ["0ab1e6f5-5cf4-444f-aa42-0cc0e8197617", "6bc95d12-6caa-4147-b98c-d15237e4f197"], "metadata": {"page_label": "17", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "46bf62b4-5558-43c6-8d65-4f3b62dae319": {"node_ids": ["ffe850f2-bd72-4464-9c2c-86375f2a5613", "373929c7-7adb-456f-a25d-de8b9ba0de15"], "metadata": {"page_label": "18", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "6810f935-b315-4872-9ab1-276a40e46e44": {"node_ids": ["b538336a-590a-467c-994e-2bf4294cf002", "4ce76e13-7b10-4e8f-91dc-76419c43ba24"], "metadata": {"page_label": "19", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "ff3f6703-3601-4572-b67c-23ebbd05d5dd": {"node_ids": ["6a937a97-dd88-45f8-b4b9-4033dff65df9"], "metadata": {"page_label": "20", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "3bff6b00-6fcb-4b8d-b1aa-6a615fdaec51": {"node_ids": ["08039658-2335-43fc-a53b-6ba055c7c852", "22bc9140-bfc2-4e55-bf78-e9467e99ae82"], "metadata": {"page_label": "21", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "d1e8ccb1-1fb3-4536-ba94-30657d180b19": {"node_ids": ["4081def5-86ef-463e-839a-9ea0a8912667", "78c3d25a-919a-49a9-8cbc-f7a9e0073335"], "metadata": {"page_label": "22", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "ef883407-1e48-4ec1-b2cf-37f0b283ce69": {"node_ids": ["4d5f0e60-2f52-4a96-b2c6-c6dbe84ab12e", "c0d67455-c6c6-4fdc-821a-4a6517ef8e6d"], "metadata": {"page_label": "23", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "7f20761a-e0eb-4a36-a29f-3d481c8bb256": {"node_ids": ["285c86b4-90ce-4894-8756-c9412336a579", "e598aaf2-00b1-4a0e-a252-b4e974b39cb9"], "metadata": {"page_label": "24", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "2d50e2be-5c8a-41a2-b2c4-7ae1942dc685": {"node_ids": ["c68a2840-4799-4755-8180-75f6c99dc3b6", "24877f06-16ec-4d78-9d87-2dec4f5827ea"], "metadata": {"page_label": "25", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "c99a54fc-b8fb-4b9e-b121-2beeb24440f4": {"node_ids": ["f1d9b863-2fa5-4e9e-8aa1-f90217396e9c"], "metadata": {"page_label": "26", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "179937f3-5518-4faf-b3e9-7f03a18a6a30": {"node_ids": ["f8117bf1-30ce-4ce1-a324-df24aeab0f9a", "dc671a17-8adf-4653-93a8-c28169f4dea2"], "metadata": {"page_label": "27", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "264887f3-d4b6-4c45-82e5-479e1ee7ef04": {"node_ids": ["6e323a8c-568d-48c5-9578-b9be33ebcc35"], "metadata": {"page_label": "28", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "38234d53-0ca9-4146-9ec4-2ade5b7c8b67": {"node_ids": ["ea6610ca-6d7b-412f-8e9a-78670ff703ac", "fa142759-8335-47be-84a3-0b137ebeeed0"], "metadata": {"page_label": "29", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "afe91798-85ed-4db9-89bd-1d14beba03b8": {"node_ids": ["92fc61de-7d9b-4c1a-b466-5f1a88e9e56b", "fe8354f0-2492-439e-baac-b334181bb272"], "metadata": {"page_label": "30", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "7fdcc2cd-5535-4b78-9fdc-fa6a5c90efce": {"node_ids": ["5a51e625-0d10-499b-8ef8-c9dfe3bd4d4b", "8f378dfb-fa87-48d0-a699-a4f8f3855126"], "metadata": {"page_label": "31", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "a13ee79e-1f7c-4c64-b3ac-fe5ba9b1b7dc": {"node_ids": ["33a24653-6f19-4a2b-94a3-8c6f20b75f22", "24833518-abeb-4715-961b-a36fd0f89d82"], "metadata": {"page_label": "32", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "c9e28ad3-7513-4b4e-a179-18849ad39c35": {"node_ids": ["23f1f6d1-c23d-4dbb-8384-29c71b880ae9"], "metadata": {"page_label": "33", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "03c6cd1f-be62-42b7-b509-9bbdaabc3213": {"node_ids": ["1578b625-b2f4-4741-844f-0084d2afd9c5"], "metadata": {"page_label": "34", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "7d6a84cb-5208-424b-a403-25580b000ae7": {"node_ids": ["10b8718a-0adb-493c-8cc0-fde9567f811b", "d95f851d-6c05-4a99-95d7-a1a3ac1c369b"], "metadata": {"page_label": "35", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "c90c7d5a-067a-4c5d-9cdc-9f8129fd09cc": {"node_ids": ["c30ca5b7-c03a-420b-a587-04e1f1329118", "533721ed-8bee-42be-aac0-68b084173b91"], "metadata": {"page_label": "36", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "82e61a9d-2eef-49e6-9f5d-41562c995589": {"node_ids": ["e64259bf-9974-4ad0-bcbd-de8b5ddcbf5a", "282ccc27-83b2-4f25-9139-03d36952d0b6"], "metadata": {"page_label": "37", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "6391cb5d-b101-4644-908b-40bdfd0e913f": {"node_ids": ["6b86910e-1423-470a-92f7-fae69e9edc93"], "metadata": {"page_label": "38", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "2f22b994-8dbb-4d88-8352-5a9fdb868e06": {"node_ids": ["edd7f8aa-bb1f-40db-90ca-263652935f4e"], "metadata": {"page_label": "39", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "fbaf96c8-70f8-4731-bbdb-85d321f0991b": {"node_ids": ["5bee5618-8f26-487c-9665-d71cfad9648a", "b1ba3042-b143-4869-9fb1-68dc374be716"], "metadata": {"page_label": "40", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "e7a885ea-3f65-4915-83cf-564f18e50bc9": {"node_ids": ["b5b1f751-916b-4796-bfd3-3abf90bab973", "79a75624-b355-4998-8258-d20e87aa68be"], "metadata": {"page_label": "41", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "733191a2-af86-45fa-b525-856ed65adfe5": {"node_ids": ["bad82dc9-395e-4c17-a51f-076ccb4582b2"], "metadata": {"page_label": "42", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "dc0ebc3a-0a26-400d-bc38-e3f109a5fe0a": {"node_ids": ["0130df9e-d4c0-47ad-9115-86c7fc1406c7", "78aa6706-abfb-43a8-a4e1-d310f77cd127"], "metadata": {"page_label": "43", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "7fc6007a-767c-4390-9164-62179c873c77": {"node_ids": ["5e64c2bd-4e5b-4f41-a281-1c8ceb4a1d82", "30934b34-7e35-49b4-93da-175808ded43b"], "metadata": {"page_label": "44", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "c185ac8c-06be-416b-9fdc-19087684730e": {"node_ids": ["41feeed9-be6d-4556-a1b2-f8a307fa74fd", "5edf799d-1457-484b-956e-d93d663b066e"], "metadata": {"page_label": "45", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "76ea6bf7-ce7f-42d0-b254-73e1c37fea3d": {"node_ids": ["15a937e0-2550-4b73-8e0c-1b64a2499e0c"], "metadata": {"page_label": "46", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "3ad6f4b3-100a-44df-b1f8-d40424f52a0f": {"node_ids": ["1560830c-1c3c-4fa2-bc50-f6ff4ed67562", "803ddf7f-7017-41ce-a794-3b3ea592321b"], "metadata": {"page_label": "47", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "a23afbef-faa8-42eb-bb0b-d4af01c2b1af": {"node_ids": ["0f924c9f-34ee-44c2-bb38-507c517a6978"], "metadata": {"page_label": "48", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "0cb8eb46-a71d-4228-be2c-3f20cb31025a": {"node_ids": ["6055b970-22d1-4fa6-a508-7022ddf32591"], "metadata": {"page_label": "49", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "92238105-858e-4f90-b6ed-c7c7dbfa0e08": {"node_ids": ["b1aff551-2170-47ff-8017-b18cf6cf58a9"], "metadata": {"page_label": "50", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "3f830bbd-7bf1-4acc-9193-84dd47a2e212": {"node_ids": ["d57c4995-c52a-41d2-a029-c28578759712", "9708271e-0459-4801-8068-9e5e6f14ef88"], "metadata": {"page_label": "51", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "3ecb5304-c784-4118-922a-f7b6eeac23f1": {"node_ids": ["8b6214a9-eda6-48b7-a92e-534c1aaa3235"], "metadata": {"page_label": "52", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "bcb391ad-c487-4189-bad3-e742a75aa340": {"node_ids": ["629f4da1-2129-4201-ad23-3f75ed5d6f27", "1e4a7246-3d4a-4eda-98f7-70b2cc54eb7f"], "metadata": {"page_label": "53", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "7b70200b-1c4f-4615-86e3-8382f9b8d2de": {"node_ids": ["5e405028-c4f0-43d1-b80a-a4a9c44ac8fe", "bf988a13-59e8-41d8-a608-5c5723a8b047"], "metadata": {"page_label": "54", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "f235f253-ab1e-43be-8cbd-800eab485f88": {"node_ids": ["4425c16b-56e9-4b99-9d78-b9c50b502c6f"], "metadata": {"page_label": "55", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "b25d40ae-ab86-4c14-9eab-038ab0b41760": {"node_ids": ["0f447729-1c78-48d0-86bb-d606f1c45cf3"], "metadata": {"page_label": "56", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "b9cb15a3-f949-45b7-9e01-bdc0bc178945": {"node_ids": ["35e4a954-31d7-47d9-b34e-bd66ccbf1b08", "b9ddd870-b29e-46c2-a979-5baf94fab778"], "metadata": {"page_label": "57", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "5a8d2dcf-bbd5-471f-8a65-1f2a555cb229": {"node_ids": ["d9d4f921-607d-4276-a6aa-7f73247e8cce", "cb37f860-9f5e-4d7f-b527-ff6225663e83"], "metadata": {"page_label": "58", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "21e929ce-3e11-4f56-b6ee-16f41a72cdfc": {"node_ids": ["4f7863fd-3cd1-464c-bde5-b55891d99915", "f79355ba-f756-4286-af95-b3985636e2d7"], "metadata": {"page_label": "59", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "8a45578d-5044-4d5f-9bf5-aef629d70917": {"node_ids": ["5401fa29-c9fb-4f3f-acc5-483f17e6799e"], "metadata": {"page_label": "60", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "1a5fc65b-7546-48c3-ba29-614b6df88236": {"node_ids": ["2930f660-dac4-4385-ab52-ad2be66041dd"], "metadata": {"page_label": "61", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "40643ffa-8800-43b2-8e36-59f56a8b69c2": {"node_ids": ["b6d570f9-739c-4d9f-b7b8-e54a825b2a05"], "metadata": {"page_label": "62", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "5835ceb4-48bc-4ca8-a95e-4b1e4e120339": {"node_ids": ["7fad35e5-2200-43a8-b31a-bbe695837b6a", "ecdee644-f8bf-456a-98c4-68f67ae521e8"], "metadata": {"page_label": "63", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "2539089e-3c01-4b47-9494-0965758b3da6": {"node_ids": ["a3fe6483-a996-4ebf-a1b0-ed60566c7a04"], "metadata": {"page_label": "64", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "b52beb20-a889-48d0-bd38-048e254e362f": {"node_ids": ["3ac4883c-6d48-4d8c-ad80-72af3481f0ba"], "metadata": {"page_label": "65", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "deb3df48-8344-41db-abd5-930649e70bb2": {"node_ids": ["5a7d29a2-48b8-4f70-9439-7fb311d1d6f0", "71c3ece8-85d4-40f3-ad05-069a7bebc348"], "metadata": {"page_label": "66", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "77469f62-cb8e-4cac-8417-5abb9faad643": {"node_ids": ["658b06c4-f315-416c-8279-48d2f79cb6e9"], "metadata": {"page_label": "67", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "54a03e67-6b84-4e5b-9b3a-a09db6c0ab87": {"node_ids": ["6d29a934-074d-4383-9454-32101b204afa"], "metadata": {"page_label": "68", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "616bfc8b-252f-4638-a3a8-6f2afa12e662": {"node_ids": ["00f4e5c9-7f38-4231-9e12-276eb2cff2c7"], "metadata": {"page_label": "69", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "6367feb9-ef1e-478d-83bb-83c04c32be85": {"node_ids": ["0dee3d18-948c-4b7e-b11b-a0cc8b8c1bda"], "metadata": {"page_label": "70", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "de08b289-6536-4ff0-98e4-a3bb8e363727": {"node_ids": ["ed812420-4b3e-4ebb-85f0-49455ce1a218"], "metadata": {"page_label": "71", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "17b71dfa-ed2e-4ed9-a506-129875e96aa1": {"node_ids": ["33f4e238-c13c-4c72-afdd-3de409192f7f"], "metadata": {"page_label": "72", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "c0f50496-e9b3-4987-9ee1-20cd9906df30": {"node_ids": ["7a5f90f9-63ed-4f86-90bf-47fc920d42d6", "ae9e0db0-35b5-483d-a429-87ea38c0b745"], "metadata": {"page_label": "73", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "3f100fe1-c9e6-475e-ae8f-fffdea4fb8be": {"node_ids": ["2cf23693-a9f0-4f80-a2d2-3382c344a5c1"], "metadata": {"page_label": "74", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "e9a9f5ea-7412-45f7-9142-30b634592df7": {"node_ids": ["7985138a-fafb-4c38-827e-5f9873cb23e0"], "metadata": {"page_label": "75", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "a1d13aff-daac-421d-b502-7622dc524df1": {"node_ids": ["d04d1808-31a3-4a6b-83fc-be775bf988ec"], "metadata": {"page_label": "76", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "46eaf67c-3fe1-4685-b76e-3415d54d0f4e": {"node_ids": ["b5507e3e-00d0-477a-bfaa-8004bc5b95d6"], "metadata": {"page_label": "77", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "36a208b8-0096-4368-8887-c275ad86fc53": {"node_ids": ["8e7b3e37-4e53-4c83-bee6-cf4cc208d13c"], "metadata": {"page_label": "78", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "a7ae0c48-21a8-40a9-8579-82ce516fd829": {"node_ids": ["1c305eb2-7ee8-408e-b649-926e26a95290"], "metadata": {"page_label": "1", "file_name": "Unmasking the Shadows of AI Investigating Deceptive Capabilities in Large Language Models.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Unmasking the Shadows of AI Investigating Deceptive Capabilities in Large Language Models.pdf", "file_type": "application/pdf", "file_size": 211088, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "c55f44e6-a7e1-4d5c-a03b-18b5bc439a42": {"node_ids": ["75e9230b-0fd4-416e-a386-f0a7a267fc57"], "metadata": {"page_label": "2", "file_name": "Unmasking the Shadows of AI Investigating Deceptive Capabilities in Large Language Models.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Unmasking the Shadows of AI Investigating Deceptive Capabilities in Large Language Models.pdf", "file_type": "application/pdf", "file_size": 211088, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "96b1ffb7-aa8d-4c60-a33b-e77c45258faa": {"node_ids": ["def97ab3-0457-48e7-857a-dfac6b7a9fcc"], "metadata": {"page_label": "3", "file_name": "Unmasking the Shadows of AI Investigating Deceptive Capabilities in Large Language Models.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Unmasking the Shadows of AI Investigating Deceptive Capabilities in Large Language Models.pdf", "file_type": "application/pdf", "file_size": 211088, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "081d9993-3af6-4d06-9ab2-16c8e4d0a73a": {"node_ids": ["28d6c7d8-0110-4516-9ba2-0e900bd9d56b"], "metadata": {"page_label": "4", "file_name": "Unmasking the Shadows of AI Investigating Deceptive Capabilities in Large Language Models.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Unmasking the Shadows of AI Investigating Deceptive Capabilities in Large Language Models.pdf", "file_type": "application/pdf", "file_size": 211088, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "19ba5d76-21bf-459b-a64c-92901a5cd433": {"node_ids": ["0ce6ba23-fa03-4bba-a83d-44dacb77a6c7"], "metadata": {"page_label": "5", "file_name": "Unmasking the Shadows of AI Investigating Deceptive Capabilities in Large Language Models.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Unmasking the Shadows of AI Investigating Deceptive Capabilities in Large Language Models.pdf", "file_type": "application/pdf", "file_size": 211088, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "a99da12d-8348-47c5-802b-c6af765971a5": {"node_ids": ["9696e663-ca63-4e83-8dd5-2e5b5a03c488"], "metadata": {"page_label": "6", "file_name": "Unmasking the Shadows of AI Investigating Deceptive Capabilities in Large Language Models.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Unmasking the Shadows of AI Investigating Deceptive Capabilities in Large Language Models.pdf", "file_type": "application/pdf", "file_size": 211088, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "a48b8dd5-8f64-4fe7-8ab0-41590a053fc9": {"node_ids": ["1987c594-f40e-40b1-bb3b-0455045f87a3"], "metadata": {"page_label": "7", "file_name": "Unmasking the Shadows of AI Investigating Deceptive Capabilities in Large Language Models.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Unmasking the Shadows of AI Investigating Deceptive Capabilities in Large Language Models.pdf", "file_type": "application/pdf", "file_size": 211088, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "62b36143-da7c-47cb-8e02-54983dfb2ddf": {"node_ids": ["b7260b9d-1811-42c5-9323-f2221a35bb18"], "metadata": {"page_label": "8", "file_name": "Unmasking the Shadows of AI Investigating Deceptive Capabilities in Large Language Models.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Unmasking the Shadows of AI Investigating Deceptive Capabilities in Large Language Models.pdf", "file_type": "application/pdf", "file_size": 211088, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "ea16b9e1-0038-4912-b08d-c05d79e992f1": {"node_ids": ["694b693d-ae7e-4e11-b716-3c305cbe79be"], "metadata": {"page_label": "9", "file_name": "Unmasking the Shadows of AI Investigating Deceptive Capabilities in Large Language Models.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Unmasking the Shadows of AI Investigating Deceptive Capabilities in Large Language Models.pdf", "file_type": "application/pdf", "file_size": 211088, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "822f4bed-8f8a-4572-b635-7b64a7802223": {"node_ids": ["769c9b37-0fd4-499e-9c0a-4b78f25fea89"], "metadata": {"page_label": "10", "file_name": "Unmasking the Shadows of AI Investigating Deceptive Capabilities in Large Language Models.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Unmasking the Shadows of AI Investigating Deceptive Capabilities in Large Language Models.pdf", "file_type": "application/pdf", "file_size": 211088, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "4b7ea0ca-7298-4661-8773-916cf7e69911": {"node_ids": ["798385e4-1a16-46bf-b13c-2390a4c320fc"], "metadata": {"page_label": "11", "file_name": "Unmasking the Shadows of AI Investigating Deceptive Capabilities in Large Language Models.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Unmasking the Shadows of AI Investigating Deceptive Capabilities in Large Language Models.pdf", "file_type": "application/pdf", "file_size": 211088, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "6d98a4fd-261a-4226-bab2-9d69678cc51d": {"node_ids": ["4753d8fc-65fe-44d2-a0d7-e62ee70f1291"], "metadata": {"page_label": "12", "file_name": "Unmasking the Shadows of AI Investigating Deceptive Capabilities in Large Language Models.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Unmasking the Shadows of AI Investigating Deceptive Capabilities in Large Language Models.pdf", "file_type": "application/pdf", "file_size": 211088, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "c11ba10e-ddae-4306-9fb4-974daf734cd8": {"node_ids": ["c3c32b5e-1242-40fd-8346-90d05bf95d4c", "ae77cfdb-e789-44e5-b28f-dd4706b16934"], "metadata": {"page_label": "13", "file_name": "Unmasking the Shadows of AI Investigating Deceptive Capabilities in Large Language Models.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Unmasking the Shadows of AI Investigating Deceptive Capabilities in Large Language Models.pdf", "file_type": "application/pdf", "file_size": 211088, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "321e862d-d329-4921-8663-5779de794cb1": {"node_ids": ["6a15031b-2a99-41cc-9295-29120d30598a", "39285f5c-9723-4f9b-8eda-5295031cabd2"], "metadata": {"page_label": "14", "file_name": "Unmasking the Shadows of AI Investigating Deceptive Capabilities in Large Language Models.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Unmasking the Shadows of AI Investigating Deceptive Capabilities in Large Language Models.pdf", "file_type": "application/pdf", "file_size": 211088, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}, "a457ed4e-d4bf-4b08-bed1-47cb18a5307e": {"node_ids": ["19db1f6d-e725-48ca-90fc-f08784195976"], "metadata": {"page_label": "15", "file_name": "Unmasking the Shadows of AI Investigating Deceptive Capabilities in Large Language Models.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Unmasking the Shadows of AI Investigating Deceptive Capabilities in Large Language Models.pdf", "file_type": "application/pdf", "file_size": 211088, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}}}, "docstore/data": {"d8ea723f-7cda-4fa1-b639-a93942fffda8": {"__data__": {"id_": "d8ea723f-7cda-4fa1-b639-a93942fffda8", "embedding": null, "metadata": {"page_label": "1", "file_name": "A cybersecurity AI agent selection and decision support framework.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\A cybersecurity AI agent selection and decision support framework.pdf", "file_type": "application/pdf", "file_size": 1115417, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ac7499e0-371d-4829-a4b8-563891562236", "node_type": "4", "metadata": {"page_label": "1", "file_name": "A cybersecurity AI agent selection and decision support framework.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\A cybersecurity AI agent selection and decision support framework.pdf", "file_type": "application/pdf", "file_size": 1115417, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "577064dc812471e5287ab89a37efadfe0f6b591ebe15856a420867b22dfd79c3", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "A cybersecurity AI agent selection and decision \nsupport framework \nMasike Malatji \nGraduate School of Business Leadership (SBL), University of South Africa (UNISA) \nMidrand, Johannesburg, South Africa, PO Box 392, Unisa, 0003 \nmalatm1@unisa.ac.za \nhttps://orcid.org/0000-0002-9893-9598 \nABSTRACT \nThis paper presents a novel, structured decision support framework that systematically aligns diverse artificial \nintelligence (AI) agent architectures\u2014reactive, cognitive, hybrid, and learning\u2014with the comprehensive National \nInstitute of Standards and Technology (NIST) Cybersecurity Framework (CSF) 2.0. By integrating agent theory \nwith industry guidelines, this framework provides a transparent and stepwise methodology for selecting and \ndeploying AI solutions to address contemporary cyber threats. Employing a granular decomposition of NIST CSF \n2.0 functions into specific tasks, the study links essential AI agent properties such as autonomy, adaptive learning, \nand real-time responsiveness to each subcategory\u2019s security requirements. In addition, it outlines graduated levels \nof autonomy (assisted, augmented, and full y autonomous) to accommodate organisations at varying stages of \ncybersecurity maturity. This holistic approach transcends isolated AI applications, providing a unified detection, \nincident response, and governance strategy. Through conceptual validation, the framework demonstrates how \ntailored AI agent deployments can align with real -world constraints and risk profiles, enhancing situational \nawareness, accelerating response times, and fortifying long -term resilience via adaptive risk management. \nUltimately, this research bridges the gap between theoretical AI constructs and operational cybersecurity \ndemands, establishing a foundation for robust, empirically validated multi -agent systems that adhere to industry \nstandards. \nKeywords: Agents; Autonomous; Cybersecurity; Detection; Framework; GenAI; Governance", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1956, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "2a68eef7-628b-4aea-8489-45beb414897d": {"__data__": {"id_": "2a68eef7-628b-4aea-8489-45beb414897d", "embedding": null, "metadata": {"page_label": "2", "file_name": "A cybersecurity AI agent selection and decision support framework.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\A cybersecurity AI agent selection and decision support framework.pdf", "file_type": "application/pdf", "file_size": 1115417, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0f9c0d06-8922-4dbe-a246-08baf420899b", "node_type": "4", "metadata": {"page_label": "2", "file_name": "A cybersecurity AI agent selection and decision support framework.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\A cybersecurity AI agent selection and decision support framework.pdf", "file_type": "application/pdf", "file_size": 1115417, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "0f6d54af25310afcc43f78565f9edd303db2b47b54360a4918caf553ef41f18b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "1 \n \n1. Introduction \nThe integration of artificial intelligence (AI) has fundamentally reshaped the contemporary cybersecurity landscape, \nemerging as a critical driver in the ongoing battle against increasingly sophisticated cyber threats. While traditional \nsecurity paradigms, reliant on rule-based or signature-based detection, struggle to keep pace with the dynamic and \nevolving nature of modern attacks \u2013 characterised by the proliferation of novel exploits, polymorphic malware, and \nzero-day vulnerabilities [1], [2] \u2013 AI offers a paradigm shift. Leveraging the power of machine learning (ML) and \ndeep learning (DL) algorithms, AI demonstrates unparalleled proficiency in real -time analysis of vast datasets, \nenabling the identification of subtle and often elusive indicat ors of compromise [3], [4].  This advancement \nempowers AI-driven systems with potent techniques such as pattern recognition, anomaly detection, and predictive \nanalytics, automating core cybersecurity functions like intrusion detection and proactive threat hunting [5], [6]. This \nautomation minimises the reliance on manual intervention. It addresses the critical shortage of skilled cybersecurity \nprofessionals by providing invaluable support to Security Operations Centres (SOCs) in maintaining continuous \nsurveillance and managing high-volume incident streams [7], [8]. \nHowever, the attributes that position AI as a formidable defensive tool \u2013  its inherent automation, rapid learning \ncapabilities, and capacity for generating novel insights \u2013 also present new avenues for exploitation by malicious \nactors. This necessitates a proactive and adaptive approach to cybersecurity. The  evolving landscape demands the \nstrategic adoption of AI-based defensive measures and the development of resilient, explainable AI (XAI) models \ncapable of dynamically adapting to evolving attack vectors while preserving transparency and fostering user trust  \n[9], [10]. \nThe true power of AI in cybersecurity lies in its synergistic combination with human expertise, enabling a proactive, \nscalable, and intelligent approach to threat management [11]. However, realising this potential necessitates a sound \nand multifaceted security strategy. This strategy must extend beyond mere technological implementation to \nencompass good governance frameworks, address critical ethical considerations, and prioritise thorough validation \nof AI model performance [12], [13] . A cornerstone of responsible AI integration in cybersecurity is its alignment \nwith established industry frameworks, standards, and guidelines . The National Institute of Standards and \nTechnology (NIST) Cybersecurity Framework (CSF), particularly its updated version 2.0 [14] , provides a critical \nfoundation for this alignment. Fig.  1 illustrates the NIST CSF 2.0, a versatile framework designed to empower \norganisations across diverse sectors, including government agencies, in effectively managing their cybersecurity \nrisks [14], [15]. \n \nFig. 1 NIST CSF 2.0: Illustrates the high-level structure of the NIST Cybersecurity Framework \n2.0, highlighting its six core functions and how they interrelate \nThe NIST CSF 2.0 offers a comprehensive taxonomy of high- level cybersecurity outcomes, serving as a valuable \ntool for organisations  of all sizes, sectors, and maturity levels. It facilitates a more precise understanding, \nassessment, prioritisation, and communication of cybersecurity efforts [14]. Notably, the NIST CSF 2.0 is not \nprescriptive in its implementation; it serves as a guiding resource, complemented by supplementary guidance on \npotential practices and controls that can be leveraged to achieve the desired security outcomes [14] . As visually \nrepresented in Fig.  1, the NIST CSF 2.0 is organised around a cohesive set of six core functions that underpin a \nresilient cybersecurity posture: Identify, Protect, Detect, Respond, Recover, and Govern. These interconnected", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3945, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c91746db-492d-42fb-9b4c-1152fc1c4db4": {"__data__": {"id_": "c91746db-492d-42fb-9b4c-1152fc1c4db4", "embedding": null, "metadata": {"page_label": "3", "file_name": "A cybersecurity AI agent selection and decision support framework.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\A cybersecurity AI agent selection and decision support framework.pdf", "file_type": "application/pdf", "file_size": 1115417, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "66b7b794-a9d2-4d13-a6e5-9dd1837e9ee3", "node_type": "4", "metadata": {"page_label": "3", "file_name": "A cybersecurity AI agent selection and decision support framework.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\A cybersecurity AI agent selection and decision support framework.pdf", "file_type": "application/pdf", "file_size": 1115417, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "3bf0849c3235faca5849cf0f906b9ff880744c18d6e033691e763686b78ae2bf", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d28f6583-9704-4b9b-9c46-2fba7bc72f19", "node_type": "1", "metadata": {}, "hash": "37550b57abe921dfff15787e8947d4ab3b943e043298e24f1974299e7d1055c7", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "2 \n \nfunctions provide a comprehensive framework for managing and mitigating organisational cybersecurity risks . \nAccording to the NIST [14] guidelines, the core functions of the NIST CSF 2.0 are defined as follows: \n\u2022 Govern: This overarching function establishes the importance of an organisation's commitment to \ndefining, communicating, and consistently monitoring its cybersecurity risk management strategy, \nexpectations, and policies. \n\u2022 Identify: This foundational function emphasises  the critical need for an organisation to understand its \ncurrent cybersecurity risks thoroughly. \n\u2022 Protect: This function focuses on implementing adequate  safeguards to manage and mitigate identified \ncybersecurity risks. \n\u2022 Detect: This involves the timely identification and analysis of potential cybersecurity attacks and \ncompromises. \n\u2022 Respond: This function outlines the necessary actions to address and minimise  the impact of detected \ncybersecurity incidents. \n\u2022 Recover: This focuses on the swift restoration of assets and operations affected by a cybersecurity \nincident to a functional state. \nThe practical implementation of the NIST CSF 2.0, however, can be significantly enhanced by leveraging the \ncapabilities of advanced technologies. As mentioned earlier, AI  has the potential to offer powerful tools to \noperationalise and strengthen these core cybersecurity functions. Indeed, as the sophistication of cyber threats \ncontinues to escalate [16], the need for comprehensive, adaptive, and scalable solutions across heterogeneous digital \necosystems becomes increasingly critical [17], [18] . With their inherent capacity for autonomous reasoning, real -\ntime decision-making, and collaborative learning [19], [20], AI agents have emerged as compelling candidates to \nmeet these evolving demands. AI agents are computational entities, often comprising software and sometimes \nhardware components, designed to exhibit autonomy, social interaction, responsiveness, and proactive behaviour  \n[21], [22], [23], [24] . These agents can perceive their environment and execute actions to achieve predefined \nobjectives [25]. While definitions may vary across disciplines, a common thread emphasises their ability to function \nindependently, adapt to dynamic conditions, and interact with other agents or systems [24], [26], [27]. Within the \ncybersecurity domain, AI agents hold significant potential for automating critical tasks , such as threat monitoring, \ndetection, and incident response, by effectively leveraging both reactive and proactive decision -making strategies.  \nFrom an architectural standpoint, the landscape of AI agents encompasses a diverse range of designs, each tailored \nto specific functionalities and environments. Key categories include virtual agents, which operate within digital \nenvironments; embodied agen ts, which possess a physical presence and interact with the physical world; reactive \nagents, characterised by their immediate response to environmental stimuli; hybrid agents, integrating multiple \narchitectural paradigms; learning agents, which possess the  capacity to adapt and improve their performance over \ntime; and cognitive or deliberative agents, characterised by higher-level functions such as planning, reasoning, and \nlearning [21], [28], [29], [30], [31], [32] . Categorising AI agents provides a framework for researchers and \npractitioners to strategically align agent capabilities and design limitations with the specific requirements of diverse \napplication domains. This is particularly useful in modern cybersecurity, where layered defence and dynamic threat \nresponse strategies demand tailored technological solutions. To this end, a n appropriate, theory-driven mapping of \nAI agent architectures to the NIST CSF 2.0 is required . Such a mapping ensures that the technical strengths of \nvarious agent types are effectively leveraged to support the core functions t hat underpin an organisation's \ncybersecurity posture. \nHowever, the optimal AI agent architecture is not a one -size-fits-all solution. For instance, while reactive agents \ndemonstrate exceptional proficiency in rapid, event -driven responses  [29], they may lack the strategic foresight \nessential for proactive threat hunting or optimal resource allocation. Conversely, learning and cognitive agents offer \nsound planning and adaptive capabilities [28] , but often come with increased computational and maintenance \ncomplexities [33]. Thus, research gaps persist in the current literature regarding the systematic and theoretically \ngrounded integration of AI agents within established cybersecurity frameworks. In other words, while the potential \nof AI agents in bolstering cyber defences has been widely acknowledged, a cohesive and standardised  framework \nfor their implementation remains largely absent. A study by Kott [34] envisioned a future in which intelligent,  \nautonomous cyber defence agents would play a pivotal role in threat mitigation. Subsequent research, including \ncontributions from  Th\u00e9ron and Kott  [35] and Kott et al.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 5085, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d28f6583-9704-4b9b-9c46-2fba7bc72f19": {"__data__": {"id_": "d28f6583-9704-4b9b-9c46-2fba7bc72f19", "embedding": null, "metadata": {"page_label": "3", "file_name": "A cybersecurity AI agent selection and decision support framework.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\A cybersecurity AI agent selection and decision support framework.pdf", "file_type": "application/pdf", "file_size": 1115417, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "66b7b794-a9d2-4d13-a6e5-9dd1837e9ee3", "node_type": "4", "metadata": {"page_label": "3", "file_name": "A cybersecurity AI agent selection and decision support framework.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\A cybersecurity AI agent selection and decision support framework.pdf", "file_type": "application/pdf", "file_size": 1115417, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "3bf0849c3235faca5849cf0f906b9ff880744c18d6e033691e763686b78ae2bf", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c91746db-492d-42fb-9b4c-1152fc1c4db4", "node_type": "1", "metadata": {"page_label": "3", "file_name": "A cybersecurity AI agent selection and decision support framework.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\A cybersecurity AI agent selection and decision support framework.pdf", "file_type": "application/pdf", "file_size": 1115417, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "d7a165ce4fc86d1cec9133df6b27784c1cad0c322fe6942e9a19ca428623115e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "For instance, while reactive agents \ndemonstrate exceptional proficiency in rapid, event -driven responses  [29], they may lack the strategic foresight \nessential for proactive threat hunting or optimal resource allocation. Conversely, learning and cognitive agents offer \nsound planning and adaptive capabilities [28] , but often come with increased computational and maintenance \ncomplexities [33]. Thus, research gaps persist in the current literature regarding the systematic and theoretically \ngrounded integration of AI agents within established cybersecurity frameworks. In other words, while the potential \nof AI agents in bolstering cyber defences has been widely acknowledged, a cohesive and standardised  framework \nfor their implementation remains largely absent. A study by Kott [34] envisioned a future in which intelligent,  \nautonomous cyber defence agents would play a pivotal role in threat mitigation. Subsequent research, including \ncontributions from  Th\u00e9ron and Kott  [35] and Kott et al.  [36], has further advanced this vision by proposing \narchitectural models for autonomous intelligent cyber defence agents capable of outperforming human response \nspeed and agility. Expanding the scope, Truong et al. [37] provided a broader perspective on the applications of AI \nagents across both offensive and defensive cybersecurity domains. Ligo et al. [38] delved into the intricacies of \nmeasuring cyber -resilience in systems that incorporate autonomous agents, highlighting the complexities of \nevaluating their effectiveness. Furthermore, Naik et al. [39] offered a review of AI techniques, including the role of \nAI agents in analysing, detecting, and mitigating  diverse cyber threats. More recently, Sharma and Jindal [40]", "mimetype": "text/plain", "start_char_idx": 4075, "end_char_idx": 5822, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "662cae6e-3041-4439-bc35-d7b071c62124": {"__data__": {"id_": "662cae6e-3041-4439-bc35-d7b071c62124", "embedding": null, "metadata": {"page_label": "4", "file_name": "A cybersecurity AI agent selection and decision support framework.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\A cybersecurity AI agent selection and decision support framework.pdf", "file_type": "application/pdf", "file_size": 1115417, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6317a77f-e5be-4269-8575-3ee8813a2132", "node_type": "4", "metadata": {"page_label": "4", "file_name": "A cybersecurity AI agent selection and decision support framework.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\A cybersecurity AI agent selection and decision support framework.pdf", "file_type": "application/pdf", "file_size": 1115417, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "df0de7f9529bca2dd355582f81bc893759c107a909c5ce6d421c47fedc431ddb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "07481227-7921-4b5e-a7f6-f10e343516eb", "node_type": "1", "metadata": {}, "hash": "03f8a7efc2398b54efcc845949b70ed3bb90058eb6c2f388d324117c7a5588d9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "3 \n \nexplored the broader implications of AI, underscoring its potential to advance intelligent agent technologies across \nvarious domains, including cybersecurity. Despite these valuable contributions, a unified, framework- driven \napproach to integrating and eva luating AI agents within established cybersecurity standards remains an area for \nfurther investigation. \nIn the evolving landscape of AI in cybersecurity, this paper aims to develop a framework for selecting, designing, \nand deploying AI agents  to address the NIST CSF 2.0 cybersecurity requirements . Th e contribution of this \nframework is that it provides a structured lens through which to understand, for instance, how the adaptive learning \ncapabilities of learning agents can enhance the Detect and Respond functions of the NIST CSF 2.0 by proactively \nadapting to emerging threat vectors  [14], [41], [42]. Similarly, it clarifies how the strategic planning inherent in \ncognitive/deliberative agents can better facilitate the Governance function by effectively balancing compliance \nrequirements with actionable threat intelligence  [14], [43]. Without a clearly defined mapping, organisations  risk \ndeploying AI agents ill-suited to their specific security needs or failing to fully capitalise on the unique strengths of \nagents optimised for particular operational contexts. Therefore, rigorous conceptual mapping is vital for informing \ndecision-makers and system architects, guiding them towards the most appropriate agent solutions tailored to their \ncybersecurity objectives. This alignment between conceptual rigour  and practical applicability paves the way for \nempirical validation and targeted tool implementations, ultimately fostering effective and sustainable AI -driven \nsecurity operations.  \nThis study is grounded in foundational concepts from agent-based AI, particularly the classification and behavioural \nproperties of AI agents as applied to cybersecurity [21], [22], [24]. The framework builds upon four core agent \narchitectures, reactive, cognitive, hybrid, and learning agents, each defined by its level of autonomy, decision-\nmaking mechanisms, and interaction with dynamic environments  [21], [28], [29], [30], [31], [32]. As mentioned \nearlier, the paper also draws from recognised cybersecurity governance models, primarily the NIST CSF 2.0, which \nserves as the anchor for mapping agent capabilities to structured cybersecurity functions. Additionally, the concept \nof graduated levels of autonomy is introduced, informing the selection logic that guides  agent deployment across \nthe Identify, Protect, Detect, Respond, Recover, and Govern functions of the NIST CSF 2.0. This layered autonomy \nspectrum, ranging from assisted [44], [45], augmented [46], [47], to fully autonomous intelligence [48], [49], [50], \nenables a capability-based interpretation of agent suitability aligned with organisational maturity, task complexity, \nand risk appetite. These theoretical constructs form the foundational lens through which the AI Agent Taxonomy \nand Decision Framework (AIATDF) is developed and validated in this paper.  \nThe methodology for developing the framework employs a matrix-based approach to map specific agent properties \n(e.g., autonomy, adaptiveness, reactivity) to the subcategories and tasks defined by the NIST CSF 2.0. This mapping \nforms the basis for identifying which agent architectures (e.g., reactive, hybrid, learning) are best suited to different \ncybersecurity functions (e.g., Detect, Respond, Recover)  [14], [28], [29] . In a nutshell, the proposed approach is \ngrounded in a structured, matrix- based conceptual mapping of agent capabilities to cybersecurity requirements. \nBelow is a concise overview of the main contributions presented in this paper: \n\u2022 AI Agent Taxonomy and Decision Framework (AIATDF):  \nPresents a structured model for classifying AI agents and systematically mapping their properties to the  \nNIST CSF 2.0 functions (Fig. 6). \n\u2022 Conceptual mapping matrix:  \nProvides a detailed mapping matrix, linking specific AI agent types and capabilities to NIST CSF 2.0 \nsubcategories, facilitating optimal architecture selection for real-world cybersecurity tasks (Table 6). \n\u2022 Graduated levels of autonomy:  \nEstablishes a preliminary capability maturity model (CMM) , enabling organisations with varying \nmaturity levels to incrementally adopt AI -driven solutions, progressing from assisted to augmented and \nautonomous intelligence (Fig. 3). \n\u2022 Holistic NIST CSF 2.0 alignment:  \nDemonstrates the integrated deployment of AI agents across all NIST CSF 2.0 functions, providing an \nend-to-end cybersecurity management approach (Table 6).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4690, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "07481227-7921-4b5e-a7f6-f10e343516eb": {"__data__": {"id_": "07481227-7921-4b5e-a7f6-f10e343516eb", "embedding": null, "metadata": {"page_label": "4", "file_name": "A cybersecurity AI agent selection and decision support framework.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\A cybersecurity AI agent selection and decision support framework.pdf", "file_type": "application/pdf", "file_size": 1115417, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6317a77f-e5be-4269-8575-3ee8813a2132", "node_type": "4", "metadata": {"page_label": "4", "file_name": "A cybersecurity AI agent selection and decision support framework.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\A cybersecurity AI agent selection and decision support framework.pdf", "file_type": "application/pdf", "file_size": 1115417, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "df0de7f9529bca2dd355582f81bc893759c107a909c5ce6d421c47fedc431ddb", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "662cae6e-3041-4439-bc35-d7b071c62124", "node_type": "1", "metadata": {"page_label": "4", "file_name": "A cybersecurity AI agent selection and decision support framework.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\A cybersecurity AI agent selection and decision support framework.pdf", "file_type": "application/pdf", "file_size": 1115417, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "86f083b8e2b86a8854f8f335759f382fccec26eab329498191d0f605c44adeb9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "6). \n\u2022 Conceptual mapping matrix:  \nProvides a detailed mapping matrix, linking specific AI agent types and capabilities to NIST CSF 2.0 \nsubcategories, facilitating optimal architecture selection for real-world cybersecurity tasks (Table 6). \n\u2022 Graduated levels of autonomy:  \nEstablishes a preliminary capability maturity model (CMM) , enabling organisations with varying \nmaturity levels to incrementally adopt AI -driven solutions, progressing from assisted to augmented and \nautonomous intelligence (Fig. 3). \n\u2022 Holistic NIST CSF 2.0 alignment:  \nDemonstrates the integrated deployment of AI agents across all NIST CSF 2.0 functions, providing an \nend-to-end cybersecurity management approach (Table 6). \n\u2022 Rigorous methodology and applied utility:  \nCombines hierarchical task analysis with AI agent theory, yielding a transparent and reproducible \nframework suitable for academic research and applied SOC deployments (Steps beneath Fig. 6). \nThe theoretical framework presented here is based on several core assumptions. Firstly, I assume that the NIST CSF \n2.0 provides sufficiently granular and actionable functions and subcategories for mapping AI agent tasks in \noperational settings. Secondly, I contend that the inherent properties of AI agents, such as autonomy, reactivity, and \nlearning capabilities, can be sufficiently characterised and conceptually aligned with established cybersecurity \nrequirements. Thirdly, the proposed framework assumes a baseline level of digital infrastructure and security  \nmaturity within deploying organisations. Despite these foundational assumptions, the study's primary limitation lies", "mimetype": "text/plain", "start_char_idx": 3982, "end_char_idx": 5618, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "4a0da9a4-05ee-491a-a6d6-5a3a274bd365": {"__data__": {"id_": "4a0da9a4-05ee-491a-a6d6-5a3a274bd365", "embedding": null, "metadata": {"page_label": "5", "file_name": "A cybersecurity AI agent selection and decision support framework.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\A cybersecurity AI agent selection and decision support framework.pdf", "file_type": "application/pdf", "file_size": 1115417, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "04f8d0a1-308f-40f8-92fe-9384ad9d72fb", "node_type": "4", "metadata": {"page_label": "5", "file_name": "A cybersecurity AI agent selection and decision support framework.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\A cybersecurity AI agent selection and decision support framework.pdf", "file_type": "application/pdf", "file_size": 1115417, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "26c2033f5b6bcd2e0d018b2b3a4177ec2ea9d0f87d6e6c1c8144bc6d3be1403e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "4 \n \nin its conceptual genesis; the AIATDF remains to be validated through empirical testing in live cybersecurity \nenvironments. Furthermore, while drawing extensively from scholarly and industry literature, my classification of \nAI agent architectures and prope rties may inherently simplify dynamically evolving, complex hybrid systems. \nThese limitations are elaborated upon in Section 6.3. \nBuilding upon the foundational understanding established in the preceding content , this paper proceeds as follows: \nSection 2 provides a detailed review of the relevant literature, explicitly  exploring the capabilities of various AI \nagent architectures, the tenets of the NIST CSF  2.0, and the rationale for systematically mapping these agent \ncapabilities to the framework's functional requirements. Subsequently, Section 3  outlines the methodological \napproach employed in this study to develop this mapping framework. The framework is presented in Section 4 and \nconceptually validated in Section 5. D iscussions of the mapping framework, its implications,  and inherent \nlimitations are provided in Section 6. Finally, Section 7 concludes the paper by offering key takeaways and practical \nrecommendations for future cybersecurity practices and suggesting promising avenues for further research in this \nevolving domain. \n2. Related Works \n2.1 NIST CSF 2.0 \nThe NIST CSF 2.0 is an updated and comprehensive set of guidelines, standards, and best practices developed by \nthe NIST to help organisations manage and reduce cybersecurity risks [14]. NIST is a non-regulatory federal agency \nthat provides the scientific and technical foundation for innovation, economic growth, and public safety in the \nUnited States of America. The NIST CSF 2.0 is designed to be flexible and adaptable for organisations of all sizes, \nsectors, and cybersecurity maturity levels [14]. \n2.1.1 The evolution of the NIST CSF \nThe NIST CSF has undergone a significant and necessary evolution from its initial release as version 1.0 in 2014 \nto the more comprehensive and globally relevant version 2.0. This progression mirrors the escalating complexity \nand scale of contemporary cybersecurity threats , necessitating more resilient and adaptable security strategies. \nWhile the foundational framework in version 1.0 established five core functions \u2013 Identify, Protect, Detect, \nRespond, and Recover [51], [52], [53]  \u2013 version 2.0 represents a notable enhancement. A key advancement in \nversion 2.0 is the introduction of a dedicated \"Govern\" function, which explicitly elevates  the importance of \ncybersecurity within an organisation's overall governance structure [14], [53] . Therefore, t he NIST CSF 2.0 is \nstructured around six core functions, which are further elaborated through twenty- two categories representing \nspecific cybersecurity outcomes and one hundred and six subcategories detailing more granular technical and \nmanagement activities [14]. Fig. 2 provides a visual representation of this layered structure.  \n \nFig. 2  NIST CSF 2.0 framework core: Depicts the layered composition of the NIST Cybersecurity Framework \n2.0, showing how the 22 categories and 106 subcategories are organised under the six core functions of the \nframework", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3238, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "39a8b576-95ee-4f2b-bf50-b61a12715802": {"__data__": {"id_": "39a8b576-95ee-4f2b-bf50-b61a12715802", "embedding": null, "metadata": {"page_label": "6", "file_name": "A cybersecurity AI agent selection and decision support framework.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\A cybersecurity AI agent selection and decision support framework.pdf", "file_type": "application/pdf", "file_size": 1115417, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4b1d74f3-f204-45e7-af3d-fe1d5c283494", "node_type": "4", "metadata": {"page_label": "6", "file_name": "A cybersecurity AI agent selection and decision support framework.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\A cybersecurity AI agent selection and decision support framework.pdf", "file_type": "application/pdf", "file_size": 1115417, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "133ad218bd9d1a81a1a54732dd429e263c99e08195eb2c567cec92ab544163b6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "5 \n \nThe twenty-two NIST CSF 2.0 categories are summarised in Table 1 [14]. \nTable 1 NIST CSF 2.0 categories: The 22 categories within the updated NIST Cybersecurity Framework 2.0 are \nlisted, grouping them by the six core functions (Govern, Identify, Protect, Detect, Respond, Recover)  \nFunction Category \nGovern o Organisational context  \no Risk management strategy  \no Roles, responsibilities, and authorities  \no Policy  \no Oversight  \no Cybersecurity supply chain risk management  \nIdentify o Asset management  \no Risk assessment  \no Improvement  \nProtect o Identity management, authentication, and access control  \no Awareness and training  \no Data security  \no Platform security  \no Technology infrastructure resilience  \nDetect o Continuous monitoring  \no Adverse event analysis  \nRespond o Incident management  \no Incident analysis  \no Incident response reporting and communication  \no Incident mitigation  \nRecover o Incident Recovery Plan Execution  \no Incident Recovery Communication \nFurthermore, the updated framework broadens its scope with a global perspective, ensuring its applicability and \nutility for worldwide organisations  [52], [53] . Version 2.0 provides more detailed guidance and actionable \nrecommendations, offering practical strategies and steps for cybersecurity professionals at all levels [53]. This \nenhanced version also explicitly addresses the rapid advancements in technology, including the Internet of Things, \ncloud computing, and big data, offering specific guidelines to manage the associated cybersecurity risks [14], [54], \n[55]. Moreover, the updated framework also emphasises a risk-based approach, equipping organisations with more \nsolid methods for identifying, assessing, and effectively managing potential threats [14], [53] . Ultimately, version \n2.0 empowers organisations to develop more comprehensive and threat -informed cybersecurity strategies tailored \nto the specific needs of diverse sectors and industries [52], [53] . The expanded functions and detailed guidance in \nversion 2.0 provide cybersecurity professionals with a more precise and actionable roadmap for implementing \neffective security measures, including hundreds of immediately applicable recommendations [14], [53] . This \nenhanced international applicability positions version 2.0 as a valuable tool for standardising cybersecurity practices \nand fostering greater global cybersecurity resilience [14], [53].  \nHowever, because the functions, categories and subcategories are designed to be high-level and adaptable to various \norganisations, they only outline what  cybersecurity outcomes should be achieved, but not necessarily how . \n\u2018Informative references\u2019 of the NIST CSF 2.0 provide the \"how\" by pointing to specific standards, guidelines, and \nbest practices that offer detailed implementation guidance [14]. This is the level at which more granular \ncybersecurity controls are found and implemented.  Thus, a clearly defined mapping framework is required to \nsystematically examine the alignment between distinct AI agent capabilities and the functional pillars of the NIST \nCSF 2.0. \n2.1.2 Rationale for mapping AI agents' capabilities to the NIST CSF 2.0 \nReflecting the rapidly evolving cybersecurity landscape, NIST has proactively initiated an examination into how \nexisting frameworks, such as the NIST CSF 2.0, can assist organisations in navigating emerging and expanding \nrisks. As a testament to this, on February 14, 2025, NIST [56] published a concept paper for a cybersecurity and AI \nworkshop, seeking public input on the critical challenge of addressing cybersecurity risks associated with the \ndevelopment and deployment of AI . The subsequent workshop, held on April 3, 2025, further underscored this \nfocus. In essence, the  NIST [56] concept paper centres  on identifying and mitigating AI -related sources of \ncybersecurity risk that can significantly impact an organisation's operational risk profile. Th e concept  paper \ncategorises these risks into three key areas: (i) cybersecurity of AI systems, (ii) AI -enabled cyber attacks, and (iii) \nAI-enabled cyber defence, which align with the categories of adversarial AI, offensive AI, and defensive AI, \nrespectively, previously described by Malatji and Tolah [57]. To quote NIST [56] directly, \u201cthere is no consistent \ntaxonomy or agreement on how AI advances inform organisations\u2019 strategies for cybersecurity risk management.\u201d", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4438, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "fcf0646c-36eb-4a5c-91c5-931b603b93c1": {"__data__": {"id_": "fcf0646c-36eb-4a5c-91c5-931b603b93c1", "embedding": null, "metadata": {"page_label": "7", "file_name": "A cybersecurity AI agent selection and decision support framework.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\A cybersecurity AI agent selection and decision support framework.pdf", "file_type": "application/pdf", "file_size": 1115417, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0a3a9eab-cd91-40b5-a082-99f565c8aa9d", "node_type": "4", "metadata": {"page_label": "7", "file_name": "A cybersecurity AI agent selection and decision support framework.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\A cybersecurity AI agent selection and decision support framework.pdf", "file_type": "application/pdf", "file_size": 1115417, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "40fc1c121908f5b9413d1076a3ce8cc676980f83021b36a58472e2485b0e9ddf", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d28ffa8e-4319-4369-85be-ed2907ea7a51", "node_type": "1", "metadata": {}, "hash": "aede6395eee76fd1d6108f956ead61880283e692a9dfa6154fa170f730e28870", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "6 \n \nThey propose a \u201cCyber AI Profile\u201d for guiding organisations deploying AI technologies and/or defending against \nAI-enabled attacks. At the time of writing, NIST is collaborating with its partners and stakeholders on the \u201cCyber \nAI Profile.\u201d \nAgainst this backdrop, this paper's rationale for  mapping AI agents to the NIST CSF 2.0 is that the strategic \nintegration of these agents with the NIST CSF 2.0 offers a  significant (autonomous ) opportunity to fortify an \norganisation's cybersecurity posture across its core functions. This involves strategically leveraging AI's inherent \ncapabilities to enhance asset identification, bolster threat detection, strengthen protective measures, streamline \nincident response, and accelerate recovery processes [4], [58], [59]. By embedding AI agents within these and other \ncybersecurity functions, organisations can anticipate even more improvements in response times and ensure more \nresilient, continuous protection against the ever -evolving threat landscape.  Therefore, a  structured mapping and \ndecision-making tool is essential for effectively aligning the technical capabilities of diverse AI agent architectures \nwith the specific security tasks that define an organisation's overall cybersecurity posture. Ultimately, the absence \nof such a structured mapping can directly contribute to security vulnerabilities and potentially lead to SOC \nunderperformance due to the deployment of ill -suited AI agent solutions.  In the next section, I review existing \nliterature on the generic application of AI on the NIST CSF 2.0. \n2.1.3 Application of AI on NIST CSF  \nThis paper distinguishes  between automation and autonomy within the context of AI/ML. From this perspective, \nthe core differentiation hinges on the degree of intelligence embedded within the system and the extent of human \ninvolvement in the decision- making process [60], [61]. In this context, automation refers to AI- enhanced systems \nthat execute predefined rules or algorithms to accomplish tasks while maintaining a significant degree of human \ncontrol over the system's actions [62], [63] . These systems, even when employing sophisticated ML  for \noptimisation, operate within the constraints of their initial programming. A typical example is a spam filter, which \nutilises algorithms and learned patterns to categorise emails based on pre -set parameters [64] , demonstrating \nadaptability but not fundamentally altering its operational logic. \nConversely, autonomy describes AI systems characterised  by their capacity for independent decision- making in \ndynamic and unpredictable environments [48], [49], [50]. These systems exhibit a greater degree of \"agency\" [65] , \nenabling them to perceive their surroundings, interpret information, formulate plans, and execute actions to achieve \nobjectives without explicit, step -by-step instructions for every possible scenario [24], [25], [26], [27]. A prime \nillustration is a self-driving car, which must navigate complex traffic scenarios, adapt to unexpected obstacles, and \nmake real-time decisions regarding speed, direction, and braking [59], [66], [67]. While reliant on algorithms and \npre-trained models, a self-driving car\u2019s ability to respond to novel and unforeseen situations distinguishes it from a \npurely automated system.  Therefore, the fundamental distinction between automation and autonomy lies in the \nsystem's inherent capacity for adaptive behaviour, independent problem -solving, and the level of human \nintervention [48], [49], [50]. Automated systems excel in optimising  predictable processes, whereas autonomous \nsystems are designed to tackle unpredictable ones, necessitating continuous learning, adaptation, and real -time \njudgment [68]. This critical distinction underpins understanding the varying potential and limitations of different \nAI applications, a theme explored throughout this paper.  \nAccording to Morovat and Panda  [69] and Dehghantanha et al. [68] , cybersecurity paradigms evolve  from \ntraditional to AI -enhanced to autonomous. Traditional cybersecurity involves signature -based or rule -based \ndefences, characterised by their reactive nature and limited capacity for dynamic analysis of novel threats [70]. This \napproach primarily responds to known threat patterns, lacking the agility to adapt to the ever -evolving adversary. \nAI-driven cybersecurity represents a significant advancement, integrating automated responses and leveraging ML \nfor more flexible threat detection [10]. This shift transitions from static, rule-driven methodologies to sophisticated \npattern-based analytics and accelerated incident triage.  This paper focuses on the most advanced paradigm : \nautonomous cybersecurity. This paradigm leverages AI agents capable of reinforcement learning (RL), continuous \nself-improvement, and predictive orchestration across complex, distributed systems  [71], [72], [73] .", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4927, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d28ffa8e-4319-4369-85be-ed2907ea7a51": {"__data__": {"id_": "d28ffa8e-4319-4369-85be-ed2907ea7a51", "embedding": null, "metadata": {"page_label": "7", "file_name": "A cybersecurity AI agent selection and decision support framework.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\A cybersecurity AI agent selection and decision support framework.pdf", "file_type": "application/pdf", "file_size": 1115417, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0a3a9eab-cd91-40b5-a082-99f565c8aa9d", "node_type": "4", "metadata": {"page_label": "7", "file_name": "A cybersecurity AI agent selection and decision support framework.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\A cybersecurity AI agent selection and decision support framework.pdf", "file_type": "application/pdf", "file_size": 1115417, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "40fc1c121908f5b9413d1076a3ce8cc676980f83021b36a58472e2485b0e9ddf", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fcf0646c-36eb-4a5c-91c5-931b603b93c1", "node_type": "1", "metadata": {"page_label": "7", "file_name": "A cybersecurity AI agent selection and decision support framework.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\A cybersecurity AI agent selection and decision support framework.pdf", "file_type": "application/pdf", "file_size": 1115417, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "ffdd3770fe1a7d561d322c39e0b9ecb88965768ede5cdf4c9b833eccab34ae32", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[68] , cybersecurity paradigms evolve  from \ntraditional to AI -enhanced to autonomous. Traditional cybersecurity involves signature -based or rule -based \ndefences, characterised by their reactive nature and limited capacity for dynamic analysis of novel threats [70]. This \napproach primarily responds to known threat patterns, lacking the agility to adapt to the ever -evolving adversary. \nAI-driven cybersecurity represents a significant advancement, integrating automated responses and leveraging ML \nfor more flexible threat detection [10]. This shift transitions from static, rule-driven methodologies to sophisticated \npattern-based analytics and accelerated incident triage.  This paper focuses on the most advanced paradigm : \nautonomous cybersecurity. This paradigm leverages AI agents capable of reinforcement learning (RL), continuous \nself-improvement, and predictive orchestration across complex, distributed systems  [71], [72], [73] . A core \nemphasis of this approach lies in developing XAI and implementing  highly personalised threat defences that can \nrapidly adapt to the dynamic and unpredictable nature of modern attack landscapes  [74], [75]. Furthermore, as AI \nagents are increasingly embedded in cybersecurity infrastructures, concerns about transparency, fairness, and \nprivacy are becoming central to their design and deployment. As highlighted by Radanliev [76] , ethical AI \ndevelopment demands proactive strategies that address data bias, algorithmic opacity, and differential privacy. \nThese ethical safeguards are critical not only for user trust but also for regulatory compliance in sensitive domains \nlike cybersecurity [9], [10], where decision-making must be both explainable and justifiable under audit conditions \n[77]. Incorporating such perspectives ensures that AI agents are not merely technically capable but also socially \nresponsible and accountable.", "mimetype": "text/plain", "start_char_idx": 3976, "end_char_idx": 5875, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "362e4686-5809-4165-b2c3-31a3998beeee": {"__data__": {"id_": "362e4686-5809-4165-b2c3-31a3998beeee", "embedding": null, "metadata": {"page_label": "8", "file_name": "A cybersecurity AI agent selection and decision support framework.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\A cybersecurity AI agent selection and decision support framework.pdf", "file_type": "application/pdf", "file_size": 1115417, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "958cd863-8c06-4326-b6b7-9b02d03b9cc9", "node_type": "4", "metadata": {"page_label": "8", "file_name": "A cybersecurity AI agent selection and decision support framework.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\A cybersecurity AI agent selection and decision support framework.pdf", "file_type": "application/pdf", "file_size": 1115417, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "481a8d7c663f11c98eb6d2233e8fcc419564977fb61f6ea4993611949a2f2a4e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "7 \n \nBuilding upon the works  of Dehghantanha et al. [68] and Morovat and Panda  [69], this paper distinguishes four  \nprimary modes of AI system intelligence in cybersecurity . Fig. 3 illustrates the graduated levels of autonomy, \nranging from purely manual (human- driven) to assisted intelligence, augmented intelligence, and ultimately , \ncomplete autonomy. \n \nFig. 3 Graduated levels of autonomy in cybersecurity: Visualises a CMM\u2019s progression from manual, human -\ndriven modes to assisted, augmented, and fully autonomous intelligence within cybersecurity contexts  \nThe manual or human-driven mode in Fig. 3 represents a state without automation or AI integration, relying entirely \non human expertise.  Assisted intelligence  enhances human capabilities and cognitive functions  by providing \ninformation, automating specific tasks, or offering insights  [44], [45]. Within this mode, AI systems act as tools to \nenhance human abilities for efficiency and effectiveness, with ultimate responsibility for outcomes remaining with \nhuman operators. A prime example is healthcare, where assistive AI aids clinicians in making more informed \ndiagnostic or management decisions, as seen in applications ranging from virtual informati cs systems for health \nmanagement to physical robotic -assisted surgeries [78]. The previously mentioned cybersecurity spam filter also \nexemplifies this category, as the human operator retains responsibility for email filtering, positioning assisted \nintelligence as a form of AI/ML -driven automation.  Augmented intelligence  goes a step further  than assisted \nintelligence. It is about a collaborative partnership between human intelligence and AI to amplify human capabilities \nby leveraging the unique strengths of both while mitigating their inherent weaknesses [46], [47].  \nWhile assisted intelligence inherently operates within the human -in-the-loop (HITL) human -machine interaction \n(HMI) paradigm, augmented intelligence possesses the capability to function across both HITL and the more \nadvanced human-out-of-the-loop (full autonomy) HMI paradigm [79], [80], [81], [82], [83]. Therefore, augmented \nintelligence exhibits characteristics of automation and autonomy , effectively representing a hybrid mode that \nbridges assisted and autonomous intelligence . In cybersecurity, augmented intelligence enhances threat detection \nand response by combining human expertise with the analytical power of AI/ ML, leading to more accurate and \nefficient handling of complex threats [84] . Finally, autonomous intelligence , as discussed at the beginning of this \nsection, encompasses systems capable of independent task execution without direct human intervention, relying on \nadvanced AI /ML techniques [48], [49], [50] . These systems are increasingly prevalent across diverse domains,", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2832, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3941b510-279e-4797-84a1-1839af8fc475": {"__data__": {"id_": "3941b510-279e-4797-84a1-1839af8fc475", "embedding": null, "metadata": {"page_label": "9", "file_name": "A cybersecurity AI agent selection and decision support framework.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\A cybersecurity AI agent selection and decision support framework.pdf", "file_type": "application/pdf", "file_size": 1115417, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "51ab4d3a-f32a-48a6-b92c-ffb52d2f78db", "node_type": "4", "metadata": {"page_label": "9", "file_name": "A cybersecurity AI agent selection and decision support framework.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\A cybersecurity AI agent selection and decision support framework.pdf", "file_type": "application/pdf", "file_size": 1115417, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "3a92265c2753374eaf9833c324aaefecbfa704f276946f47a85cbd378fb63e25", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "8 \n \nincluding healthcare, military, transportation, and cybersecurity [83], [85], [86]. This paper focuses explicitly on AI \nagents operating as autonomous intelligent entities within the cybersecurity domain , while acknowledging their \npotential deployment in augmented intelligence mode. To the best of my knowledge, only Salesforce [87] released \na similar AI agent CMM with graduated levels of autonomy, as shown in Fig. 3, on April 10, 2025. In other words, \nthere is a shortage of scholarly, well -defined CMMs for the adoption of  AI agents. This avenue forms part of my \nfuture research as it is beyond the scope of this paper. \nBeyond the current paradigm of autonomous AI agents in cybersecurity, emerging research heralds a future of \ntransformative capabilities. Frontier research explores quantum-safe AI algorithms to fortify defences against future \nquantum attacks [88] and privacy-preserving ML, including homomorphic encryption and differential privacy [89], \nto ensure data security without compromising agent efficacy. Federated learning is also emerging [90], promising \ndecentralised AI pipelines that enable collaborative learning while preserving data locality and regulatory \ncompliance. Though currently in development, these cutting-edge technologies hold the potential to redefine agent \narchitectures and significantly enhance cybersecurity resilience.  In this paper, t hirty-five units of analysis were \nobtained from a systematic literature review (SLR) process through Scopus (12 papers), IEEE (5 papers), and \nGoogle Scholar (18 papers) databases.  The key phrase \u201cAI agents in cybersecurity\u201d was utilised to search for the \nrelevant literature. The notable outcomes of the SLR are summarised in Table 2. \nTable 2  Literature review of the application of AI on NIST CSF: Summarises existing studies that intersect \ncybersecurity, AI agents, and NIST CSF, indicating whether each study addressed the NIST CSF and its functions", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1961, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "7ceba8cf-fd94-4296-b757-ee6f59baa14e": {"__data__": {"id_": "7ceba8cf-fd94-4296-b757-ee6f59baa14e", "embedding": null, "metadata": {"page_label": "10", "file_name": "A cybersecurity AI agent selection and decision support framework.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\A cybersecurity AI agent selection and decision support framework.pdf", "file_type": "application/pdf", "file_size": 1115417, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d150f798-78fb-4979-94e1-a803e1e43a06", "node_type": "4", "metadata": {"page_label": "10", "file_name": "A cybersecurity AI agent selection and decision support framework.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\A cybersecurity AI agent selection and decision support framework.pdf", "file_type": "application/pdf", "file_size": 1115417, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "10a61caf3302a3ff46ebfd1769d6bf5e32a4c1a811cebc904cb7d51b5f18b7e0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "39b60357-2add-471c-abb5-6c3ca1d9fa21", "node_type": "1", "metadata": {}, "hash": "f8cf2dea22d4a132c656b487cdaf3a64a746e285f8df8df8d41f0a4a0d74b706", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "9 \n \nAuthor Purpose of the study Focus on  \ncybersecurity \nFocus on  \nAI agents \nFocus on \nNIST CSF  \nNIST [56] Evaluate how an updated NIST CSF 2.0 can assist \norganisations facing new or expanded AI-related risks.  \uf0fc \uf0fb Potentially \nCronin [72] Reviews generic AI agents and those that use Generative AI. \uf0fb \uf0fc \uf0fb \nYu et al. [91] Explore the potential for blockchain to be a foundational \ninfrastructure for AI agents in the metaverse. \uf0fb \uf0fc \uf0fb \nHan et al. [92] Propose a multi-agent system ( MAS) to enhance financial \ninvestment research. \uf0fb \uf0fc \uf0fb \nChan et al. [71] Evaluate metrics to increase visibility into AI agents. \uf0fb \uf0fc \uf0fb \nChen et al. [93] Explore the role of 6G in realising AI agents\u2019 potential. \uf0fb \uf0fc \uf0fb \nBovo et al. [94] Explore large language models ( LLM) agents in extended \nreality environments. \uf0fb \uf0fc \uf0fb \nBaabdullah [95] Explores the impact of decision -making efficiency facilitated \nby generative conversational AI agents. \uf0fb \uf0fc \uf0fb \nHuang et al. [96] Explore embodied AI agent systems. \uf0fb \uf0fc \uf0fb \nAgashe et al. [97] Propose agent S, an open agentic framework.  \uf0fb \uf0fc \uf0fb \nKim and Saad [98] Propose a novel continual learning algorithm for AI agents. \uf0fb \uf0fc \uf0fb \nPleshakova et al. [99] Create a neural network architecture for multi-agent tasks.  \uf0fc \uf0fc \uf0fc \nSharma and Jindal [40] Explore the role of AI agents in various domains. \uf0fb \uf0fc \uf0fb \nDehghantanha et al. [68] \nProvide an overview of the current state of autonomous \ncybersecurity, highlighting the key challenges and \nopportunities. \n\uf0fc \uf0fc \uf0fb \nHauptman et al. [100] Explore how a team\u2019s work cycle could guide an AI agent\u2019s \nchanging level of autonomy. \uf0fc \uf0fc \uf0fc \nKott  et al. [36] Advance the reference architecture work on the AICA. \uf0fc \uf0fc \uf0fb \nKaur et al. [101] Analysed 236 AI use cases for cybersecurity provisioning \nagainst the NIST CSF 1.1. \uf0fc \uf0fc \uf0fc \nCa\u00f1as [102] Studies human and AI agents' responsibility when interacting. \uf0fb \uf0fc \uf0fb \nNaik [39] Review the application of AI techniques in fighting various \ncyberattacks. \uf0fc \uf0fb \uf0fb \nRoy [103] Propose a MAS that detects and neutralises unseen cyber \nanomalies. \uf0fc \uf0fc \uf0fb \nLi et al. [104] Train AI agents using the Cyber Gym for Intelligent Learning. \uf0fb \uf0fc \uf0fb \nLigo et al. [38] Examine approaches to measuring cyber -resilience systems \nwith autonomous agents. \uf0fc \uf0fc \uf0fb \nPrasad et al. [77] Evaluate AI agents\u2019 decisions using the Testing with Concept \nActivation Vectors XAI technique. \uf0fc \uf0fc \uf0fb \nAshktorab  et al. [105] Investigate the social perceptions of AI agents. \uf0fb \uf0fc \uf0fb \nZolotukhin et al. [106] Explore attack mitigation techniques in software -defined \nnetworking (SDN) environments. \uf0fc \uf0fc \uf0fb \nCao et al. [107] Address fifth-generation (5G) SDN challenges with AI agents.  \uf0fb \uf0fc \uf0fb \nFranco et al. [108] Introduce a cybersecurity-driven conversational agent. \uf0fc \uf0fc \uf0fb \nMorovat and Panda [69] Review the impact of AI on cybersecurity. \uf0fc \uf0fb \uf0fb \nTruong et al.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2808, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "39b60357-2add-471c-abb5-6c3ca1d9fa21": {"__data__": {"id_": "39b60357-2add-471c-abb5-6c3ca1d9fa21", "embedding": null, "metadata": {"page_label": "10", "file_name": "A cybersecurity AI agent selection and decision support framework.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\A cybersecurity AI agent selection and decision support framework.pdf", "file_type": "application/pdf", "file_size": 1115417, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d150f798-78fb-4979-94e1-a803e1e43a06", "node_type": "4", "metadata": {"page_label": "10", "file_name": "A cybersecurity AI agent selection and decision support framework.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\A cybersecurity AI agent selection and decision support framework.pdf", "file_type": "application/pdf", "file_size": 1115417, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "10a61caf3302a3ff46ebfd1769d6bf5e32a4c1a811cebc904cb7d51b5f18b7e0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7ceba8cf-fd94-4296-b757-ee6f59baa14e", "node_type": "1", "metadata": {"page_label": "10", "file_name": "A cybersecurity AI agent selection and decision support framework.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\A cybersecurity AI agent selection and decision support framework.pdf", "file_type": "application/pdf", "file_size": 1115417, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "b11133b711ab32a79f66637ba71ea4cf1132a7521d47098d08cdb951d9387171", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[77] Evaluate AI agents\u2019 decisions using the Testing with Concept \nActivation Vectors XAI technique. \uf0fc \uf0fc \uf0fb \nAshktorab  et al. [105] Investigate the social perceptions of AI agents. \uf0fb \uf0fc \uf0fb \nZolotukhin et al. [106] Explore attack mitigation techniques in software -defined \nnetworking (SDN) environments. \uf0fc \uf0fc \uf0fb \nCao et al. [107] Address fifth-generation (5G) SDN challenges with AI agents.  \uf0fb \uf0fc \uf0fb \nFranco et al. [108] Introduce a cybersecurity-driven conversational agent. \uf0fc \uf0fc \uf0fb \nMorovat and Panda [69] Review the impact of AI on cybersecurity. \uf0fc \uf0fb \uf0fb \nTruong et al. [37] Provide an overview of how AI can be used in cybersecurity \nfor offensive and defensive AI. \uf0fc \uf0fb \uf0fb \nKott and Th\u00e9ron [109] Advance work on AICA and introduce its high-level reference \narchitecture. \uf0fc \uf0fc \uf0fb \nTh\u00e9ron and Kott [35] Advance work on AICA. \uf0fc \uf0fc \uf0fb \nTh\u00e9ron et al. [110] Introduce the concept and architecture of an autonomous \nintelligent cyber-defence agent (AICA). \uf0fc \uf0fc \uf0fb \nGrzonka et al. [111] Present a MAS cloud monitoring model. \uf0fb \uf0fc \uf0fb \nYampolskiy [112] Explains AI safety vs cybersecurity. \uf0fb \uf0fc \uf0fb \nPetrovi\u0107 [113] Explores AI agents in virtual worlds. \uf0fb \uf0fc \uf0fb \nA significant research gap is evident in the literature, as only three studies, indicated in Table 2, have examined the \nconvergence of cybersecurity, AI agents, and the NIST CSF. Pleshakova et al. [99] briefly model attackers by neural \nnetworks, highlighting the capabilities of decentralised LLMs, or autonomous LLM agent swarms, into distinct \ncyber operations categories aligned with four functions of the NIST CSF 1.1 (Identify, Protect, Detect, Respond) . \nNotwithstanding that the current paper concerns the NIST CSF 2.0, Pleshakova et al. [99] do not map any AI agent \ncapabilities to the NIST CSF functions . Hauptman et al. [100] further narrowed the scope, concentrating solely on \nthe Response function of the NIST CSF 1.1. Notably, Kaur et al. [101] recognised the importance of governance in \ncybersecurity preceding the release of NIST CSF 2.0. They astutely argued for AI's role in policy enforcement and \nrisk monitoring, aligning with the framework's eventual inclusion of the 'Govern' function. However, as shown in \nFig. 4, Kaur et al. [101] specification of AI-driven techniques was ultimately grounded in the outdated NIST CSF \n1.1, missing critical updates and changes in NIST CSF 2.0.", "mimetype": "text/plain", "start_char_idx": 2246, "end_char_idx": 4587, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "974db033-555a-4661-979c-0618c0541dea": {"__data__": {"id_": "974db033-555a-4661-979c-0618c0541dea", "embedding": null, "metadata": {"page_label": "11", "file_name": "A cybersecurity AI agent selection and decision support framework.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\A cybersecurity AI agent selection and decision support framework.pdf", "file_type": "application/pdf", "file_size": 1115417, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "adc017cc-ac1a-4f0e-ac74-76d2445ee2bc", "node_type": "4", "metadata": {"page_label": "11", "file_name": "A cybersecurity AI agent selection and decision support framework.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\A cybersecurity AI agent selection and decision support framework.pdf", "file_type": "application/pdf", "file_size": 1115417, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "5f4a247bf0865aff1d93e73498e3f1b6302a00414c6b3d87bea118c3df2999a5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "10 \n \n \nFig. 4 AI domain per the NIST CSF 1.1 function: Shows how AI-based techniques map to five original NIST CSF \n1.1 functions (Identify, Protect, Detect, Respond, Recover) before the addition of the Govern function  \nA further limitation in Kaur et al. [101]  approach is their reliance on terms like 'AI -based,' 'AI-powered,' and 'AI \ncan automate,' suggesting a focus on automation or  assisted intelligence (refer to Fig.  3). This contrasts with the \nexploration of AI agents (autonomous intelligence), a critical aspect of this paper. The phrase 'AI agent' is used only \nonce in their work. While the AI automation techniques in Fig. 4, such as planning, reasoning and learning, exhibit \nsome overlap with basic AI agent properties  discussed in the intr oduction section and elaborated upon further in \nSection 2.2.3, the study, like Pleshakova et al. [99] and Hauptman et al. [100], did not address the NIST CSF 2.0. \nTherefore, the systematic mapping of AI agents to the current NIST CSF 2.0 remains a conspicuous research gap \nthat requires further investigation. \n2.2 Foundations of AI agents \n2.2.1 Large-language models \nDriven by DL  architectures and massive pre- training, LLMs have emerged as a transformative force in n atural \nlanguage processing [114]. With transformer-based architectures like OpenAI\u2019s Generative Pre-trained Transformer \n(GPT) series, Google\u2019s Gemini series, and Anthropic\u2019s Claude series , LLMs  showcase unparalleled abilities in \ncapturing and generating human- like text [92], [115] . However, this general -purpose proficiency masks critical \nlimitations. LLMs, without customisation , struggle with domain- specific knowledge and proprietary data, making \nthem unsuitable for many real -world applications  [114]. The prohibitive cost and resource demands of training \nLLMs from scratch further necessitate the development of tailored solutions  [116]. As a result, the field has \nwitnessed an explosion of customisation  strategies, broadly falling into two distinct categories, designed to adapt \nLLMs to specific application contexts: \n\u2022 Parameter-efficient fine-tuning (PEFT) or frozen model adaptation: PEFT techniques adapt pre-trained \nLLMs by training only a small subset of newly introduced parameters, preserving/freezing  the original \nmodel's weights and significantly reducing computational costs [116].  \n\u2022 Full fine-tuning: In contrast, full fine -tuning modifies all parameters of a pre -trained LLM to optimise \nperformance on a specific task, demanding more computational resources and potentially altering the \nmodel's general knowledge [117].", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2612, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "7ef21053-d125-42f8-9ccb-7b613ecae9b1": {"__data__": {"id_": "7ef21053-d125-42f8-9ccb-7b613ecae9b1", "embedding": null, "metadata": {"page_label": "12", "file_name": "A cybersecurity AI agent selection and decision support framework.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\A cybersecurity AI agent selection and decision support framework.pdf", "file_type": "application/pdf", "file_size": 1115417, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0f1c73c9-b684-4671-aa71-47e6eb8ac488", "node_type": "4", "metadata": {"page_label": "12", "file_name": "A cybersecurity AI agent selection and decision support framework.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\A cybersecurity AI agent selection and decision support framework.pdf", "file_type": "application/pdf", "file_size": 1115417, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "585632617bd840d020eab73f1835620130ddd1e4fc00d3fb599c87c94f769de2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b3b37b44-10e0-4975-8265-9ce4ee96e7f2", "node_type": "1", "metadata": {}, "hash": "7f3a1658e260a1db7ac01a676e2e2406e4304446e47f06594feeb079a140b01b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "11 \n \nAfter selecting a foundational LLM, a customisation strategy is required  to facilitate the adaptation of the LLM \nfor specialised applications. This paper presents a spectrum of five prevalent customisation strategies, ordered by \nprogressively increasing resource demands and computational expenditure: \n\u2022 Prompt engineering (Strategy: PEFT-adjacent): This strategy, which includes in-context learning and \nrequires minimal resource investment, focuses on carefully crafting input prompts to elicit desired LLM \nresponses, effectively steering the model's inherent capabilities without parameter modification  [118]. \n\u2022 Retrieval augmented generation (RAG) (S trategy: PEFT-adjacent): Employing moderate resource \nconsumption, RAG augments LLM responses with external knowledge retrieval, enhancing accuracy \nand relevance by incorporating real-time information [119]. \n\u2022 Agent frameworks ( Strategy: PEFT-adjacent/Hybrid): These agentic frameworks, with increasingly \nresource-intensive demands, enable LLMs to interact with external environments and tools, facilitating \nthe execution of complex tasks through dynamic interaction behaviour [71] . This approach can be seen \nas a hybrid, as some agents may modify small parameters while others do not. \n\u2022 Fine-tuning (Strategy: Full fine-tuning): This strategy involves significant resource allocation to adapt \nLLM parameters using domain -specific datasets, resulting in enhanced performance for targeted \napplications [117], [118]. \n\u2022 Reinforcement learning from h uman feedback (RLHF) ( Strategy: Full fine-tuning/Hybrid): Requiring \nmaximum resource expenditure, RLHF refines LLM behaviour  through human preference -based \nlearning, aligning the model's outputs with desired human values and preferences  [119]. This also has \nhybrid qualities, as some implementations only modify the \"reward\" model, while others modify the base \nLLM. \nCustomisation s trategies designated as 'PEFT -adjacent/Hybrid' represent approaches that, while not strictly \nadhering to standard PEFT methodologies, either share core principles of minimising  pre-trained weight \nmodifications or integrate aspects of PEFT and full fine -tuning. Similarly, the designation 'Full fine-\ntuning/Hybrid' indicates strategies that predominantly rely on full fine -tuning, modifying all model parameters, \nbut may also integrate aspects of parameter -efficient methods or possess implementations that vary, resulting in \na combination of full and parameter-efficient fine-tuning characteristics.  \nThis paper focuses on agent frameworks , a customisation  strategy that enables LLMs to interact with external \nenvironments and tools. On November 25, 2024, Anthropic [120] introduced the Model Context Protocol (MCP), \na new standard for connecting AI models to diverse external data systems, including business tools, content \nrepositories, and development environments.   Thus, the MCP is a n open standard designed to simplify the \nintegration of AI agents with real -world data , providing context to underlying LLMs [121]. Additionally, in \ncollaboration with a wide range of industry partners (Deloitte, Langchain, Salesforce, Cohere, and fifty- six \nothers), Google's launch of the Agent2Agent (A2A) protocol on April 9, 2025, an open standard that complements \nthe MCP, is poised to accelerate the evolution of interconnected AI agents by enabling cross -vendor \ninteroperability between agents [122]. A good understanding of AI agents necessitates thoroughly examining their \ncore principles, inherent properties, and the range of agent types. \n2.2.2 Defining AI agents \nBuilding on the discussion of LLM customisation strategies, this paper explores the critical role of AI agents within \nagentic frameworks. To contextualise this, it is essential to clarify the concept of 'agency' in AI systems. 'Agency' \ndenotes the extent to which an AI system's behaviour is goal-directed, directly impacts its environment, and enables \nit to achieve long-term objectives with minimal human intervention [71]. This implies a shift from passive AI tools \nto active decision-making entities. 'Agentic AI,' therefore, represents the overarching framework that empowers AI \nagents to operate with heightened autonomy. It encompasses the broader capabilities that enable LLMs to function \nas agents, facilitating complex interactions and decision -making processes [123]. In this context, an AI agent is a \nspecific instantiation or component within an agentic AI system designed to execute particular tasks or functions. \nThis distinction between the broader agentic AI framework and the specific AI agent implementation is crucial for \nunderstanding the nuanced application of LLMs in autonomous systems.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4731, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b3b37b44-10e0-4975-8265-9ce4ee96e7f2": {"__data__": {"id_": "b3b37b44-10e0-4975-8265-9ce4ee96e7f2", "embedding": null, "metadata": {"page_label": "12", "file_name": "A cybersecurity AI agent selection and decision support framework.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\A cybersecurity AI agent selection and decision support framework.pdf", "file_type": "application/pdf", "file_size": 1115417, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0f1c73c9-b684-4671-aa71-47e6eb8ac488", "node_type": "4", "metadata": {"page_label": "12", "file_name": "A cybersecurity AI agent selection and decision support framework.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\A cybersecurity AI agent selection and decision support framework.pdf", "file_type": "application/pdf", "file_size": 1115417, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "585632617bd840d020eab73f1835620130ddd1e4fc00d3fb599c87c94f769de2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7ef21053-d125-42f8-9ccb-7b613ecae9b1", "node_type": "1", "metadata": {"page_label": "12", "file_name": "A cybersecurity AI agent selection and decision support framework.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\A cybersecurity AI agent selection and decision support framework.pdf", "file_type": "application/pdf", "file_size": 1115417, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "633dbfee8a0808326607ed3b5d4a47a47442ed6d6ad23b8bc228bb7b6173f0b5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "To contextualise this, it is essential to clarify the concept of 'agency' in AI systems. 'Agency' \ndenotes the extent to which an AI system's behaviour is goal-directed, directly impacts its environment, and enables \nit to achieve long-term objectives with minimal human intervention [71]. This implies a shift from passive AI tools \nto active decision-making entities. 'Agentic AI,' therefore, represents the overarching framework that empowers AI \nagents to operate with heightened autonomy. It encompasses the broader capabilities that enable LLMs to function \nas agents, facilitating complex interactions and decision -making processes [123]. In this context, an AI agent is a \nspecific instantiation or component within an agentic AI system designed to execute particular tasks or functions. \nThis distinction between the broader agentic AI framework and the specific AI agent implementation is crucial for \nunderstanding the nuanced application of LLMs in autonomous systems. Recognising that agentic AI is a system  \nand an AI agent is a component within that system is very important. \nWhile scholarly definitions of AI agents vary, they converge on the fundamental concept of systems that perceive \ntheir environment and autonomously execute actions to optimise goal achievement [25] . These systems span from \nbasic rule-based programs to sophisticated learning and reasoning entities [30] . Despite definitional nuances, the \ncore tenets of AI agents consistently emphasise their capacity for independent and intelligent operation within a \ngiven environment [26], [27]. Table 3 summarises AI agent definitions according to several authors.", "mimetype": "text/plain", "start_char_idx": 3750, "end_char_idx": 5401, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ce69075e-e4fe-4ec6-99ef-a35dd5ac653f": {"__data__": {"id_": "ce69075e-e4fe-4ec6-99ef-a35dd5ac653f", "embedding": null, "metadata": {"page_label": "13", "file_name": "A cybersecurity AI agent selection and decision support framework.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\A cybersecurity AI agent selection and decision support framework.pdf", "file_type": "application/pdf", "file_size": 1115417, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3a6e1496-8dd5-4de9-9768-3f24e646a775", "node_type": "4", "metadata": {"page_label": "13", "file_name": "A cybersecurity AI agent selection and decision support framework.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\A cybersecurity AI agent selection and decision support framework.pdf", "file_type": "application/pdf", "file_size": 1115417, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "ae6ca83e09b0144f3edc4367838ba897b0a217f5a72f8df8fcdbf2b9734bb323", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "12 \n \nTable 3  AI agent definitions: Provides various scholarly definitions of AI agents, capturing themes such as \nautonomy, adaptability, and environment interaction \nAI agent definition Author \nA programmed entity that performs operations on behalf of another user or program with a certain \ndegree of independence. \nAlrfai et al. [124] \nSystems capable of executing tasks without human intervention. Dehghantanha et al. [68] \nSoftware or a collection of software that resides and operates on one or more computing devices, \nperceives its environment, and executes purposeful actions on the environment (and on itself) to \nachieve the agent\u2019s goals. \nKott  et al. [36] \n Entities that can perform certain functions without human intervention, including self-activating, \nself-sufficiency, and persistent computation. \nLigo et al. [38] \nAn entity that is a mixture of hardware and software and uses sensors to perceive its surroundings \nand actuators to make changes. \nSharma and Jindal [40] \nPieces of software or hardware with a processing unit capable of making wise decisions about their \ncourses of action in uncertain and adverse environments. \nTh\u00e9ron and Kott [35] \nComputer-assisted systems that can generate text, images, audio, or videos. Baabdullah [95] \nAn entity with self-adaption and intelligence in an environment can understand its environment and \ncontrol its decision behaviour; it conducts self-regulation and self-learning after it perceives changes \nin the environment, and it can proactively complete preset tasks. \nCao et al. [107] \nSystems capable of pursuing complex goals with limited supervision. Chan  et al. [71] \nSystems or software that can perform tasks or functions independently, without human intervention, \nin various environments and situations are designed to make decisions and take actions based on \ntheir programming and the data they receive from their surroundings. \nCronin [72] \nAutonomous systems that interact with their environment to achieve specific objectives, often using \nLLMs to enhance natural language processing capabilities. \nHan  et al. [92] \nAn intelligent entity that can autonomously execute appropriate and contextually relevant actions \nbased on sensory input, whether in a physical, virtual, or mixed-reality environment. \nHuang  et al. [114] \nSystems that perceive their environment and take actions to maximise their chances of achieving \ntheir goals.  \nRossi [25] \nSoftware programs that respond to states and events in their environment, independent from direct \ninstruction by the user or owner, acting on behalf and in the owner's interest.  \nB\u00f6sser [30] \nAs evidenced by the diverse definitions presented in Table 3, the absence of a unified conceptualisation of AI agents \nhas resulted in a proliferation of varied architectural and methodological approaches. This lack of consistency \ndirectly impacts the design and implementation of agent ic systems, necessitating a thorough examination of the \nfield's dominant trends and ongoing projects to understand the resulting complexities in modelling and application \n[26]. Using the various definitions in Table 3, I asked ChatGPT 4.5 to analyse themes and patterns and construct a \nsingle definition. The exact prompt is as follows: \nI have compiled a collection of AI agent definitions from various scholarly research papers. Please analyse  the \nfollowing definitions, identify the recurring themes and patterns, and construct a unified definition that encapsulates \nthe core concepts shared across these definitions. Aim for a definition that is both comprehensive and concise .  \nDefinitions: [I inserted my list of AI agent definitions from Table 3 here, each clearly separated.] \nSpecifically, please focus on identifying: \n\u2022 Common elements: What core capabilities or characteristics are consistently mentioned?  \n\u2022 Variations: Where do the definitions differ, and what are the key points of divergence?  \n\u2022 Essential components: What elements are indispensable for a system to be considered an AI agent?  \nBased on your analysis, provide a unified definition of an AI agent that reflects the essence of these definitions.  \nThe resulting definition is as follows: An AI agent is an autonomous computational entity\u2014comprised of software \nor hardware and software integration\u2014 that independently perceives its environment, intelligently processes \ninformation, makes contextually appropriate decisions, and proactively executes purposef ul actions to achieve \ndefined goals, adapting dynamically through self-regulation and learning to changing circumstances. \nAlthough AI agent properties are explored in detail in the next section, the ChatGPT prompt above also yielded \ncore AI agent characteristics emanating from the studies in Table 3. These are: \n\u2022 Autonomy: Capacity to operate independently.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4830, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8893edb3-58b0-43ee-813c-164af9d5d5b4": {"__data__": {"id_": "8893edb3-58b0-43ee-813c-164af9d5d5b4", "embedding": null, "metadata": {"page_label": "14", "file_name": "A cybersecurity AI agent selection and decision support framework.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\A cybersecurity AI agent selection and decision support framework.pdf", "file_type": "application/pdf", "file_size": 1115417, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "010eda0c-8a04-4332-9be4-a79e76b5d00a", "node_type": "4", "metadata": {"page_label": "14", "file_name": "A cybersecurity AI agent selection and decision support framework.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\A cybersecurity AI agent selection and decision support framework.pdf", "file_type": "application/pdf", "file_size": 1115417, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "2a91bd164a2655b49df072de5f99342666150f24959b63186681850bf6fe40a0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9c6b14b4-8b8f-48f7-9c7c-293adbe5d950", "node_type": "1", "metadata": {}, "hash": "29bb0a2883c3e94567732512ae3d6e9ad0ea010f8edec573a6d68dd5aa25bcbb", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "13 \n \n\u2022 Environmental perception: The capability to gather and interpret environmental data. \n\u2022 Decision-making: Ability to independently select appropriate actions or behaviours. \n\u2022 Goal-oriented action: Pursuit and achievement of specified objectives through purposeful interactions. \n\u2022 Adaptability and learning: The capability for self-adaptation and learning. \nIn addition to the above, the following section provides a thorough exploration of key AI agent properties. \n2.2.3 AI agent properties \nAI agents possess several key properties that enable them to interact with their environment and make decisions to \nachieve specific goals. A literature review reveals several properties, as shown in Table 4 .  \nTable 4 AI agent properties: Lists the key properties of AI agents (e.g., autonomy, learning) identified through a \nliterature review, with brief descriptions and source references \nCore property Description Author \nAutonomy Operate independently without human intervention. \nPhillips-Wren, Leite et al., B\u00f6sser, Gu and Li , \nand Liu et al. [22], [23], [30], [125], [126] \nAdaptive learning Learn from past experiences and adapt to new situations. \nRabuzin et al., Leite et al., Abiodun and Khuen, \nSethy  et al., and Mazumder and Liu [22], [24], \n[26], [127], [128] \nProactive goal pursuit Take initiative to achieve goals. \nPhillips-Wren, Leite et al., Jameel et al., Liu et \nal., and Pantoja et al. [22], [23], [125], [129], \n[130] \nReactive responsiveness Respond to environmental changes in real-time. \nPhillips-Wren, Leite et al., Jameel et al., Liu et \nal., and Pantoja et al. [22], [23], [125], [129], \n[130] \nInter-agent \ncommunication Share information and coordinate actions with other agents.  \nPhillips-Wren, Leite et al., Jameel et al., Liu et \nal., Pantoja et al., and Sethy et al. [23], [125], \n[128], [129], [130] \nCollaborative interaction Collaborate in MAS to achieve shared goals. \nLeng et al., Phillips-Wren, Jameel et al., Liu et \nal, and Sethy et al. [23], [125], [128], [129], \n[131] \nKnowledge \nrepresentation and \nreasoning Store and utilise knowledge for decision-making. Sennott  et al. [132] \nEthical reasoning and \ndecision-making Embed ethical decision-making capabilities into AI systems. \nB\u00f6sser, Rossi and Mattei, and Gu and Li [30], \n[126], [133] \nMoral agency (where \napplicable) \nEnsure actions by agents with significant autonomy and \ndecision-making align with moral codes and societal norms. B\u00f6sser, and Gu and Li [30], [126] \nSocial interaction \nInteract with humans and other agents through various \ninterfaces to influence their decisions, confidence, and trust \nthrough transparency and reliability.  \nNickles et al., Phillips-Wren, Leite et al., Rossi \nand Mattei, Pitardi and Marriott, Chakraborty, \nHe and Jazizadeh, and Peng et al. [19], [22], \n[125], [133], [134], [135], [136], [137] \nTrustworthiness Build user trust through reliability and transparency. \nPitardi and Marriott, and He and Jazizadeh \n[135], [137] \nDomain-specific \ncompetence \nApplications in various fields, such as  smart homes, \nhealthcare, and military operations. \nB\u00f6sser, Rossi, Preethiya et al., Rashid and \nKausik, and Liu et al. [25], [30], [32], [138], \n[139] \nExplainability \nProvide clear and understandable explanations for actions \nand decisions. \nChakraborty, Majumdar, and Simran et al. \n[136], [140], [141] \nResilience \nMaintain performance and functionality in the face of \nunexpected inputs, noise, or adversarial attacks. \nCam, Falowo  et al., and Rafferty and \nMacdermott [142], [143], [144] \nEmbodiment (if \napplicable) \nIn applicable instances, build a physical presence or virtual \nrepresentation in an environment, enabling it to interact with \nthe world through sensors and actuators.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3742, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "9c6b14b4-8b8f-48f7-9c7c-293adbe5d950": {"__data__": {"id_": "9c6b14b4-8b8f-48f7-9c7c-293adbe5d950", "embedding": null, "metadata": {"page_label": "14", "file_name": "A cybersecurity AI agent selection and decision support framework.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\A cybersecurity AI agent selection and decision support framework.pdf", "file_type": "application/pdf", "file_size": 1115417, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "010eda0c-8a04-4332-9be4-a79e76b5d00a", "node_type": "4", "metadata": {"page_label": "14", "file_name": "A cybersecurity AI agent selection and decision support framework.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\A cybersecurity AI agent selection and decision support framework.pdf", "file_type": "application/pdf", "file_size": 1115417, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "2a91bd164a2655b49df072de5f99342666150f24959b63186681850bf6fe40a0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8893edb3-58b0-43ee-813c-164af9d5d5b4", "node_type": "1", "metadata": {"page_label": "14", "file_name": "A cybersecurity AI agent selection and decision support framework.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\A cybersecurity AI agent selection and decision support framework.pdf", "file_type": "application/pdf", "file_size": 1115417, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "91ef8883aebe6f1105b188dff828d29c6a1f72a3986a3651136cb9e7398646f9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "B\u00f6sser, Rossi, Preethiya et al., Rashid and \nKausik, and Liu et al. [25], [30], [32], [138], \n[139] \nExplainability \nProvide clear and understandable explanations for actions \nand decisions. \nChakraborty, Majumdar, and Simran et al. \n[136], [140], [141] \nResilience \nMaintain performance and functionality in the face of \nunexpected inputs, noise, or adversarial attacks. \nCam, Falowo  et al., and Rafferty and \nMacdermott [142], [143], [144] \nEmbodiment (if \napplicable) \nIn applicable instances, build a physical presence or virtual \nrepresentation in an environment, enabling it to interact with \nthe world through sensors and actuators. \nAbiodun and Khuen, Weng and Ho, Bovo  et al., \nPreethiya et al., and Liu et al. [31], [32], [94], \n[127], [139] \nLong-term memory Retain and utilise information over extended periods. \nChan et al., Kim and Saad, Chen et al., and Deng \net al. [71], [93], [98], [123] \nAgency \ntransfer/Delegation \nTransfer or delegate parts of the agency or decision-making \nto other agents or humans. \nNeff and Nagy, Baird and Maruping, and \nCandrian and Scherer [65], [145], [146] \nAs shown in Table 4, AI agents' functionality is enhanced by various core properties, including autonomy, \nadaptability, and reasoning [23], [30]. While functionalities such as planning, goal-oriented behaviour, and explicit \ndecision-making are undeniably crucial for AI agent operations  [28], [30], [127], my review in Table 4 prioritises \nthe examination of broader, foundational properties that underpin these capabilities. These specific functionalities \nare often observed as emergent behaviours or direct applications of core attributes like autonomy, adaptive learning, \nand (knowledge representation and) reasoning. For example, sound reasoning inherently enables informed decision-\nmaking [30], [132], and planning is intrinsically linked to goal- oriented actions [29], both of which are facilitated", "mimetype": "text/plain", "start_char_idx": 3102, "end_char_idx": 5021, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "527b6e2d-0492-42a5-bf0e-fc2fe031d79a": {"__data__": {"id_": "527b6e2d-0492-42a5-bf0e-fc2fe031d79a", "embedding": null, "metadata": {"page_label": "15", "file_name": "A cybersecurity AI agent selection and decision support framework.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\A cybersecurity AI agent selection and decision support framework.pdf", "file_type": "application/pdf", "file_size": 1115417, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0591e821-5b81-42e4-838f-6f6de18f7fc3", "node_type": "4", "metadata": {"page_label": "15", "file_name": "A cybersecurity AI agent selection and decision support framework.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\A cybersecurity AI agent selection and decision support framework.pdf", "file_type": "application/pdf", "file_size": 1115417, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "ab0ddb6d9d28ee1064e8c2cbce8c46dc14220907a0e32f2468d362f5d6069a7e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "14 \n \nby a high degree of autonomy  [41], [147]. Furthermore, domain- specific competence integrates and applies these \nfoundational properties within context. In addition, properties such as explainability, resilience , and long- term \nmemory are becoming increasingly relevant as AI agents are deployed in more complex and critical applications  \n[123], [140], [142] . Therefore, this paper  focuses on the core properties \u2014autonomy, adaptive learning, proactive \ngoal pursuit, reasoning, and others detailed in Table 4\u2014to provide a framework for understanding the fundamental \nprinciples that govern AI agent behaviour while acknowledging the inherent interconnectedness and hierarchical \nrelationships among these attributes. \nHowever, semantic gaps in communication languages challenge the development of AI agents with these properties, \nand there is a need for firm ethical boundaries [133], [134]. In other words, ethical considerations like fairness, data \nprivacy and security , and transparency are essential for responsible AI development [128], [129], [136] . As \nRadanliev et al. [148] articulate, autonomous AI systems must be evaluated not only for their technical performance \nand alignment with established frameworks, such as the NIST CSF, but also, crucially, for their potential to heighten \ncyber risks, compro mise privacy, or operate  beyond adequate human control. These profound concerns \nfundamentally reshape the discourse, underscoring the imperative for agentic frameworks that inherently embed \nethical safeguards and responsible AI principles throughout the entire deployment lifecycl e. \nThe limitations of AI agents extend beyond the absence of a unified definition, current technological constraints, \nand ethical considerations. Notably, legal and regulatory frameworks present a substantial challenge, particularly \nconcerning the classification of autonomous AI systems. Whether such systems should be granted legal personhood \nrather than being treated as mere objects is a subject of ongoing debate [149]. However, a thorough examination of \nthese legal implications is beyond the scope of this pa per. Building on the discussion of AI agent properties  and \ntheir inherent limitations, the following section provides an overview of their classification, highlighting the diverse \ntypes and their respective applications. \n2.2.4 AI agent types  \nA clear differentiation based on their functional and task-specific architectures is required to understand the diverse \napplications of AI agents.  In the same SLR exercise described in Section 2.1.3, one of the themes extracted from \nthe qualitative data was under the category of \u2018AI agent types.\u2019 I asked ChatGPT 4.5 to analyse this \u2018AI agent type\u2019 \ndata, and the exact prompt is as follows: \nYou are an expert in artificial intelligence and agent -based systems, tasked with analysing the findings of a \nsystematic literature review (SLR) on AI agent types. I will provide you with a series of notes extracted from the \nSLR, describing various entities that authors have identified as AI agents. Your task is to: \n1. Categorise the data into broader, more accepted 'AI agent type' categories . These categories should \nreflect the core, fundamental distinctions in AI agent design and functionality established by reputable \nacademic and industry sources. \n2. Identify and describe any subcategories or variations mentioned in the notes . Explain how these \nsubcategories relate to the main categories, highlighting their specific characteristics and differences.  \n3. Acknowledge and clarify any instances where authors may refer to specific implementations, \napplications, or variations rather than distinct agent types. Explain the nuances of these distinctions. \n4. Present your analysis in a clear, structured table format. The table should include:  \no 'Main AI Agent Category' \no 'Subcategories/Variations' \no 'Description and Nuances' \n5. Provide a concise summary of the key findings and trends identified in the SLR data.  \nPlease ensure your analysis is grounded in established AI agent literature and reflects a comprehensive \nunderstanding of the field. \n[Insert your notes from the SLR here. In my case, I attached a document with my SLR notes.] \nTable 5 presents a structured analysis of the SLR data, organised into AI agent categories, including subcategories \nand nuances that are clarified. \nTable 5 AI agent types: Classifies AI agent architectures (e.g., reactive, cognitive) and subcategories or variations, \nshowing how each addresses distinct design and operational nuances", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4588, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "20891a2a-bc5a-4515-ac40-c3fbd7f6bc7d": {"__data__": {"id_": "20891a2a-bc5a-4515-ac40-c3fbd7f6bc7d", "embedding": null, "metadata": {"page_label": "16", "file_name": "A cybersecurity AI agent selection and decision support framework.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\A cybersecurity AI agent selection and decision support framework.pdf", "file_type": "application/pdf", "file_size": 1115417, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b92297f3-bcb0-4c62-97fb-f22f4c86396f", "node_type": "4", "metadata": {"page_label": "16", "file_name": "A cybersecurity AI agent selection and decision support framework.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\A cybersecurity AI agent selection and decision support framework.pdf", "file_type": "application/pdf", "file_size": 1115417, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "1de291695f509e23f7a968ad5b0fb589366af63431ba7c2e85ec424e052ffed9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "30d2bdda-48ac-4131-b09d-dec60ba55975", "node_type": "1", "metadata": {}, "hash": "f1cfa67e9a7dc4b13ad4f88f24926145b0fa255082ea10f313b51e59c36db58b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "15 \n \nAI agent category Subcategories/Variations Description and nuances \nAutonomy-based agents o Autonomous agents \no Adaptive autonomous agents \no Continual learning -enabled \nAI agents (learning agents) \nAgents perform tasks independently, with autonomous agents operating \nwithout humans [41], [150]. Adaptive autonomous agents dynamically \nadjust autonomy levels according to contextual requirements [100]. \nLearning agents continuously self -learn, adapting to novel situations \nautonomously [23], [24]. \nInteraction complexity \n(Agent cognition) \no Simple reflex agents \no Model-based reflex agents \no Goal-based agents \no Utility-based agents \no Cognitive agents \nCategorised by cognitive complexity, simple reflex agents react directly \nto current percepts without history. Model-based reflex agents maintain \ninternal states by tracking environmental aspects [151]. Goal -based \nagents explicitly pursue defined objectives, guiding their actions. Utility-\nbased agents select actions that maximise outcomes' desirability or utility \n[151]. Cognitive or deliberative agents are intelligent agents that aim to \nemulate human-like cognitive processes [152]. They are responsible for \nhigher-level tasks such as learning, planning, conflict resolution, and \ntask management [28].  \nEmbodiment o Embodied agents \no Virtual agents Embodied agents possess physical forms ( such as robots and drones ) \ncapable of physical  actions [31], [32]. Virtual a gents exist entirely as \nsoftware, operating digitally, such as chatbots and virtual assistants [31]. \nBehavioural approach o Reactive agents \no Proactive (Goal -oriented) \nagents \no Hybrid agents \no Learning agents \nReactive agents respond directly to stimuli without using internal states \nor past experiences [29]. Proactive a gents utilise internal states, \nplanning, and past experiences to achieve goals [29]. Hybrid agents \nincorporate both reactive and proactive functionalities, dynamically \nadjusting to their surroundings [29]. \nCollaboration and \ncoordination \no Cooperative agents \no Centralised agents \no Decentralised agents \no Multi-agent systems \n(Swarm, Cohort, Society of \nagents) \no Autonomous collaborative \nagents \nCooperative agents collaborate toward common goals in MASs [150], \n[153]. Centralised agents are centrally coordinated, whereas \nDecentralised agents coordinate independently through communication \n[153]. MASs demonstrate collective behaviours through horizontal \n(swarm) or vertical (cohort) coordination, enabling emergent properties \nand self-organisation [35], [36], [110]. Autonomous collaborative agents \nindependently perform complete tasks, interacting with others when \nnecessary [35], [36]. \nEthics and morality o Artificial moral agents Designed explicitly to facilitate ethical decision -making, adhering to \nmoral standards is crucial in ethically sensitive applications such as  \nhealthcare or autonomous vehicles [126]. \nArchitectural design \nparadigms \no Thinking-type architecture \n(Symbolic/Decision-based) \no Response-type architecture \n(Event-driven) \no Mixed-type architecture \n(Hybrid) \nThinking-type agents execute actions through symbolic reasoning and \ndecision-making processes. Response-type agents primarily react to \nexternal perceptions or events. Mixed-type agents combine symbolic \nreasoning and reactive perception-driven behaviour [107]. \nAgent implementation \ncontexts \no Specialised agents \n(Functional specialisation) \no Fully functional agents \n(Holistic capability) \nSpecialised agents focus on performing discrete functional roles as part \nof broader collaborative agent societies. Fully f unctional agents \nindependently execute all necessary functions for their designed purpose \nand are capable of autonomous operation [35], [36]. \nSpecific implementations and \napplications (Contextual \nexamples) \no Self-driving cars \no Drones \no Robotic vacuum cleaners \no Chatbots and virtual \nassistants \no Text-based generative \nagents \no Automated trading systems \no Robotic space exploration \nprobes \no Internet information agents \nContextual examples demonstrating practical implementations utilising \ncore agent characteristics (autonomy, cognition, embodiment). \nExamples: autonomous vehicles, drones, home robots, automated \nfinancial trading [72], robotic space exploration [40], conversational AI \nand generative text-based agents [72]. Such examples reflect real-world \napplications rather than distinct agent types. \nAs evidenced through Table 5, t he SLR identifies distinct  AI agent categories based on autonomy, cognitive \ncomplexity, embodiment, behavioural  approach, coordination structure, ethical considerations, and architectural \ndesign. Trends indicate a growing emphasis on adaptive autonomy, continual learning capabilities, ethical and moral \nagency, and multi-agent collaboration for complex problem-solving.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4859, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "30d2bdda-48ac-4131-b09d-dec60ba55975": {"__data__": {"id_": "30d2bdda-48ac-4131-b09d-dec60ba55975", "embedding": null, "metadata": {"page_label": "16", "file_name": "A cybersecurity AI agent selection and decision support framework.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\A cybersecurity AI agent selection and decision support framework.pdf", "file_type": "application/pdf", "file_size": 1115417, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b92297f3-bcb0-4c62-97fb-f22f4c86396f", "node_type": "4", "metadata": {"page_label": "16", "file_name": "A cybersecurity AI agent selection and decision support framework.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\A cybersecurity AI agent selection and decision support framework.pdf", "file_type": "application/pdf", "file_size": 1115417, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "1de291695f509e23f7a968ad5b0fb589366af63431ba7c2e85ec424e052ffed9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "20891a2a-bc5a-4515-ac40-c3fbd7f6bc7d", "node_type": "1", "metadata": {"page_label": "16", "file_name": "A cybersecurity AI agent selection and decision support framework.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\A cybersecurity AI agent selection and decision support framework.pdf", "file_type": "application/pdf", "file_size": 1115417, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "4f67a62e570232efec5bb3f0d7c3b53f7645c98fd6e79af909a78e46cba0a0d4", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Examples: autonomous vehicles, drones, home robots, automated \nfinancial trading [72], robotic space exploration [40], conversational AI \nand generative text-based agents [72]. Such examples reflect real-world \napplications rather than distinct agent types. \nAs evidenced through Table 5, t he SLR identifies distinct  AI agent categories based on autonomy, cognitive \ncomplexity, embodiment, behavioural  approach, coordination structure, ethical considerations, and architectural \ndesign. Trends indicate a growing emphasis on adaptive autonomy, continual learning capabilities, ethical and moral \nagency, and multi-agent collaboration for complex problem-solving. Contextual examples provided in the literature, \nlisted in the last row of Table 5, emphasise practical applications rather than new agent categories, highlighting how \ntheoretical constructs of AI agents manifest in varied real-world scenarios. It is clear from Table 5 that not all agent", "mimetype": "text/plain", "start_char_idx": 4193, "end_char_idx": 5149, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "fd834def-22e7-43dc-aebf-e385bddce180": {"__data__": {"id_": "fd834def-22e7-43dc-aebf-e385bddce180", "embedding": null, "metadata": {"page_label": "17", "file_name": "A cybersecurity AI agent selection and decision support framework.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\A cybersecurity AI agent selection and decision support framework.pdf", "file_type": "application/pdf", "file_size": 1115417, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d251dafc-c95d-4067-9111-eca2d7dd67ed", "node_type": "4", "metadata": {"page_label": "17", "file_name": "A cybersecurity AI agent selection and decision support framework.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\A cybersecurity AI agent selection and decision support framework.pdf", "file_type": "application/pdf", "file_size": 1115417, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "2775f2c6259e67678afe2022670a9f39cfdbd97fccee40e5c8f85c7812ed58dd", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "16 \n \ntypes are equally suited to every security task. The framework proposed in this paper aims to provide a systematic \nand contextually relevant approach to selecting AI agents suitable for cybersecurity tasks.  \n3. Method for developing an AI agent selection framework  \nDriven by the need for a structured approach to AI agent selection, design, and deployment, informed by a \nsystematic categorisation of agent types and their characteristics, I present the AI A gent Taxonomy and Decision \nFramework (AIATDF). This framework, grounded in literature-derived agent properties and categories, provides a \ndecision-making tool for mapping suitable AI agents to the NIST CSF 2.0. The framework development method, \nguided by the AI agent properties in Table 4, the architecture/types in Table 5, and the NIST CSF 2.0 core structure \nin Fig. 2, is visually represented in Fig. 5. \n \nFig. 5 AIATDF development method: Outlines the overarching method employed in the study for the \ndevelopment of the AI agent taxonomy and decision framework \nEssentially, Fig. 5 says the following: \n1) Establish agent property-NIST CSF 2.0 task alignment: \no Map AI agent properties to specific NIST CSF 2.0 task requirements. \no Method: Develop a comprehensive mapping matrix. \n2) Conduct architectural suitability assessment:  \no Evaluate the architectural suitability of various AI agent types for NIST CSF 2.0 task \nrequirements. \no Method: Perform a comparative analysis of agent architectures. \n3) Implement and iteratively refine agent deployment:  \no Deploy selected agents and establish a continuous refinement process. \no Method: Implement a feedback-driven iterative optimisation cycle. \nExecuting the steps above yields the AIATDF presented and explained in the next section.  \n4. AI agent taxonomy and decision framework \nIn the AIATDF description, \u2018taxonomy\u2019 emphasises the systematic categorisation of agent types derived from the \nliterature, and \u2018decision framework\u2019 indicates a structured approach for guiding the selection, design, and \ndeployment decisions of AI agents based on identified agent categories, variations, characteristics, and application \ncontexts. Following the three framework development steps  outlined in the methods section, Fig.  6 represents the \nAIATDF of this paper.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2295, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "196f2e54-008c-49fa-a236-ce7f97720913": {"__data__": {"id_": "196f2e54-008c-49fa-a236-ce7f97720913", "embedding": null, "metadata": {"page_label": "18", "file_name": "A cybersecurity AI agent selection and decision support framework.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\A cybersecurity AI agent selection and decision support framework.pdf", "file_type": "application/pdf", "file_size": 1115417, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ac764fbb-d2ed-4832-ab34-50ce58d65ed6", "node_type": "4", "metadata": {"page_label": "18", "file_name": "A cybersecurity AI agent selection and decision support framework.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\A cybersecurity AI agent selection and decision support framework.pdf", "file_type": "application/pdf", "file_size": 1115417, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "0277067319e8189aaf9c8ddf280436887890038ed8c015c361acce74ff96fe5e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "17 \n \n \nFig. 6 AI agent taxonomy and decision support framework: Provides an overview of the six main stages of the \nproposed AIATDF, showing how each step contributes to choosing and deploying AI agents aligned with the \nNIST CSF 2.0  \nThe AIATDF comprises six main stages as follows:  \n1) Contextual cybersecurity task decomposition:  \no Break down complex cybersecurity tasks into granular subtasks within the NIST CSF 2.0.  \no Method: Hierarchical task analysis  of the NIST CSF 2.0\u2019s subcategories through the \n\u2018informative references\u2019 functionality. \n2) Mapping agent properties to NIST CSF 2.0: \no Link AI agent properties to the NIST CSF 2.0 cybersecurity task-specific requirements. \no Method: Create a mapping matrix. \n3) AI agent-type architectural suitability analysis: \no Assess AI agent architecture suitability for  NIST CSF 2.0 cybersecurity task-specific \nrequirements. \no Method: Comparative analysis of agent architectures. \n4) Performance evaluation metrics for agent effectiveness:  \no Define metrics to assess AI agent(s) performance in NIST CSF 2.0 objectives. \no Method: Use industry best practices to define key performance indicators.  \n5) Design, develop, and deploy AI agent(s) \no Create and implement AI agent solutions that meet the architectural and performance \nrequirements identified in the previous stages. \no Method: Use established software development practices (e.g., Agile or DevSecOps) and pilot \ntesting to integrate the chosen agent architectures into the organisation\u2019s SOC or equivalent \ncybersecurity workflow. \n6) Iterative framework refinement and validation: \no Establish a feedback loop for continuous improvement of the agentic AI framework.  \no Method: Collect data and feedback from cybersecurity practitioners and end-users.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1778, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "04c6973a-b190-4aba-b52d-4dc378388e8a": {"__data__": {"id_": "04c6973a-b190-4aba-b52d-4dc378388e8a", "embedding": null, "metadata": {"page_label": "19", "file_name": "A cybersecurity AI agent selection and decision support framework.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\A cybersecurity AI agent selection and decision support framework.pdf", "file_type": "application/pdf", "file_size": 1115417, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c518dd2c-1601-4e39-bd26-680434916d24", "node_type": "4", "metadata": {"page_label": "19", "file_name": "A cybersecurity AI agent selection and decision support framework.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\A cybersecurity AI agent selection and decision support framework.pdf", "file_type": "application/pdf", "file_size": 1115417, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "a159dc290c0c293f7d81e2fa0441f7dcd7cd26bc99811fecca8e4ce1f468c621", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2c6e4549-324f-4d63-93df-94c0ddebf0a8", "node_type": "1", "metadata": {}, "hash": "7073b8b218a22078138019fe687a2623c47e8da68ea8cd689be0626de3be122c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "18 \n \nIn addition to the six main stages of the AIATDF , there are two extra conditional stages in Fig. 6. These are the \n\u2018governance decision\u2019 and \u2018ethical and legal considerations\u2019 stages . As discussed in the literature review section , \nthe NIST CSF 2.0 has a \u2018govern\u2019 function that ensures the integration of ethical and legal considerations, the \u2018yes\u2019 \noption of the \u2018governance decision\u2019  box in Fig. 6. The \u2018no\u2019 governance option in the AIATDF  demonstrates that \nany acceptable cybersecurity framework can be utilised in place of the NIST CSF 2.0, and the agentic AI framework \nwill still be applicable. In other words, any widely recognised cybersecurity standards, guidelines, and frameworks \ncan be utilised with the AIATDF. In the next section, I use the NIST CSF 2.0 to validate the AIATDF. \n5. Validation of the AIATDF \nTo establish the validity of the AIATDF \u2014specifically, its ability to represent its intended construct accurately  \n[154]\u2014this test focuses on mapping AI agent properties to six core functions of the NIST CSF 2.0 (Fig. 2). Notably, \nthe initial hierarchical task analysis stage of the AIATDF, which decomposes the NIST CSF 2.0 (sub)categories via \nthe framework's 'informative references,' falls beyond the scope of this agentic AI framework ( Fig. 6) and is \ntherefore not validated in this paper. Instead, the validation process commences with the crucial step of aligning AI \nagent properties (Table 4) to the NIST CSF 2.0 functions. As stage 2 of the AIATDF dictates, this alignment must \nbe documented in a mapping matrix, which directly links  AI agent characteristics to specific cybersecurity task -\nspecific requirements (NIST CSF 2.0 functions in this validation exercise). Furthermore, the AIATDF validation \ntest recognises that AI agent types (Table 5)\u2014which directly influence the agentic AI framework architecture\u2014are \nderived from these properties. Therefore, the validation extends to stage 3, which assesses the suitability of AI agent \narchitectures for the NIST CSF 2.0 task-specific cybersecurity requirements, a process also reflected in the mapping \nmatrix.  \nFor demonstrative purposes, the mapping matrix (Table 6) showcases the application of reactive, cognitive, hybrid, \nand learning AI agent types; however, it is emphasised  that this mapping strategy is universally applicable across \nall AI agent types.  In other words , Table 6 validates  a simplified version of the AIATDF conceptual framework. \nThus, real-world deployments may use MAS [92] for a single cybersecurity function or combine multiple functions \nwithin the same agentic system. \nTable 6. Mapping matrix for selection, design, and deployment of AI agents: Aligns four AI agent types\u2014reactive, \ncognitive, hybrid, and learning\u2014with the six NIST CSF 2.0 functions, offering a practical guide to optimal agent \ndeployment \n Reactive agents Cognitive agents Hybrid agents Learning agents \nGovern o Enforces static policy \nchecks in real-time \no Limited scope for policy \nevolution \no Policy-driven decision -\nmaking & and compliance \nmanagement  \no Tracks strategic goals, \ncompliance, risk appetite  \no Ensures immediate policy \nadherence and higher -\nlevel oversight \no Facilitates cross -\nfunctional governance \ntasks \no Learning-based \ngovernance \noptimisation (e.g., \nbalancing privacy \no vs.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3325, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "2c6e4549-324f-4d63-93df-94c0ddebf0a8": {"__data__": {"id_": "2c6e4549-324f-4d63-93df-94c0ddebf0a8", "embedding": null, "metadata": {"page_label": "19", "file_name": "A cybersecurity AI agent selection and decision support framework.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\A cybersecurity AI agent selection and decision support framework.pdf", "file_type": "application/pdf", "file_size": 1115417, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c518dd2c-1601-4e39-bd26-680434916d24", "node_type": "4", "metadata": {"page_label": "19", "file_name": "A cybersecurity AI agent selection and decision support framework.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\A cybersecurity AI agent selection and decision support framework.pdf", "file_type": "application/pdf", "file_size": 1115417, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "a159dc290c0c293f7d81e2fa0441f7dcd7cd26bc99811fecca8e4ce1f468c621", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "04c6973a-b190-4aba-b52d-4dc378388e8a", "node_type": "1", "metadata": {"page_label": "19", "file_name": "A cybersecurity AI agent selection and decision support framework.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\A cybersecurity AI agent selection and decision support framework.pdf", "file_type": "application/pdf", "file_size": 1115417, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "118263da12e7af1c0047f2e7ea3364cc42603a6714b5924b89340e474cc47c7f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Thus, real-world deployments may use MAS [92] for a single cybersecurity function or combine multiple functions \nwithin the same agentic system. \nTable 6. Mapping matrix for selection, design, and deployment of AI agents: Aligns four AI agent types\u2014reactive, \ncognitive, hybrid, and learning\u2014with the six NIST CSF 2.0 functions, offering a practical guide to optimal agent \ndeployment \n Reactive agents Cognitive agents Hybrid agents Learning agents \nGovern o Enforces static policy \nchecks in real-time \no Limited scope for policy \nevolution \no Policy-driven decision -\nmaking & and compliance \nmanagement  \no Tracks strategic goals, \ncompliance, risk appetite  \no Ensures immediate policy \nadherence and higher -\nlevel oversight \no Facilitates cross -\nfunctional governance \ntasks \no Learning-based \ngovernance \noptimisation (e.g., \nbalancing privacy \no vs. security trade-offs)  \no Automates policy \nrefinement for dynamic \nchanges \nIdentify \n \no Rapid scanning of known \nassets and threats \no Trigger-based checks for \nnew devices or \nvulnerabilities  \no Strategic planning for \ncomprehensive asset \ninventories \no Incorporates \norganisational risk models \nand compliance \no Balances immediate \nscanning with higher -\nlevel threat modelling \no Layered architecture for \nreal-\ntime triggers and \ndeeper analysis \no Adapts risk profiles \nover time via continual \nlearning \no Discovers unknown \nvulnerabilities from \nhistorical patterns \nProtect o Immediate enforcement of \naccess rules  \no Real-time reactive \nlockdowns under \nsuspicious activity  \no Policy-driven \nconfiguration \nmanagement \no Plans system -wide \nchanges and resource \nallocations \no Integrates rules -based \ntriggers with strategic \naccess control \no Quick local responses \nplus deliberative \noversight \no Uses anomaly detection \nto refine protective \nmeasures \no Learns evolving user \naccess patterns for \ndynamic policy updates \nDetect o Real-time intrusion \ntriggers via signature -\nbased checks \no High-speed event -driven \nanomaly alerts  \no Considers multi -step \nreasoning for stealthy \nthreats \no Goal-based analysis of \nsuspicious patterns \no Fast detection loop \n(reactive layer)  \no Complex threat \ncorrelation (deliberative \nlayer) \no Adaptive models refine \ndetection thresholds \nover time \no Identifies zero -day \nexploits using ML or \nRL \nRespond o Automated containment \n(block IP, isolate assets), \nminimal strategic \nforesight but instant action \no Deliberate coordination of \nmulti-step mitigation \nplans \no Accounts for long -term \nsystem impacts \no Instant quarantines \ncombined with policy -\nbased orchestration \no Layered approach to \nreduce escalation time \no RL to optimise \nresponse effectiveness \no Learns from each \nincident to improve \nfuture mitigations", "mimetype": "text/plain", "start_char_idx": 2466, "end_char_idx": 5220, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "cae362ed-f7cf-4fbd-93cd-502f12ca4484": {"__data__": {"id_": "cae362ed-f7cf-4fbd-93cd-502f12ca4484", "embedding": null, "metadata": {"page_label": "20", "file_name": "A cybersecurity AI agent selection and decision support framework.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\A cybersecurity AI agent selection and decision support framework.pdf", "file_type": "application/pdf", "file_size": 1115417, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6900c6ce-0e26-4bf5-be6b-d349443630af", "node_type": "4", "metadata": {"page_label": "20", "file_name": "A cybersecurity AI agent selection and decision support framework.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\A cybersecurity AI agent selection and decision support framework.pdf", "file_type": "application/pdf", "file_size": 1115417, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "5143c12d79f9826daf9d895b8b4602db2a07f76b159dfffb5e903e1dcac30447", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b0343ff0-b755-45a6-a6f2-1232907b0b35", "node_type": "1", "metadata": {}, "hash": "acdc157df75b738ab5eef5d4dd99be88f1bf87437e44438b58888533adaa4ed7", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "19 \n \nRecover o Scripted fallback to \nrestore systems quickly \no Lacks deeper analysis of \nroot causes  \no Strategic resource \nallocation for restoration \no Coordinated plan to \nrebuild or recon Fig. \ncritical infra \no Rapid initial recovery plus \nholistic follow-up \no Reacts quickly yet \nintegrates deliberative \nanalysis for improvement \no Adaptive post -incident \nlearning (root -cause \nanalysis) \no Uses data from each \nbreach to refine future \nrecovery blueprints \nAs shown in Table 6, each row corresponds to one of the six NIST CSF 2.0  functions, and each column highlights \nwhich and how selected AI agent types can address the cyber security task-specific requirements linked to each  \nfunction. The bullet points summarise the key capabilities or actions of AI agents . The AI agent capabilities a re \nderived from Table 5.  For instance, reactive agents rely on stimulus -response mechanisms  and thus  react to \nenvironmental changes in near real -time [29]. Therefore, they would suit any trigger-based or event -driven \ncybersecurity activities in the NIST CSF 2.0. The cognitive agents are responsible for higher -level functions such \nas planning, reasoning, learning,  and task management  [28], [152]. They would be appropriate for NIST CSF 2.0 \ncybersecurity tasks that require coordination, strategic planning, multi-step reasoning, and decision-making. Hybrid \nagents combine all types of AI agents , as shown  in Table 6.  For instance, they combine reactive and cognitive \nstrategies, typically arranged in layered architectures, so quick reactive behaviours can coexist with higher -level \nreasoning processe s [22], [29].  They are, therefore , suitable for cybersecurity activities that require immediate \nreaction to events and strategic oversight that requires continuous planning, reasoning and decision-making. Lastly, \nlearning agents can enhance performance by revising internal models based on new data or feedback loops [41], \n[150], making them well -suited for ever-evolving threat landscapes . I discuss the implications of the AIATDF in \nthe next section. \n6. Discussions \nThe proposed framework of the study, the AIATDF, was presented in Fig. 6 and conceptually validated in Table 6. \nThe AI ATDF demonstrates how AI agents can automate (assisted intelligence), autonomize (autonomous \nintelligence), or combine automation and autonomy (augmented intelligence) to meet the NIST CSF 2.0 \ncybersecurity task-specific requirements.  \n6.1 Implications of the proposed framework  \n6.1.1 Theoretical insights \nThis paper introduces a framework that aligns core AI agent capabilities , such as autonomy, adaptive learning, \nproactivity, and reactivity, with the six fundamental functions of the NIST CSF 2.0. This integration establishes a \nconceptual bridge, translating abstract AI agent theory into actionable cybersecurity strategies defined by the NIST \nCSF. In contrast to fragmented, single -capability AI  deployments, this holistic framework provides a unified \nperspective, defining how diverse agent types  can address the complex demands inherent in each NIST CSF 2.0 \nfunction. By mapping AI agent capabilities  to the task -specific outcomes of the NIST CSF 2.0, th e AIATDF \nmitigates the risks associated with haphazard AI adoption. Rather than deploying advanced AI tools without a clear \nintegration strategy, this  approach fosters an  alignment between agent theory and NIST CSF constructs. \nAdditionally, this alignment enables the design of AI systems that excel in threat detection and response, adapt  to \nevolving vulnerabilities, engage in strategic planning, and enhance governance. Combining theoretical concepts, \nsuch as autonomy and AI agent cooperation, with practical objectives, like reduced incident response times and \nenhanced compliance, empowers organisations to invest in AI -driven cybersecurity solutions with confidence . \nConsequently, the  AIATDF enhances the theoretical rigour and real -world applicability of agent -based defence \nstrategies, ensuring alignment with recognised cybersecurity standards. \nOne of the key contributions of this study is that the AIATDF also distinguishes between assisted, autonomous, and \naugmented modes of AI systems intelligence. This contrasts with traditional agent taxonomies that obscure critical \ndistinctions in agent autonomy through broad, overlapping categories.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4387, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b0343ff0-b755-45a6-a6f2-1232907b0b35": {"__data__": {"id_": "b0343ff0-b755-45a6-a6f2-1232907b0b35", "embedding": null, "metadata": {"page_label": "20", "file_name": "A cybersecurity AI agent selection and decision support framework.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\A cybersecurity AI agent selection and decision support framework.pdf", "file_type": "application/pdf", "file_size": 1115417, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6900c6ce-0e26-4bf5-be6b-d349443630af", "node_type": "4", "metadata": {"page_label": "20", "file_name": "A cybersecurity AI agent selection and decision support framework.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\A cybersecurity AI agent selection and decision support framework.pdf", "file_type": "application/pdf", "file_size": 1115417, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "5143c12d79f9826daf9d895b8b4602db2a07f76b159dfffb5e903e1dcac30447", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cae362ed-f7cf-4fbd-93cd-502f12ca4484", "node_type": "1", "metadata": {"page_label": "20", "file_name": "A cybersecurity AI agent selection and decision support framework.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\A cybersecurity AI agent selection and decision support framework.pdf", "file_type": "application/pdf", "file_size": 1115417, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "0db68b807580d314ca8358176c05549433138cbbc1ec4dea15dd4dacdf8d40b1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Additionally, this alignment enables the design of AI systems that excel in threat detection and response, adapt  to \nevolving vulnerabilities, engage in strategic planning, and enhance governance. Combining theoretical concepts, \nsuch as autonomy and AI agent cooperation, with practical objectives, like reduced incident response times and \nenhanced compliance, empowers organisations to invest in AI -driven cybersecurity solutions with confidence . \nConsequently, the  AIATDF enhances the theoretical rigour and real -world applicability of agent -based defence \nstrategies, ensuring alignment with recognised cybersecurity standards. \nOne of the key contributions of this study is that the AIATDF also distinguishes between assisted, autonomous, and \naugmented modes of AI systems intelligence. This contrasts with traditional agent taxonomies that obscure critical \ndistinctions in agent autonomy through broad, overlapping categories. The AIATDF illuminates the nuanced \nspectrum of AI agent capabilities by explicitly mapping each intelligence mode to specific cybersecurity \nfunctions\u2014ranging from rapid, automated tasks (assisted intelligence) to complete human-out-of-the-loop decision-\nmaking (autonomous intelligence) . In other words, the AIATDF highlights the graduated spectrum of AI agent \ncapabilities. This clarity is crucial; for instance, it describes why a reactive AI agent may be sufficient for immediate, \nrule-based detection tasks within the Identify or Protect functions, while a learning or cognitive AI agent is \nindispensable for strategic governance or comprehensive vulnerability assessments.  Consequently, the AIATDF \ndemonstrates that \"autonomy\" is not a monolithic concept but a dynamic attribute that must be tailored to the \nspecific demands of each security context. This granular approach underscores the importance of aligning AI agent \narchitectures with functional requirements. The AIATDF advances the theoretical understanding of agentic AI \noperations in cybersecurity by emphasising these roles . It reveals how diverse AI agent architectures can be", "mimetype": "text/plain", "start_char_idx": 3446, "end_char_idx": 5543, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "99ce0cb6-1223-4a61-a443-f179ecbd059c": {"__data__": {"id_": "99ce0cb6-1223-4a61-a443-f179ecbd059c", "embedding": null, "metadata": {"page_label": "21", "file_name": "A cybersecurity AI agent selection and decision support framework.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\A cybersecurity AI agent selection and decision support framework.pdf", "file_type": "application/pdf", "file_size": 1115417, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "23995e31-dcbf-46a9-941e-599c6e4c9b5a", "node_type": "4", "metadata": {"page_label": "21", "file_name": "A cybersecurity AI agent selection and decision support framework.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\A cybersecurity AI agent selection and decision support framework.pdf", "file_type": "application/pdf", "file_size": 1115417, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "186fac0d0f66dde8296a1d124a5017ab7818a3708dd56fad126ca172093d0a9c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5e402284-a5a9-4a38-b50e-9d18b637bf0b", "node_type": "1", "metadata": {}, "hash": "abba7c601df33e3140af7d99f647ccf58a15766e41fc23b96a6aad04bd1af4eb", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "20 \n \nsynergistically deployed to achieve operational efficiency and adaptive resilience, moving beyond simplistic \nclassifications to a more nuanced, context-aware deployment strategy. \nFurthermore, t he AIATDF distinguishes itself by treating AI agent architecture  and operational modes  as \nintrinsically interdependent dimensions, thereby providing a unified analytical lens for evaluating and deploying AI \nagents. Traditional approaches often treat these aspects in isolation, focusing either on architectural sophistication \nor the desired level of automation, neglecting their critical interplay. In contrast, the AIATDF demonstrates that an \nAI agent\u2019s architectural complexity directly dictates its potential for autonomy, and conversely, the intended \nautonomy influences the required architectural design. For example, while learning agents can unlock advanced \nautonomous capabilities, they may still operate in assisted or augmented modes when strategic human oversight is \nessential. This multidimensional perspective compels researchers to transcend simplistic, single-axis classifications, \nsuch as \"low\" versus \"high\" autonomy, and examine how an AI agent\u2019s structural design and functional objectives \nconverge to address specific cybersecurity demands. This integrated approach facilitates the development of more \nsophisticated theoretical models, situating layered AI agent architectures within established organisational  risk \nmanagement standards , such as  the NIST CSF. Consequently, the AIATDF  fosters a holistic understanding that \nconnects context-aware AI agent behaviours with the hierarchical needs of enterprise security, spanning real -time \ndefence to strategic governance. This unified perspective bridges the gap between AI agent design and operational \ndeployment, enhancing theoretical and practical applications. \nWhile AI agents promise significant operational efficiency and scalability in cybersecurity, their deployment \nintroduces novel cyber risks, especially within critical infrastructure  domains. As Radanliev et al. [148]  and \nRadanliev [76] emphasise, the rapid integration of AI into critical systems necessitates a thorough  examination of \nboth ethical risks and their associated governance structures. This study recognises that autonomous and semi -\nautonomous AI agents, particularly those interacting with sensitive data streams, can amplify adversarial threats \nsuch as model inversion attacks, data poisoning, and adversarial learning. Such vulnerabilities jeopardise not only \nsystem integrity but also intellectual property and classified information. Consequently, the AIATDF must \nexplicitly integrate principles of responsible AI deployment to pre -empt unintended harms. Radanliev [76] and \nRadanliev et al . [148] advocate for ethical frameworks that prioritise transparency, human oversight, contextual \naccountability, and resilience against manipulation \u2014considerations that become acutely critical in high- stakes \nenvironments, such as  biomedical research, national security operations, and AI -driven policymaking, where the \nconsequences of compromise are profound. Therefore, my proposed maturity- based framework necessitates \nsupplementation with institutional risk assessments, comprehensi ve model audit trails, and thorough ethical \nreadiness evaluations prior to full -scale deployment. Future enhancements to the AIATDF should embed ethical \nassurance layers, including XAI protocols, bias detection modules, and accountability matrices, directly into agentic \nsystem design, fostering not just technical adequacy but also societal trustworthiness.  \n6.1.2 Practical significance \nThe AIATDF offers a structured, step-by-step methodology that enables the mapping of AI agent capabilities to the \nsubcategories of the NIST CSF 2.0, thereby empowering SOC managers and cybersecurity architects with a \nvaluable tool. It eliminates the reliance on inefficient trial- and-error approaches or generic AI solutions, allowing \npractitioners to systematically identify the optimal AI agent type for each subcategory's operational objectives. For \nexample, reactive agents can be strategically deployed for high- volume, rapid-response tasks, such as  real-time \nanomaly detection. In contrast, cognitive agents may be reserved for strategic planning and governance functions.  \nThis targeted approach minimises  resource waste and efficiently allocates computational infrastructure and staff \ntraining. By providing clear guidance, the AIATDF accelerates the deployment of effective AI -driven solutions, \noptimising their impact by ensuring a n alignment between AI agent strengths and specific cybersecurity \nrequirements. Open standards , such as the MCP, can further streamline these deployments by simplifying the \nintegration of LLM -driven agents with diverse enterprise tools and data repositories, thereby expanding the \nAIATDF\u2019s practical reach.  \nFurthermore, t he AIATDF offers scalability, empowering organisations across varying cybersecurity maturity \nlevels to adopt AI-driven capabilities incrementally.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 5100, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5e402284-a5a9-4a38-b50e-9d18b637bf0b": {"__data__": {"id_": "5e402284-a5a9-4a38-b50e-9d18b637bf0b", "embedding": null, "metadata": {"page_label": "21", "file_name": "A cybersecurity AI agent selection and decision support framework.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\A cybersecurity AI agent selection and decision support framework.pdf", "file_type": "application/pdf", "file_size": 1115417, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "23995e31-dcbf-46a9-941e-599c6e4c9b5a", "node_type": "4", "metadata": {"page_label": "21", "file_name": "A cybersecurity AI agent selection and decision support framework.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\A cybersecurity AI agent selection and decision support framework.pdf", "file_type": "application/pdf", "file_size": 1115417, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "186fac0d0f66dde8296a1d124a5017ab7818a3708dd56fad126ca172093d0a9c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "99ce0cb6-1223-4a61-a443-f179ecbd059c", "node_type": "1", "metadata": {"page_label": "21", "file_name": "A cybersecurity AI agent selection and decision support framework.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\A cybersecurity AI agent selection and decision support framework.pdf", "file_type": "application/pdf", "file_size": 1115417, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "26409a0c88a436f37f01dafa8ef0bc4632461b0360a69221d6b98c0163377c93", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "For \nexample, reactive agents can be strategically deployed for high- volume, rapid-response tasks, such as  real-time \nanomaly detection. In contrast, cognitive agents may be reserved for strategic planning and governance functions.  \nThis targeted approach minimises  resource waste and efficiently allocates computational infrastructure and staff \ntraining. By providing clear guidance, the AIATDF accelerates the deployment of effective AI -driven solutions, \noptimising their impact by ensuring a n alignment between AI agent strengths and specific cybersecurity \nrequirements. Open standards , such as the MCP, can further streamline these deployments by simplifying the \nintegration of LLM -driven agents with diverse enterprise tools and data repositories, thereby expanding the \nAIATDF\u2019s practical reach.  \nFurthermore, t he AIATDF offers scalability, empowering organisations across varying cybersecurity maturity \nlevels to adopt AI-driven capabilities incrementally. The framework facilitates a strategic entry point for resource-\nconstrained teams through assisted intelligence, focusing on automated alerts, anomaly detection, and \nrecommendation systems that enhance existing human oversight. As organisational confidence and resource \nallocation expand, the AIATDF supports transitioning to more advanced autonomous solutions, including AI agent-\ndriven incident response and strategic threat analysis. This phased approach minimises  operational disruption and \nfosters stakeholder buy- in, ensuring that each increment of autonomy is implemented optimally  for maximum \nimpact. The framework also  provides a roadmap for continuous advancement, guiding organisations  from \nfoundational automation to sophisticated, fully autonomous AI agent deployments. This strategy enables \norganisations to adapt and evolve their cybersecurity posture in response to the ever-changing threat landscape, \nensuring sustained resilience and operational efficiency.", "mimetype": "text/plain", "start_char_idx": 4122, "end_char_idx": 6089, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c9f59cfb-a61b-49d5-ba37-9cf4ff88f3cb": {"__data__": {"id_": "c9f59cfb-a61b-49d5-ba37-9cf4ff88f3cb", "embedding": null, "metadata": {"page_label": "22", "file_name": "A cybersecurity AI agent selection and decision support framework.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\A cybersecurity AI agent selection and decision support framework.pdf", "file_type": "application/pdf", "file_size": 1115417, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e7692e97-75d4-452d-8eda-eecdfa47a4fc", "node_type": "4", "metadata": {"page_label": "22", "file_name": "A cybersecurity AI agent selection and decision support framework.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\A cybersecurity AI agent selection and decision support framework.pdf", "file_type": "application/pdf", "file_size": 1115417, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "488e7794bd8ba9bdb78a101aab3a9ab4cd1690dae3f68e2bd46ebe3726c22c10", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ddeae525-791f-4ad5-9795-8eab22c13f61", "node_type": "1", "metadata": {}, "hash": "173e3a3f80cd9a89482e995bcc180415cae89cfd806a02025305fd470814ee39", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "21 \n \n6.2 Comparisons with prior work \nIt was discussed in Section 2.1.3, and shown in Table 2, that only three studies from prior works mapped AI \nsolutions to the NIST CSF. These studies were conducted by Pleshakova et al. [99], Hauptman et al. [100] , and \nKaur et al. [101]. This paper distinguishes itself from prior research by presenting a detailed, NIST CSF 2.0-aligned \nframework for the deployment of AI agents in cybersecurity. Unlike earlier works that focused on limited NIST \nCSF 1.1 functions [99] or explored AI -based methods without systematic mapping, the AIATDF explicitly \nintegrates a broad spectrum of AI agent architectures with all six NIST CSF 2.0 functions, including the critical \n'Govern' function. This approach addresses the fragmented scope observed in studies that concentrated on narrower \nfunctions or outdated frameworks, and it builds upon research recognising AI's importance in risk management \n[101] by formalising the mapping of AI agent taxonomies to security tasks. Furthermore, while some studies \nexplored limited AI autonomy within specific NIST CSF functions  [100], the AIATDF spans the entire \ncybersecurity lifecycle, clarifying how varying degrees of agent autonomy\u2014assisted, augmented, and fully \nautonomous\u2014intersect with NIST CSF 2.0 subcategories. By systematically linking AI agent properties to these \nsubcategories, the AIATDF provides a promising , theory-driven mapping mechanism, a feature notably absent in \nprior partial alignments. Thus, t he AIATDF bridges previously disparate strands of AI agent research, MAS  \napproaches, and the evolving NIST CSF 2.0, offering a unified mapping framework that extends and reinforces the \napplicability of AI in modern cybersecurity practices. \n6.3 Limitations of the study \nAlthough the mapping framework is rigorously grounded in peer -reviewed AI agent research and the established \nNIST CSF 2.0 guidelines, it remains  a theoretical construct. While this conceptual foundation provides a strong \nanalytical framework, the AIATDF  lacks empirical validation  within realistic cybersecurity environments. \nConsequently, critical questions regarding practical performance\u2014such as false positive rates in anomaly detection \nor mean time to respond  during active attacks\u2014remain unanswered. To bridge this gap, pilot studies or proof -of-\nconcept deployments are  essential. These empirical investigations would provide invaluable insights, verifying \nwhether each AI agent type delivers the predicted benefits under real -world operational constraints, including \nlimited computational resources and dynamic threat vector s. This real -world validation is crucial for refining the \nframework, ensuring its applicability and accuracy in reflecting the complexities inherent in cybersecurity \noperations. \nAlthough the AIATDF provides distinct AI agent categories for analytical clarity, real-world implementations rarely \nadhere to such rigid classifications. AI agent architectures often exhibit fluidity, particularly in hybrid or evolving \nsystems. For instance, an initially reactive agent may progressively integrate learning components, or a cognitive \nagent may acquire adaptive capabilities over time. Consequently, an organisation's deployment may deviate from \nthese idealised categorisations, reflecting a more dynamic and integrated expression of AI agent behaviours. This \ninherent overlap does not invalidate the AIATDF's utility. Instead, it underscores the necessity for context-sensitive \napplications, emphasising that effective cybersecurity solutions frequently necessitate tailored combinations of AI \nagent functionalities  rather than strict adherence to singular agent types. This context -sensitive approach \nacknowledges the dynamic and complex nature of real -world cybersecurity challenges , demanding a flexible and \nadaptable integration of AI capabilities. \nThe rapid convergence of AI innovation and the evolution of sophisticated threat actors  necessitate a dynamic \napproach to this mapping. Specifically, current AI agent properties may become obsolete or superseded by emerging \nparadigms, such as quantum -safe AI algorithms or next -generation RL. Therefore, the AIATDF  must be \nperiodically reviewed and updated to maintain its operational relevance. This proactive adaptation should \nincorporate novel attack vectors, advanced computational models, and evolving industry best practices.  Failure to \ndo so risks deploying AI agents that, while theoretically sound in the current context, may prove inadequate in \naddressing the dynamic and unpredictable nature of future cyber threats . This continuous evolution is crucial to \nensuring that AI-driven cybersecurity solutions remain effective and resilient in the face of emerging challenges.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4786, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ddeae525-791f-4ad5-9795-8eab22c13f61": {"__data__": {"id_": "ddeae525-791f-4ad5-9795-8eab22c13f61", "embedding": null, "metadata": {"page_label": "22", "file_name": "A cybersecurity AI agent selection and decision support framework.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\A cybersecurity AI agent selection and decision support framework.pdf", "file_type": "application/pdf", "file_size": 1115417, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e7692e97-75d4-452d-8eda-eecdfa47a4fc", "node_type": "4", "metadata": {"page_label": "22", "file_name": "A cybersecurity AI agent selection and decision support framework.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\A cybersecurity AI agent selection and decision support framework.pdf", "file_type": "application/pdf", "file_size": 1115417, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "488e7794bd8ba9bdb78a101aab3a9ab4cd1690dae3f68e2bd46ebe3726c22c10", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c9f59cfb-a61b-49d5-ba37-9cf4ff88f3cb", "node_type": "1", "metadata": {"page_label": "22", "file_name": "A cybersecurity AI agent selection and decision support framework.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\A cybersecurity AI agent selection and decision support framework.pdf", "file_type": "application/pdf", "file_size": 1115417, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "65c4841b85500a248ecfa12826f086ac1144b28e0b4e166f1cc81a9ac2d33e39", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The rapid convergence of AI innovation and the evolution of sophisticated threat actors  necessitate a dynamic \napproach to this mapping. Specifically, current AI agent properties may become obsolete or superseded by emerging \nparadigms, such as quantum -safe AI algorithms or next -generation RL. Therefore, the AIATDF  must be \nperiodically reviewed and updated to maintain its operational relevance. This proactive adaptation should \nincorporate novel attack vectors, advanced computational models, and evolving industry best practices.  Failure to \ndo so risks deploying AI agents that, while theoretically sound in the current context, may prove inadequate in \naddressing the dynamic and unpredictable nature of future cyber threats . This continuous evolution is crucial to \nensuring that AI-driven cybersecurity solutions remain effective and resilient in the face of emerging challenges.  \nWhile the AIATDF provides a promising technical blueprint for aligning AI agent capabilities with specific \ncybersecurity tasks, it does not explicitly address critical organisational factors that influence the sustainable \nsuccess of AI-driven initiatives. These could be placed under \u2018legacy workflows\u2019 in Fig.  6, and their impact on the \nAI agent deployment studied. For example, these factors may include organisational culture, budgetary constraints, \nand staff readiness. In this regard , even a perfectly aligned learning AI agent may fail to deliver its intended value \nwithout organisational buy-in, adequate training budgets, or a workforce ready to collaborate effectively with \nautonomous systems.  Consequently, organisations must conduct readiness assessments and develop change \nmanagement plans to foster internal acceptance and ensure staff possess the requisite skills and tools for seamless \nintegration with AI agents. Without these preparatory measures, even the most technic ally sophisticated AI \ndeployments risk underutilisation , resistance, or premature abandonment. This underscores the imperative that", "mimetype": "text/plain", "start_char_idx": 3891, "end_char_idx": 5920, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a3d1a7a6-d4f6-45c7-b9b0-0181d4e6140b": {"__data__": {"id_": "a3d1a7a6-d4f6-45c7-b9b0-0181d4e6140b", "embedding": null, "metadata": {"page_label": "23", "file_name": "A cybersecurity AI agent selection and decision support framework.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\A cybersecurity AI agent selection and decision support framework.pdf", "file_type": "application/pdf", "file_size": 1115417, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "126e363a-3782-48c3-ac37-112382f9bffb", "node_type": "4", "metadata": {"page_label": "23", "file_name": "A cybersecurity AI agent selection and decision support framework.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\A cybersecurity AI agent selection and decision support framework.pdf", "file_type": "application/pdf", "file_size": 1115417, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "8467afd73dcd4d8157cb28a627e53c3d94112e80f44eb051a446d04ff0631c15", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "22 \n \ntechnological excellence must be coupled with strong organisational alignment to achieve optimal and sustainable \noutcomes. \n7. Conclusion \nThis paper aimed to develop a framework for selecting, designing, and deploying AI agents to address the NIST \nCSF 2.0 cybersecurity requirements, based on identified agent categories, properties, and application contexts. The \nstudy introduced an AI agent taxonomy and decision framework to address this research aim . The AIATDF \nsystematically aligns AI agent types , including reactive, cognitive, hybrid, and learning, with the NIST CSF 2.0 \nfunctions (Identify, Protect, Detect, Respond, Recover, Govern), categories, and subcategories. \n7.1 Summary of key findings  \nThis paper's key findings are encapsulated within five distinct perspectives: (i) theoretical synthesis, (ii) a practical \nmapping instrument, (iii) a phased adoption strategy, (iv) versatile agent architectures, and (v) framework \nlimitations. Firstly, the AI ATDF achieves a novel theoretical synthesis by integrating agent -based AI theory with \nthe structured cybersecurity outcomes defined by the NIST CSF 2.0. This integration, primarily through the \ndifferentiation of assisted, autonomous, and augmented intelligence modes, enhances the theoretical understanding \nof AI agent autonomy's application in cybersecurity. Secondly, the AI ATDF functions as a practical mapping \ninstrument, utilising a matrix to demonstrate the suitability of specific AI agent types for the NIST CSF 2.0. This \nprovides SOC managers and cybersecurity architects with a structured, risk-mitigating guide for selecting AI agents. \nThirdly, the AI ATDF supports a phased adoption approach , enabling organisations  to incrementally integrate AI \nsolutions, from basic automated alerts to advanced autonomous and hybrid systems. Fourthly, as detailed in Section \n5, conceptual validation demonstrates the AIATDF's comprehensive scope, encompassing all six NIST CSF 2.0 \nfunctions and highlighting the potential of a MAS architecture in addressing diverse cyber threats. Finally, while \nthe AIATDF presents a promising theoretical foundation, it requires empirical validation and further development \nto incorporate socio -technical factors and the evolving capabilities of AI . These five perspectives highlight the \npotential of a theoretically grounded and practically applicable approach to deploying cybersecurity AI agents . \nAdditionally, they offer academic insight and actionable guidance for enhancing AI -driven defences within the \nNIST CSF 2.0 framework. \n7.2 Implications for future research \nThis study opens several promising research avenues. Firstly, empirical validation and field studies are necessary \nto establish the applicability of the AIATDF . Pilot deployments in operational environments, measuring metrics \nsuch as response times, false positive/negative rates, and resource overhead across diverse organisational  contexts \n(e.g., small vs. large enterprises), would provide essential data for model refinement. Secondly, exploring  adaptive \nand evolving AI agent architectures is imperative, given the rapid evolution of cyber threats and AI technologies. \nFor example, emerging open standards such as the MCP could significantly improve the integration of external data \nsources into AI agent architectures, rendering the AIATDF approach more scalable and easier to deploy in diverse \norganisational environments. Therefore, future research should investigate how MAS  architectures with adaptive \nlearning layers, leveraging techniques such as reinforcement, transfer, or federated learning, can maintain their \nefficacy in dynamic threat landscapes and distributed data scenarios. Thirdly, future research should integrate socio-\ntechnical factors into AI agent deployment methodologies. Explicitly factoring in organisational  culture, budget \nconstraints, and workforce skill sets would facilitate holistic readiness assessments and deepen the understanding \nof seamless, autonomous,  or augmented intelligence implementation. Fourthly, developing  ethical and regulatory \nframeworks is necessary to address the escalating ethical and compliance concerns associated with advanced AI \nagents. Studies could define guidelines for operationalising responsible AI practices within the NIST CSF 2.0, \nparticularly in high-stakes scenarios. Fifthly, despite the emergence of AI agents CMMs from grey literature, there \nare no scholarly, well -defined CMMs for adopting AI agents. Future research could provide a structured path \n(CMM) outlining key stages of AI agent adoption progression and actiona ble steps for advancement. Ultimately, \nlongitudinal studies examining the long- term deployment of AI agents would provide valuable  insights into the \nevolution of agent-based solutions. These studies would inform continuous improvement mechanisms, ensuring the \nAIATDF's sustained relevance in the face of evolving AI capabilities and cybersecurity threats. \nStatements and Declarations: \nEthical Approval and Consent to Participate \nNot applicable.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 5086, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f5ee87f7-db19-4c24-9b54-4ddd1f4f5079": {"__data__": {"id_": "f5ee87f7-db19-4c24-9b54-4ddd1f4f5079", "embedding": null, "metadata": {"page_label": "24", "file_name": "A cybersecurity AI agent selection and decision support framework.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\A cybersecurity AI agent selection and decision support framework.pdf", "file_type": "application/pdf", "file_size": 1115417, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "599f904d-f72e-4f5f-be6d-e9c5d25483ce", "node_type": "4", "metadata": {"page_label": "24", "file_name": "A cybersecurity AI agent selection and decision support framework.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\A cybersecurity AI agent selection and decision support framework.pdf", "file_type": "application/pdf", "file_size": 1115417, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "6d856465026edbb93dd7205950a70b03696996777c780e53f9bdc9d3ca712726", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "23 \n \nConsent for Publication \nThe author declares consent for publication. \nCompeting Interests \nThe author has no relevant financial or non-financial interests to declare. \nFunding \nNo funding was received to conduct this study. \nAvailability of Supporting Data \nNo datasets were generated or analysed during the current study. Only textual data from a systematic literature \nreview underpin the study.  \nAuthor\u2019s Contributions \nConceptualisation, methodology, writing\u2014original draft preparation, and writing\u2014final draft review and editing.  \nAcknowledgements \nThe author would like to acknowledge the current \u2018read and publish\u2019 agreements negotiated by the South African \nNational Library and Information Consortium (SANLiC) as having contributed to the open -access publication of \nthis paper without the author facing any charges. \nAuthors' Information \nMasike Malatji \nGraduate School of Business Leadership (SBL), University of South Africa (UNISA), Midrand, Johannesburg, \nSouth Africa \nDeclaration of Generative AI and AI-assisted Technologies in the Writing Process \nWhile preparing this work, the author used Scopus AI (Beta) to review some articles, ChatGPT 4.5 to analyse textual \ndata, Google Gemini and ChatGPT 4.5 to structure the initial ideas and logical flow of the paper and improve its \nreadability, and Grammarly for English language editing. After using these AI -powered tools, the author reviewed \nand edited the content as needed and took full responsibility for the publication's content . \nReferences \n1. Gupta M, Akiri C, Aryal K, Parker E, Praharaj L (2023) From ChatGPT to ThreatGPT: Impact of generative \nAI in cybersecurity and privacy. IEEE Access 11: 80218\u201345. https://doi.org/10.1109/ACCESS.2023.3300381 \n2. Mahboubi A, Luong K, Aboutorab H, Bui HT, Jarrad G, Bahutair M, Camtepe S, Pogrebna G, Ahmed E, \nBarry B, Gately H (2024) Evolving techniques in cyber threat hunting: A systematic review. Journal of \nNetwork and Computer Applications 232: 104004. https://doi.org/10.1016/j.jnca.2024.104004 \n3. Villal\u00f3n-Huerta A, Ripoll- Ripoll I, Marco -Gisbert H (2022) Key requirements for the detection and sharing \nof behavioral indicators of compromise. Electronics 11(3): 416. https://doi.org/10.3390/electronics11030416 \n4. Pinto A, Herrera L, Donoso Y, Gutierrez JA (2023) Survey on intrusion detection systems based on machine \nlearning techniques for the protection of critical infrastructure. Sensors 23(5): 2415. \nhttps://doi.org/10.3390/s23052415. \n5. Priyalakshmi V, Devi R (2023) Analysis and implementation of normalisation techniques on KDD\u201999 data set \nfor IDS and IPS. In: Saraswat M, Chowdhury C, Kumar Mandal C, Gandomi AH (eds) Proceedings of \nInternational Conference on Data Science and Applications . Springer Nature, Singapore, pp. 51\u2013 70. \nhttps://doi.org/10.1007/978-981-19-6634-7_5 \n6. Namakshenas D, Yazdinejad A, Dehghantanha A, Srivastava G (2024) Federated quantum -based privacy -\npreserving threat detection model for consumer internet of things. IEEE Transactions on Consumer Electronics \n70(3): 5829\u201338. https://doi.org/10.1109/TCE.2024.3377550 \n7. Kshetri N (2021) Economics of artificial intelligence in cybersecurity. IT Professional 23(5): 73 \u201377. \nhttps://doi.org/10.1109/MITP.2021.3100177", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3259, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1b54ab41-3a7d-4272-9b71-f9f5a9dabd14": {"__data__": {"id_": "1b54ab41-3a7d-4272-9b71-f9f5a9dabd14", "embedding": null, "metadata": {"page_label": "25", "file_name": "A cybersecurity AI agent selection and decision support framework.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\A cybersecurity AI agent selection and decision support framework.pdf", "file_type": "application/pdf", "file_size": 1115417, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c0673f5c-d45b-41c3-9220-951c9694195f", "node_type": "4", "metadata": {"page_label": "25", "file_name": "A cybersecurity AI agent selection and decision support framework.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\A cybersecurity AI agent selection and decision support framework.pdf", "file_type": "application/pdf", "file_size": 1115417, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "34e1b60b756c1935083620dc991c54ad28e70fb6bc4a9a5d554fb499937e7724", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3caa9e0e-1b46-4b01-84c0-8c4b45ec31e1", "node_type": "1", "metadata": {}, "hash": "a4fc77394ad1fe412201b4cc3070753429198aff0f662c5565d4c9a1bea14b58", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "24 \n \n8. Jalalvand F, Chhetri MB, Nepal S, Paris C (2024) Alert prioritisation in security operations centres: A \nsystematic survey on criteria and methods. ACM Computing Surveys 57(2) : 1 -36. \nhttps://doi.org/10.1145/3695462 \n9. Sindiramutty SJ (2023) Autonomous threat hunting: a future paradigm for AI-driven threat intelligence. arXiv. \nhttps://doi.org/10.48550/arXiv.2401.00286 \n10. Chen W, Zhang J (2024) Elevating security operations: the role of ai -driven automation in enhancing soc \nefficiency and efficacy. Journal of Artificial Intelligence and Machine Learning in Management 8(2): 1 \u201313. \n11. Sarker IH, Janicke H, Mohammad N, Watters P, Nepal S (2024) AI potentiality and  awareness: A position \npaper from  the perspective of  human-AI teaming in  cybersecurity. In: Vasant P, Panchenko V, Munapo E, \nWeber G-W, Thomas JJ, Intan R, Arefin MS (eds) Intel ligent Computing and Optimization. Springer Nature \nSwitzerland, Cham, pp. 140\u201349. https://doi.org/10.1007/978-3-031-50887-5_14 \n12. Shoaee H, Bagherinejad J, Rezaee Nour J (2022) Towards the analysis of information technology governance \nand productivity based on COBIT framework: An empirical study in e-banking. Tehnicki Vjesnik - Technical \nGazette 29 (6). https://doi.org/10.17559/TV-20220115074214 \n13. Vats V, Nizam MB, Liu M, Wang Z, Ho R, Prasad MS, Titterton V, Malreddy SV, Aggarwal R, Xu Y, Ding \nL, Mehta J, Grinnell N, Liu L, Zhong S, Gandamani D, Tang X, Ghosalkar R, Shen C, Shen R, Hussain N, \nRavichandran K, Davis J (2024) A survey on human- AI team ing with large pre -trained models. arXiv. \nhttps://doi.org/10.48550/arXiv.2403.04931 \n14. NIST (2024) The NIST Cybersecurity Framework (CSF) 2.0. https://www.nist.gov/cyberframework. \nAccessed 07 April 2025. \n15. Lungu N, Rababah AA, Dash BB, Syed AH, Barik L, Rout S, Tembo S, Lubobya C, Patra SS (2024) NIST \nCSF-2.0 compliant GPU shader execution. Engineering, Technology & Applied Science Research 14(4): \n15187\u201393. https://doi.org/10.48084/etasr.7351 \n16. Horan C, Saiedian H (2021) Cyber crime investigation: Landscape, challenges, and future research directions. \nJournal of Cybersecurity and Privacy 1 (4): 580\u201396. https://doi.org/10.3390/jcp1040029 \n17. Aslan \u00d6, Aktu\u011f SS, Ozkan -Okay M, Yilmaz AA, Akin E. A comprehensive review of cyber security \nvulnerabilities, threats, attacks, and solutions. Electronics 12(6): 1333. \nhttps://doi.org/10.3390/electronics12061333 \n18. Gonzalez C, Aggarwal P, Cranford EA, Lebiere C (2023) Adaptive cyberdefense with deception: A human\u2013\nAI cognitive approach. In: Bao T, Tambe M, Wang C (eds) Cyber deception: Techniques, strategies, and \nhuman aspects, pp. 41 \u201357. Springer International Publis hing, Cham. https://doi.org/10.1007/978- 3-031-\n16613-6_3 \n19. Peng L, Li D, Zhang Z, Zhang T, Huang A, Yang S, Hu Y (2024) Human- AI collaboration: Unraveling  the \neffects of user proficiency and AI agent capability in intelligent decision support systems.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2942, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3caa9e0e-1b46-4b01-84c0-8c4b45ec31e1": {"__data__": {"id_": "3caa9e0e-1b46-4b01-84c0-8c4b45ec31e1", "embedding": null, "metadata": {"page_label": "25", "file_name": "A cybersecurity AI agent selection and decision support framework.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\A cybersecurity AI agent selection and decision support framework.pdf", "file_type": "application/pdf", "file_size": 1115417, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c0673f5c-d45b-41c3-9220-951c9694195f", "node_type": "4", "metadata": {"page_label": "25", "file_name": "A cybersecurity AI agent selection and decision support framework.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\A cybersecurity AI agent selection and decision support framework.pdf", "file_type": "application/pdf", "file_size": 1115417, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "34e1b60b756c1935083620dc991c54ad28e70fb6bc4a9a5d554fb499937e7724", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1b54ab41-3a7d-4272-9b71-f9f5a9dabd14", "node_type": "1", "metadata": {"page_label": "25", "file_name": "A cybersecurity AI agent selection and decision support framework.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\A cybersecurity AI agent selection and decision support framework.pdf", "file_type": "application/pdf", "file_size": 1115417, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "60941d6f98bd45e057ae74a4bb5a83b062d1629aacba0bae1b8927bc26b78b82", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Electronics 12(6): 1333. \nhttps://doi.org/10.3390/electronics12061333 \n18. Gonzalez C, Aggarwal P, Cranford EA, Lebiere C (2023) Adaptive cyberdefense with deception: A human\u2013\nAI cognitive approach. In: Bao T, Tambe M, Wang C (eds) Cyber deception: Techniques, strategies, and \nhuman aspects, pp. 41 \u201357. Springer International Publis hing, Cham. https://doi.org/10.1007/978- 3-031-\n16613-6_3 \n19. Peng L, Li D, Zhang Z, Zhang T, Huang A, Yang S, Hu Y (2024) Human- AI collaboration: Unraveling  the \neffects of user proficiency and AI agent capability in intelligent decision support systems. International Journal \nof Industrial Ergonomics 103: 103629. https://doi.org/10.1016/j.ergon.2024.103629 \n20. Putta P, Mills E, Garg N, Motwani S, Finn C, Garg D, Rafailov R (2024) Agent Q: Advanced reasoning and \nlearning for autonomous AI agents. arXiv. https://doi.org/10.48550/arXiv.2408.07199  \n21. Horn J, Hallin N, Taheri H, O\u2019Rourke M, Edwards D (2013) Intentional state -ascription in multi- agent \nsystems. In: M\u00fcller VC (eds) Philosophy and theory of artificial intelligence, pp. 225\u2013 35. Springer, Berlin. \nhttps://doi.org/10.1007/978-3-642-31674-6_17 \n22. Leite A, Girardi R, Novais P (2013) Using ontologies in hybrid software agent architectures. In: 2013 \nIEEE/WIC/ACM International Joint Conferences on Web Intelligence (WI) and Intelligent Agent \nTechnologies (IAT), 3: 155\u201358. https://doi.org/10.1109/WI-IAT.2013.172 \n23. Liu B, Mazumder S, Robertson E, Grigsby S (2023) AI autonomy: Self\u2010initiated open\u2010world continual learning \nand adaptation. AI Magazine 44(2): 185\u201399. https://doi.org/10.1002/aaai.12087 \n24. Mazumder S, Liu B (2024) Open- world continual learning: A framework. In: Mazumder S, Liu B (eds) \nLifelong and continual learning dialogue systems. Springer International Publishing, Cham, pp. 21\u2013 47. \nhttps://doi.org/10.1007/978-3-031-48189-5_2 \n25. Rossi N (2023) Applications of artificial intelligence in healthcare. TKS Publisher 41(2): 49\u2013 51. \nhttps://www.teknoscienze.com/tks_article/applications-of-artificial-intelligence-in-healthcare/. Accessed 07 \nApril 2025. \n26. Rabuzin K, Malekovi\u0107 M, Ba\u010da M (2006) A survey of the properties of agents. Journal of Information and \nOrganizational Sciences 30(1): 155\u201370. https://hrcak.srce.hr/20873 \n27. Garro A, M\u00fchlh\u00e4user M, Tundis A, Mariani S, Omicini A, Vizzari G (2018) Intelligent agents and \nenvironment. In: Encyclopedia of Bioinformatics and Computational Biology: ABC of Bioinformatics, 1:  \n309\u201314. Elsevier. https://doi.org/10.1016/B978-0-12-809633-8.20327-0 \n28. Fern\u00e1ndez JM, Pav\u00f3n J (2010) Talking agents: A distributed architecture for interactive artistic installations. \nIntegrated Computer-Aided Engineering 17(3): 243\u201359. https://doi.org/10.3233/ICA-2010-0341", "mimetype": "text/plain", "start_char_idx": 2349, "end_char_idx": 5106, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "82e90a79-617c-4aa9-a794-dc0d054e5694": {"__data__": {"id_": "82e90a79-617c-4aa9-a794-dc0d054e5694", "embedding": null, "metadata": {"page_label": "26", "file_name": "A cybersecurity AI agent selection and decision support framework.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\A cybersecurity AI agent selection and decision support framework.pdf", "file_type": "application/pdf", "file_size": 1115417, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "513abfd1-07ca-4784-bd5e-41c0c3f8dd0f", "node_type": "4", "metadata": {"page_label": "26", "file_name": "A cybersecurity AI agent selection and decision support framework.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\A cybersecurity AI agent selection and decision support framework.pdf", "file_type": "application/pdf", "file_size": 1115417, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "e0a1f1de902c2937a4eca4e0035bb90f6405d912cd0cfdc9446ba8d5f756cb31", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ef63d4b0-c211-4e84-8711-c84ea2c36260", "node_type": "1", "metadata": {}, "hash": "8d6d9126119949c3094b86548d22ec1c639fc1abd8c614e79b3188f5760091d2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "25 \n \n29. Kefalas P, Stamatopoulou I (2011) Towards modelling of reactive, goal-oriented and hybrid intelligent agents \nusing p systems. In: Gheorghe M, Hinze T, P\u0103un G, Rozenberg G, Salomaa A (eds) Membrane Computing. \nSpringer, Berlin, pp. 265\u201372. https://doi.org/10.1007/978-3-642-18123-8_21 \n30. B\u00f6sser T, (2015) Autonomous agents. In: Wright JD (ed) International encyclopedia of the social & behavioral \nsciences, 2nd edition. Elsevier, Oxford, pp. 309\u201313. https://doi.org/10.1016/B978-0-08-097086-8.43011-4 \n31. Weng Y-H, Ho C-h (2020) Embodiment and algorithms for human\u2013robot interaction. In: Barfield W (ed) The \nCambridge Handbook of the Law of Algorithms. Cambridge University Press, Cambridge, pp. 736 \u201356. \nhttps://doi.org/10.1017/9781108680844.035 \n32. Preethiya T, Subbiah P, Pandiarajan T, Ojo S, Vijayalakshmi S (2024) Artificial intelligence in robotics. In: \nModeling, simulation, and control of ai robotics and autonomous systems. IGI Global Scientific Publishing, \npp. 152\u201365. https://doi.org/10.4018/979-8-3693-1962-8.ch009 \n33. Dash S (2025) Green AI: Enhancing sustainability and energy efficiency in ai -integrated enterprise systems. \nIEEE Access 13: 21216\u201328. https://doi.org/10.1109/ACCESS.2025.3532838 \n34. Kott A (2018) Intelligent autonomous agents are key to cyber defense of the future army networks. The Cyber \nDefense Review 3(3): 57 \u201370. https://cyberdefensereview.army.mil/CDR-Content/Articles/Article-\nView/Article/1716477/intelligent-autonomous-agents-are-key-to-cyber-defense-of-the-future-army-netwo/. \nAccessed 09 April 2025. \n35. Th\u00e9ron P, Kott A (2019) When autonomous intelligent goodware will fight autonomous intelligent malware: \na possible future of cyber defense. In: 2019 IEEE Military Communications Conference (MILCOM), pp. 1\u2013\n7. https://doi.org/10.1109/MILCOM47813.2019.9021038 \n36. Kott A, Th\u00e9ron P, Dra\u0161ar M, Dushku E, LeBlanc B, Losiewicz P, Guarino A, Mancini L, Panico A, Pihelgas \nM, Rzadca K, Gaspari FD (2023) Autonomous intelligent cyber-defense agent (AICA) reference architecture, \nrelease 2.0. arXiv. https://doi.org/10.48550/arXiv.1803.10664 \n37. Truong TC, Diep QB, Zelinka I (2020) Artificial intelligence in the cyber domain: offense and defense. \nSymmetry 12(3): 410. https://doi.org/10.3390/sym12030410 \n38. Ligo AK, Kott A, Linkov I (2021) How to measure cyber -resilience of a system with autonomous agents: \napproaches and challenges. IEEE Engineering Management Review 49(2): 89\u2013 97. \nhttps://doi.org/10.1109/EMR.2021.3074288 \n39. Naik B, Mehta A, Yagnik H, Shah M (2022) The impacts of artificial intelligence techniques in augmentation \nof cybersecurity: A comprehensive review. Complex & Intelligent Systems 8(2): 1763\u2013 80. \nhttps://doi.org/10.1007/s40747-021-00494-8 \n40. Sharma N, Jindal N (2024) Emerging artificial intelligence applications: Metaverse, IoT, cybersecurity, \nhealthcare - An overview.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2883, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ef63d4b0-c211-4e84-8711-c84ea2c36260": {"__data__": {"id_": "ef63d4b0-c211-4e84-8711-c84ea2c36260", "embedding": null, "metadata": {"page_label": "26", "file_name": "A cybersecurity AI agent selection and decision support framework.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\A cybersecurity AI agent selection and decision support framework.pdf", "file_type": "application/pdf", "file_size": 1115417, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "513abfd1-07ca-4784-bd5e-41c0c3f8dd0f", "node_type": "4", "metadata": {"page_label": "26", "file_name": "A cybersecurity AI agent selection and decision support framework.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\A cybersecurity AI agent selection and decision support framework.pdf", "file_type": "application/pdf", "file_size": 1115417, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "e0a1f1de902c2937a4eca4e0035bb90f6405d912cd0cfdc9446ba8d5f756cb31", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "82e90a79-617c-4aa9-a794-dc0d054e5694", "node_type": "1", "metadata": {"page_label": "26", "file_name": "A cybersecurity AI agent selection and decision support framework.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\A cybersecurity AI agent selection and decision support framework.pdf", "file_type": "application/pdf", "file_size": 1115417, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "420b65cc39c69b520652a173bb58a69d76ff361bd4a88dfbc2fd01ed52c8f7df", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Ligo AK, Kott A, Linkov I (2021) How to measure cyber -resilience of a system with autonomous agents: \napproaches and challenges. IEEE Engineering Management Review 49(2): 89\u2013 97. \nhttps://doi.org/10.1109/EMR.2021.3074288 \n39. Naik B, Mehta A, Yagnik H, Shah M (2022) The impacts of artificial intelligence techniques in augmentation \nof cybersecurity: A comprehensive review. Complex & Intelligent Systems 8(2): 1763\u2013 80. \nhttps://doi.org/10.1007/s40747-021-00494-8 \n40. Sharma N, Jindal N (2024) Emerging artificial intelligence applications: Metaverse, IoT, cybersecurity, \nhealthcare - An overview. Multimedia Tools and Applications 83(19): 57317\u2013 45. \nhttps://doi.org/10.1007/s11042-023-17890-6 \n41. Dodig-Crnkovic G, Burgin M (2024) A systematic approach to autonomous agents. Philosophies 9(2): 44. \nhttps://doi.org/10.3390/philosophies9020044 \n42. Srivastava A, Stager S (2024) Cognitive computing with deep learning based cybersecurity solution for human \ncomputer interface applications In: 2024 International Conference on Data Science and Network Security \n(ICDSNS), pp. 1\u20136. https://doi.org/10.1109/ICDSNS62112.2024.10691061 \n43. Xu W, Gao Z (2024) Applying HCAI in developing effective human-AI teaming: A perspective from human-\nAI joint cognitive systems. Interactions 31(1): 32\u201337. https://doi.org/10.1145/3635116 \n44. Abramoff MD (2021) Autonomous artificial intelligence safety and trust. In: Grzybowski A (ed) Artificial \nIntelligence in Ophthalmology. Springer International Publishing, Cham, pp. 55\u2013 67. \nhttps://doi.org/10.1007/978-3-030-78601-4_4 \n45. Freitas MP, Piai VA, Farias RH, Fernandes AMR, de Moraes Rossetto AG, Leithardt  VRQ (2022) Artificial \nintelligence of things applied to assistive technology: A systematic literature review. Sensors 22(21): 8531. \nhttps://doi.org/10.3390/s22218531 \n46. Yau K-LA, Lee HJ, Chong Y-W, Ling MH, Syed AR, Wu C, Goh HG (2021) Augmented intelligence: Surveys \nof literature and expert opinion to understand relations between human intelligence and artificial intelligence. \nIEEE Access 9: 136744\u201361. https://doi.org/10.1109/ACCESS.2021.3115494 \n47. Yau K-L, Saleem Y, Chong Y-W, Fan X, Eyu JM, Chieng D (2024) The augmented intelligence perspective \non human- in-the-loop reinforcement learning: Review, concept designs, and future directions. IEEE \nTransactions on Human-Machine Systems 54(6): 762\u201377. https://doi.org/10.1109/THMS.2024.3467370 \n48. Simmler M, Frischknecht R (2021) A taxonomy of human\u2013machine collaboration: Capturing automation and \ntechnical autonomy. AI & Society 36(1): 239\u201350. https://doi.org/10.1007/s00146-020-01004-z \n49. Hinsen S, Hofmann P, J\u00f6hnk J, Urbach N (2022) How can organizations design purposeful human -ai \ninteractions: a practical perspective from existing use cases and interviews. In: Hawaii International \nConference on System Sciences. https://aisel.aisnet.org/hicss-55/cl/human-ai_collaboration/2", "mimetype": "text/plain", "start_char_idx": 2281, "end_char_idx": 5192, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e15480fd-fdc3-4b54-9af9-92b563abac6e": {"__data__": {"id_": "e15480fd-fdc3-4b54-9af9-92b563abac6e", "embedding": null, "metadata": {"page_label": "27", "file_name": "A cybersecurity AI agent selection and decision support framework.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\A cybersecurity AI agent selection and decision support framework.pdf", "file_type": "application/pdf", "file_size": 1115417, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f39e0524-039c-4e15-9417-39925ee5588e", "node_type": "4", "metadata": {"page_label": "27", "file_name": "A cybersecurity AI agent selection and decision support framework.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\A cybersecurity AI agent selection and decision support framework.pdf", "file_type": "application/pdf", "file_size": 1115417, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "7d8657c776abebaaf406e0332a70119c4590221934ca4cd588910a9f5592a3bb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "62db4191-b1c1-4c79-8fe9-2816ce69c04f", "node_type": "1", "metadata": {}, "hash": "cd75e053dbe70cfeb005b0a206a3cadee07a7f1995352060bfea8b6e8655d9ea", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "26 \n \n50. Roch N, Sievers H, Sch\u00f6ni L, Zimmermann V (2024) Navigating autonomy: Unveiling security experts\u2019 \nperspectives on augmented intelligence in cybersecurity. Usenix Association, pp. 41\u2013 60. \nhttps://www.usenix.org/conference/soups2024/presentation/roch. Accessed 07 April 2025. \n51. Dedeke A (2017) Cybersecurity framework adoption: using capability levels for implementation tiers and \nprofiles. IEEE Security & Privacy 15(5): 47\u201354. https://doi.org/10.1109/MSP.2017.3681063 \n52. Dimitrov V, Kaloyanova K, Petrov M (2021) Adapted SANS cybersecurity policies for NIST cybersecurity \nframework. In: CEUR Workshop Proceedings, 2933:  293\u2013301. https://ceur-ws.org/Vol-2933/paper29.pdf. \nAccessed 26 March 2025.  \n53. Edwards J, (2024) A comprehensive guide to the NIST cybersecurity framework 2.0: Strategies, \nimplementation, and best practice. John Wiley & Sons, Ltd. https://doi.org/10.1002/9781394280391.fmatter. \n54. Velasco JM (2017) The situation and evolution of the managed services of cybersecurity, towards 3.0 and \nbeyond. In: Ram\u00edrez JM, Garc\u00eda- Segura LA (eds) Cyberspace: Risks and Benefits for Society, Security and \nDevelopment. Springer International Publishing, Cham, pp. 153 \u201364. https://doi.org/10.1007/978-3-319-\n54975-0_9 \n55. Axon L, Fletcher K, Scott AS, Stolz M, Hannigan R, Kaafarani AE, Goldsmith M, Creese S (2022) Emerging \ncybersecurity capability gaps in the industrial internet of things: overview and research agenda. Digital Threats \n3(4): 34:1-34:27. https://doi.org/10.1145/3503920 \n56. NIST (2025) Cyber AI profile. https://www.nccoe.nist.gov/projects/cyber-ai-profile. Accessed 05 April 2025. \n57. Malatji M, Tolah A (2024) Artificial intelligence (AI) cybersecurity dimensions: A comprehensive framework \nfor understanding adversarial and offensive AI. AI and Ethics. https://doi.org/10.1007/s43681- 024-00427-4. \n58. Dash B, Ansari MF, Sharma P, Ali A (2022) Threats and opportunities with ai -based cyber security intrusion \ndetection: A review. International Journal of Software Engineering & Applications 13(5). \nhttps://ssrn.com/abstract=4323258 \n59. Bela\u00efd A, (2024) Human -machine collaboration for incident response in cybersecurity operations for \nautonomous vehicles. African Journal of Artificial Intelligence and Sustainable Development 4(1): 297\u2013 321. \nhttps://africansciencegroup.com/index.php/AJAISD/article/view/98 \n60. Johnson M, Bradshaw JM, Feltovich PJ (2018) Tomorrow\u2019s human\u2013 machine design tools: from levels of \nautomation to interdependencies. Journal of Cognitive Engineering and Decision Making 12(1): 77 \u201382. \nhttps://doi.org/10.1177/1555343417736462 \n61. Ivanov SH (2023) Automated decision- making. Foresight 25(1): 4 \u201319. https://doi.org/10.1108/FS-09-2021-\n0183 \n62. Altamimi S, Altamimi B, C\u00f4t\u00e9 D, Shirmohammadi S (2023) Toward a superintelligent action recommender \nfor network operation centers using reinforcement learning. IEEE Access 11:  20216\u201329. \nhttps://doi.org/10.1109/ACCESS.2023.3248652 \n63. Tilbury J, Flowerday S (2024) Humans and automation: augmenting security operation centers. Journal of \nCybersecurity and Privacy 4(3): 388\u2013409.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3126, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "62db4191-b1c1-4c79-8fe9-2816ce69c04f": {"__data__": {"id_": "62db4191-b1c1-4c79-8fe9-2816ce69c04f", "embedding": null, "metadata": {"page_label": "27", "file_name": "A cybersecurity AI agent selection and decision support framework.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\A cybersecurity AI agent selection and decision support framework.pdf", "file_type": "application/pdf", "file_size": 1115417, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f39e0524-039c-4e15-9417-39925ee5588e", "node_type": "4", "metadata": {"page_label": "27", "file_name": "A cybersecurity AI agent selection and decision support framework.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\A cybersecurity AI agent selection and decision support framework.pdf", "file_type": "application/pdf", "file_size": 1115417, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "7d8657c776abebaaf406e0332a70119c4590221934ca4cd588910a9f5592a3bb", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e15480fd-fdc3-4b54-9af9-92b563abac6e", "node_type": "1", "metadata": {"page_label": "27", "file_name": "A cybersecurity AI agent selection and decision support framework.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\A cybersecurity AI agent selection and decision support framework.pdf", "file_type": "application/pdf", "file_size": 1115417, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "095e12c0cfd4698f269933060a6db7b5541f706c4bdf8eece1dffb4960a4e418", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "https://doi.org/10.1177/1555343417736462 \n61. Ivanov SH (2023) Automated decision- making. Foresight 25(1): 4 \u201319. https://doi.org/10.1108/FS-09-2021-\n0183 \n62. Altamimi S, Altamimi B, C\u00f4t\u00e9 D, Shirmohammadi S (2023) Toward a superintelligent action recommender \nfor network operation centers using reinforcement learning. IEEE Access 11:  20216\u201329. \nhttps://doi.org/10.1109/ACCESS.2023.3248652 \n63. Tilbury J, Flowerday S (2024) Humans and automation: augmenting security operation centers. Journal of \nCybersecurity and Privacy 4(3): 388\u2013409. https://doi.org/10.3390/jcp4030020 \n64. Mageshkumar N, Vijayaraj A, Arunpriya N, Sangeetha A (2022) Efficient spam filtering through intelligent \ntext modification detection using machine learning. Materials Today: Proceedings, International Conference \non Advanced Materials for Innovation and Sus tainability, 64: 848\u201358. \nhttps://doi.org/10.1016/j.matpr.2022.05.364 \n65. Neff G, Nagy P (2016) Automation, algorithms, and politics| talking to bots: Symbiotic agency and the case \nof Tay. International Journal of Communication 10:17. https://ijoc.org/index.php/ijoc/article/view/6277 . \n66. Girdhar M, Hong J, Moore J (2023) Cybersecurity of autonomous vehicles: a systematic literature review of \nadversarial attacks and defense models. IEEE Open Journal of Vehicular Technology 4:  417\u201337. \nhttps://doi.org/10.1109/OJVT.2023.3265363 \n67. Mudhivarthi BR, Thakur P, Singh G (2023) Aspects of cyber security in autonomous and connected vehicles. \nApplied Sciences 13(5): 3014. https://doi.org/10.3390/app13053014 \n68. Dehghantanha A, Yazdinejad A, Parizi RM (2024) Autonomous cybersecurity: evolving challenges, emerging \nopportunities, and future research trajectories. In: Proceedings of the Workshop on Autonomous \nCybersecurity. Association for Computing Machinery, pp. 1\u2013 10, New York, NY, USA. \nhttps://doi.org/10.1145/3689933.3690832 \n69. Morovat K, Panda B (2020) A survey of artificial intelligence in cybersecurity. In: 2020 International \nConference on Computational Science and Computational Intelligence (CSCI), pp. 109\u2013 15. \nhttps://doi.org/10.1109/CSCI51800.2020.00026 \n70. Rai HM, Galymzada A, Almas K, Nurzhan D, Alibek M (2024) Fortifying cyber defenses: a deep dive into \nthe development of an AI -powered network intrusion detection system. In: Tanwar S, Singh PK, Ganzha M, \nEpiphaniou G (eds) Proceedings of 5\nth International Conference on Computing, Communications, and Cyber-\nSecurity. Springer Nature, Singapore, pp. 809\u201321. https://doi.org/10.1007/978-981-97-2550-2_58 \n71. Chan A, Ezell C, Kaufmann M, Wie K, Hammond L, Bradley H, Bluemke E, Rajkumar N, Krueger D, Kolt \nN, Heim L, Anderljung M (2024) Visibility into AI agents\u2019. In: Proceedings of the 2024 ACM Conference on", "mimetype": "text/plain", "start_char_idx": 2583, "end_char_idx": 5331, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "2c16ae91-bd75-4f5c-95c4-e41637e0dba4": {"__data__": {"id_": "2c16ae91-bd75-4f5c-95c4-e41637e0dba4", "embedding": null, "metadata": {"page_label": "28", "file_name": "A cybersecurity AI agent selection and decision support framework.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\A cybersecurity AI agent selection and decision support framework.pdf", "file_type": "application/pdf", "file_size": 1115417, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1b74cad1-591c-4e89-81c0-0fd0159d645a", "node_type": "4", "metadata": {"page_label": "28", "file_name": "A cybersecurity AI agent selection and decision support framework.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\A cybersecurity AI agent selection and decision support framework.pdf", "file_type": "application/pdf", "file_size": 1115417, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "89306f5f6d11f9f69594821258392cd19f4dc75c996c366dda1c5a8bc883a2e0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "be19f4f1-817d-48d3-a3e0-b43fb59a1afe", "node_type": "1", "metadata": {}, "hash": "5609049253117c1cd70b88db6787246df85d5408bf95a0f0df8966b5a9d9543e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "27 \n \nFairness, Accountability, and Transparency. Association for Computing Machinery, New York, NY, USA, \npp. 958\u201373. https://doi.org/10.1145/3630106.3658948 \n72. Cronin I (2024) Autonomous AI agents: Decision- making, data, and algorithms. In: Cronin I (ed) \nUnderstanding generative ai business applications: A guide to technical principles and real-world applications, \npp. 165\u201380. Apress, Berkeley, CA, USA. https://doi.org/10.1007/979-8-8688-0282-9_11 \n73. Jiang Y-H, Li R., Zhou Y, Qi C, Hu H, Wei Y, Jiang B, Wu Y (2024) AI agent for education: Von Neumann \nmulti-agent system framework. arXiv. https://doi.org/10.48550/arXiv.2501.00083 \n74. Capuano N, Fenza G, Loia V, Stanzione C (2022) Explainable artificial intelligence in cybersecurity: A survey. \nIEEE Access 10: 93575\u2013600. https://doi.org/10.1109/ACCESS.2022.3204171 \n75. Desai B, Patil K, Mehta I, Patil A (2024) Explainable AI in cybersecurity: A comprehensive framework for \nenhancing transparency, trust, and human-ai collaboration. In: 2024 International Seminar on Application for \nTechnology of Information and Communication (iSemantic), pp. 135 \u201350. \nhttps://doi.org/10.1109/iSemantic63362.2024.10762690 \n76. Radanliev P (2025) AI ethics: Integrating transparency, fairness, and privacy in AI development. Applied \nArtificial Intelligence 39(1): 2463722. https://doi.org/10.1080/08839514.2025.2463722 \n77. Prasad RR, Robinson RRR, Thomas C, Balakrishnan N (2021) Evaluation of strategic decision taken by \nautonomous agent using explainable AI. In: 4 th International Conference on Security and Privacy (ISEA -\nISAP), pp. 1\u20138. https://doi.org/10.1109/ISEA-ISAP54304.2021.9689715 \n78. Hamet P, Tremblay J (2017) Artificial intelligence in medicine. Metabolism \u2013  Clinical and Experimental \n69:S36\u201340. https://doi.org/10.1016/j.metabol.2017.01.011 \n79. Berberian B, Somon B, Saha\u00ef A, Gouraud J (2017) The out -of-the-loop brain: A neuroeconomic approach of \nthe human automation interaction. Annual Reviews in Control 44:  303\u201315. \nhttps://doi.org/10.1016/j.arcontrol.2017.09.010 \n80. Hellebrandt T, Huebser L, Adam T, Heine I, Schmitt RH (2021) Augmented intelligence \u2013 mensch trifft \nk\u00fcnstliche intelligenz: intelligentes zusammenwirken von mensch und ki f\u00fcr bessere entscheidungen und \nhandlungen in der produktion. Zeitschrift F\u00fcr Wirtscha ftlichen Fabrikbetrieb 116(6): 433 \u201337. \nhttps://doi.org/10.1515/zwf-2021-0104 \n81. Jain H, Padmanabhan B, Pavlou PA, Raghu TS (2021) Editorial for the special section on humans, algorithms, \nand augmented intelligence: The future of work, organizations, and society. Information Systems Research \n32(3): 675\u201387. https://doi.org/10.1287/isre.2021.1046 \n82. Karunamurthy A,  Kiruthivasan R, Gauthamkrishna S (2023) Human-in-the-Loop intelligence: Advancing AI-\ncentric cybersecurity for the future. Quing: International Journal of Multidisciplinary Scientific Research and \nDevelopment 2(3): 20\u201343. https://doi.org/10.54368/qijmsrd.2.3.0011 \n83.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2960, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "be19f4f1-817d-48d3-a3e0-b43fb59a1afe": {"__data__": {"id_": "be19f4f1-817d-48d3-a3e0-b43fb59a1afe", "embedding": null, "metadata": {"page_label": "28", "file_name": "A cybersecurity AI agent selection and decision support framework.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\A cybersecurity AI agent selection and decision support framework.pdf", "file_type": "application/pdf", "file_size": 1115417, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1b74cad1-591c-4e89-81c0-0fd0159d645a", "node_type": "4", "metadata": {"page_label": "28", "file_name": "A cybersecurity AI agent selection and decision support framework.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\A cybersecurity AI agent selection and decision support framework.pdf", "file_type": "application/pdf", "file_size": 1115417, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "89306f5f6d11f9f69594821258392cd19f4dc75c996c366dda1c5a8bc883a2e0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2c16ae91-bd75-4f5c-95c4-e41637e0dba4", "node_type": "1", "metadata": {"page_label": "28", "file_name": "A cybersecurity AI agent selection and decision support framework.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\A cybersecurity AI agent selection and decision support framework.pdf", "file_type": "application/pdf", "file_size": 1115417, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "3c4d1016e004d48d40fdf2964d99f9958e50c0ce2e5b1596b0c854c906466f38", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "https://doi.org/10.1515/zwf-2021-0104 \n81. Jain H, Padmanabhan B, Pavlou PA, Raghu TS (2021) Editorial for the special section on humans, algorithms, \nand augmented intelligence: The future of work, organizations, and society. Information Systems Research \n32(3): 675\u201387. https://doi.org/10.1287/isre.2021.1046 \n82. Karunamurthy A,  Kiruthivasan R, Gauthamkrishna S (2023) Human-in-the-Loop intelligence: Advancing AI-\ncentric cybersecurity for the future. Quing: International Journal of Multidisciplinary Scientific Research and \nDevelopment 2(3): 20\u201343. https://doi.org/10.54368/qijmsrd.2.3.0011 \n83. Ahdadou M, Aajly A, Tahrouch M (2024) Unlocking the potential of augmented intelligence: A discussion on \nits role in boardroom decision- making. International Journal of Disclosure and Governance 21(3): 433\u2013 46. \nhttps://doi.org/10.1057/s41310-023-00207-2 \n84. Gore S, Hamsa S, Roychowdhury S, Patil G, Gore S, Karmode S (2023) Augmented intelligence in machine \nlearning for cybersecurity: Enhancing threat detection and human -machine collaboration. In: 2023 Second \nInternational Conference on Augmented Intelligence and Sustainable Systems (ICAISS), pp. 638\u2013 44. \nhttps://doi.org/10.1109/ICAISS58487.2023.10250514 \n85. Caballero-Martin D, Lopez -Guede JM, Estevez J, Gra\u00f1a M (2024) Artificial intelligence applied to drone \ncontrol: a state of the art. Drones 8(7): 296. https://doi.org/10.3390/drones8070296  \n86. Chen L, Zhang W, Song Y, Chen J (2024) Machine learning for human\u2013 machine systems with advanced \npersistent threats. IEEE Transactions on Human- Machine Systems 54(6): 753\u2013 61. \nhttps://doi.org/10.1109/THMS.2024.3439625 \n87. Salesforce (2025) The agentic maturity model: A 4- step roadmap for CIOs to succeed in the agentic era. \nhttps://www.salesforce.com/news/stories/agentic-maturity-model/. Accessed 15 April 2025 \n88. Paul S, Choudhury NR, Pandit B, Dawn A (2025) Integration of AI and quantum computing in cybersecurity: \nA comprehensive review. In: Integration of AI, Quantum Computing, and Semiconductor Technology. IGI \nGlobal Scientific Publishing, pp. 287\u2013308. https://doi.org/10.4018/979-8-3693-7076-6.ch014 \n89. Kuru K, Kuru K (2025) UMetaBE -DPPML: Urban metaverse & blockchain -enabled decentralised privacy -\npreserving machine learning verification and authentication with metaverse immersive devices. Internet of \nThings and Cyber-Physical Systems. https://doi.org/10.1016/j.iotcps.2025.02.001 \n90. Qi P, Chiaro D, Guzzo A, Ianni M, Fortino G, Piccialli F (2024) Model aggregation techniques in federated \nlearning: A comprehensive survey. Future Generation Computer Systems 150:  272\u201393. \nhttps://doi.org/10.1016/j.future.2023.09.008 \n91. Yu E, Yue W, Jianzheng S, Xun W (2024) Blockchain- based AI agent and autonomous world infrastructure. \nIn: 2024 IEEE Conference on Artificial Intelligence (CAI), pp. 278 \u201383. \nhttps://doi.org/10.1109/CAI59869.2024.00061", "mimetype": "text/plain", "start_char_idx": 2357, "end_char_idx": 5260, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8857792d-dd67-4049-8a81-4868c4eec805": {"__data__": {"id_": "8857792d-dd67-4049-8a81-4868c4eec805", "embedding": null, "metadata": {"page_label": "29", "file_name": "A cybersecurity AI agent selection and decision support framework.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\A cybersecurity AI agent selection and decision support framework.pdf", "file_type": "application/pdf", "file_size": 1115417, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1a304a07-df7f-4b7f-8c85-ed37974052af", "node_type": "4", "metadata": {"page_label": "29", "file_name": "A cybersecurity AI agent selection and decision support framework.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\A cybersecurity AI agent selection and decision support framework.pdf", "file_type": "application/pdf", "file_size": 1115417, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "83e35233197ea1182512605c58a47977e8b711ecf705d5a57562f8066e64e374", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f63d9ba7-9141-4a28-9f2b-d5298112a4f8", "node_type": "1", "metadata": {}, "hash": "036d4fb8daf89d6de078422bb5770c25eecbf987f88ad58665936d17275f2288", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "28 \n \n92. Han X, Wang N, Che S, Yang H, Zhang K, Xu SX (2024) Enhancing investment analysis: Optimizing AI -\nagent collaboration in financial research. In: Proceedings of the 5th ACM International Conference on AI in \nFinance. Association for Computing Machinery, New Y ork, NY, USA, pp. 538 \u201346. \nhttps://doi.org/10.1145/3677052.3698645 \n93. Chen Z, Sun Q, Li N, Li X, Wang Y, \u2160 C-L (2024) Enabling mobile AI agent in 6G era: Architecture and key \ntechnologies. IEEE Network 38(5): 66\u201375. https://doi.org/10.1109/MNET.2024.3422309 \n94. Bovo R, Abreu S, Ahuja K, Gonzalez EJ, Cheng L -T, Gonzalez- Franco M (2024) EmBARDiment: an \nembodied AI agent for productivity in XR. arXiv. https://doi.org/10.48550/arXiv.2408.08158  \n95. Baabdullah AM (2024) Generative conversational AI agent for managerial practices: the role of qi dimensions, \nnovelty seeking and ethical concerns. Technological Forecasting and Social Change 198:  122951. \nhttps://doi.org/10.1016/j.techfore.2023.122951 \n96. Huang Q, Wake N, Sarkar B, Durante Z, Gong R, Taori R, Noda Y,  Terzopoulos D, Kuno N, Famoti A, \nLlorens A, Langford J, Vo H, Fei-Fei L, Ikeuchi K, Gao J (2024) Position paper: Agent AI towards a holistic \nintelligence. arXiv. https://doi.org/10.48550/arXiv.2403.00833 \n97. Agashe S, Han J, Gan S, Yang J, Li A, Wang XE (2024) Agent S: An open agentic framework that uses \ncomputers like a human. arXiv.Org. https://arxiv.org/abs/2410.08164v1 \n98. Kim M, Saad W (2024) Analysis of the memorization and generalization capabilities of AI agents: Are \ncontinual learners robust? In: 2024 IEEE International Conference on Acoustics, Speech and Signal Processing \n(ICASSP), 6840\u201344. https://doi.org/10.1109/ICASSP48485.2024.10447575 \n99. Pleshakova E, Osipov A, Gataullin S, Gataullin T, Vasilakos A (2024) Next gen cybersecurity paradigm \ntowards artificial general intelligence: Russian market challenges and future global technological trends. \nJournal of Computer Virology and Hacking Techniques 20(3): 429 \u201340. https://doi.org/10.1007/s11416-024-\n00529-x \n100. Hauptman AI, Schelble BG, McNeese NJ, Madathil KC (2023) Adapt and overcome: perceptions of adaptive \nautonomous agents for human- AI teaming. Computers in Human Behavior 138:  107451. \nhttps://doi.org/10.1016/j.chb.2022.107451 \n101. Kaur R, Gabrijel\u010di\u010d D, Klobu\u010dar T (2023) Artificial intelligence for cybersecurity: Literature review and future \nresearch directions. Information Fusion 97: 101804. https://doi.org/10.1016/j.inffus.2023.101804 \n102. Ca\u00f1as JJ (2022) AI and ethics when human beings collaborate with AI agents. Frontiers in Psychology 13. \nhttps://doi.org/10.3389/fpsyg.2022.836650 \n103. Roy SD, Debbarma S, Guerrero JM (2022) Machine learning based multi -agent system for detecting and \nneutralizing unseen cyber-attacks in AGC and HVDC systems. IEEE Journal on Emerging and Selected Topics \nin Circuits and Systems 12(1): 182\u201393. https://doi.org/10.1109/JETCAS.2022.3142055 \n104.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2947, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f63d9ba7-9141-4a28-9f2b-d5298112a4f8": {"__data__": {"id_": "f63d9ba7-9141-4a28-9f2b-d5298112a4f8", "embedding": null, "metadata": {"page_label": "29", "file_name": "A cybersecurity AI agent selection and decision support framework.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\A cybersecurity AI agent selection and decision support framework.pdf", "file_type": "application/pdf", "file_size": 1115417, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1a304a07-df7f-4b7f-8c85-ed37974052af", "node_type": "4", "metadata": {"page_label": "29", "file_name": "A cybersecurity AI agent selection and decision support framework.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\A cybersecurity AI agent selection and decision support framework.pdf", "file_type": "application/pdf", "file_size": 1115417, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "83e35233197ea1182512605c58a47977e8b711ecf705d5a57562f8066e64e374", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8857792d-dd67-4049-8a81-4868c4eec805", "node_type": "1", "metadata": {"page_label": "29", "file_name": "A cybersecurity AI agent selection and decision support framework.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\A cybersecurity AI agent selection and decision support framework.pdf", "file_type": "application/pdf", "file_size": 1115417, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "29c391fbb604dd5782c42e29fa7a240ac34349f9b086a17204dea92424483e8c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Information Fusion 97: 101804. https://doi.org/10.1016/j.inffus.2023.101804 \n102. Ca\u00f1as JJ (2022) AI and ethics when human beings collaborate with AI agents. Frontiers in Psychology 13. \nhttps://doi.org/10.3389/fpsyg.2022.836650 \n103. Roy SD, Debbarma S, Guerrero JM (2022) Machine learning based multi -agent system for detecting and \nneutralizing unseen cyber-attacks in AGC and HVDC systems. IEEE Journal on Emerging and Selected Topics \nin Circuits and Systems 12(1): 182\u201393. https://doi.org/10.1109/JETCAS.2022.3142055 \n104. Li L, El Rami J -PS, Taylor A, Rao JH, Kunz T (2022) Enabling a network AI gym for autonomous cyber \nagents. In: 2022 International Conference on Computational Science and Computational Intelligence (CSCI), \npp. 172\u201377. https://doi.org/10.1109/CSCI58124.2022.00034 \n105. Ashktorab Z, Dugan C, Johnson J, Pan Q, Zhang W, Kumaravel S, Campbell M (2021) Effects of \ncommunication directionality and ai agent differences in human- ai interaction. In: Proceedings of the 2021 \nCHI Conference on Human Factors in Computing Systems. Association for Computing Machinery, New York, \npp. 1\u201315. https://doi.org/10.1145/3411764.3445256 \n106. Zolotukhin M, Kumar S, H\u00e4m\u00e4l\u00e4inen T (2020) Reinforcement learning for attack mitigation in SDN -enabled \nnetworks. In: 2020 6 th IEEE Conference on Network Softwarization (NetSoft), pp. 282 \u201386. \nhttps://doi.org/10.1109/NetSoft48620.2020.9165383 \n107. Cao Y, Wang R, Chen M, Barnawi A (2020) AI agent in software -defined network: agent -based network \nservice prediction and wireless resource scheduling optimization. IEEE Internet of Things Journal 7(7): 5816\u2013\n26. https://doi.org/10.1109/JIOT.2019.2950730 \n108. Franco MF, Rodrigues B, Scheid EJ, Jacobs A, Killer C, Granville LZ, Stiller B (2020) SecBot: A business -\ndriven conversational agent for cybersecurity planning and management. In: 2020 16th International \nConference on Network and Service Management (CNSM),  pp. 1\u2013 7. \nhttps://doi.org/10.23919/CNSM50824.2020.9269037 \n109. Kott A, Th\u00e9ron P (2020) Doers, not watchers: Intelligent autonomous agents are a path to cyber resilience. \nIEEE Security & Privacy 18(3): 62\u201366. https://doi.org/10.1109/MSEC.2020.2983714 \n110. Th\u00e9ron P, Kott A, Dra\u0161ar M, Rzadca K, LeBlanc B, Pihelgas M, Mancini L, Panico A (2018) Towards an \nactive, autonomous and intelligent cyber defense of military systems: The NATO AICA reference architecture. \nIn: 2018 International Conference on Military Communications and Information Systems (ICMCIS), pp. 1\u20139. \nhttps://doi.org/10.1109/ICMCIS.2018.8398730 \n111. Grzonka D, Jak\u00f3bik A, Ko\u0142odziej J, Pllana S (2018) Using a multi-agent system and artificial intelligence for \nmonitoring and improving the cloud performance and security. Future Generation Computer Systems 86:  \n1106\u201317. https://doi.org/10.1016/j.future.2017.05.046.", "mimetype": "text/plain", "start_char_idx": 2418, "end_char_idx": 5244, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d730d98c-69ee-40a3-8d27-3d6ae993ac53": {"__data__": {"id_": "d730d98c-69ee-40a3-8d27-3d6ae993ac53", "embedding": null, "metadata": {"page_label": "30", "file_name": "A cybersecurity AI agent selection and decision support framework.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\A cybersecurity AI agent selection and decision support framework.pdf", "file_type": "application/pdf", "file_size": 1115417, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "285e0372-1d38-4d5c-a3a0-5bc305c26aa1", "node_type": "4", "metadata": {"page_label": "30", "file_name": "A cybersecurity AI agent selection and decision support framework.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\A cybersecurity AI agent selection and decision support framework.pdf", "file_type": "application/pdf", "file_size": 1115417, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "fc559e373b81480d5b1407bc6db2b70175912c85639986266e802791090fa33c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "57ac611c-2374-43f1-b2ec-1d68d7f69ecf", "node_type": "1", "metadata": {}, "hash": "9592592a3da6a7a20b2b51d0d134cddac3302942593a8161268f5d387a120dfb", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "29 \n \n112. Yampolskiy RV (2018) Predicting future AI failures from histric examples. Foresight 21(1): 138\u2013 52. \nhttps://doi.org/10.1108/FS-04-2018-0034 \n113. Petrovi\u0107 VM (2018) Artificial intelligence and virtual worlds \u2013 toward human-level AI agents. IEEE Access \n6: 39976\u201388. https://doi.org/10.1109/ACCESS.2018.2855970 \n114. Huang X, Lian J, Lei Y, Yao J, Lian D, Xie X (2024) Recommender AI agent: Integrating large language \nmodels for interactive recommendations. arXiv. https://doi.org/10.48550/arXiv.2308.16505  \n115. Hochmair HH, Juh\u00e1sz L, Kemp T (2024) Correctness comparison of ChatGPT -4, Gemini, Claude -3, and \nCopilot for spatial tasks. Transactions in GIS 28(7): 2219\u201331. https://doi.org/10.1111/tgis.13233 \n116. Mao Y, Ge Y, Fan Y, Xu W,  Mi Y, Hu Z, Gao Y (2025) A survey on LoRA of large language models. \nFrontiers of Computer Science 19 (7): 197605. https://doi.org/10.1007/s11704 -024-40663-9 \n117. Ding N, Qin Y, Yang G, Wei F, Yang Z, Su Y, Hu S, Chen Y, Chan C -M, Chen W, Yi J, Zhao W, Wang X, \nLiu Z, Zheng H -T, Chen J, Liu Y, Tang J, Li J, Sun M (2023) Parameter -efficient fine-tuning of large-scale \npre-trained language models. Nature Machine Intelligence 5(3): 220\u201335. https://doi.org/10.1038/s42256-023-\n00626-4 \n118. Trad F, Chehab A (2024) Prompt engineering or fine -tuning? a case study on phishing detection with large \nlanguage models. Machine Learning and Knowledge Extraction 6(1): 367\u2013 84. \nhttps://doi.org/10.3390/make6010018 \n119. Anisuzzaman DM, Malins JG, Friedman PA, Attia ZI (2025) Fine -tuning large language models for \nspecialized use cases. Mayo Clinic Proceedings: Digital Health 3(1): 100184. \nhttps://doi.org/10.1016/j.mcpdig.2024.11.005 \n120. Anthropic (2024) Introducing the model context protocol. https://www.anthropic.com/news/model-context-\nprotocol. Accessed 07 April 2025. \n121. Hou X, Zhao Y, Wang S, Wang H (2025) Model context protocol (MCP): Landscape, security threats, and \nfuture research directions. arXiv. https://doi.org/10.48550/arXiv.2503.23278 \n122. Surapaneni R, Jha M, Vakoc M, Segal T (2025) Announcing the Agent2Agent protocol (A2A). Google \ndevelopers blog. https://developers.googleblog.com/en/a2a-a-new-era-of-agent-interoperability/. Accessed 11 \nApril 2025. \n123. Deng Z, Guo Y, Han C, Ma W, Xiong J, Wen S, Xiang Y (2025) AI agents under threat: A survey of key \nsecurity challenges and future pathways. ACM Comput. Surv. 57(7): 182:1- 182:36. \nhttps://doi.org/10.1145/3716628 \n124. Alrfai MM, Alqudah H, Lutfi A, Al- Kofahi M, Alrawad M, Almaiah MA (2023) The influence of artificial \nintelligence on the aiss efficiency: moderating effect of the cyber security. Cogent Social Sciences 9(2): \n2243719. https://doi.org/10.1080/23311886.2023.2243719 \n125. Phillips-Wren G (2008) Intelligent decision support to assist real -time collaboration.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2830, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "57ac611c-2374-43f1-b2ec-1d68d7f69ecf": {"__data__": {"id_": "57ac611c-2374-43f1-b2ec-1d68d7f69ecf", "embedding": null, "metadata": {"page_label": "30", "file_name": "A cybersecurity AI agent selection and decision support framework.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\A cybersecurity AI agent selection and decision support framework.pdf", "file_type": "application/pdf", "file_size": 1115417, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "285e0372-1d38-4d5c-a3a0-5bc305c26aa1", "node_type": "4", "metadata": {"page_label": "30", "file_name": "A cybersecurity AI agent selection and decision support framework.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\A cybersecurity AI agent selection and decision support framework.pdf", "file_type": "application/pdf", "file_size": 1115417, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "fc559e373b81480d5b1407bc6db2b70175912c85639986266e802791090fa33c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d730d98c-69ee-40a3-8d27-3d6ae993ac53", "node_type": "1", "metadata": {"page_label": "30", "file_name": "A cybersecurity AI agent selection and decision support framework.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\A cybersecurity AI agent selection and decision support framework.pdf", "file_type": "application/pdf", "file_size": 1115417, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "b94bc95166447d5f1479089a5ca6f56ffbc6a01c725dc7433198c1e8857eeb93", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Deng Z, Guo Y, Han C, Ma W, Xiong J, Wen S, Xiang Y (2025) AI agents under threat: A survey of key \nsecurity challenges and future pathways. ACM Comput. Surv. 57(7): 182:1- 182:36. \nhttps://doi.org/10.1145/3716628 \n124. Alrfai MM, Alqudah H, Lutfi A, Al- Kofahi M, Alrawad M, Almaiah MA (2023) The influence of artificial \nintelligence on the aiss efficiency: moderating effect of the cyber security. Cogent Social Sciences 9(2): \n2243719. https://doi.org/10.1080/23311886.2023.2243719 \n125. Phillips-Wren G (2008) Intelligent decision support to assist real -time collaboration. In: 2008 International \nSymposium on Collaborative Technologies and Systems, pp. 375 \u2013375. \nhttps://doi.org/10.1109/CTS.2008.4543953 \n126. Gu TL, Li L (2021) Artificial moral agents and their design methodology: retrospect and prospect. Chinese \nJournal of Computers 44(3): 632\u201351. https://www.doi.org/http://dx.doi.org/10.11897/SP.J.1016.2021.00632. \n127. Abiodun IA, Khuen CW (2014) A multi agent framework (MAFSNUD) for Belief -Desire-Intention (BDI) \nmodel\u2019s decision-making problem in dynamic situations: An overview. Advanced Science Letters 20(1): 91\u2013\n96. https://doi.org/10.1166/asl.2014.5305 \n128. Sethy A, Shaik N, Yadavalli PK, Anandaraj SP (2023) 9 AI: Issues, concerns, and ethical considerations. In: \nde Albuquerque VH, Raj P, Yadav SP (eds) Toward Artificial General Intelligence: Deep Learning, Neural \nNetworks, Generative AI. De Gruyter Brill, Be rlin, Germany  , pp. 189\u2013 212. \nhttps://doi.org/10.1515/9783111323749-009 \n129. Jameel T, Ali R, Toheed I (2020) Ethics of artificial intelligence: Research challenges and potential solutions. \nIn: 2020 3\nrd International Conference on Computing, Mathematics and Engineering Technologies (iCoMET), \npp. 1\u20136. https://doi.org/10.1109/iCoMET48670.2020.9073911 \n130. Pantoja CE, de Jesus VS, Lazarin NM, Viterbo J (2023) A spin- off version of  ason for IoT and embedded \nmulti-agent systems. In: Naldi MC, Bianchi RAC (eds) Intelligent Systems. Springer Nature, Cham, pp. 382\u2013\n96. https://doi.org/10.1007/978-3-031-45368-7_25 \n131. Leng J, Fyfe C, Jain L (2008) Simulation and reinforcement learning with soccer agents. Multiagent and Grid \nSystems 4(4): 415\u201336. https://doi.org/10.3233/MGS-2008-4407 \n132. Sennott SC, Akagi L, Lee M, Rhodes A (2019) AAC and artificial intelligence (AI). Topics in Language \nDisorders 39(4): 389. https://doi.org/10.1097/TLD.0000000000000197 \n133. Rossi F, Mattei N (2019) Building ethically bounded AI. In: Proceedings of the AAAI Conference on Artificial \nIntelligence 33(01): 9785\u201389. https://doi.org/10.1609/aaai.v33i01.33019785 \n134. Nickles M, Rovatsos M, Weiss G (2004) Empirical -rational semantics of agent communication. In: \nProceedings of the Third International Joint Conference on Autonomous Agents and Multiagent Systems,", "mimetype": "text/plain", "start_char_idx": 2251, "end_char_idx": 5064, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "faf46dba-5b94-4561-9b1d-6257c8e2f895": {"__data__": {"id_": "faf46dba-5b94-4561-9b1d-6257c8e2f895", "embedding": null, "metadata": {"page_label": "31", "file_name": "A cybersecurity AI agent selection and decision support framework.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\A cybersecurity AI agent selection and decision support framework.pdf", "file_type": "application/pdf", "file_size": 1115417, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b1c1759b-075a-4b9d-8632-3d6c8b8757e4", "node_type": "4", "metadata": {"page_label": "31", "file_name": "A cybersecurity AI agent selection and decision support framework.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\A cybersecurity AI agent selection and decision support framework.pdf", "file_type": "application/pdf", "file_size": 1115417, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "ff02da890b0b52878346a70e6cd1a55bae952b8f9df05261865073ec4820ba5c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e48d354d-824e-4e24-bdd6-61f14429cfb8", "node_type": "1", "metadata": {}, "hash": "af93cc035474f3a88ca545c68cd725916c5d6983a2c1babee393ae2cae367f5f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "30 \n \n2004. AAMAS 2004. 2: 94\u2013 101. \nhttp://www.doi.org/https://doi.ieeecomputersociety.org/10.1109/AAMAS.2004.10055  \n135. Pitardi V, Marriott HR (2022) Challenging vulnerability perceptions towards voice activated assistants: An \nabstract. In: Allen J, Jochims B, Wu S (eds) Celebrating the past and future of marketing and discovery with \nsocial impact.  Springer International P ublishing, Cham, pp. 255\u2013 56. https://doi.org/10.1007/978-3-030-\n95346-1_88 \n136. Chakraborty S (2023) AI and ethics: navigating the moral landscape. In: Investigating the impact of AI on \nethics and spirituality. IGI Global Scientific Publishing, pp. 25\u201333. https://doi.org/10.4018/978-1-6684-9196-\n6.ch002 \n137. He T, Jazizadeh F (2024) Trust in human-AI interaction: review of empirical research on trust in AI-powered \nsmart home ecosystems. In: Computing in Civil Engineering 2023. ASCE , Reston, Virginia: USA, pp. 530\u2013\n38. https://doi.org/10.1061/9780784485224.064 \n138. Rashid AB, and Kausik AK (2024) AI Revolutionizing industries worldwide: A comprehensive overview of \nits diverse applications. Hybrid Advances 7: 100277. https://doi.org/10.1016/j.hybadv.2024.100277 \n139. Liu Y, Cao X, Chen T, Jiang Y, You J, Wu M, Wang X, Feng M, Jin Y, Chen J (2025) From screens to scenes: \na survey of embodied ai in healthcare. Information Fusion 119: 103033. \nhttps://doi.org/10.1016/j.inffus.2025.103033 \n140. Majumdar S, (2024) Standards for LLM security. In: Kucharavy A, Plancherel O, Mulder V, Mermoud A, \nLenders V (eds) Large language models in cybersecurity: Threats, exposure and mitigation. Springer Nature, \nCham, pp. 225\u201331. https://doi.org/10.1007/978-3-031-54827-7_25 \n141. Simran, Kumar S, Hans A (2024) The AI shield and red AI framework: Machine learning solutions for cyber \nthreat intelligence(CTI). In: 2024 International Conference on Intelligent Systems for Cybersecurity (ISCS), \npp. 1\u20136. https://doi.org/10.1109/ISCS61804.2024.10581195 \n142. Cam H (2020) Cyber resilience using autonomous agents and reinforcement learning. In: Artificial intelligence \nand machine learning for multi -domain operations applications II, 11413:  219\u201334. SPIE. \nhttps://doi.org/10.1117/12.2559319 \n143. Falowo OI,  Botsyoe LE, Koshoedo K, Ozer M (2024) Enhancing cybersecurity with artificial immune systems \nand general intelligence: A new frontier in threat detection and response. IEEE Access 12:  123811\u201322. \nhttps://doi.org/10.1109/ACCESS.2024.3454543 \n144. Rafferty L, Macdermott A (2024) Adaptive defence of the Internet of Things (IoT) using the Belief -Desire-\nIntention (BDI) model for social robots. In: Proceedings of the 57th Hawaii International Conference on System \nSciences, pp. 1722\u201332. Waikiki, Hawaii. https://scholarspace.manoa.hawaii.edu/items/86168302- 9592-45e4-\n9794-2dbbb78e614c \n145. Baird A, Maruping LM (2021) The next generation of research on IS use:  A theoretical framework of \ndelegation to and from agentic is artefacts.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2945, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e48d354d-824e-4e24-bdd6-61f14429cfb8": {"__data__": {"id_": "e48d354d-824e-4e24-bdd6-61f14429cfb8", "embedding": null, "metadata": {"page_label": "31", "file_name": "A cybersecurity AI agent selection and decision support framework.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\A cybersecurity AI agent selection and decision support framework.pdf", "file_type": "application/pdf", "file_size": 1115417, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b1c1759b-075a-4b9d-8632-3d6c8b8757e4", "node_type": "4", "metadata": {"page_label": "31", "file_name": "A cybersecurity AI agent selection and decision support framework.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\A cybersecurity AI agent selection and decision support framework.pdf", "file_type": "application/pdf", "file_size": 1115417, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "ff02da890b0b52878346a70e6cd1a55bae952b8f9df05261865073ec4820ba5c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "faf46dba-5b94-4561-9b1d-6257c8e2f895", "node_type": "1", "metadata": {"page_label": "31", "file_name": "A cybersecurity AI agent selection and decision support framework.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\A cybersecurity AI agent selection and decision support framework.pdf", "file_type": "application/pdf", "file_size": 1115417, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "3c2e268054bb184919d9ffbb4d458744d0f432255b033baf2969113ddbf212b1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "IEEE Access 12:  123811\u201322. \nhttps://doi.org/10.1109/ACCESS.2024.3454543 \n144. Rafferty L, Macdermott A (2024) Adaptive defence of the Internet of Things (IoT) using the Belief -Desire-\nIntention (BDI) model for social robots. In: Proceedings of the 57th Hawaii International Conference on System \nSciences, pp. 1722\u201332. Waikiki, Hawaii. https://scholarspace.manoa.hawaii.edu/items/86168302- 9592-45e4-\n9794-2dbbb78e614c \n145. Baird A, Maruping LM (2021) The next generation of research on IS use:  A theoretical framework of \ndelegation to and from agentic is artefacts. Management Information Systems Quarterly 45(1b): 315\u2013 41. \nhttps://www.doi.org/10.25300/MISQ/2021/15882 \n146. Candrian C, Scherer A (2022) Rise of the machines: delegating decisions to autonomous AI. Computers in \nHuman Behavior 134107308. https://doi.org/10.1016/j.chb.2022.107308 \n147. Dorri A, Kanhere SS, Jurdak R (2018) Multi -agent systems: A survey. IEEE Access 6:  28573\u201393. \nhttps://doi.org/10.1109/ACCESS.2018.2831228 \n148. Radanliev P, Santos O, Brandon-Jones A, Joinson A (2024) Ethics and responsible AI deployment. Frontiers \nin Artificial Intelligence 7: 1\u201317. https://doi.org/10.3389/frai.2024.1377011 \n149. Alhalalmeh A, Al-Tarawneh A (2025) Artificial intelligence and the law: The complexities of technology and \nlegalities. In: Hannoon  A, Mahmood A (eds) Intelligence -driven circular economy: Regeneration towards \nsustainability and social responsibility, volume 2. Springer Nature Switzerland, Cham, pp. 641\u2013 49. \nhttps://doi.org/10.1007/978-3-031-74220-0_50 \n150. Bhateja N, Sethi N, Kumar D (2018) Study of ant colony optimization technique for coalition formation in \nmulti agent systems. In: 2018 International Conference on Circuits and Systems in Digital Enterprise \nTechnology, pp. 1\u20134. https://doi.org/10.1109/ICCSDET.2018.8821175 \n151. Russell SJ, Norvig P (2020) Artificial intelligence: A modern approach. 4 th ed. Pearson, Hoboken, NJ, USA. \n152. Ye P, Wang T, Wang F-Y (2018) A survey of cognitive architectures in the past 20 years. IEEE Transactions \non Cybernetics 48(12): 3280\u201390. https://doi.org/10.1109/TCYB.2018.2857704 \n153. Andreadis G, Klazoglou P, Niotaki K, Bouzakis K-D (2014) Classification and review of multi-agents systems \nin the manufacturing section. Procedia Engineering, 69: 282\u201390. https://doi.org/10.1016/j.proeng.2014.02.233 \n154. Lim M (2024) A typology of validity: content, face, convergent, discriminant, nomological and predictive \nvalidity. Journal of Trade Science 12(3): 155\u201379. https://doi.org/10.1108/JTS-03-2024-0016", "mimetype": "text/plain", "start_char_idx": 2374, "end_char_idx": 4935, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "81aff390-b5e4-4853-b83f-5bda8c5237d1": {"__data__": {"id_": "81aff390-b5e4-4853-b83f-5bda8c5237d1", "embedding": null, "metadata": {"page_label": "1", "file_name": "A dissemination workshop for introducing young Italian students to NLP.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\A dissemination workshop for introducing young Italian students to NLP.pdf", "file_type": "application/pdf", "file_size": 3675277, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "db4b926c-c2fa-48b9-b971-d445cbb2a32d", "node_type": "4", "metadata": {"page_label": "1", "file_name": "A dissemination workshop for introducing young Italian students to NLP.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\A dissemination workshop for introducing young Italian students to NLP.pdf", "file_type": "application/pdf", "file_size": 3675277, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "a7cdee1c928411ab19eff5cb842a86ad8b517179b1d7380511c2e9e151229f90", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "A dissemination workshop for introducing young Italian students to NLP\nLucio Messina\nIndependent Researcher\nlucio.messina@autistici.org\nLucia Busso\nAston University\nl.busso@aston.ac.uk\nClaudia Roberta Combei\nUniversity of Bologna\nclaudiaroberta.combei@unibo.it\nLudovica Pannitto\nUniversity of Trento\nludovica.pannitto@unitn.it\nAlessio Miaschi\nUniversity of Pisa\nalessio.miaschi@phd.unipi.it\nGabriele Sarti\nUniversity of Trieste\ngsarti@sissa.it\nMalvina Nissim\nUniversity of Groningen\nm.nissim@rug.nl\nAbstract\nWe describe and make available the game-\nbased material developed for a laboratory run\nat several Italian science festivals to popularize\nNLP among young students.\n1 Introduction\nThe present paper aims at describing in detail the\nteaching materials developed and used for a series\nof interactive dissemination workshops on NLP\nand computational linguistics1. These workshops\nwere designed and delivered by the authors on be-\nhalf of the Italian Association for Computational\nLinguistics (AILC, www.ai-lc.it), with the\naim of popularizing Natural Language Processing\n(NLP) among young Italian students (13+) and the\ngeneral public. The workshops were run in the\ncontext of nationwide popular science festivals and\nopen-day events both onsite (at BergamoScienza,\nand Scuola Internazionale Superiore di Studi Avan-\nzati [SISSA], Trieste) and online (at Festival della\nScienza di Genova, the BRIGHT European Re-\nsearchers\u2019 Night, high school ITS Tullio Buzzi in\nPrato and the second edition of Science Web Festi-\nval, engaging over 700 participants in Center and\nNorthern Italy from 2019 to 2021.2\nThe core approach of the workshop remained\nthe same throughout all the events. However, the\nmaterials and activities were adapted to a variety\nof different formats and time-slots, ranging from\n30 to 90 minutes. We \ufb01nd that this workshop \u2013\n1In this discussion, and throughout the paper, we con\ufb02ate\nthe terms Natural Language Processing and Computational\nLinguistics and use them interchangeably.\n2Links to events are in the repository\u2019s README \ufb01le.\nthanks to its modular nature \u2013 can \ufb01t different tar-\nget audiences and different time slots, depending\non the level of interactive engagement required\nfrom participants and on the level of granularity\nof the presentation itself. Other than on the level\nof engagement expected of participants, time re-\nquired can also vary depending on the participants\u2019\nbackground and metalinguistic awareness.\nOur interactive workshops took the form of mod-\nular games where participants, guided by trained\ntutors, acted as if they were computers that had to\nrecognize speech and text, as well as to generate\nwritten sentences in a mysterious language they\nknew nothing about.\nThe present contribution only describes the\nteaching materials and provide a general outline\nof the activities composing the workshop. For a\ndetailed discussion and re\ufb02ection on the workshop\ngenesis and goals and on how it was received by\nthe participants see (Pannitto et al., 2021).\nThe teaching support consist in an interactive\npresentation plus hands-on material, either in hard-\ncopy or digital form. We share a sample presen-\ntation3 and an open-access repository4 containing\nboth printable materials to download and scripts to\nreproduce them on different input data.\n2 Workshop and materials\nThe activity contains both more theoretical and\nhands-on parts, which are cast as games.\n3https://docs.google.com/presentation/\nd/1ebES_K8o3I2ND_1iMyBlQmrH733eQ6m_\nK8a0tON8reo/edit?usp=sharing\n4https://bitbucket.org/melfnt/\nmalvisindi\narXiv:2104.12405v2  [cs.CL]  14 May 2021", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3587, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ff7157e1-2771-4895-8631-9df2d75eaa83": {"__data__": {"id_": "ff7157e1-2771-4895-8631-9df2d75eaa83", "embedding": null, "metadata": {"page_label": "2", "file_name": "A dissemination workshop for introducing young Italian students to NLP.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\A dissemination workshop for introducing young Italian students to NLP.pdf", "file_type": "application/pdf", "file_size": 3675277, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "bfd03b52-815b-4501-a310-d4bcefc0cae7", "node_type": "4", "metadata": {"page_label": "2", "file_name": "A dissemination workshop for introducing young Italian students to NLP.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\A dissemination workshop for introducing young Italian students to NLP.pdf", "file_type": "application/pdf", "file_size": 3675277, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "2663eb4044736c92e687e10fa2b6d337b99bb5bec1fdb002ceb7165396aed3cc", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Awareness The \ufb01rst part consists of a brief intro-\nduction to (computational) linguistics, focusing on\nsome common misconceptions (slides 3-5), and on\nexamples of linguistic questions (slides 6-12). Due\nto their increasing popularity, we chose vocal as-\nsistants as practical examples of NLP technologies\naccounting for how humans and machines differ\nin processing speech in particular and language in\ngeneral (slides 20-39).\nGames The core of the activity is inspired by the\nword saladpuzzle (Radev and Pustejovsky, 2013)\nand is organized as a game revolving around a fun-\ndamental problem in NLP: given a set of words,\nparticipants are asked to determine the most likely\nordering for a sentence containing those words.\nThis is a trivial problem when approached in a\nknown language (i.e., consider reordering the to-\nkens garden, my, is, the, in, dog), but an apparently\nimpossible task when semantics is not accessible,\nwhich is the most common situation for simple\nNLP algorithms.\nTo make participants deal with language as a\ncomputer would do, we asked them to compose\nsentences using tokens obtained by transliterating\nand annotating 60 sentences from the well known\nfairy tale \"Snow White\" to a set of symbols. We\nproduced two possible versions of the masked ma-\nterials: either replacing each word with a random\nsequence of DINGs (e.g. ?o\u00a7\u0006 for the word morn-\ning) or replacing them with a corresponding non-\nword (for example croto for the word morning).\nThe grammatical structure of each sentence is repre-\nsented by horizontal lines on top of it representing\nphrases (such as noun or verb phrases), while the\nparts of speech are indicated by numbers from 0 to\n9 placed as superscripts on each word (Figure 1).\nFigure 1: The \ufb01rst sentence of the corpus \"on a\nsnowy day a queen was sewing by her\nwindow\" translated using DINGs (above) and using\nnon-words (below)\nParticipants were divided into two teams, one\nteam would be working on masked Italian and the\nother on masked English. Both teams were given\nthe corpus in A3 format and were told that the texts\nare written in a \ufb01ctional language.\nTwo activities were then run, focusing on two\nFigure 2: Example cards, both showing a word. Left:\na card for the \ufb01rst activity, with a button loop to thread\nit in a sentence. Right: a card for the second activity,\nwith part of speech (number) at the bottom.\nFigure 3: Possible rules extracted from the corpus.\nEach rule is made of felt strips for phrases, cards with\nnumbers for parts of speech, and \u201c=\u201d cards.\ndifferent algorithms for sentence generation. In\nthe \ufb01rst, participants received a deck of cards each\nequipped with a button loop (Figure 2) and show-\ning a token from the corpus. Participants had to\ncreate new valid sentences by rearranging the cards\naccording to the bigrams\u2019 distribution in the cor-\npus. Using the bracelet method(slides 52-61), they\ncould physically thread tokens into sentences.\nIn the second activity (slides 63-92), the partici-\npants extracted grammatical rules from the corpus,\nand used them to generate new sentences. In or-\nder to write the grammar, participants were given\nfelt strips reproducing the colors of the annotation,\na deck of cards with numbers (identifying parts\nof speech) and a deck of \u201c=\u201d symbols (Figure 3).\nWith a new deck of words (Figure 2), not all present\nin the corpus, participants had to generate a sen-\ntence using the previously composed rules.\nRe\ufb02ection and Outlook By superimposing a\nplexiglass frame on the A3 corpus pages (Figure 4),\nthe true nature of the corpora was eventually re-\nvealed. The participants could see the original texts\n(in Italian and English) and translate the sentences\nthey had created previously.\nThe activity ended with a discussion of recent\nNLP technologies and their commercial applica-\ntions (slides 93-96), and of what it takes to become\na computational linguist today (slides 97-99).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3884, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "7e029afa-a2a8-4bf9-810e-a45009ec1bd3": {"__data__": {"id_": "7e029afa-a2a8-4bf9-810e-a45009ec1bd3", "embedding": null, "metadata": {"page_label": "3", "file_name": "A dissemination workshop for introducing young Italian students to NLP.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\A dissemination workshop for introducing young Italian students to NLP.pdf", "file_type": "application/pdf", "file_size": 3675277, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e5d7d61a-3801-4a38-8bc1-dd5064fb1c22", "node_type": "4", "metadata": {"page_label": "3", "file_name": "A dissemination workshop for introducing young Italian students to NLP.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\A dissemination workshop for introducing young Italian students to NLP.pdf", "file_type": "application/pdf", "file_size": 3675277, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "19b09248e881212245acbc6ec7b8542f49ee0037cab09b88309b9b1fe21a8541", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Figure 4: A session of the workshop: the original lan-\nguage of the corpus has just been revealed by superim-\nposing plexiglass supports on the corpus tables.\n3 Activity Preparation\nThe preparation of the activity consists of several\nsteps: (1) creating and tagging the corpora with\nmorpho-syntactic and syntactic categories as de-\nscribed in the repository; (2) choosing the words to\ninclude in the card decks: these must be manually\nselected but scripts are provided to generate possi-\nble sentences based on bigram co-occurrences, and\nto extract all the possible grammar rules present in\nthe annotation; (3) when the produced sentences\nand grammar are satisfactory, scripts are provided\nto generate (i) the printable formats of corpora and\ndecks of cards, (ii) a dictionary to support the trans-\nlation of sentences in the last part of the work-\nshop, and (iii) clear-text corpora; (4) sentences\nfrom the clear-text corpora have to be manually\ncut and glued on a transparent support that can be\nsuperimposed on the printed corpora to reveal the\nsentences; (5) \ufb01nally, some manual work is neces-\nsary: producing strips of felt or any material with\nthe same colors used in the corpus; cutting threads;\nattaching a button loop to the relevant cards, etc.\n4 Reusability\nIn the spirit of open science and to encourage the\npopularization of NLP, the teaching materials and\nsource code are freely available in our repository\n(see footnotes 2-3 for the links). The print-ready\nmaterial is released under CC BY-NC; the source\ncode is distributed under the GNU gpl license ver-\nsion 3. All scripts work for Python versions 3.6 or\nabove and the overall process requires python3,\nlualatex, pdftk and pdfnup as detailed in\nthe README.md \ufb01le in the repository, which con-\ntains all necessary instructions.\nAcknowledgements\nWe would like to thank the board of the Italian As-\nsociation for Computational Linguistics (AILC) for\nthe support given to the workshop development and\ndelivery. We are also grateful to BergamoScienza,\nFestival della Scienza di Genova, Science Web Fes-\ntival for hosting the activity during the festivals; to\nILC-CNR \u201cAntonio Zampolli\" and ColingLab (Uni-\nversity of Pisa) for hosting us during the European\nNight of Research; and to the Scuola Internazionale\nSuperiore di Studi Avanzati (SISSA) and ITI Tullio\nBuzzi for hosting our activities with their students.\nWe also thank Dr. Mirko Lai, who has collaborated\non the development of the web interface for the\nonline versions of our activity.\nReferences\nLudovica Pannitto, Lucia Busso, Claudia Roberta\nCombei, Lucio Messina, Alessio Miaschi, Gabriele\nSarti, and Malvina Nissim. 2021. Teaching NLP\nwith bracelets and restaurant menus: An interactive\nworkshop for italian students. In Proceedings of the\nFifth Workshop on Teaching NLP and CL, Online\nevent. Association for Computational Linguistics.\nDragomir Radev and James Pustejovsky. 2013.Puzzles\nin logic, languages and computation: the red book.\nSpringer.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2975, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d22e6934-8eda-46b6-8517-caf4972fee3c": {"__data__": {"id_": "d22e6934-8eda-46b6-8517-caf4972fee3c", "embedding": null, "metadata": {"page_label": "1", "file_name": "ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 18789005, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "54c0018c-5d06-4f2d-b667-40a030539f52", "node_type": "4", "metadata": {"page_label": "1", "file_name": "ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 18789005, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "fcbaa3c9a6b110b571da04f14547c498f901183b81db69968d41796ead5b5793", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "ARLBench: Flexible and Efficient Benchmarking for\nHyperparameter Optimization in Reinforcement\nLearning\nJannis Becktepe*1\nbecktepe@stud.uni-hannover.de\nJulian Dierkes*2\ndierkes@aim.rwth-aachen.de\nCarolin Benjamins1 Aditya Mohan1 David Salinas3 Raghu Rajan3\nFrank Hutter4,3 Holger H. Hoos2 Marius Lindauer1,5 Theresa Eimer1\n1 Leibniz University Hannover, 2 RWTH Aachen University,3 University of Freiburg,\n4 ELLIS Institute T\u00fcbingen, 5 L3S Research Center\n*Both authors contributed equally to this work\nAbstract\nHyperparameters are a critical factor in reliably training well-performing reinforce-\nment learning (RL) agents. Unfortunately, developing and evaluating automated\napproaches for tuning such hyperparameters is both costly and time-consuming. As\na result, such approaches are often only evaluated on a single domain or algorithm,\nmaking comparisons difficult and limiting insights into their generalizability. We\npropose ARLBench, a benchmark for hyperparameter optimization (HPO) in RL\nthat allows comparisons of diverse HPO approaches while being highly efficient in\nevaluation. To enable research into HPO in RL, even in settings with low compute\nresources, we select a representative subset of HPO tasks spanning a variety of\nalgorithm and environment combinations. This selection allows for generating a\nperformance profile of an automated RL (AutoRL) method using only a fraction\nof the compute previously necessary, enabling a broader range of researchers to\nwork on HPO in RL. With the extensive and large-scale dataset on hyperparameter\nlandscapes that our selection is based on, ARLBench is an efficient, flexible, and\nfuture-oriented foundation for research on AutoRL. Both the benchmark and the\ndataset are available at https://github.com/automl/arlbench.\n1 Introduction\nDeep Reinforcement Learning (RL) algorithms require careful configuration of many different design\ndecisions and hyperparameters to reliably work in practice [Farsang and Szegletes, 2021, Pislar\net al., 2022], such as learning rates [Gulde et al., 2020] or batch sizes [Obando-Ceron et al., 2023].\nAutomated reinforcement learning (AutoRL) [Parker-Holder et al., 2022], a sub-field of automated\nmachine learning (AutoML), makes these design decisions in a data-driven manner. In fact, recent\nwork has shown that such a data-driven approach offers the best way of navigating hyperparameters\nin RL [Zhang et al., 2021, Eimer et al., 2023], due to the complex and changing hyperparameter\noptimization landscapes encountered [Mohan et al., 2023].\nResearch on hyperparameter optimization (HPO) for RL has been gaining traction in recent\nyears [Jaderberg et al., 2017, Parker-Holder et al., 2020, Franke et al., 2021, Wan et al., 2022].\n17th European Workshop on Reinforcement Learning (EWRL 2024).\narXiv:2409.18827v1  [cs.LG]  27 Sep 2024", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2826, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "6b2df7ad-c417-479b-b13a-a3574c0e5432": {"__data__": {"id_": "6b2df7ad-c417-479b-b13a-a3574c0e5432", "embedding": null, "metadata": {"page_label": "2", "file_name": "ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 18789005, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9b912e35-b125-42e8-be12-70fdb75056ad", "node_type": "4", "metadata": {"page_label": "2", "file_name": "ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 18789005, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "c5b418cf98be147b154006bd4f5f655bd755794dbbd6a8c65a321b1ab579d765", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "aecc4bee-8cc7-4d4d-8567-5dadbd103570", "node_type": "1", "metadata": {}, "hash": "80a681a058384c4e263a06cef39ff9a50c9e534378ae7d4d9666c9825ae674d1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Figure 1: Running time comparison for an HPO method of 32 RL runs using 10 seeds each on the\nfull environment set and our subsets between ARLBench and StableBaselines3 (SB3) [Raffin et al.,\n2021]. This results in speedup factors due to JAX of 3.59 for PPO, 2.87 for DQN, and 5.78 for SAC\nof ARLBench, compared to SB3 on the full set. The subset selection further decreases the running\ntime by a factor of 2.67 for PPO, 2.49 for DQN, and 2.0 for SAC. Comparing ARLBench on the\nsubset to SB3 on the full set, the total speedups are 9.6 for PPO, 7.14 for DQN, and 11.61 for SAC.\nRunning time comparisons for each environment category can be found in Appendix E. Note the bars\nfor some domains, especially on ARLBench, may be very small due to low running time.\nWhile such approaches promise to streamline the application of RL by providing users with well-\nperforming hyperparameter configurations for their RL tasks, it is hard to discern their actual quality;\neach HPO method is usually evaluated on a limited number of environments, combined with a\ndifferent HPO configuration space (see, e.g., the differences between Parker-Holder et al. [2020]\nand Shala et al. [2024]). This inability to compare HPO approaches and AutoRL approaches more\nbroadly leads to a lack of clarity and, ultimately, a lack of adoption of an approach that shows great\npromise in making RL overall more efficient and easier to apply.\nOne reason for the inconsistent evaluations in the current HPO literature is the wealth of RL algorithms\nand environments, each with its own challenges. While some environments require the processing of\nimage observations [Bellemare et al., 2013, Cobbe et al., 2020], others focus more on finding the\noptimal solutions in settings with sparse reward signals [Nikulin et al., 2023]. It is fundamentally\nunclear which environment and algorithm combinations should be considered representative tasks for\nthe current scope of RL research and thus useful as evaluation settings for AutoRL approaches.\nWe focus on the followingquestion: Which environments should we evaluate a given RL algorithm on\nto obtain a reliable performance estimate of an AutoRL method? To answer it, we first implement\nhighly efficient and configurable versions of three popular RL algorithms: DQN [Mnih et al., 2015],\nPPO [Schulman et al., 2017], and SAC [Haarnoja et al., 2018]. We subsequently conduct a large-scale\nstudy across different environment domains (ALE games [Bellemare et al., 2017], Classic Control and\nBox2D simulations [Brockman et al., 2016, Towers et al., 2023], Brax robot walkers [Freeman et al.,\n2021], and grid-based exploration [Nikulin et al., 2023]) to generate hyperparameter landscapes for\nthese algorithms. This study, which we publish as a meta-dataset, allows us to assess the performance\nof given hyperparameter configurations for each algorithm and environment.\nBased on the scores, we follow the method proposed by Aitchison et al. [2023] to find the subset of en-\nvironments with the highest capability for predicting the average performance across all environments\nin order to model the RL task space. This subset thus matches the tasks the RL community cares about\nbetter than previous work on HPO for RL, while reducing computational demands for evaluation.\nThis provides the research community with an empirically sound benchmark for HPO, which we dub\nARLBench. It is highly efficient, taking only 937 GPU hours to evaluate an HPO budget of 32 full\nRL trainings for 10 runs each on all three algorithm subsets. StableBaselines3 [Raffin et al., 2021]\n(SB3) on the full set of environments would take 8 163 GPU hours, resulting in average speedup\nfactors of 9.6 for PPO, 7.14 for DQN, and 11.61 for SAC as shown in Figure 1.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3741, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "aecc4bee-8cc7-4d4d-8567-5dadbd103570": {"__data__": {"id_": "aecc4bee-8cc7-4d4d-8567-5dadbd103570", "embedding": null, "metadata": {"page_label": "2", "file_name": "ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 18789005, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9b912e35-b125-42e8-be12-70fdb75056ad", "node_type": "4", "metadata": {"page_label": "2", "file_name": "ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 18789005, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "c5b418cf98be147b154006bd4f5f655bd755794dbbd6a8c65a321b1ab579d765", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6b2df7ad-c417-479b-b13a-a3574c0e5432", "node_type": "1", "metadata": {"page_label": "2", "file_name": "ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 18789005, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "a41a6a8d33115e037de35e8455fdd2c8cc81f90584a4a65999c210a1eb2cb4f8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[2023] to find the subset of en-\nvironments with the highest capability for predicting the average performance across all environments\nin order to model the RL task space. This subset thus matches the tasks the RL community cares about\nbetter than previous work on HPO for RL, while reducing computational demands for evaluation.\nThis provides the research community with an empirically sound benchmark for HPO, which we dub\nARLBench. It is highly efficient, taking only 937 GPU hours to evaluate an HPO budget of 32 full\nRL trainings for 10 runs each on all three algorithm subsets. StableBaselines3 [Raffin et al., 2021]\n(SB3) on the full set of environments would take 8 163 GPU hours, resulting in average speedup\nfactors of 9.6 for PPO, 7.14 for DQN, and 11.61 for SAC as shown in Figure 1.\nARLBench is designed with current AutoRL and AutoML methods in mind; partial execution as used\nin many contemporary HPO methods [Li et al., 2017, Awad et al., 2021, Lindauer et al., 2022] is\nbuilt into the benchmark structure just like dynamic optimization in arbitrary intervals, as, e.g., in\npopulation-based training [Jaderberg et al., 2017] variations. Moreover, various training data from\nARLBench, including performance measures such as evaluation rewards and gradient history, can be\n2", "mimetype": "text/plain", "start_char_idx": 2946, "end_char_idx": 4234, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "636a652b-b1c7-45a3-aa59-9fc0b81a0806": {"__data__": {"id_": "636a652b-b1c7-45a3-aa59-9fc0b81a0806", "embedding": null, "metadata": {"page_label": "3", "file_name": "ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 18789005, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "93fad366-2ad9-4df2-8f76-fe61187b2bce", "node_type": "4", "metadata": {"page_label": "3", "file_name": "ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 18789005, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "6e198e5b85c6b0d30d723eae98f1e9ca738a5709bf5f367a746056fa7a0f8b30", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "19ff54e0-8e86-4fb5-8024-95be13cb6234", "node_type": "1", "metadata": {}, "hash": "e92fd272a881bbf281c4d1188672b551148359d1837ba97f3eadc205d3b09c98", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "used in adaptive HPO methods. ARLBench additionally supports large configuration spaces, making\nmost low-level design decisions and architectures configurable for each algorithm. This flexibility\nand running time efficiency spawns a range of new insights into approaches to AutoRL.\nIn short, our key contributions are: (i) A highly efficient benchmark for HPO in RL, which natively\nsupports diverse categories of HPO approaches; (ii) an environment subset selection for standardized\ncomparisons that covers the RL task space, both (i) and (ii) together improving computational feasi-\nbility by an order of magnitude; (iii) a set of performance data on our benchmark with over 100 000\ntotal runs spanning various RL algorithms, environments, seeds, and configurations (equivalent to\n32 588GPU hours).\n2 Related Work: Benchmarking HPO for RL\nSeveral works study the impact that such hyperparameter settings have on RL algorithms [Henderson\net al., 2018, Andrychowicz et al., 2021, Obando-Ceron and Castro, 2021, Obando-Ceron et al., 2023]\nand show that they mostly do not transfer across environments [Ceron et al., 2024]. Automated\nconfiguration of these algorithms, on the other hand, is not as common, especially compared to\nthe body of work in AutoML [Hutter et al., 2019]. Previous work has shown, however, that HPO\napproaches can find high-performing hyperparameter configurations quite efficiently [Xu et al., 2018,\nParker-Holder et al., 2020, Zhang et al., 2021, Franke et al., 2021, Flennerhag et al., 2022, Wan et al.,\n2022]. These approaches range from standard HPO, including multi-fidelity optimization [Falkner\net al., 2018, Awad et al., 2021], and algorithm configuration tools from AutoML [Schede et al., 2022,\nDierkes et al., 2024] to novel strategies aiming to adapt to the dynamic nature of RL algorithms.\nMost popular is the population-based training (PBT) line of work [Jaderberg et al., 2017, Wan et al.,\n2022], which evolves hyperparameter schedules via a population of agents, resulting in a dynamic\nconfiguration strategy. For adaptive dynamic HPO, second-order optimization can be used to learn\nhyperparameter schedules online [Xu et al., 2018, Flennerhag et al., 2022]. Most of these, however,\nare not directly comparable due to different algorithms, environments, and configuration spaces in\ntheir experiments, making it difficult to find clear state-of-the-art and thus promising directions for\nfuture work [Eimer et al., 2023].\nBesides this lack of comparisons in HPO for RL, the cost of training and evaluation is a significant\nfactor hindering progress in the field. Tabular benchmarks [Ying et al., 2019, Klein and Hutter, 2019]\noffer a low-cost option when benchmarking HPO. Such benchmarks are essentially databases, from\nwhich the results of running a given algorithm are looked up rather than performing actual runs.\nCurrently, the only benchmark library for HPO in RL is a tabular benchmark: HPO-RL-Bench [Shala\net al., 2024]. It contains results for five RL algorithms on 22 different environments with three\nrandom seeds each. HPO-RL-Bench offers significantly reduced configuration spaces of only up to\nthree hyperparameters, narrowed down from typically larger spaces of 10 to 13 hyperparameters,\ne.g., in ARLBench and SB3 [Raffin et al., 2021], and is based solely on a pre-computed dataset. Its\ndynamic option is further reduced to only two hyperparameters, each with three possible values at\ntwo switching points. We believe, therefore, that HPO-RL-Bench and ARLBench will fulfill different\nroles: HPO-RL-Bench can provide zero-cost evaluations of expensive domains, while for ARLBench,\nwe prioritized flexibility in what and when to configure while still allowing fast evaluations.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3725, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "19ff54e0-8e86-4fb5-8024-95be13cb6234": {"__data__": {"id_": "19ff54e0-8e86-4fb5-8024-95be13cb6234", "embedding": null, "metadata": {"page_label": "3", "file_name": "ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 18789005, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "93fad366-2ad9-4df2-8f76-fe61187b2bce", "node_type": "4", "metadata": {"page_label": "3", "file_name": "ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 18789005, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "6e198e5b85c6b0d30d723eae98f1e9ca738a5709bf5f367a746056fa7a0f8b30", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "636a652b-b1c7-45a3-aa59-9fc0b81a0806", "node_type": "1", "metadata": {"page_label": "3", "file_name": "ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 18789005, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "d9de52d3db47dd51b83321fd0d61a20719d9539e9a8a5687672277c66106cbad", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "It contains results for five RL algorithms on 22 different environments with three\nrandom seeds each. HPO-RL-Bench offers significantly reduced configuration spaces of only up to\nthree hyperparameters, narrowed down from typically larger spaces of 10 to 13 hyperparameters,\ne.g., in ARLBench and SB3 [Raffin et al., 2021], and is based solely on a pre-computed dataset. Its\ndynamic option is further reduced to only two hyperparameters, each with three possible values at\ntwo switching points. We believe, therefore, that HPO-RL-Bench and ARLBench will fulfill different\nroles: HPO-RL-Bench can provide zero-cost evaluations of expensive domains, while for ARLBench,\nwe prioritized flexibility in what and when to configure while still allowing fast evaluations.\nBenchmarks are essential in the broader AutoML domain; Benchmarks such as\nHPOBench [Eggensperger et al., 2021], HPO-B [Pineda et al., 2021] and YAHPO-Gym [Pfisterer\net al., 2022] have been contributing to research progress in HPO. In contrast, ARLBench focuses\nexclusively on RL, a domain that has only been included with a single toy scenario in HPOBench\nso far. Given that Mohan et al. [2023] have shown that the RL HPO landscapes do not seem\nas benign as Pushak and Hoos [2018] describe the HPO landscapes for supervised learning\noverall (see Appendix H.1), it is necessary to offer a dedicated RL benchmark with a diverse\ntask set. The NAS-Bench benchmarks for neural architecture search (NAS) are examples of\nbenchmarking supporting efficient research: NAS-Bench-101 [Ying et al., 2019] is a tabular\nbenchmark, which NAS-Bench-201 [Dong and Yang, 2020] extends to a larger configuration space,\nand NAS-Bench-301 [Zela et al., 2022] uses this data to propose surrogate models [Eggensperger\net al., 2014, Klein et al., 2019] that can predict performance even for unseen architectures. Building\non these, several dozens of specialized NAS benchmarks have been developed [Mehta et al., 2022].\nWe expect that benchmarking HPO in RL will similarly become a focal point within the community\ntowards advancing the configuration of RL algorithms.\n3", "mimetype": "text/plain", "start_char_idx": 2963, "end_char_idx": 5070, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1f512c41-97df-4a6c-9194-f7dcb170b248": {"__data__": {"id_": "1f512c41-97df-4a6c-9194-f7dcb170b248", "embedding": null, "metadata": {"page_label": "4", "file_name": "ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 18789005, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e75ddac4-8bcb-43a5-ac3a-a61218f561c3", "node_type": "4", "metadata": {"page_label": "4", "file_name": "ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 18789005, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "b5397efe52f7d773e12723c55fdac600c1aca8753ba94425e9c66fb3bbba0082", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Figure 2: Overview of the ARLBench framework. The AutoRL environment, providing aGymnasium-\nlike interface [Towers et al., 2023], is the interaction point for HPO methods. At optimization step t,\nthe optimizer selects a hyperparameter configuration \u03bbt and a training budget (number of steps) bt.\nThen, the RL algorithm is trained using the given configuration and budget. As a result, the AutoRL\nenvironment returns the training result in the form of optimization objectives ot, e.g., the evaluation\nreturn and runtime, and state features xt, e.g., gradients during training.\n3 Implementing ARLBench\nIn this section, we discuss the implementation of the ARLBench framework. Notably, we elaborate on\nessential considerations for the benchmark and its two main components: the AutoRL Environment\nHPO interface and the RL algorithm implementations.\n3.1 Benchmark Desiderata for ARLBench\nGiven the limitations of HPO-RL-Bench [Shala et al., 2024] compared to the kinds of methods\nwe see in HPO for RL, our three main priorities in constructing ARLBench are (i) enabling the\nlarge configuration spaces required for RL, (ii) prioritizing fast execution times, and (iii) supporting\ndynamic and reactive hyperparameter schedules.\nConfiguration Space Size. Eimer et al. [2023] have shown that most hyperparameters contribute to\nthe training success of RL algorithms. Furthermore, our knowledge of how hyperparameters act on\nRL algorithms continues to expand, most recently, e.g., by showing the importance of batch sizes in\ncertain RL settings [Obando-Ceron et al., 2023]. Thus, limiting the configurability of a benchmark\nwill lead to the insights we gather outpacing the benchmarking capabilities of the community.\nTherefore, we enable large and flexible configuration spaces for all algorithms. To achieve this,\nhowever, we cannot simply extend the tabular HPO-RL-Bench, as the computational expense required\nfor larger configuration spaces would grow exponentially in the number of hyperparameters. A\nlong-term solution would be to train surrogate models to predict performance. However, as the data\nrequirements for reliable and dynamic surrogates in RL are presently unclear, we focus on building a\ngood online benchmark first and use it to generate preliminary landscapes. We hope this approach\nallows the building of better RL-specific surrogate models in future work.\nRunning Time. An alternative to using surrogate models is building an efficient way of evaluating\nhyperparameter configurations in RL. JAX [Bradbury et al., 2018] enables significant efficiency gains,\nleading to RL agents training on many domains in mere minutes or seconds [Lu, 2022, Toledo, 2024].\nWe exploit this while providing RL algorithms that are easy to configure for commonly used HPO\nmethods, including multi-objective and multi-fidelity optimization.\nDynamic Configuration. Finally, we aim to enable dynamic configurations that allow hyperpa-\nrameter settings to be adjusted during a single RL training session, recognizing that the optimal\nhyperparameters can evolve as training progresses [Mohan et al., 2023]. One way of doing this is\nby providing checkpoint capabilities that support the seamless continuation of RL training. Most\npopulation-based methods, for example, find schedules with 10\u201320 hyperparameter changes during a\nsingle training run [Jaderberg et al., 2017, Parker-Holder et al., 2022, Wan et al., 2022], while other\nmethods, such as hyperparameter adaptation via meta-gradients, can configure much more often and\neven require information about the current algorithm state.\n4", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3574, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d32ec64c-1971-4029-a7a7-47186f07483d": {"__data__": {"id_": "d32ec64c-1971-4029-a7a7-47186f07483d", "embedding": null, "metadata": {"page_label": "5", "file_name": "ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 18789005, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f6a7c0a4-f9d2-4aa8-9a41-e76cddf15c5f", "node_type": "4", "metadata": {"page_label": "5", "file_name": "ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 18789005, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "bcda864a5646bfd77b549e5a45834eb959896d6a1cd6fca9bdd58664133d1980", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c8930588-0bdf-45a0-9568-f2b3a59d1b1a", "node_type": "1", "metadata": {}, "hash": "11d54d21b2ce5ea26e03e72900e0213d31013f427c7c58faf0a206258d8ccd37", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "3.2 The HPO Interface: The AutoRL Environment\nAs shown in Figure 2, the AutoRL Environment is the main building block of ARLBench and connects\nall the critical parts for HPO in RL. It provides a powerful, flexible, and dynamic interface to support\nvarious HPO methods in an interface that, for ease of use, functions similarly to Gymnasium [Towers\net al., 2023]. During the optimization, the HPO method selects a hyperparameter configuration \u03bbt\nand training budget bt for the current optimization step t. Given these, the AutoRL Environment\nsets up the algorithm and RL environment and performs the actual RL training. In addition to an\nevaluation reward, data such as gradients and losses are collected during training. Depending on\nthe user\u2019s preferences, the AutoRL Environment then extracts optimization objectives, such as the\naverage evaluation reward, training running time, or carbon emissions [Courty et al., 2024], as well\nas optional information on the RL algorithm\u2019s internal state, e.g., the variance of the gradients.\nThe AutoRL Environment supports static and dynamic HPO methods. While static methods start the\ninner RL training from scratch for each configuration, dynamic approaches can keep the training state,\nwhich includes the neural network parameters, optimizer state, and replay buffer. To support the latter,\nwe integrate an easy-to-use yet powerful checkpointing mechanism. This enables HPO methods to\nrestore, duplicate, or checkpoint the training state at any point during the dynamic optimization.\n3.3 RL Training\nTo address the computational efficiency of RL algorithms, we implement the entire training pipeline\nusing JAX [Bradbury et al., 2018]. We re-implement DQN [Mnih et al., 2015], PPO [Schulman\net al., 2017], and SAC [Haarnoja et al., 2018] in order to make them highly configurable, enable\ndynamic execution, and ensure compatibility with different target environments. Wherever we use\ncode from external sources (Freeman et al. [2021], Lu [2022], Toledo et al. [2023]; licensed under\nApache-2.0), it is referenced in the code. We compare our implementation to SB3 [Raffin et al.,\n2021] in Appendix E and find very similar learning curves for the speedups in Figure 1. We support a\nrange of environment frameworks, particularly Brax [Freeman et al., 2021], Gymnax [Lange, 2022],\nGymnasium [Towers et al., 2023], Envpool [Weng et al., 2022], and XLand [Nikulin et al., 2023].\nThis results in a broad coverage of RL domains, including robotic simulations, grid worlds, and\nvideo games, such as the ALE [Bellemare et al., 2013]. We ensure compatibility with these different\nenvironments and their APIs with our own ARLBenchEnvironment class, allowing for future updates\nand continued support of changing interfaces in RL.\n4 Finding Representative Benchmarking Settings\nHighly efficient implementations are crucial for efficient benchmarking of HPO methods for RL.\nHowever, they represent just a fraction of the overall picture: prior work has focused primarily on a\nsingle-task domain, due to a lack of insight regarding which RL domains to target. To tackle this\nissue, we aim to find a subset of RL environments representative of the broader RL field. First,\nwe study the hyperparameter landscapes for a large set of environments using random sampling of\nconfigurations. To ensure the feasibility of our experiments in terms of computational resources, we\nselect a representative subset of environments from each domain. In particular, we select a total of\n21 environments: five ALE games (Atari-5), three Box2D environments, four Brax walkers, five\nclassic control environments, and four XLand environments (see Appendix D). Then, we use the\nAtari-5 [Aitchison et al., 2023] method to find a set of environments for testing HPO approaches in\nRL. Ultimately, we validate that this subset is representative of the HPO landscape of all RL tasks we\nconsider. In total, we spent 10 105h on CPUs and 32 588h on GPUs (see Appendix I).\n4.1 Data Collection\nFor each combination of algorithm and environment, we aim to estimate the hyperparameter landscape,\ni.e., the relationship between a certain hyperparameter configuration and its performance.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4179, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c8930588-0bdf-45a0-9568-f2b3a59d1b1a": {"__data__": {"id_": "c8930588-0bdf-45a0-9568-f2b3a59d1b1a", "embedding": null, "metadata": {"page_label": "5", "file_name": "ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 18789005, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f6a7c0a4-f9d2-4aa8-9a41-e76cddf15c5f", "node_type": "4", "metadata": {"page_label": "5", "file_name": "ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 18789005, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "bcda864a5646bfd77b549e5a45834eb959896d6a1cd6fca9bdd58664133d1980", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d32ec64c-1971-4029-a7a7-47186f07483d", "node_type": "1", "metadata": {"page_label": "5", "file_name": "ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 18789005, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "11337bd60c057f96b81e64dacd67e08c0b0bb069225f8a2e7638d9f87919c98d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In particular, we select a total of\n21 environments: five ALE games (Atari-5), three Box2D environments, four Brax walkers, five\nclassic control environments, and four XLand environments (see Appendix D). Then, we use the\nAtari-5 [Aitchison et al., 2023] method to find a set of environments for testing HPO approaches in\nRL. Ultimately, we validate that this subset is representative of the HPO landscape of all RL tasks we\nconsider. In total, we spent 10 105h on CPUs and 32 588h on GPUs (see Appendix I).\n4.1 Data Collection\nFor each combination of algorithm and environment, we aim to estimate the hyperparameter landscape,\ni.e., the relationship between a certain hyperparameter configuration and its performance. Therefore,\nwe run an RL algorithm on 256 Sobol-sampled configurations [Sobol, 1967]. With configuration\nspaces ranging from 10 to 13 hyperparameters, this is roughly equivalent to the search space covering\ninitial design recommendations of Jones et al. [1998]. We run each configuration for 10 random\nseeds. The performance is measured by evaluating the final policy induced by the configuration on a\ndedicated evaluation environment with a different random seed. We collect 128 episodes and calculate\n5", "mimetype": "text/plain", "start_char_idx": 3461, "end_char_idx": 4683, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f579576f-3dbc-4dbd-8a80-318cebc52c5f": {"__data__": {"id_": "f579576f-3dbc-4dbd-8a80-318cebc52c5f", "embedding": null, "metadata": {"page_label": "6", "file_name": "ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 18789005, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9d2c7fb5-b6e0-4a53-8fa4-af7c73ed831a", "node_type": "4", "metadata": {"page_label": "6", "file_name": "ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 18789005, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "460687a00e7a1e6827ca77944994f1764047b4748734008db08fd510fe9d97f5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Figure 3: Comparison of the Spearman correlation for different subset sizes with confidence intervals\nfrom 5-fold cross-validation on the configurations.\nthe average cumulative reward. This dataset can be found on GitHub as well as on Huggingface:\nhttps://huggingface.co/datasets/autorl-org/arlbench.\n4.2 Subset Selection\nBased on the collected evaluation rewards, we aim to find a subset of environments on which to\nevaluate an AutoRL method. Due to discrete and continuous action spaces, the algorithms differ in\nthe environments they are compatible with. Therefore, we perform this selection for each algorithm\nindividually. The set of environments for PPO contains all 21 evaluated environments, while DQN is\nlimited to discrete action spaces (13 environments), and SAC only supports continuous action spaces\n(8 environments). The full sets of environments per algorithm are listed in Appendix D. Additional\ndetails on the subset selection are stated in Appendix G.1.\nFinding an optimal subset. For selecting an optimal subset, we use the method proposed by Aitchi-\nson et al. [2023]. Let \u039b be the set of hyperparameter configurations for an algorithm and E the\ncorresponding set of environments. For each evaluated hyperparameter configuration \u03bb \u2208 \u039b and\nenvironment e \u2208 E, we are given a performance score pe\n\u03bb. We define pE\n\u03bb := 1\n|E| \u00b7 P\ne\u2208E pe\n\u03bb as the\naverage score of a configuration \u03bb across all environments. Given a subset of environments I \u2282 Eof\nsize C \u2208 N, we use a linear regression model f to predict pE\n\u03bb from the scores pe\n\u03bb for all e \u2208 I, i.e.,\n\u02c6pE\n\u03bb := f(pe1\n\u03bb , \u00b7\u00b7\u00b7 , peC\n\u03bb ). An optimal subset I\u2217 of size C is defined as\nI\u2217 \u2208 arg min\nI={e1,\u00b7\u00b7\u00b7,eC}\u2282E\nd(\u02c6pE, pE) with \u02c6pE = (\u02c6pE\n\u03bb)\u03bb\u2208\u039b and \u02c6pE\n\u03bb = f(pe1\n\u03bb , \u00b7\u00b7\u00b7 , peC\n\u03bb ), (1)\nwhere d is a distance metric between the predicted and target hyperparameter landscapes, i.e., the\nvector of predicted scores \u02c6pE = (\u02c6pE\n\u03bb)\u03bb\u2208\u039b and the vector of target scores pE = (pE\n\u03bb)\u03bb\u2208\u039b spanning\nacross the configurations \u03bb \u2208 \u039b. The performance attained on the subset then provides the best\napproximation of the performance across all environments for subsets of size C. Note that the\nenvironment selection does not take HPO behavior into account. We could perform the subselection\nto approximate HPO results directly. However, the performance discrepancies we see for HPO\nmethods in the literature [Eimer et al., 2023, Shala et al., 2024] suggest that we do not yet know how\nto best apply HPO methods to RL. Therefore, we currently lack reliable methodologies to obtain the\nnecessary data allowing us to infer direct relationships between environments and performance of\nHPO methods.\nSelection Strategy. Although reward scales vary drastically across environments, we lack the human\nexpert scores [Aitchison et al., 2023] to normalize rewards per environment. Instead, we apply a rank-\nbased normalization method to obtain the performances pe\n\u03bb. For an environment e, we train policies\nusing each hyperparameter configuration \u03bb across 10 different random seeds and evaluate each\nresulting policy to obtain its mean return. The performance pe\n\u03bb of a configuration \u03bb is then determined\nby the average rank with respect to the mean return over its 10 random seeds when compared to all\nother configurations within the same environment e. Now, we can fit a linear model to predict the\naverage ranks across the full set, given the ranks on the subset. We use the Spearman correlation\ncoefficient \u03c1p as a similarity metric, leading tod(\u02c6pE\n\u03bb, pE\n\u03bb) := 1\u2212\u03c1p(\u02c6pE\n\u03bb, pE\n\u03bb) in Equation 1. Our choice\nof \u03c1p is motivated by our interest in capturing relationships between two return distributions robustly\n6", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3645, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "2be3c88a-7d34-46fc-ba01-99ad7ac78bca": {"__data__": {"id_": "2be3c88a-7d34-46fc-ba01-99ad7ac78bca", "embedding": null, "metadata": {"page_label": "7", "file_name": "ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 18789005, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "cb641677-5bfb-48b8-9e3c-20035e771c2f", "node_type": "4", "metadata": {"page_label": "7", "file_name": "ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 18789005, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "bfbdabd52933c7c8d165a350aba1a162e24876c0854bd7d5321fe5ae88f78c9c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Figure 4: Selected set of representative environments per algorithm. For PPO, the discrete variant of\nLunarLander was selected.\nFigure 5: Comparison of the return distributions over hyperparameter configurations of PPO on all 21\nenvironments (left) and the selected subset of 5 environments (right). For the same comparisons for\nDQN and SAC, see Appendix H.2.\nby focusing on relative rankings rather than exact values. Figure 3 shows the Spearman correlation\ncoefficient for the top three subsets of different sizes with confidence intervals computed using 5-fold\ncross-validation on the configurations. The results are fairly consistent for all algorithms except for\nvery small subsets for PPO. Furthermore, they exhibit a high correlation to the full environment\nset, even when considering only a few environments. Based on these correlations, we select five\nenvironments for PPO and DQN from their respective full sets of 21 and 13 environments. The\nselected PPO subset shows a correlation of 0.95, while the DQN subset has a correlation of 0.92. For\nSAC, we select a subset of four environments, achieving a correlation of 0.94 with the full set of\neight environments. Further details can be found in Figure 4 and Appendix G.3. A single training on\nall environments in all three subsets takes around 2.93 GPU hours, compared to 7.12 GPU hours for\nthe full set of environments, where the ALE environments are limited to Atari-5 in the full set.\n4.3 Validating ARLBench\nHaving selected a subset per algorithm, we still need to ensure this subset is representative of the full\nenvironment set from an HPO perspective. To investigate this, we examine (i) the HPO landscape, in\nparticular the return distributions, budget correlations, and hyperparameter importance, and (ii) the\nperformance of different HPO optimizers on the subset and full environment set. For most of the\nfollowing analysis, we use DeepCA VE [Sass et al., 2022], as a monitoring package for HPO. We use\n95% confidence intervals in our reported results as suggested by Agarwal et al. [2021].\nComparing HPO Landscapes. We first analyze the differences in HPO landscapes between the full\nenvironment set and our subset. This is especially important since we do not use HPO performance\ndata for the selection but still want to ensure that HPO approaches will encounter the same overall\nlandscape characteristics on the subsets as on all benchmarks. We argue that this yields the first\ninsights into the consistency of HPO performance on the subset and full set of environments. To see\nif the overall RL algorithm performance changes, Figure 5 shows the distribution of returns in our\nrandom samples of PPO, normalized per domain by the performance scores seen in our pre-study.\nFor the Box2D and Brax environments, we set fixed minimum scores of -200 and -2000, respectively,\nto mitigate artificially low performance caused by numerical instabilities. We see that the subset\nincludes a diverse selection of return distributions: from a large bias of configurations towards the\n7", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3044, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5323d89a-2e66-4d57-b5ad-2fbeb11ece0e": {"__data__": {"id_": "5323d89a-2e66-4d57-b5ad-2fbeb11ece0e", "embedding": null, "metadata": {"page_label": "8", "file_name": "ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 18789005, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "47d7f859-a471-4a50-b2bb-c9ed011d899b", "node_type": "4", "metadata": {"page_label": "8", "file_name": "ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 18789005, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "2a3bfb94d0316479cf3888e67a01395701106bc725b0843ebaa0c55412a34c84", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "PPO Subset PPO DQN Subset DQN SAC Subset SAC\n#HPs \u2265 5% 2.2 1.2 2.54 2.75 1.86 1.25\n#Interactions \u2265 5% 1.62 2.2 1.3 1.2 1.0 1.0\nTable 1: Number of hyperparameters and hyperparameter interactions with over 5% importance on\nthe full set and subset for each algorithm.\nlower end of the performance spectrum (in BattleZone, LunarLander, and Humanoid), an even spread\nbiased towards higher performance (in EmptyRandom) to a dense concentration of performances\ntowards the middle (in Phoenix). Most environment domains show a similar trend to BattleZone,\nLunarLander, and EmptyRandom: there is a wide spread of configurations, with a bias towards low\nperformances. Our subset thus captures this dominant trend as well as the tendency of XLand and\nClassic Control for a more even performance distribution. The Phoenix environment reflects the\nopposite behavior, ensuring that similar environments outside of the typical performance distribution\nare included in our subset. These different patterns in performance with regard to hyperparameter\nsettings suggest that the selected subset is likely to test these variations in HPO behavior.\nAdditionally, for the environment domains, the performance at different points in training is similarly\ncorrelated with the final performance, see Appendix H.4 for the full analysis. This shows that a\nlarge proportion of the RL algorithms\u2019 behavior regarding their hyperparameters is preserved in the\nsubset selection (see Appendix H.2 for full results). In our fANOV A analysis [Hutter et al., 2014]\n(see Appendix H.3 for full results), we verify that the number of important hyperparameters stays\nconsistent. For most algorithm-environment combinations, only two to four hyperparameters have an\nimportance of at least 5%, though the specific important ones differ, similar to common observations\nin HPO [Bergstra and Bengio, 2012]. Table 1 shows that the number of important hyperparameters\nand their interactions remain consistent in the subset, with the highest deviation being between 2.2\nhyperparameters above 5% importance on average for PPO on the whole environment set and 1.2 on\nthe subset. Our results, along with the observed similarities in return distributions, suggest that the\nmain properties of the HPO landscapes are preserved in our subselection.\nFigure 6: Comparison of HPO methods\u2019 scores on the subset and full environment set (higher is\nbetter). Top: Performance distributions over optimizer runs and environments. Medians and means\nare visualized using black and dotted gray lines, respectively. Bottom: HPO anytime performance\nwith 95% confidence intervals. See Appendices G.3 for PPO and SAC , and J for details. We note\nthat we do not consider inter-quartile means to prevent disregarding environments (top), especially\nsince we are using only three optimizer runs (bottom).\nComparing HPO Optimizers. To further validate the subset selection, we run four HPO optimizers\nwith a budget of 32 full training runs each for all algorithms and environments. We use five runs, i.e.,\nrandom seeds for each HPO optimizer and each configuration is evaluated on three random seeds,\nfollowing recommendations by Eimer et al. [2023]. We believe five seeds are a good compromise\n8", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3221, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c4dba48c-e801-451d-8aef-0a56bb377c3e": {"__data__": {"id_": "c4dba48c-e801-451d-8aef-0a56bb377c3e", "embedding": null, "metadata": {"page_label": "9", "file_name": "ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 18789005, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3fd785be-8bb9-4078-aefb-20303d24b412", "node_type": "4", "metadata": {"page_label": "9", "file_name": "ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 18789005, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "628638e8ef873a29b220608393e68758596aecd46f2ee6e08fceecacd6ce932e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "26a79565-3381-45fe-8fc9-cc3634f4a7aa", "node_type": "1", "metadata": {}, "hash": "b9ff6cdb0d228783072328877b51771944156704fc6388c4afbc0f246763f3d6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "to obtain valid insights while accounting for the associated high computational demand. While\nstatistically significant insights might require many more seeds, we believe five seeds are sufficient for\nobtaining preliminary insights into the compatibility of the full set of environments and our chosen\nsubsets. To reflect the current range of HPO tools for RL, we select random search (RS) [Bergstra and\nBengio, 2012], PBT [Jaderberg et al., 2017], the Bayesian optimization tool SMAC [Lindauer et al.,\n2022] as well as SMAC in combination with the Hyperband scheduler [Li et al., 2017] (SMAC+HB).\nWe compare the results on the subsets and the full set of environments.\nFigure 7: The hyperparameter landscape\nof PPO on Ant. Lighter is better, (mean\nperformance over 10 seeds). Similar con-\nfigurations perform very differently: high\nreturns occur next to almost failure modes.\nFigure 6 shows the HPO optimizer scores, normalized per\ndomain by the performances seen in our pre-study, for\neach algorithm on the subset and all environments. The\noverall performance of each HPO optimizer is represented\nby the mean performance across all environments in the\nrespective set of environments. We observe that the scores\nare distributed similarly between the full set and the subset\non each algorithm for the final scores. Median and mean\nscores for all algorithms closely align with the respective\nscores of the subsets in terms of ranking.\nFor HPO anytime performance, the relative order remains\nconsistent across both sets for RS, PBT, and SMAC+HB,\nwith the only major difference that RS scores higher on the\ncomplete set earlier on; This, however, is due to merely\na slight difference in scores, which is still within the con-\nfidence interval of RS. Our analysis shows that overall,\nthe best mean HPO optimizer performance is achieved by\nSMAC (without HB) and RS. Although the confidence intervals overlap, the overall trends we observe\nin the subsets and full sets of environments stay consistent. In previous work [Eimer et al., 2023, Shala\net al., 2024], multi-fidelity optimizers were shown to perform quite well, and PBT performed worst\noverall. Here, multi-fidelity optimization still outperforms PBT for DQN, but black-box optimization\nwith SMAC without HB and RS overperform in comparison. This is likely due to the wider variety of\nenvironment domains we consider, some of which might not be suitable for the partial evaluations\nof multi-fidelity approaches (e.g. XLand). Another important factor is the inclusion of SAC where\nRS is especially strong. Even for PPO and DQN, however, it is striking how closely SMAC as a\nstate-of-the-art HPO optimizer compares to RS, which typically performs worse than SMAC and\nother state-of-the-art HPO methods in standard supervised ML settings [Turner et al., 2021, Lindauer\net al., 2022].\nLooking at a partial hyperparameter landscape in Figure 7, we see a possible reason: this is far from\nthe benign HPO landscapes Pushak and Hoos [2018] found for supervised learning. This shows\nthat simply applying common HPO packages will not be sufficient to solve HPO for all RL tasks; a\ndedicated, specific effort is needed. We present further landscape plots in Appendix H.1, showing the\ncontrast between benign and adverse landscapes we found during our experiments.\n5 Limitations and Future Work\nDue to the dimensions of complexity involved in this topic, including the computational expense and\nwealth of RL algorithms and environments, ARLBench has some limitations. First, we manually\nselected the underlying set of algorithms and environments from those used in the RL community\nat large. This gave rise to a focus on model-free learning in combination with base versions of\nPPO, DQN, and SAC. In the future, we will cover extensions to these algorithms, such as advanced\ntypes of replay strategies [Kapturowski et al., 2019], multi-step or exploration strategies [Amin\net al., 2021, Pislar et al., 2022]. Additionally, we would like to enable ARLBench to evaluate policy\ngeneralization, ensuring that optimized policies perform well in previously unseen environments [Kirk\net al., 2023, Benjamins et al., 2023, Mohan et al., 2024, Benjamins et al., 2024].", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4200, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "26a79565-3381-45fe-8fc9-cc3634f4a7aa": {"__data__": {"id_": "26a79565-3381-45fe-8fc9-cc3634f4a7aa", "embedding": null, "metadata": {"page_label": "9", "file_name": "ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 18789005, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3fd785be-8bb9-4078-aefb-20303d24b412", "node_type": "4", "metadata": {"page_label": "9", "file_name": "ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 18789005, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "628638e8ef873a29b220608393e68758596aecd46f2ee6e08fceecacd6ce932e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c4dba48c-e801-451d-8aef-0a56bb377c3e", "node_type": "1", "metadata": {"page_label": "9", "file_name": "ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 18789005, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "db6d8266fb66362501569e27b7a134147027de5756bf04dd3cef7a4373f40045", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "First, we manually\nselected the underlying set of algorithms and environments from those used in the RL community\nat large. This gave rise to a focus on model-free learning in combination with base versions of\nPPO, DQN, and SAC. In the future, we will cover extensions to these algorithms, such as advanced\ntypes of replay strategies [Kapturowski et al., 2019], multi-step or exploration strategies [Amin\net al., 2021, Pislar et al., 2022]. Additionally, we would like to enable ARLBench to evaluate policy\ngeneralization, ensuring that optimized policies perform well in previously unseen environments [Kirk\net al., 2023, Benjamins et al., 2023, Mohan et al., 2024, Benjamins et al., 2024]. Further research on\nhyperparameter landscapes in RL [Mohan et al., 2023] can inform useful future additions.\nUsing a selected subset reduces computational costs but may increase variance due to the smaller\nsample size, requiring careful experimental design to ensure statistically significant results. Addi-\ntionally, the selection of subsets could also consider training time, aiming for the most informative\n9", "mimetype": "text/plain", "start_char_idx": 3509, "end_char_idx": 4612, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "47bb971b-a435-479c-b074-10081c2cdff4": {"__data__": {"id_": "47bb971b-a435-479c-b074-10081c2cdff4", "embedding": null, "metadata": {"page_label": "10", "file_name": "ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 18789005, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "fef459fe-9ea0-4559-a73f-25461bf42691", "node_type": "4", "metadata": {"page_label": "10", "file_name": "ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 18789005, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "7b1f407e5e0b75c8789b7b500ed89d9a155d65babb6df6839787f9f8d0b95f3f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "59a16964-71c1-4933-aadb-d6a80803bed8", "node_type": "1", "metadata": {}, "hash": "28db53e74cc57160950a54b0a44f74b7b44f9d307653c9c0780cb7de4bc4e161", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "and the least costly subsets. Currently, we prioritize higher validation accuracy over reduced running\ntime, even if one environment is slightly less important but significantly faster.\nThe computational cost itself remains a limitation of the benchmark. While our setting is much\ncheaper to evaluate and enables many more research groups to do thorough research on AutoRL,\nit is still by no means as cheap as surrogate or table lookups would be. Our highest priority is the\nflexibility of large configuration spaces and dynamic configuration, representing real-world HPO\napplications of RL; we do not see purely tabular benchmarks as an alternative in this exploratory\nphase of the field. Instead, we believe surrogate models [Eggensperger et al., 2015, 2018, Zela et al.,\n2022] will be crucial for more efficient HPO in RL, though modeling the dynamic nature of HPO in\nsurrogates remains an open challenge. Our published meta-dataset, the largest one for AutoRL to\ndate, enables the first steps towards such dynamic surrogates.\nFurthermore, there are additional elements of AutoRL research our benchmark does not yet fully\nsupport. We designed it to be future-oriented, with a benchmark structure that can, in principle,\nsupport second-order optimization methods, learning based on internal algorithmic aspects, such as\nlosses and activation functions, or architecture search. However, we believe integrating these aspects\ninto ARLBench first requires research into how state-based HPO in RL and NAS for RL should\nbe approached. The same holds for concepts such as discovering RL algorithms [Co-Reyes et al.,\n2021, Jackson et al., 2024], where no standard interface exists for evaluating a learned algorithm.\nNonetheless, our environment subsets can aid in the evaluation of these approaches. Integrating\nAutoRL for environment components, such as environment design [Jiang et al., 2021, Parker-Holder\net al., 2022], into ARLBench poses a challenge because most RL environments do not inherently\nsupport these approaches. Improving the compatibility of the environment frameworks in ARLBench\nwill facilitate the integration of these methods into the benchmark.\n6 Conclusion\nWe propose a benchmark for HPO in RL that supports this emerging field of research by (i) providing\na general, easily integrable and extensible way of evaluating various paradigms for HPO in RL; (ii) re-\nducing computational costs with highly efficient implementations, while expanding the evaluation\ncoverage of HPO methods by selecting informative environment subsets, achieving over 16 times\nthe efficiency compared to standard frameworks; (iii) publishing a large set of performance data for\nfuture use in AutoRL research. Such a concerted effort is necessary to help the community work in\na common direction and democratize AutoRL as a research field. While its set of algorithms and\nenvironments will evolve within the coming years, ARLBench is built to allow for easy extension,\ne.g., to AutoML paradigms such as NAS, which are currently underrepresented in RL. Therefore,\nARLBench will catalyze the development of increasingly efficient HPO methods for RL that perform\nwell across algorithms and environments.\nReferences\nR. Agarwal, M. Schwarzer, P. Samuel Castro, A. C. Courville, and M. G. Bellemare. Deep reinforce-\nment learning at the edge of the statistical precipice. In Proc. of NeurIPS\u201921, 2021.\nM. Aitchison, P. Sweetser, and M. Hutter. Atari-5: Distilling the arcade learning environment down\nto five games. In Proc. of ICML\u201923. PMLR, 2023.\nS. Amin, M. Gomrokchi, H. Satija, H. van Hoof, and D. Precup. A survey of exploration methods in\nreinforcement learning. arXiv preprint arXiv:2109.00157, 2021.\nM. Andrychowicz, A. Raichuk, P. Sta\u00b4nczyk, M. Orsini, S. Girgin, Rapha\u00ebl Marinier, L. Hussenot,\nM. Geist, O. Pietquin, M. Michalski, S. Gelly, and O. Bachem. What matters for on-policy deep\nactor-critic methods? A large-scale study. In Proc. of ICLR\u201921, 2021.\nN. Awad, N. Mallik, and F. Hutter.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3990, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "59a16964-71c1-4933-aadb-d6a80803bed8": {"__data__": {"id_": "59a16964-71c1-4933-aadb-d6a80803bed8", "embedding": null, "metadata": {"page_label": "10", "file_name": "ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 18789005, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "fef459fe-9ea0-4559-a73f-25461bf42691", "node_type": "4", "metadata": {"page_label": "10", "file_name": "ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 18789005, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "7b1f407e5e0b75c8789b7b500ed89d9a155d65babb6df6839787f9f8d0b95f3f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "47bb971b-a435-479c-b074-10081c2cdff4", "node_type": "1", "metadata": {"page_label": "10", "file_name": "ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 18789005, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "f6fde3ee38a5ae7775226fc4e865dc225c012203c2cf20f96f23f1f10a07ae86", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "PMLR, 2023.\nS. Amin, M. Gomrokchi, H. Satija, H. van Hoof, and D. Precup. A survey of exploration methods in\nreinforcement learning. arXiv preprint arXiv:2109.00157, 2021.\nM. Andrychowicz, A. Raichuk, P. Sta\u00b4nczyk, M. Orsini, S. Girgin, Rapha\u00ebl Marinier, L. Hussenot,\nM. Geist, O. Pietquin, M. Michalski, S. Gelly, and O. Bachem. What matters for on-policy deep\nactor-critic methods? A large-scale study. In Proc. of ICLR\u201921, 2021.\nN. Awad, N. Mallik, and F. Hutter. DEHB: Evolutionary hyberband for scalable, robust and efficient\nhyperparameter optimization. In Proc. of IJCAI\u201921, 2021.\nM. G. Bellemare, Y . Naddaf, J. Veness, and M. Bowling. The arcade learning environment: An\nevaluation platform for general agents. Journal Artificial Intelligence Research, 47:253\u2013279, 2013.\n10", "mimetype": "text/plain", "start_char_idx": 3524, "end_char_idx": 4306, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "64c05e30-4a51-4f25-90eb-ffd528fc6f67": {"__data__": {"id_": "64c05e30-4a51-4f25-90eb-ffd528fc6f67", "embedding": null, "metadata": {"page_label": "11", "file_name": "ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 18789005, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1d654ea2-8161-40e8-b3a6-c024089c2824", "node_type": "4", "metadata": {"page_label": "11", "file_name": "ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 18789005, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "cf7c1bf1c145f8babb2cf0c30fd0e931dae99ea53b0f3aed30b7ec5ce5f6d3cc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ee81acce-1b2d-41ff-afe1-96786e9b4fd2", "node_type": "1", "metadata": {}, "hash": "dd65a671c883ebc92be20d50cb09ea6198a43689ca9f245e5f1edf9d2663c4aa", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "M. G. Bellemare, W. Dabney, and R\u00e9mi R. Munos. A distributional perspective on reinforcement\nlearning. In Proc. of ICML\u201917, 2017.\nC. Benjamins, T. Eimer, F. Schubert, A. Mohan, S. D\u00f6hler, A. Biedenkapp, B. Rosenhan, F. Hutter,\nand M. Lindauer. Contextualize me \u2013 the case for context in reinforcement learning. Transactions\non Machine Learning Research, 2023.\nC. Benjamins, G. Cenikj, A. Nikolikj, A. Mohan, T. Eftimov, and M. Lindauer. Instance selection\nfor dynamic algorithm configuration with reinforcement learning: Improving generalization. In\nProceedings of the Genetic and Evolutionary Computation Conference Companion, 2024.\nJ. Bergstra and Y . Bengio. Random search for hyper-parameter optimization.Journal of Machine\nLearning Research, 13:281\u2013305, 2012.\nJ. Bradbury, R. Frostig, P. Hawkins, M. Johnson, C. Leary, D. Maclaurin, G. Necula, A. Paszke, J. Van-\nderPlas, S. Wanderman-Milne, and Q. Zhang. JAX: composable transformations of Python+NumPy\nprograms, 2018. URL http://github.com/google/jax.\nG. Brockman, V . Cheung, L. Pettersson, J. Schneider, J. Schulman, J. Tang, and W. Zaremba. OpenAI\ngym. arxiv preprint, arXiv:1606.01540, 2016.\nJ. S. O. Ceron, J. G. M. Ara\u00fajo, A. Courville, and P. S. Castro. On the consistency of hyper-parameter\nselection in value-based deep reinforcement learning. Reinforcement Learning Journal, 1, 2024.\nJ. Co-Reyes, Y . Miao, D. Peng, E. Real, Q. Le, S. Levine, H. Lee, and A. Faust. Evolving reinforce-\nment learning algorithms. In Proc. of ICLR\u201921, 2021.\nK. Cobbe, C. Hesse, J. Hilton, and J. Schulman. Leveraging procedural generation to benchmark\nreinforcement learning. In Proc. of ICML\u201920, 2020.\nB. Courty, V . Schmidt, S. Luccioni, Goyal-Kamal, M. Coutarel, B. Feld, J. Lecourt, L. Connell,\nA. Saboni, Inimaz, supatomic, M. L\u00e9val, L. Blanche, A. Cruveiller, ouminasara, F. Zhao, A. Joshi,\nA. Bogroff, H. de Lavoreille, N. Laskaris, E. Abati, D. Blank, Z. Wang, A. Catovic, M. Alencon,\nM. St\u02db ech\u0142y, C. Bauer, Lucas-Otavio, JPW, and MinervaBooks. mlco2/codecarbon: v2.4.1, May\n2024.\nJ. Dierkes, E. Cramer, H. Hoos, and S. Trimpe. Combining automated optimisation of hyperparameters\nand reward shape. Reinforcement Learning Journal, 1, 2024.\nX. Dong and Y . Yang. NAS-Bench-201: Extending the scope of reproducible neural architecture\nsearch. In Proc. of ICLR\u201920, 2020.\nK. Eggensperger, F. Hutter, H. Hoos, and K. Leyton-Brown. Surrogate benchmarks for hyperparameter\noptimization. In MetaSel\u201914, 2014.\nK. Eggensperger, F. Hutter, H. Hoos, and K. Leyton-Brown. Efficient benchmarking of hyperparame-\nter optimizers via surrogates. In Proc. of AAAI\u201915, 2015.\nK. Eggensperger, M. Lindauer, H. Hoos, F. Hutter, and K. Leyton-Brown.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2681, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ee81acce-1b2d-41ff-afe1-96786e9b4fd2": {"__data__": {"id_": "ee81acce-1b2d-41ff-afe1-96786e9b4fd2", "embedding": null, "metadata": {"page_label": "11", "file_name": "ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 18789005, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1d654ea2-8161-40e8-b3a6-c024089c2824", "node_type": "4", "metadata": {"page_label": "11", "file_name": "ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 18789005, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "cf7c1bf1c145f8babb2cf0c30fd0e931dae99ea53b0f3aed30b7ec5ce5f6d3cc", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "64c05e30-4a51-4f25-90eb-ffd528fc6f67", "node_type": "1", "metadata": {"page_label": "11", "file_name": "ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 18789005, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "44bc682b6e7004986486bd7bd44244236c6214ba722603083ee6c3b9ca7be1bd", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Reinforcement Learning Journal, 1, 2024.\nX. Dong and Y . Yang. NAS-Bench-201: Extending the scope of reproducible neural architecture\nsearch. In Proc. of ICLR\u201920, 2020.\nK. Eggensperger, F. Hutter, H. Hoos, and K. Leyton-Brown. Surrogate benchmarks for hyperparameter\noptimization. In MetaSel\u201914, 2014.\nK. Eggensperger, F. Hutter, H. Hoos, and K. Leyton-Brown. Efficient benchmarking of hyperparame-\nter optimizers via surrogates. In Proc. of AAAI\u201915, 2015.\nK. Eggensperger, M. Lindauer, H. Hoos, F. Hutter, and K. Leyton-Brown. Efficient benchmarking of\nalgorithm configurators via model-based surrogates. Machine Learning, 107(1):15\u201341, 2018.\nK. Eggensperger, P. M\u00fcller, N. Mallik, M. Feurer, R. Sass, A. Klein, N. Awad, M. Lindauer, and\nF. Hutter. HPOBench: A collection of reproducible multi-fidelity benchmark problems for HPO.\nIn Proc. of NeurIPS\u201921 Datasets and Benchmarks Track, 2021.\nT. Eimer, M. Lindauer, and R. Raileanu. Hyperparameters in reinforcement learning and how to tune\nthem. In Proc. of ICML\u201923, 2023.\nS. Falkner, A. Klein, and F. Hutter. BOHB: Robust and efficient Hyperparameter Optimization at\nscale. In Proc. of ICML\u201918, pages 1437\u20131446, 2018.\nM. Farsang and L. Szegletes. Decaying clipping range in proximal policy optimization. In 15th\nIEEE International Symposium on Applied Computational Intelligence and Informatics, SACI\n2021. IEEE, 2021.\n11", "mimetype": "text/plain", "start_char_idx": 2154, "end_char_idx": 3526, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "fe902f5a-e6b8-4f09-9021-59f9eda26eea": {"__data__": {"id_": "fe902f5a-e6b8-4f09-9021-59f9eda26eea", "embedding": null, "metadata": {"page_label": "12", "file_name": "ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 18789005, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e2d4fbf6-11e7-49fd-b4ac-62173546f7a9", "node_type": "4", "metadata": {"page_label": "12", "file_name": "ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 18789005, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "2fe55a75ed6aec34c233d84167ba714a14c2ccbcf88378bb58f92464c722ee33", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b489ef1c-b4b7-498e-a170-290fb373da18", "node_type": "1", "metadata": {}, "hash": "9dfee20c0fe89ed9d73f2c7f6246fe893c26cdadfae38113d6f25e5c02e73b3c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "S. Flennerhag, Y . Schroecker, T. Zahavy, H. van Hasselt, D. Silver, and S. Singh. Bootstrapped\nmeta-learning. In Proc. of ICLR\u201922, 2022.\nJ. Franke, G. K\u00f6hler, A. Biedenkapp, and F. Hutter. Sample-efficient automated deep reinforcement\nlearning. In Proc. of ICLR\u201921, 2021.\nC. Freeman, E. Frey, A. Raichuk, S. Girgin, I. Mordatch, and O. Bachem. Brax - A differentiable\nphysics engine for large scale rigid body simulation. In Proceedings of the Neural Information\nProcessing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks\n2021, 2021.\nR. Gulde, M. Tuscher, A. Csiszar, O. Riedel, and A. Verl. Deep reinforcement learning using cyclical\nlearning rates. In Third International Conference on Artificial Intelligence for Industries, AI4I,\n2020.\nT. Haarnoja, A. Zhou, P. Abbeel, and S. Levine. Soft actor-critic: Off-policy maximum entropy deep\nreinforcement learning with a stochastic actor. In Proc. of ICML\u201918, 2018.\nP. Henderson, R. Islam, P. Bachman, J. Pineau, D. Precup, and D. Meger. Deep reinforcement\nlearning that matters. In Proc. of AAAI\u201918, 2018.\nF. Hutter, H. Hoos, and K. Leyton-Brown. An efficient approach for assessing hyperparameter\nimportance. In Proc. of ICML\u201914, 2014.\nF. Hutter, L. Kotthoff, and J. Vanschoren, editors.Automated Machine Learning: Methods, Systems,\nChallenges. Springer, 2019. Available for free at http://automl.org/book.\nM. Jackson, C. Lu, L. Kirsch, R. Lange, S. Whiteson, and J. Foerster. Discovering temporally-aware\nreinforcement learning algorithms. In Proc. of ICLR\u201924, 2024.\nM. Jaderberg, V . Dalibard, S. Osindero, W. Czarnecki, J. Donahue, A. Razavi, O. Vinyals, T. Green,\nI. Dunning, K. Simonyan, C. Fernando, and K. Kavukcuoglu. Population based training of neural\nnetworks. arXiv:1711.09846 [cs.LG], 2017.\nM. Jiang, E. Grefenstette, and T. Rockt\u00e4schel. Prioritized level replay. In Proc. of ICML\u201921, 2021.\nD. Jones, M. Schonlau, and W. Welch. Efficient global optimization of expensive black box functions.\nJournal of Global Optimization, 13:455\u2013492, 1998.\nS. Kapturowski, G. Ostrovski, J. Quan, R. Munos, and W. Dabney. Recurrent experience replay in\ndistributed reinforcement learning. In Proc. of ICLR\u201919, 2019.\nR. Kirk, A. Zhang, E. Grefenstette, and T. Rockt\u00e4schel. A survey of zero-shot generalisation in deep\nreinforcement learning. Journal of Artificial Intelligence Research, 76:201\u2013264, 2023.\nA. Klein and F. Hutter. Tabular benchmarks for joint architecture and hyperparameter optimization.\narXiv:1905.04970[cs.LG], 2019.\nA. Klein, Z. Dai, F. Hutter, N. Lawrence, and J. Gonzalez. Meta-surrogate benchmarking for\nhyperparameter optimization. In Proc. of NeurIPS\u201919, 2019.\nR. Lange. gymnax: A JAX-based reinforcement learning environment library, 2022. URL http:\n//github.com/RobertTLange/gymnax.\nL. Li, K. Jamieson, G. DeSalvo, A. Rostamizadeh, and A. Talwalkar.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2850, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b489ef1c-b4b7-498e-a170-290fb373da18": {"__data__": {"id_": "b489ef1c-b4b7-498e-a170-290fb373da18", "embedding": null, "metadata": {"page_label": "12", "file_name": "ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 18789005, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e2d4fbf6-11e7-49fd-b4ac-62173546f7a9", "node_type": "4", "metadata": {"page_label": "12", "file_name": "ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 18789005, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "2fe55a75ed6aec34c233d84167ba714a14c2ccbcf88378bb58f92464c722ee33", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fe902f5a-e6b8-4f09-9021-59f9eda26eea", "node_type": "1", "metadata": {"page_label": "12", "file_name": "ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 18789005, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "7d825b54e088a0bf8a0502b065db6d866177b850e1274d65da27e26efc87b56b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "A survey of zero-shot generalisation in deep\nreinforcement learning. Journal of Artificial Intelligence Research, 76:201\u2013264, 2023.\nA. Klein and F. Hutter. Tabular benchmarks for joint architecture and hyperparameter optimization.\narXiv:1905.04970[cs.LG], 2019.\nA. Klein, Z. Dai, F. Hutter, N. Lawrence, and J. Gonzalez. Meta-surrogate benchmarking for\nhyperparameter optimization. In Proc. of NeurIPS\u201919, 2019.\nR. Lange. gymnax: A JAX-based reinforcement learning environment library, 2022. URL http:\n//github.com/RobertTLange/gymnax.\nL. Li, K. Jamieson, G. DeSalvo, A. Rostamizadeh, and A. Talwalkar. Hyperband: Bandit-based\nconfiguration evaluation for hyperparameter optimization. In Proc. of ICLR\u201917, 2017.\nM. Lindauer, K. Eggensperger, M. Feurer, A. Biedenkapp, D. Deng, C. Benjamins, T. Ruhkopf,\nR. Sass, and F. Hutter. SMAC3: A versatile bayesian optimization package for hyperparameter\noptimization. Journal of Machine Learning Research, 23(54):1\u20139, 2022.\nC. Lu. PureJaxRL (end-to-end RL training in pure JAX), 2022. URL https://github.com/\nluchris429/purejaxrl.\n12", "mimetype": "text/plain", "start_char_idx": 2248, "end_char_idx": 3322, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f9514b9c-0be6-4e9e-9c02-2670b3a84890": {"__data__": {"id_": "f9514b9c-0be6-4e9e-9c02-2670b3a84890", "embedding": null, "metadata": {"page_label": "13", "file_name": "ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 18789005, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5312f52d-471a-4c62-89b7-e62c0bb9c629", "node_type": "4", "metadata": {"page_label": "13", "file_name": "ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 18789005, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "b532a0c7e1df13b55719b36e12c32efb443b17bff772304dd71dbbec0312a5b6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2e80c844-f3f3-4c57-9d71-530fd2c70210", "node_type": "1", "metadata": {}, "hash": "aa7e408dc7a6348aabbe8e868029f6ece65a1838b8ea5b1edf99e112b5ca85a6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Y . Mehta, C. White, A. Zela, A. Krishnakumar, G. Zabergja, S. Moradian, M. Safari, K. Yu, and\nF. Hutter. NAS-Bench-Suite: NAS evaluation is (now) surprisingly easy. In Proc. of ICLR\u201922,\n2022.\nV . Mnih, K. Kavukcuoglu, D. Silver, A. Rusu, J. Veness, M. Bellemare, A. Graves, M. Riedmiller,\nA. Fidjeland, G. Ostrovski, S. Petersen, C. Beattie, A. Sadik, I. Antonoglou, H. King, D. Kumaran,\nD. Wierstra, S. Legg, and D. Hassabis. Human-level control through deep reinforcement learning.\nNature, 518(7540):529\u2013533, 02 2015.\nA. Mohan, C. Benjamins, K. Wienecke, A. Dockhorn, and M. Lindauer. AutoRL hyperparameter\nlandscapes. In Proc. of AutoML Conf\u201923. PMLR, 2023.\nA. Mohan, A. Zhang, and M. Lindauer. Structure in deep reinforcement learning: A survey and open\nproblems. Journal of Artificial Intelligence Research, 79:1167\u20131236, 2024.\nA. Nikulin, V . Kurenkov, I. Zisman, V . Sinii, A. Agarkov, and S. Kolesnikov. XLand-minigrid: Scal-\nable meta-reinforcement learning environments in JAX. In NeurIPS\u201923 Workshop on Intrinsically-\nMotivated and Open-Ended Learning, 2023.\nJ. Obando-Ceron and P. Castro. Revisiting rainbow: Promoting more insightful and inclusive deep\nreinforcement learning research. In Proc. of ICML\u201921, 2021.\nJ. Obando-Ceron, M. Bellemare, and P. Castro. Small batch deep reinforcement learning. In Proc. of\nNeurIPS\u201923, 2023.\nJ. Parker-Holder, V . Nguyen, and S. J. Roberts. Provably efficient online hyperparameter optimization\nwith population-based bandits. In Proc. of NeurIPS\u201920, 2020.\nJ. Parker-Holder, M. Jiang, M. Dennis, M. Samvelyan, J. Foerster, E. Grefenstette, and T. Rockt\u00e4schel.\nEvolving curricula with regret-based environment design. In Proc. of ICML\u201922, 2022.\nJ. Parker-Holder, R. Rajan, X. Song, A. Biedenkapp, Y . Miao, T. Eimer, B. Zhang, V . Nguyen,\nR. Calandra, A. Faust, F. Hutter, and M. Lindauer. Automated reinforcement learning (AutoRL):\nA survey and open problems. Journal of Artificial Intelligence Research, 74:517\u2013568, 2022.\nF. Pfisterer, L. Schneider, J. Moosbauer, M. Binder, and B. Bischl. YAHPO Gym \u2013 an efficient\nmulti-objective multi-fidelity benchmark for hyperparameter optimization. In Proc. of AutoML\nConf\u201922. PMLR, 2022.\nS. Pineda, H. Jomaa, M. Wistuba, and J. Grabocka. HPO-B: A large-scale reproducible benchmark\nfor black-box HPO based on OpenML. In Proc. of NeurIPS\u201921 Datasets and Benchmarks Track,\n2021.\nM. Pislar, D. Szepesvari, G. Ostrovski, D. Borsa, and T. Schaul. When should agents explore? In\nProc. of ICLR\u201922, 2022.\nY . Pushak and H. Hoos. Algorithm configuration landscapes: - more benign than expected? In Proc.\nof PPSN\u201918, 2018.\nA. Raffin. RL baselines3 zoo. https://github.com/DLR-RM/rl-baselines3-zoo , 2020.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2687, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "2e80c844-f3f3-4c57-9d71-530fd2c70210": {"__data__": {"id_": "2e80c844-f3f3-4c57-9d71-530fd2c70210", "embedding": null, "metadata": {"page_label": "13", "file_name": "ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 18789005, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5312f52d-471a-4c62-89b7-e62c0bb9c629", "node_type": "4", "metadata": {"page_label": "13", "file_name": "ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 18789005, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "b532a0c7e1df13b55719b36e12c32efb443b17bff772304dd71dbbec0312a5b6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f9514b9c-0be6-4e9e-9c02-2670b3a84890", "node_type": "1", "metadata": {"page_label": "13", "file_name": "ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 18789005, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "1bf922db4ac7cd01fac0c38ee9ae319268be15fd51e0acb79dddedcefdf81cab", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "PMLR, 2022.\nS. Pineda, H. Jomaa, M. Wistuba, and J. Grabocka. HPO-B: A large-scale reproducible benchmark\nfor black-box HPO based on OpenML. In Proc. of NeurIPS\u201921 Datasets and Benchmarks Track,\n2021.\nM. Pislar, D. Szepesvari, G. Ostrovski, D. Borsa, and T. Schaul. When should agents explore? In\nProc. of ICLR\u201922, 2022.\nY . Pushak and H. Hoos. Algorithm configuration landscapes: - more benign than expected? In Proc.\nof PPSN\u201918, 2018.\nA. Raffin. RL baselines3 zoo. https://github.com/DLR-RM/rl-baselines3-zoo , 2020.\nA. Raffin, A. Hill, A. Gleave, A. Kanervisto, M. Ernestus, and N. Dormann. Stable-baselines3:\nReliable reinforcement learning implementations. Journal of Machine Learning Research, 22:\n268:1\u2013268:8, 2021.\nR. Sass, E. Bergman, A. Biedenkapp, F. Hutter, and M. Lindauer. Deepcave: An interactive analysis\ntool for automated machine learning. In ICML ReALML Workshop, 2022.\nE. Schede, J. Brandt, A. Tornede, M. Wever, V . Bengs, E. H\u00fcllermeier, and K. Tierney. A survey of\nmethods for automated algorithm configuration. Journal of Artificial Intelligence Research, 75:\n425\u2013487, 2022.\nJ. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov. Proximal policy optimization\nalgorithms. arXiv:1707.06347 [cs.LG], 2017.\n13", "mimetype": "text/plain", "start_char_idx": 2169, "end_char_idx": 3407, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "20b619fb-704f-4b06-97df-07a6417faf4c": {"__data__": {"id_": "20b619fb-704f-4b06-97df-07a6417faf4c", "embedding": null, "metadata": {"page_label": "14", "file_name": "ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 18789005, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3066ea3c-0d8a-4182-8bd3-c2df08edc055", "node_type": "4", "metadata": {"page_label": "14", "file_name": "ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 18789005, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "33aa895932d3b1e6d60b38a9f1a1b3e49af0bff449e7105980353047adca7f30", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "G. Shala, S. P. Arango, A. Biedenkapp, F. Hutter, and J. Grabocka. HPO-RL-Bench: A zero-cost\nbenchmark for HPO in reinforcement learning. In Proc. of AutoML Conf\u201924. PMLR, 2024.\nI. Sobol. On the distribution of points in a cube and the approximate evaluation of integrals. USSR\nComputational Mathematics and Mathematical Physics, 7(4):86\u2013112, 1967.\nE. Toledo. Stoix: Distributed single-agent reinforcement learning end-to-end in JAX, April 2024.\nURL https://github.com/EdanToledo/Stoix.\nE. Toledo, L. Midgley, D. Byrne, C. R. Tilbury, M. Macfarlane, C. Courtot, and A. Laterre. Flashbax:\nStreamlining experience replay buffers for reinforcement learning with JAX, 2023. URL https:\n//github.com/instadeepai/flashbax/.\nM. Towers, J. Terry, A. Kwiatkowski, J. Balis, G. de Cola, T. Deleu, M. Goul\u00e3o, A. Kallinteris,\nA. KG, M. Krimmel, R. Perez-Vicente, A. Pierr\u00e9, S. Schulhoff, J. Tai, A. Shen, and O. Younis.\nGymnasium, 2023. URL https://zenodo.org/record/8127025.\nR. Turner, D. Eriksson, M. McCourt, J. Kiili, E. Laaksonen, Z. Xu, and I. Guyon. Bayesian optimiza-\ntion is superior to random search for machine learning hyperparameter tuning: Analysis of the\nBlack-Box Optimization Challenge 2020. In Proc. of NeurIPS\u201920 Competition and Demonstration,\n2021.\nC. V oelcker, M. Hussing, and E. Eaton. Can we hop in general? a discussion of benchmark selection\nand design using the hopper environment. In Finding the Frame: An RLC Workshop for Examining\nConceptual Frameworks, 2024.\nX. Wan, C. Lu, J. Parker-Holder, P. Ball, V . Nguyen, B. Ru, and M. Osborne. Bayesian generational\npopulation-based training. In Proc. of AutoML Conf\u201922. PMLR, 2022.\nJ. Weng, M. Lin, S. Huang, B. Liu, D. Makoviichuk, V . Makoviychuk, Z. Liu, Y . Song, T. Luo,\nY . Jiang, Z. Xu, and S. Yan. EnvPool: A highly parallel reinforcement learning environment\nexecution engine. In Proc. of NeurIPS\u201922, 2022.\nZ. Xu, H. van Hasselt, and D. Silver. Meta-gradient reinforcement learning. In Proc. of NeurIPS\u201918,\n2018.\nC. Ying, A. Klein, E. Christiansen, E. Real, K. Murphy, and F. Hutter. NAS-Bench-101: Towards\nreproducible neural architecture search. In Proc. of ICML\u201919, 2019.\nA. Zela, J. Siems, L. Zimmer, J. Lukasik, M. Keuper, and F. Hutter. Surrogate NAS benchmarks:\nGoing beyond the limited search spaces of tabular NAS benchmarks. In Proc. of ICLR\u201922, 2022.\nB. Zhang, R. Rajan, L. Pineda, N. Lambert, A. Biedenkapp, K. Chua, F. Hutter, and R. Calandra. On\nthe importance of hyperparameter optimization for model-based reinforcement learning. In Proc.\nof AISTATS\u201921, 2021.\n14", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2548, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1121b124-b685-45d2-869e-aa87e1d805cf": {"__data__": {"id_": "1121b124-b685-45d2-869e-aa87e1d805cf", "embedding": null, "metadata": {"page_label": "15", "file_name": "ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 18789005, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "66cb1223-55f7-46b6-948e-64b7c60f34e0", "node_type": "4", "metadata": {"page_label": "15", "file_name": "ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 18789005, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "39c977b6944e9c3da6377725efd510ba100a2ecd2373288e9fe5041a2cbde9d4", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Checklist\n1. For all authors...\n(a) Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s\ncontributions and scope? [Yes]\n(b) Did you describe the limitations of your work? [Yes]\n(c) Did you discuss any potential negative societal impacts of your work? [NA]\n(d) Have you read the ethics review guidelines and ensured that your paper conforms to\nthem? [Yes]\n2. If you are including theoretical results...\n(a) Did you state the full set of assumptions of all theoretical results? [NA]\n(b) Did you include complete proofs of all theoretical results? [NA]\n3. If you ran experiments (e.g. for benchmarks)...\n(a) Did you include the code, data, and instructions needed to reproduce the main experi-\nmental results (either in the supplemental material or as a URL)? [Yes]\n(b) Did you specify all the training details (e.g., data splits, hyperparameters, how they\nwere chosen)? [Yes]\n(c) Did you report error bars (e.g., with respect to the random seed after running experi-\nments multiple times)? [Yes]\n(d) Did you include the total amount of compute and the type of resources used (e.g., type\nof GPUs, internal cluster, or cloud provider)? [Yes]\n4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets...\n(a) If your work uses existing assets, did you cite the creators? [Yes]\n(b) Did you mention the license of the assets? [Yes]\n(c) Did you include any new assets either in the supplemental material or as a URL? [Yes]\n(d) Did you discuss whether and how consent was obtained from people whose data you\u2019re\nusing/curating? [NA]\n(e) Did you discuss whether the data you are using/curating contains personally identifiable\ninformation or offensive content? [NA]\n5. If you used crowdsourcing or conducted research with human subjects...\n(a) Did you include the full text of instructions given to participants and screenshots, if\napplicable? [NA]\n(b) Did you describe any potential participant risks, with links to Institutional Review\nBoard (IRB) approvals, if applicable? [NA]\n(c) Did you include the estimated hourly wage paid to participants and the total amount\nspent on participant compensation? [NA]\n15", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2172, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "6ecf0ce4-36a4-4950-8b27-ba2925a00db5": {"__data__": {"id_": "6ecf0ce4-36a4-4950-8b27-ba2925a00db5", "embedding": null, "metadata": {"page_label": "16", "file_name": "ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 18789005, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "977d7711-45c1-4695-b982-1da44768bb07", "node_type": "4", "metadata": {"page_label": "16", "file_name": "ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 18789005, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "b6d0b142202a3dbaf71df18068f84e9754c077dea392b63176a2dcbeb39da061", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "A Dataset Description\nOur dataset is hosted on HuggingFace for easy and continued access: https://huggingface.co/\ndatasets/autorl-org/arlbench. See the page there for in-depth information on data value and\ndistributions. The croissant meta-data can also be found here: https://github.com/automl/\narlbench/blob/experiments/croissant_metadata.json. Everything needed to reproduce\nthe data can be found in the \u2019experiments\u2019 branch of our GitHub repository: https://github.com/\nautoml/arlbench/tree/experiments It is intended to be used in continued research on AutoRL,\ne.g., by using it in warm-starting HPO optimizers, proposing novel analysis methods or meta-learning\non it. The dataset is in CSV format, making it easily readable. We license it under the BSD-3 license.\nB Reproducing Our Results\nBelow we describe our hardware setup and steps for reproducing our experiments.\nB.1 Execution Environment\nTo conduct the experiments detailed in this paper, we pooled various computing resources. Below,\nwe describe the different hardware setups used for CPU and GPU-based training.\nCPU Jobs. Compute nodes with CPUs of type AMD Milan 7763, 2.45 GHz, each 2x 64 cores,\n128GB main memory\nGPU Jobs.\nV100 Cluster: Compute nodes with CPUs of type Intel Xeon Platinum 8160, 2.1 GHz, each 2x 24\ncores, 180GB main memory. Each node comes with 16 GPUs of type NVIDIA V100-SXM2\nwith NVLink and 32 GB HBM2, 5120 CUDA cores, 640 Tensor cores, 128 GB main\nmemory\nA100 Cluster: Compute nodes with CPUs of type AMD Milan 7763, 2.45 GHz, each 2x 64 cores,\n126GB main memory. Each node comes with 1 GPU of type NVIDIA A100 with NVLink\nand 40 GB HBM2, 6,912 CUDA cores, 432 Tensor cores, 16 GB main memory\nH100 Cluster: Compute nodes with CPUs of type Intel Xeon 8468 Sapphire, 2.1 GHz, each 2x 48\ncores, 512GB main memory. Each node comes with 4 GPUs of type NVIDIA H100 with\nNVLink and 96 GB HBM2e, 16,896 CUDA cores, 528 Tensor cores, 512 GB main memory\nB.2 Experiment Code\nWe provide code and runscripts for all of our dependencies in the \u2019experiments\u2019 branch of our\nrepository: https://github.com/automl/arlbench/tree/experiments.\nAll scripts relating to the dataset creation and HPO optimizer runs are in \u2019runscripts\u2019. For the\nperformance over time plots, see \u2019runtime_comparison\u2019. \u2019rs_data_analysis\u2019 contains the analysis\nof the HPO landscapes. For the subset selection, see \u2019subset_selection\u2019. The subset validation and\nperformance over time plots for the HPO optimizers can be found in \u2019subset_validation\u2019. Additionally,\nwe provide all of our raw data in \u2019results_finished\u2019 with \u2019results_combined\u2019 containing dataset\naggregates. Instructions for the usage of all of these can be found in the ReadMe file of that branch.\nC Maintenance Plan\nFollowing [Eggensperger et al., 2021] and [Pfisterer et al., 2022], we provide a maintenance plan\nfor the future of ARLBench. For our feature roadmap, see: https://github.com/orgs/automl/\nprojects/17\nWho Maintains. ARLBench is being developed and maintained as a cooperation between the\nInstitute of AI at the Leibniz University of Hannover and the chair for AI Methodology at the RWTH\nAachen University.\n16", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3133, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d7975a85-b489-45c5-9464-6d178db2937f": {"__data__": {"id_": "d7975a85-b489-45c5-9464-6d178db2937f", "embedding": null, "metadata": {"page_label": "17", "file_name": "ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 18789005, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c02d6c69-294b-470a-942d-4b12350a0dc1", "node_type": "4", "metadata": {"page_label": "17", "file_name": "ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 18789005, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "b9c00f78624f44327f3a618d851ac6585266c516264b8aa929bcaafa8fcb3a19", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Contact. Improvement requests, issues and questions can be asked via issue in our GitHub repository:\nhttps://github.com/automl/arlbench. The contact e-mails we provide can be used for the\nsame purpose.\nErrata. There are no errata.\nLibrary Updates. We plan on updating the library with new features, specifically more extensive state\nfeatures, more algorithms and added environment frameworks. We also welcome updates via external\npull requests which we will test and integrate into ARLBench. Changes will be communicated via\nthe changelog of our GitHub and PyPI releases.\nSupport for Older Versions. Older versions of ARLBench will continue to be available on PyPI and\nGitHub, but we will only provide limited support.\nContributions. Contributions to ARLBench from external parties are welcome in any form, be\nextensions to other environment frameworks, added algorithms or extensions of the core interface. We\ndescribe the contribution process in our documentation: https://automl.github.io/arlbench/\nmain/CONTRIBUTING.html. These contributions are managed via GitHub pull requests.\nDependencies. All of our dependencies are listed here in the GitHub repository: https://github.\ncom/automl/arlbench/blob/main/pyproject.toml.\nD Overview of all Environments\nTables 2, 3 and 4 provide an overview of all environments we executed for a given RL algorithm,\nincluding the underlying framework used and the number of environment steps for training.\nCategory Framework Name #timesteps\nALE Envpool BattleZone-v5 107\nALE Envpool DoubleDunk-v5 107\nALE Envpool Phoenix-v5 107\nALE Envpool Qbert-v5 107\nALE Envpool NameThisGame-v5 107\nBox2D Envpool LunarLander-v2 106\nBox2D Envpool LunarLanderContinuous-v2 106\nBox2D Envpool BipedalWalker-v3 106\nWalker Brax Ant 5 \u00b7 107\nWalker Brax HalfCheetah 5 \u00b7 107\nWalker Brax Hopper 5 \u00b7 107\nWalker Brax Humanoid 5 \u00b7 107\nClassic Control Gymnax Acrobot-v1 106\nClassic Control Gymnax CartPole-v1 105\nClassic Control Gymnax MountainCarContinuous-v0 2 \u00b7 104\nClassic Control Gymnax MountainCar-v0 106\nClassic Control Gymnax Pendulum-v1 105\nxland XLand-xland xland-DoorKey-5x5 106\nxland XLand-xland xland-EmptyRandom-5x5 105\nxland XLand-xland xland-FourRooms 106\nxland XLand-xland xland-Unlock 106\nTable 2: Environments for PPO.\n17", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2249, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f0c14dff-579d-4aa6-b557-609646e3d187": {"__data__": {"id_": "f0c14dff-579d-4aa6-b557-609646e3d187", "embedding": null, "metadata": {"page_label": "18", "file_name": "ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 18789005, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "421542e0-89b9-4ac0-be2f-3f46dd755936", "node_type": "4", "metadata": {"page_label": "18", "file_name": "ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 18789005, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "70a1e36966b7ffc15c75ce0824fbb156b6c99fc94a13121216a2f849784c44ad", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Category Framework Name #timesteps\nALE Envpool BattleZone-v5 107\nALE Envpool DoubleDunk-v5 107\nALE Envpool Phoenix-v5 107\nALE Envpool Qbert-v5 107\nALE Envpool NameThisGame-v5 107\nBox2D Envpool LunarLander-v2 106\nClassic Control Gymnax Acrobot-v1 105\nClassic Control Gymnax CartPole-v1 5 \u00b7 104\nClassic Control Gymnax MountainCar-v0 12 \u00b7 104\nxland XLand-xland xland-DoorKey-5x5 106\nxland XLand-xland xland-EmptyRandom-5x5 105\nxland XLand-xland xland-FourRooms 106\nxland XLand-xland xland-Unlock 106\nTable 3: Environments for DQN.\nCategory Framework Name #timesteps\nBox2D Envpool LunarLanderContinuous-v2 5 \u00b7 105\nBox2D Envpool BipedalWalker-v2 5 \u00b7 105\nWalker Brax Ant 5 \u00b7 106\nWalker Brax HalfCheetah 5 \u00b7 106\nWalker Brax Hopper 5 \u00b7 106\nWalker Brax Humanoid 5 \u00b7 106\nClassic Control Gymnax MountainCarContinuous-v0 5 \u00b7 104\nClassic Control Gymnax Pendulum-v1 2 \u00b7 104\nTable 4: Environments for SAC.\nE Performance Comparisons with Other RL Frameworks\nTo validate the correctness of our implementations beyond unit testing, we compare their performance\non a range of environments to established RL frameworks in terms of reward achieved and running\ntime. Additionally, running time comparisons for each environment category are shown in Figures 8,\n9, 10, 11, and12.\nFigure 13 compares the resulting learning curves between ARLBench, SB3 and the Brax default\nagent. We use the Brax agent instead of SB3 since SB3 performed significantly worse than we\nexpected. In most of our tests we observed very similar behavior with the other framworks and\nARLBench outperforming the other two times and showing comparable learning curves for all\nother experiments. In the case of DQN, where SB3 performed better on CartPole and worse on\nPong, SB3\u2019s results look noisy, possibly causing this discrepancy in both directions. SB3 also\noutperforms ARLbench on Pendulum, though this difference is fairly slight. For PPO on Ant,\nARLBench performs quite a bit better than the Brax default agent, though their performances of SAC\nare the same. Inconsistencies in learning curves can be due to differences in the implementations of\nalgorithms1 and environments [V oelcker et al., 2024]. Overall, this shows that our algorithms perform\non par with other commonly used implementations.\n1https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/\n18", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2329, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ca9b4ec0-af45-4100-8415-e91b88997757": {"__data__": {"id_": "ca9b4ec0-af45-4100-8415-e91b88997757", "embedding": null, "metadata": {"page_label": "19", "file_name": "ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 18789005, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "74817cbc-3a70-4f7e-afcf-319e94e9405c", "node_type": "4", "metadata": {"page_label": "19", "file_name": "ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 18789005, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "9bc9c22e732d10759f8974f0e3fdd7d9469da3a4d532b5acb378e303a6f2b989", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Figure 8: Running time comparison for an HPO method of32 RL runs, using 10 seeds each on the full\nenvironment set and our subsets between ARLBench and SB3 for ALE. JAX-related speedup factors\nare 3.21 for PPO and 2.83 for DQN. Total speedup factors of the ARLBench subset compared to the\nfull set of environments in SB3 are 8.03 for PPO and 7.08 for DQN. Note: As ALE environments\nhave discrete action spaces, SAC is left out in this figure.\nFigure 9: Running time comparison for an HPO method of 32 RL runs, using 10 seeds each on the\nfull environment set and our subsets between ARLBench and SB3 for Box2D. JAX-related speedup\nfactors are 1.97 for PPO, 2.04 for DQN, and 6.64 for SAC. Total speedup factors of the ARLBench\nsubset compared to the full set of environments in SB3 are 5.92 for PPO and 13.27 for SAC. As no\nBox2D environment is part of the DQN subset, there is no total speedup factor for DQN.\nFigure 10: Running time comparison for an HPO method of 32 RL runs, using 10 seeds each on the\nfull environment set and our subsets between ARLBench and SB3 for Classic Control. JAX-related\nspeedup factors are 4.72 for PPO, 1.89 for DQN, and 6.10 for SAC. Total speedup factors of the\nARLBench subset compared to the full set of environments in SB3 are 5.68 for DQN and 12.2 for\nSAC. As no Classic Control environment are part of the PPO subset, there is no total speedup factor\nfor PPO.\n19", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1399, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "095659a5-e835-49c9-88a5-6d805e252209": {"__data__": {"id_": "095659a5-e835-49c9-88a5-6d805e252209", "embedding": null, "metadata": {"page_label": "20", "file_name": "ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 18789005, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c9639cdd-73e5-4d6e-b377-ce6712cb0b81", "node_type": "4", "metadata": {"page_label": "20", "file_name": "ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 18789005, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "1ec5c1fe25b62ea7ddf9f78846acca25cbf827198a1c7cb466724774c4ced6bb", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Figure 11: Running time comparison for an HPO method of 32 RL runs, using 10 seeds each on the\nfull environment set and our subsets between ARLBench and SB3 for Brax/MuJoCo. JAX-related\nspeedup factors are 5.4 for PPO and 5.64 for SAC. Total speedup factors of the ARLBench subset\ncompared to the full set of environments in SB3 are 21.62 for PPO, and 11.28 for SAC. Note: As\nMuJoCo environments have continuious action spaces DQN is left out in this figure.\nFigure 12: Running time comparison for an HPO method of 32 RL runs, using 10 seeds each on\nthe full environment set and our subsets between ARLBench and SB3 for XLand. JAX-related\nspeedup factors are 10.02 for PPO and 3.72 for DQN. Total speedup factors of the ARLBench subset\ncompared to the full set of environments in SB3 are 40.07 for PPO and 7.42 for DQN. Note: As ALE\nenvironments have discrete action spaces SAC is left out in this figure.\n20", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 908, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "038aa9a8-eda7-4a76-a321-7a3111fa3843": {"__data__": {"id_": "038aa9a8-eda7-4a76-a321-7a3111fa3843", "embedding": null, "metadata": {"page_label": "21", "file_name": "ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 18789005, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "172d5992-a73d-4be5-a1e6-fb1f1ef24036", "node_type": "4", "metadata": {"page_label": "21", "file_name": "ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 18789005, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "9f0cfb891bbb603d205abedb5dca9ef845c9534c2801078942fa6748739aa211", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Figure 13: Performance comparisons of ARLBench and the Brax default agent, StableBaselines3 [Raf-\nfin et al., 2021]\nTable 5 show the speedups we achieve in terms of running time over StableBaselines3 (SB3) [Raffin\net al., 2021] on all subsets while Tables 6, 7 and 8 list the same for each environment individually.\nAs already discussed, we see a consistently large speedup, most pronounced for the Brax walkers\nwith a factor of 8.57 for PPO and 10.67 for SAC. The lowest speedups we observe are still close to a\nfactor of 2: 1.89 for DQN CartPole as well as 1.91 and 1.97 respectively for PPO LunarLander and\nLunarLanderContinuous.\nAlgorithm Set ARLBench SB3 Speedup\nPPO All 2h 7.18h 3.59\nPPO Subset 0.74h 2.58h 3.48\nDQN All 3.87h 11.10h 2.87\nDQN Subset 1.55h 4.49h 2.89\nSAC All 1.25h 7.23h 5.78\nSAC Subset 0.62h 3.61h 5.82\nSum All 7.12h 25.51h 3.58\nSum Subset 2.91h 10.68h 3.67\nTable 5: Running time comparisons for a single RL training between ARLBench and StableBaselines3\n(SB3) [Raffin et al., 2021] on the set of all environments and the selected subset. The numbers are\nbased on the results in Tables 6, 7, and 8. For each environment category, we use the running times\nfrom the experiments to estimate the overall running time for this category.\n21", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1256, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c8537cde-2dc5-4bf0-bd57-d729dc28a90c": {"__data__": {"id_": "c8537cde-2dc5-4bf0-bd57-d729dc28a90c", "embedding": null, "metadata": {"page_label": "22", "file_name": "ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 18789005, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4b5a143f-9773-4431-a175-8cacbb178d4d", "node_type": "4", "metadata": {"page_label": "22", "file_name": "ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 18789005, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "8c8bbfca9418d55b436c28d10a3e3a878fb6b84298932f3e269193684fd06e12", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Category Framework Name ARLBench SB3 Speedup\nClassic Control Envpool CartPole-v1 5.42s 25.54s 4.72\nClassic Control Envpool Pendulum-v1 5.98s 21.87s 3.66\nBox2D Envpool LunarLander-v2 125.87s 248.54s 1.97\nBox2D Envpool LunarLanderContinuous-v2 162.77s 311.25s 1.91\nMinigrid XLand Minigrid-DoorKey-5x5 54.38s 544.71s 10.01\nALE Envpool Pong-v5 1161.11s 3728.58s 3.21\nWalker Envpool Ant 194.09s 1048.84s\nWalker Brax Ant 122.28s 1048.84s* 8.57\nAverage 4.86\nTable 6: Speedup of ARLBench PPO compared to StableBaselines3 (SB3) [Raffin et al., 2021] on\ndifferent envrionments. *Note: Since SB3 is not compatible with Brax without manual interface\nadapation, we compare the results of MuJoCo + SB3 and Brax + ARLBench.\nCategory Framework Name ARLBench SB3 Speedup\nClassic Control Envpool CartPole-v1 21.5s 40.68s 1.89\nBox2D Envpool LunarLander-v2 95.27s 194.61s 2.04\nMinigrid XLand Minigrid-DoorKey-5x5 187.73s 697.64s 3.71\nALE Envpool Pong-v5 2602.69s 7373.40s 2.83\nAverage 2.15\nTable 7: Speedup of ARLBench DQN compared to StableBaselines3 (SB3) [Raffin et al., 2021] on\ndifferent envrionments.\nCategory Framework Name ARLBench SB3 Speedup\nClassic Control Envpool Pendulum-v1 17.32s 105.67s 6.10\nBox2D Envpool LunarLanderContinuous-v2 365.45s 2425.04s 6.64\nWalker Envpool Ant 930.06s 5245.17s\nWalker Brax Ant 491.70s 5245.17s* 10.67\nAverage 7.80\nTable 8: Speedup of ARLBench SAC compared to StableBaselines3 (SB3) [Raffin et al., 2021] on\ndifferent envrionments. *Note: Since SB3 is not compatible with Brax without manual interface\nadapation, we compare the results of MuJoCo + SB3 and Brax + ARLBench.\nF Algorithm Search Spaces\nFor all algorithms, we used extensive search spaces covering almost all hyperparameters that are\ncommonly optimized. The search spaces for PPO, DQN and SAC are presented in Table 9, 10 and 11\nrespectively. We choose not to optimize some hyperparameters to keep the computational resources\nconstant for each training. The default values for these hyperparameters for each environment domain\nhave been inferred from stable-baselines3 zoo Raffin [2020] and Google Brax\u2019s hyperparameter\nsweeps Freeman et al. [2021] and are shown in Tables 9, 10 and 11 accordingly. The search space\nfor the batch sizes was set to one power of two below and above its baseline value.\n22", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2287, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "13384d8f-78b1-4a39-a226-92d45f6438f1": {"__data__": {"id_": "13384d8f-78b1-4a39-a226-92d45f6438f1", "embedding": null, "metadata": {"page_label": "23", "file_name": "ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 18789005, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "92aad19a-bea7-4108-a444-0579e31347d6", "node_type": "4", "metadata": {"page_label": "23", "file_name": "ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 18789005, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "0253affd518bdb51c2cdef370bacd21524e4da225ad0a49484a150c00a190875", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Hyperparameter Box2D XLand ALE CC Brax\nbatch size {32, 64, 128} {128, 256, 512} {512, 1024, 2048}\nnumber of environments 16 8 2048\nnumber of steps 1024 32 128 32 512\nupdate epochs 4 10 4\nlearning rate log([10\u22126, 10\u22121])\nentropy coefficient [0.0, 0.5]\ngae lambda [0.8, 0.9999]\npolicy clipping [0.0, 0.5]\nvalue clipping [0.0, 0.5]\nnormalize advantages {Yes, No}\nvalue function coefficient [0.0, 1.0]\nmax gradient norm [0.0, 1.0]\nTable 9: The hyperparameter search space for PPO. To keep the computational costs feasible we\nchoose not to optimize the number of steps per epoch and update epochs.\nHyperparameter ALE Box2D CC XLand\nbatch size {16, 32, 64} {64, 128, 256} {32, 64, 128}\nnumber of environments 8 4 1 4\nbuffer priority sampling {Yes, No}\nbuffer \u03b1 [0.01, 1.0]\nbuffer \u03b2 [0.01, 1.0]\nbuffer \u03f5 log([10\u22127, 10\u22123])\nbuffer size [1024, 106]\ninitial epsilon [0.5, 1.0]\ntarget epsilon [0.001, 0.2]\nlearning rate log([10\u22126, 10\u22121])\nlearning starts [1, 2048]\nuse target network {Yes, No}\ntarget update interval [1, 2000]\nTable 10: The hyperparameter search space for DQN. The target update interval is a conditional\nhyperparameter that is only optimized when a target network is used. Similarly, buffer \u03b1, \u03b2 and \u03f5 are\nonly optimized when priority sampling is used. If the number of training steps is smaller than the\nupper limit of the buffer size, the buffer size limit is reduced accordingly.\n23", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1389, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "aa9fe60e-26f8-4409-ad57-21771ca2f3b0": {"__data__": {"id_": "aa9fe60e-26f8-4409-ad57-21771ca2f3b0", "embedding": null, "metadata": {"page_label": "24", "file_name": "ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 18789005, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "bda5dfbd-7903-40b5-ba83-8e3df737970f", "node_type": "4", "metadata": {"page_label": "24", "file_name": "ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 18789005, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "753d1abb38d8e310c9ca54607a74fc226f30f2d1e9115dabdb5f59a53a2d5a6b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Hyperparameter Box2D CC Brax\nbatch size {128, 256, 512} {256, 512, 1024} {512, 1024, 2048}\nnumber of environments 1 1 64\nbuffer priority sampling {Yes, No}\nbuffer \u03b1 [0.01, 1.0]\nbuffer \u03b2 [0.01, 1.0]\nbuffer \u03f5 log([10\u22127, 10\u22123])\nbuffer size [1024, 106]\nlearning rate log([10\u22126, 10\u22121])\nlearning starts [1, 2048]\nuse target network {Yes, No}\ntau [0.01, 1.0]\nreward scale log([0.1, 10])\nTable 11: The hyperparameter search space for SAC. The hyperparameter tau is a conditional\nparameter that is only optimized when a target network is used. Similarly, buffer \u03b1, \u03b2 and \u03f5 are only\noptimized when priority sampling is used. If the number of training steps is smaller than the upper\nlimit of the buffer size, the buffer size limit is reduced accordingly.\nG Subset Selection\nWe provide additional information on the subset selection in the form of explanations, alternative\nselection methods, and a more detailed look into the results, including environment weights.\nG.1 Additional Explanation\nWe select the subset based on the hyperparameter landscapes obtained through Sobol sampling. For\neach randomly sampled hyperparameter configuration, the RL algorithm is trained and evaluated on a\nseparate evaluation environment. As evaluation metric, we collect the cumulative episode rewards,\ni.e., return of 128 episodes and calculate the mean. The mean return for environment e \u2208 Eand\nhyperparameter configuration \u03bb \u2208 \u039b is denoted as re\n\u03bb and calculated as\nre\n\u03bb = Es\u223cS\n\"\n1\n128 \u00b7\n128X\ni=1\nTX\nt=1\nR(i,s)\nt\n#\n(2)\nwhere S is the set of 10 random seeds, T is the number of steps in the i-th evaluation episode, and\nRt corresponds to the reward at time step t in the i-th episode for seed s. As reward ranges differ\nacross environments, we have to apply normalization to compare the corresponding results. However,\nnormalization based on human expert scores is not possible as done by Aitchison et al. [2023] for\nthe selection of Atari-5. We apply rank-based normalization to compare the rewards of different\nenvironments. By ranking the rewards re\n\u03bb of all configurations \u03bb \u2208 \u039b for a given environment e, with\nhigher rewards corresponding to higher ranks, and normalizing these ranks to the interval [0, 1], we\nobtain the performance scores pe\n\u03bb. The performance score pe\n\u03bb for each configuration \u03bb in environment\ne is given by:\npe\n\u03bb = rank(re\n\u03bb) \u2212 min\u03bb\u2032\u2208\u039b rank(re\n\u03bb\u2032 )\nmax\u03bb\u2032\u2208\u039b rank(re\n\u03bb\u2032 ) \u2212 min\u03bb\u2032\u2208\u039b rank(re\n\u03bb\u2032 ), (3)\nwhere rank(re\n\u03bb) denotes the rank of the return re\n\u03bb among all returns in environment e.\nFor the regression model, we use the LinearRegression class from the scikit-learn2 package. This\nrelies on the ordinary least squares method for fitting, which is invariant to permutation of features,\ni.e., environments.\n2https://scikit-learn.org\n24", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2738, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "de41b2a1-bb42-4f87-aa67-a19c14d90a55": {"__data__": {"id_": "de41b2a1-bb42-4f87-aa67-a19c14d90a55", "embedding": null, "metadata": {"page_label": "25", "file_name": "ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 18789005, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1d5bff59-7709-4441-a877-da551aa6e560", "node_type": "4", "metadata": {"page_label": "25", "file_name": "ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 18789005, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "590f824815b0fcc755e6076ed50b9f06087270fdd30aab22aaae60c7e3949a19", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "G.2 Alternative Methods\nIn addition to our chosen method of rank-based normalization in combination with the Spearman\ncorrelation as a distance metric, we compare alternative normalization methods as well as MSE for\nthe distance. Figure 14 shows the validation error of different combinations while Figure 15 shows\nthe resulting Spearman correlation to the full environment set. While MSE might produce a good\nvalidation error, the resulting correlation is significantly worse than using the Spearman correlation\nfor the distance. Min-max normalization performs slightly worse than rank normalization for the\nvalidation error. Therefore we chose rank-based normalization with Spearman correlation for our\nsubset selection.\nFigure 14: Comparison of the validation error of different ranking methods and error functions based\non the subset size.\nFigure 15: Comparison of the validation error of different ranking methods and error functions based\non the subset size. Please note, that not all lines in this plot are visible due to overlaps. The reason is\nthat the approaches Ranks + Spearman and Ranks + MSE as well as Min-Max + MSE and Min-Max\nSpearman each results in the exact same Spearman correlation and thus are not distinguishable in the\nplot.\nG.3 Extended Subset Results\nIn addition to the environments in the subsets, we also provide the exact weights for each environment\nin the subsets in Table 12. Furthermore, Figures 16 and 17 show the optimization-over-time results\nfor DQN and SAC.\n25", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1499, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "aca76b28-4d08-4817-9461-a646fb9c2d47": {"__data__": {"id_": "aca76b28-4d08-4817-9461-a646fb9c2d47", "embedding": null, "metadata": {"page_label": "26", "file_name": "ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 18789005, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b380c5f2-b67f-477b-9c58-c85eb4cd8879", "node_type": "4", "metadata": {"page_label": "26", "file_name": "ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 18789005, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "fff786c0cd729be09621dd1dc9e5386b81051cae6ae93813e6f41980ca077d1f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Algorithm Environments (with predicted weights) \u03c1s\nPPO 0.21\u00d7 LunarLander, 0.21\u00d7 Humanoid, 0.18\u00d7 BattleZone, 0.96\n0.12\u00d7 Phoenix, 0.23\u00d7 XLand-EmptyRandom\nDQN 0.33\u00d7 Acrobot, 0.11\u00d7 NameThisGame, 0.22\u00d7 DoubleDunk 0.96\n0.12\u00d7 XLand-FourRooms, 0.18\u00d7 XLand-EmptyRandom\nSAC 0.32\u00d7 BipedalWalker,0.31\u00d7 HalfCheetah, 0.97\n0.15\u00d7 Hopper, 0.19\u00d7 MountainCarContinuous\nTable 12: The environment subsets selected for each algorithm with their Spearman correlation to the\nfull environment set.\nFigure 16: Anytime performance of the HPO methods for PPO.\nFigure 17: Anytime performance of the HPO methods for SAC.\nH Landscape Analysis\nWe use DeepCave [Sass et al., 2022] to analyze our performance dataset with regards to performance\ndistribution, hyperparameter importance and budget correlation over time. Please note that in some\ncases, results can be missing, due to consistent numerical errors in the analysis, e.g., in the case of\nSAC on Halfcheetah.\nH.1 Landscape Behaviour\nAlgorithm configuration landscapes are often found to show relatively benign structure, characterized\nby unimodal responses and compensatory or negligible interactions [Pushak and Hoos, 2018]. How-\never, in our experiments, we observe that some partial hyperparameter landscapes deviate from these\ntraits, displaying challenging structure instead. Figure 18 highlights the contrast between benign and\nadverse landscapes in our experiments, providing further insight into their differing characteristics.\n26", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1464, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "2c43f437-63d2-4def-aac7-0fb718e620b4": {"__data__": {"id_": "2c43f437-63d2-4def-aac7-0fb718e620b4", "embedding": null, "metadata": {"page_label": "27", "file_name": "ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 18789005, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "52030ca6-5ac9-426c-86af-f1b4f687e3da", "node_type": "4", "metadata": {"page_label": "27", "file_name": "ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 18789005, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "027c49f5cb51424169ec0b138a2b5dd36f1d2ec0f5263426fb72bf6d93db79f9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "ALE Qbert (DQN) CC Acrobot (PPO)\nCC Mountain Car (PPO) XLand Unlock (PPO)\nBrax Ant (PPO) Brax Hopper (PPO)\nFigure 18: Comparison of adverse landscapes on the left with typical benign landscapes on the\nright. Lighter is better, mean performance over 10 seeds. Adverse landscapes exhibit multi-modality,\nwhereas benign landscapes are uni-modal and display minimal to no hyperparameter interaction.\nH.2 Performance Distributions\nCompleting the results from the performance distributions comparison in Section 4.3, Figures 19 and\n20 show the distribution of scores for the domains and subsets of DQN and SAC, respectively. Just\nlike for PPO, there are fairly direct correspondences between selected environments and the score\ndistributions of the full domains. The only seeming exception is Box2D for DQN which has a lot\nof low scores that are not directly represented by one selected environment. Acrobot in that subset,\nhowever, covers a lot of such bad configurations even though it has higher performances overall.\n27", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1017, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1f2dec87-169e-4b85-ab8e-25d65f598eb7": {"__data__": {"id_": "1f2dec87-169e-4b85-ab8e-25d65f598eb7", "embedding": null, "metadata": {"page_label": "28", "file_name": "ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 18789005, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "be9bc060-3ff2-4dd7-8069-31c76fb3da0b", "node_type": "4", "metadata": {"page_label": "28", "file_name": "ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 18789005, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "cf6fa4affaa369336257984e9d12701c84ff9439de3d293ed880b607262fdd2f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Figure 19: Score distribution across environment domains and the selected subset of DQN.\nFigure 20: Score distribution across environment domains and the selected subset of SAC.\nH.3 Hyperparameter Importances\nTables 13, 14 and 15 show extended information on the number of important hyperparameters for\neach environment domain as well as the subset and full environment set. The set of top 3 interactions\nare shown in Table 16 for PPO, Table 17 for DQN and Table 18 for SAC. We also include the full set\nof importance plots for each environment in Figures 21, 22, 23 and 24 for PPO, Figures 25, 26 and 27\nfor DQN and Figure 28 for SAC.\nALE Box2D CC Xland Brax All Subset\n#HPs with over 10% importance 1.6 1.5 1.0 1.75 0.75 1.3 1.0\n#HPs with over 5% importance 1.6 1.5 3.4 2.25 1.75 2.2 1.2\n#HPs with over 3% importance 2.0 3.0 4.0 2.5 3.0 2.9 2.0\nTable 13: Fraction of hyperparameters with over 10%, 5%, and 3% importance on the full set and\nsubset for PPO.\nALE Box2D CC Xland All Subset\n#HPs with over 10% importance 2.0 1.0 1.0 2.25 1.77 2.0\n#HPs with over 5% importance 2.8 1.0 1.33 3.5 2.54 2.75\n#HPs with over 3% importance 3.8 2.0 2.33 4.0 3.38 3.5\nTable 14: Fraction of hyperparameters with over 10%, 5%, and 3% importance on the full set and\nsubset for DQN.\n28", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1268, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "9e34941c-a110-4bbc-be25-be12f0693b0c": {"__data__": {"id_": "9e34941c-a110-4bbc-be25-be12f0693b0c", "embedding": null, "metadata": {"page_label": "29", "file_name": "ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 18789005, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b2678f8e-18c8-4e4a-ab66-90bffda3334a", "node_type": "4", "metadata": {"page_label": "29", "file_name": "ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 18789005, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "02fbe89170501df1d583988364445e7d2c53297e7ff8db748a38a949e9189d7d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Box2D CC Brax All Subset\n#HPs with over 10% importance 1.0 1.5 1.0 1.14 0.75\n#HPs with over 5% importance 1.0 3.0 1.5 1.86 1.25\n#HPs with over 3% importance 1.0 3.5 2.75 2.71 2.5\nTable 15: Fraction of hyperparameters with over 10%, 5%, and 3% importance on the full set and\nsubset for SAC.\nALE BattleZone ALE DoubleDunk\nALE Phoenix ALE QBert\nALE NameThisGame\nFigure 21: Hyperparameter importances for PPO: ALE.\n29", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 413, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "905dfce7-a1e7-4f3c-af86-f8e98f1f38d3": {"__data__": {"id_": "905dfce7-a1e7-4f3c-af86-f8e98f1f38d3", "embedding": null, "metadata": {"page_label": "30", "file_name": "ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 18789005, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0db00f16-1ab3-44b3-864f-239a8b6c0c83", "node_type": "4", "metadata": {"page_label": "30", "file_name": "ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 18789005, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "52f34ac20b9965f01ddbe35e44a9f9665a387fc03c685b45385742909c831a77", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Environment Highest Interaction 2nd Highest Interaction 3rd Highest Interaction\nALE QBert (ent coef, learning rate) : 0.38 (learning rate, vf coef) : 0.02 (learning rate, max grad norm) : 0.01\nALE DoubleDunk (ent coef, vf coef) : 0.09 (ent coef, vf clip eps) : 0.07 (ent coef, normalize advantage) : 0.04\nALE Phoenix (learning rate, vf clip eps) : 0.11 (ent coef, learning rate) : 0.08 (learning rate, vf coef) : 0.08\nALE NameThisGame (clip eps, vf clip eps) : 0.06 (ent coef, vf coef) : 0.05 (clip eps, ent coef) : 0.05\nALE BattleZone (learning rate, normalize advantage) : 0.05 (learning rate, vf coef) : 0.05 (clip eps, learning rate) : 0.04\nBox2D LunarLander (ent coef, learning rate) : 0.02 (clip eps, learning rate) : 0.02 (learning rate, vf coef) : 0.02\nBox2D LunarLanderContinuous (clip eps, normalize advantage) : 0.09 (clip eps, learning rate) : 0.07 (clip eps, max grad norm) : 0.04\nBox2D BipedalWalker (learning rate, normalize advantage) : 0.03 (ent coef, normalize advantage) : 0.02 (ent coef, vf clip eps) : 0.02\nCC Acrobot (learning rate, minibatch size) : 0.07 (max grad norm, minibatch size) : 0.05 (learning rate, max grad norm) : 0.02\nCC CartPole (ent coef, learning rate) : 0.06 (gae lambda, learning rate) : 0.04 (learning rate, vf coef) : 0.03\nCC MountainCar (clip eps, ent coef) : 0.07 (max grad norm, vf coef) : 0.04 (ent coef, vf coef) : 0.04\nCC Pendulum (learning rate, max grad norm) : 0.05 (ent coef, learning rate) : 0.04 (ent coef, normalize advantage) : 0.03\nCC ContinuousMountainCar (clip eps, ent coef) : 0.04 (ent coef, normalize advantage) : 0.03 (normalize advantage, vf coef) : 0.02\nXLand DoorKey (learning rate, normalize advantage) : 0.19 (max grad norm, minibatch size) : 0.13 (ent coef, normalize advantage) : 0.03\nXLand EmptyRandom (learning rate, normalize advantage) : 0.08 (learning rate, vf coef) : 0.03 (ent coef, learning rate) : 0.02\nXLand FourRooms (learning rate, vf clip eps) : 0.04 (ent coef, learning rate) : 0.04 (clip eps, vf clip eps) : 0.03\nXLand Unlock (clip eps, ent coef) : 0.05 (ent coef, max grad norm) : 0.04 (ent coef, vf clip eps) : 0.03\nBrax Ant (clip eps, learning rate) : 0.09 (learning rate, max grad norm) : 0.06 (learning rate, minibatch size) : 0.06\nBrax Hopper (learning rate, vf clip eps) : 0.08 (learning rate, max grad norm) : 0.08 (learning rate, vf coef) : 0.07\nBrax Hopper (clip eps, learning rate) : nan (clip eps, vf coef) : nan (learning rate, vf coef) : nan\nBrax Humanoid (vf clip eps, vf coef) : 0.22 (gae lambda, vf coef) : 0.08 (clip eps, vf coef) : 0.07\nTable 16: Interactions effects for PPO via fANOV A.\n30", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2597, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f7d4e4db-c9f0-4f00-83c8-a60cac82696d": {"__data__": {"id_": "f7d4e4db-c9f0-4f00-83c8-a60cac82696d", "embedding": null, "metadata": {"page_label": "31", "file_name": "ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 18789005, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "75ac6d6d-352b-4f98-a0bd-368c6ed4130d", "node_type": "4", "metadata": {"page_label": "31", "file_name": "ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 18789005, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "2c13d87dade1bb2a47f4ff7db26400baa731f4d5916c8f4a3647497d83dfb2d4", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Environment Highest Interaction 2nd Highest Interaction 3rd Highest Interaction\nALE Qbert (buffer batch size, learning rate) : 0.08 (learning rate, target epsilon) : 0.06 (learning rate, target update interval) : 0.02\nALE DoubleDunk (buffer alpha, learning rate) : 0.05 (buffer alpha, target epsilon) : 0.04 (buffer alpha, target update interval) : 0.03\nALE Phoenix (buffer batch size, target epsilon) : 0.05 (buffer alpha, target epsilon) : 0.04 (buffer alpha, learning rate) : 0.04\nALE NameThisGame (learning rate, learning starts) : 0.05 (learning rate, target epsilon) : 0.04 (buffer alpha, learning rate) : 0.04\nALE BattleZone (buffer size, initial epsilon) : 0.07 (buffer alpha, learning rate) : 0.03 (buffer alpha, initial epsilon) : 0.03\nBox2D LunarLander (buffer size, learning rate) : 0.04 (buffer alpha, learning rate) : 0.03 (learning rate, learning starts) : 0.03\nCC Acrobot (learning rate, target update interval) : 0.05 (buffer alpha, learning rate) : 0.01 (initial epsilon, learning rate) : 0.01\nCC CartPole (buffer epsilon, learning rate) : 0.03 (learning rate, target epsilon) : 0.02 (learning rate, learning starts) : 0.02\nCC MountainCar (learning rate, target update interval) : 0.11 (buffer alpha, learning rate) : 0.04 (buffer alpha, target update interval) : 0.02\nXLand DoorKey (buffer alpha, learning rate) : 0.06 (buffer size, target epsilon) : 0.06 (learning rate, learning starts) : 0.04\nXLand EmptyRandom (buffer alpha, learning rate) : 0.09 (buffer alpha, initial epsilon) : 0.06 (buffer alpha, target epsilon) : 0.05\nXLand FourRooms (buffer alpha, learning rate) : 0.04 (buffer size, target epsilon) : 0.04 (buffer epsilon, learning starts) : 0.02\nXLand Unlock (buffer alpha, learning starts) : 0.06 (buffer alpha, buffer size) : 0.06 (buffer alpha, learning rate) : 0.04\nTable 17: Interaction effects for DQN hyperparameters via fANOV A.\n31", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1871, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "4c3d9a1d-e919-4e2f-a20d-3e36318e751c": {"__data__": {"id_": "4c3d9a1d-e919-4e2f-a20d-3e36318e751c", "embedding": null, "metadata": {"page_label": "32", "file_name": "ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 18789005, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e7c3358f-0526-4599-8e79-510d33e5ccb0", "node_type": "4", "metadata": {"page_label": "32", "file_name": "ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 18789005, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "175dc6f9f07cab6c94406620c23f833387f44b94dccd1a9b11142aa682431382", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Environment Highest Interaction 2nd Highest Interaction 3rd Highest Interaction\nBox2D LunarLanderContinuous (buffer batch size, learning rate) : 0.04 (buffer size, learning rate) : 0.03 (buffer batch size, buffer size) : 0.02\nBox2D BipedalWalker (learning rate, reward scale) : 0.05 (buffer alpha, learning rate) : 0.04 (buffer beta, learning rate) : 0.02\nCC Pendulum (alpha, alpha auto) : nan (alpha, buffer alpha) : nan (alpha, buffer batch size) : nan\nCC MountainCarContinuous (buffer size, tau) : 0.1 (buffer alpha, buffer beta) : 0.05 (buffer alpha, learning starts) : 0.04\nBrax Ant (learning rate, tau) : 0.1 (learning rate, reward scale) : 0.07 (buffer size, learning rate) : 0.02\nBrax HalfCheetah (buffer beta, buffer size) : 0.03 (buffer beta, tau) : 0.03 (buffer beta, learning starts) : 0.03\nBrax Hopper (learning rate, tau) : 0.15 (learning rate, reward scale) : 0.09 (learning rate, use target network) : 0.03\nBrax Humanoid (buffer alpha, tau) : 0.05 (buffer beta, learning starts) : 0.03 (buffer batch size, learning starts) : 0.02\nTable 18: Interaction effects for SAC hyperparameters via fANOV A.\n32", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1115, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "47be159d-1115-48f6-9e8f-e694ce5475a2": {"__data__": {"id_": "47be159d-1115-48f6-9e8f-e694ce5475a2", "embedding": null, "metadata": {"page_label": "33", "file_name": "ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 18789005, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1202ddf4-35be-44e0-a50b-828eb2214f51", "node_type": "4", "metadata": {"page_label": "33", "file_name": "ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 18789005, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "65f548656df65f51831fe0527bc9239bdec5077f586bb33cc1540e2c067aed1f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Brax Ant Brax Hopper\nBrax Humanoid Box2D LunarLander\nBox2D BipedalWalker\nFigure 22: Hyperparameter importances for PPO: Brax and Box2D.\n33", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 138, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ec754534-c0cd-477c-9935-9e9072ae23ee": {"__data__": {"id_": "ec754534-c0cd-477c-9935-9e9072ae23ee", "embedding": null, "metadata": {"page_label": "34", "file_name": "ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 18789005, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "40ab73bf-551a-4c7a-b187-2ab6de80c003", "node_type": "4", "metadata": {"page_label": "34", "file_name": "ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 18789005, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "67c56123971eedff24af4affdd3a7840acc09363d3e29bc0e9da024bbb7fbcd9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "CC CartPole CC Cont. Mt Car\nCC Mt Car CC Pendulum\nCC Acrobot\nFigure 23: Hyperparameter importances for PPO: Classic Control.\n34", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 127, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "cc9edfa4-b41c-4f8a-9dce-ef190a745511": {"__data__": {"id_": "cc9edfa4-b41c-4f8a-9dce-ef190a745511", "embedding": null, "metadata": {"page_label": "35", "file_name": "ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 18789005, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2cbed6e4-0a7c-4751-a651-52749f1a6c5b", "node_type": "4", "metadata": {"page_label": "35", "file_name": "ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 18789005, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "cab0e6ba1c08d55394d62e142ffaa7049ab00bf000c98e7f801c91b9243ae3bf", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "XLand DoorKey XLand Unlock\nXLand FourRooms XLand EmptyRandom\nFigure 24: Hyperparameter importances for PPO: XLand.\n35", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 117, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "9c24ac73-4ba8-4e98-a3c5-c0ab98e13e40": {"__data__": {"id_": "9c24ac73-4ba8-4e98-a3c5-c0ab98e13e40", "embedding": null, "metadata": {"page_label": "36", "file_name": "ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 18789005, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "719b18b2-98fd-42d5-851f-b599fda485c3", "node_type": "4", "metadata": {"page_label": "36", "file_name": "ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 18789005, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "f3428cfca611d591eea35dee7fc468f1d9d0be1d6e939adaf6d5808fcbaf709f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "ALE BattleZone ALE DoubleDunk\nALE Phoenix ALE QBert\nALE NameThisGame\nFigure 25: Hyperparameter importances for DQN: ALE.\n36", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 123, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "26bd3f70-0b07-46d6-be7b-5a3d198382f6": {"__data__": {"id_": "26bd3f70-0b07-46d6-be7b-5a3d198382f6", "embedding": null, "metadata": {"page_label": "37", "file_name": "ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 18789005, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "23f78ec7-dd81-4ceb-abe6-8758531c22b3", "node_type": "4", "metadata": {"page_label": "37", "file_name": "ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 18789005, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "99edf2ebbca946aeff4a710657ef4670f20de038960422532292bf63450f7f32", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "CC CartPole CC Mt Car\nCC\nAcrobot Box2D LunarLander\nFigure 26: Hyperparameter importances for DQN: Classic Control and Box2D.\nXLand DoorKey XLand Unlock\nXLand FourRooms XLand EmptyRandom\nFigure 27: Hyperparameter importances for DQN: XLand.\n37", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 242, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8c0f5768-51d1-49a0-959d-24137b11d742": {"__data__": {"id_": "8c0f5768-51d1-49a0-959d-24137b11d742", "embedding": null, "metadata": {"page_label": "38", "file_name": "ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 18789005, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ab8ffe0a-1b6e-4183-b065-c154033730a8", "node_type": "4", "metadata": {"page_label": "38", "file_name": "ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 18789005, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "b525c0463485504af21d488d4870ea00c7b933c35a0218728afbec36811e9ae6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Brax Ant Brax Hopper\nBrax Humanoid CC Cont. Mt Car\nCC Pendulum Box2D BipedalWalker\nFigure 28: Hyperparameter importances for SAC.\n38", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 132, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "97c8df9d-f9fb-46f4-a96c-99fede51fc28": {"__data__": {"id_": "97c8df9d-f9fb-46f4-a96c-99fede51fc28", "embedding": null, "metadata": {"page_label": "39", "file_name": "ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 18789005, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "cd73c69f-1842-4df7-badf-dcd8a335ce67", "node_type": "4", "metadata": {"page_label": "39", "file_name": "ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 18789005, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "4989cc0c9701acad07f9d15575d64bb258686454d1cdafc33355631b9d1776ad", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "H.4 Budget Correlations\nWe show the budget correlation plots for all budgets (Figures 29, 30, 31 and 32 for PPO, Figures 33,\n34 and 35 for DQN and Figure 36 for SAC). We see that most correlations are strong or very strong\nwith some numerical inconsistencies in mountaincar and brax halfcheetah. XLand is the the only\ndomain with a strong trend over time, for DQN most correlations only become strong after about\n30-40% of training while the same is true for DoorKey of PPO. These are consistent in the domains,\nthough: we see low or no correlations for Brax (likely due to numerical issues) and in Classic Control,\nstrong correlations otherwise. ALE and Box2D are other domains with strong correlations while\nXLand tends to need a warmup phase.\nALE BattleZone ALE DoubleDunk\nALE Phoenix ALE QBert\nALE NameThisGame\nFigure 29: Budget correlations for PPO: ALE.\n39", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 862, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "82cf48be-d09d-43ea-b837-cfe29b706ca6": {"__data__": {"id_": "82cf48be-d09d-43ea-b837-cfe29b706ca6", "embedding": null, "metadata": {"page_label": "40", "file_name": "ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 18789005, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "785f800a-a126-42a7-9386-bcedeec6db60", "node_type": "4", "metadata": {"page_label": "40", "file_name": "ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 18789005, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "f9f9d40132dcb04e09cf11790f00c866a8dfa9745c44bb60f894b9f578276ae5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Brax Ant Brax Hopper\nBrax Humanoid Box2D LunarLander\nFigure 30: Budget correlations for PPO: Brax and Box2D.\n40", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 111, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b8117518-d915-4af7-8db3-328ecc5e68b3": {"__data__": {"id_": "b8117518-d915-4af7-8db3-328ecc5e68b3", "embedding": null, "metadata": {"page_label": "41", "file_name": "ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 18789005, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7464b228-9029-41fd-8004-b8efec212ad5", "node_type": "4", "metadata": {"page_label": "41", "file_name": "ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 18789005, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "3ece0eda22840eb32f7bc935d82e0ab0a5376cd6554f7f37cb0f38cf3e33b2ee", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "CC CartPole CC Cont. Mt Car\nCC Mt Car CC Pendulum\nCC Acrobot\nFigure 31: Budget correlations for PPO: Classic Control.\n41", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 120, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "95815446-9a5c-4022-a768-b3cbc4581d16": {"__data__": {"id_": "95815446-9a5c-4022-a768-b3cbc4581d16", "embedding": null, "metadata": {"page_label": "42", "file_name": "ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 18789005, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "63e2cf92-f789-4614-a348-79e8023ec8f9", "node_type": "4", "metadata": {"page_label": "42", "file_name": "ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 18789005, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "21632bb990287575e123969668038629823b8ad057393c2926ec11fa45ee0666", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "XLand DoorKey XLand Unlock\nXLand FourRooms XLand EmptyRandom\nFigure 32: Budget correlations for PPO: XLand.\n42", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 110, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "98459118-1db2-4db3-b6b0-a4b7fe998470": {"__data__": {"id_": "98459118-1db2-4db3-b6b0-a4b7fe998470", "embedding": null, "metadata": {"page_label": "43", "file_name": "ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 18789005, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3c43b511-d047-477b-9ed0-767b9e119d32", "node_type": "4", "metadata": {"page_label": "43", "file_name": "ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 18789005, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "93254a95b4760a369cf152139700dbd87e14bdcd39143efac679d33c7a858caa", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "ALE BattleZone ALE DoubleDunk\nALE Phoenix ALE QBert\nALE NameThisGame\nFigure 33: Budget correlations for DQN: ALE.\n43", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 116, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3c85efd1-b1fd-4827-aee1-b6388a5bd45a": {"__data__": {"id_": "3c85efd1-b1fd-4827-aee1-b6388a5bd45a", "embedding": null, "metadata": {"page_label": "44", "file_name": "ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 18789005, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9569d4d3-8096-4a1f-bff2-a0dbcf3f27fb", "node_type": "4", "metadata": {"page_label": "44", "file_name": "ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 18789005, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "98e0d7c98b14adb62bc810536c6e13da99346ee98a1c27c38329c536083808f4", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "CC CartPole CC Mt Car\nCC\nAcrobot Box2D LunarLander\nFigure 34: Budget correlations for DQN: Classic Control and Box2D.\nXLand DoorKey XLand Unlock\nXLand FourRooms XLand EmptyRandom\nFigure 35: Budget correlations for DQN: XLand.\n44", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 228, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d87f3985-8d37-433a-9e3b-478f9d9918cd": {"__data__": {"id_": "d87f3985-8d37-433a-9e3b-478f9d9918cd", "embedding": null, "metadata": {"page_label": "45", "file_name": "ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 18789005, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c0be8d2d-0c6b-483a-802f-cb32f8777943", "node_type": "4", "metadata": {"page_label": "45", "file_name": "ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 18789005, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "a6f8fbfdd030bcacc6267eddf1222191798fa832c14830df23a6de00cd332f23", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Brax Ant Brax Hopper\nBrax Humanoid CC Cont. Mt Car\nCC Pendulum\nFigure 36: Budget correlations for SAC.\nI Resource Consumption\nAll running time results are stated in Table 19 and were obtained using the same setup on the H100\ncluster as described in Appendix B.1.\n45", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 265, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a0142fbc-da85-4fc1-b4ae-5e8f0a176d7a": {"__data__": {"id_": "a0142fbc-da85-4fc1-b4ae-5e8f0a176d7a", "embedding": null, "metadata": {"page_label": "46", "file_name": "ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 18789005, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e664eb65-e022-4f27-a078-8b36382578f3", "node_type": "4", "metadata": {"page_label": "46", "file_name": "ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 18789005, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "aa476ea20312a01028acc5d25dbc0d92fef3e823ab80d059d50d12c7fbb7c099", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Algorithm Environment Platform Running Time [s] Total Running Time [h]\nDQN Acrobot-v1 CPU 26.1 37.13\nDQN BattleZone-v5 GPU 2967.69 4220.72\nDQN CartPole-v1 CPU 10.27 14.6\nDQN DoubleDunk-v5 GPU 2918.08 4150.16\nDQN LunarLander-v2 CPU 34.47 49.03\nDQN MiniGrid-DoorKey-5x5 CPU 81.44 115.82\nDQN MiniGrid-EmptyRandom-5x5 CPU 30.32 43.12\nDQN MiniGrid-FourRooms CPU 172.31 245.07\nDQN MiniGrid-Unlock CPU 94.68 134.65\nDQN MountainCar-v0 CPU 19.4 27.59\nDQN NameThisGame-v5 GPU 2970.15 4224.22\nDQN Phoenix-v5 GPU 2710.29 3854.64\nDQN Qbert-v5 CPU 2943.79 4186.73\nPPO Acrobot-v1 CPU 15.34 21.82\nPPO BattleZone-v5 GPU 1154.29 1641.66\nPPO BipedalWalker-v3 CPU 89.83 127.76\nPPO CartPole-v1 CPU 7.95 11.3\nPPO DoubleDunk-v5 GPU 1083.08 1540.38\nPPO LunarLander-v2 CPU 162.97 231.78\nPPO LunarLanderContinuous-v2 CPU 300.47 427.33\nPPO MiniGrid-DoorKey-5x5 CPU 81.23 115.52\nPPO MiniGrid-EmptyRandom-5x5 CPU 26.37 37.5\nPPO MiniGrid-FourRooms CPU 179.84 255.77\nPPO MiniGrid-Unlock CPU 112.33 159.76\nPPO MountainCar-v0 CPU 13.21 18.79\nPPO MountainCarContinuous-v0 CPU 7.68 10.93\nPPO NameThisGame-v5 GPU 1130.46 1607.76\nPPO Pendulum-v1 CPU 13.81 19.64\nPPO Phoenix-v5 GPU 955.17 1358.46\nPPO Qbert-v5 CPU 1145.07 1628.54\nPPO ant GPU 220.87 314.13\nPPO halfcheetah GPU 851.99 1211.73\nPPO hopper GPU 458.43 651.98\nPPO humanoid GPU 338.6 481.57\nSAC BipedalWalker-v3 CPU 486.32 691.66\nSAC LunarLanderContinuous-v2 CPU 381.22 542.17\nSAC MountainCarContinuous-v0 CPU 557.13 792.36\nSAC Pendulum-v1 CPU 111.76 158.95\nSAC ant GPU 824.95 1173.26\nSAC halfcheetah GPU 2194.59 3121.2\nSAC hopper GPU 1263.28 1796.66\nSAC humanoid GPU 871.83 1239.93\nTable 19: Running times of algorithms and environments and respective platforms they were executed\non. The column Running Time represents the duration, in seconds, of a single training session, while\nTotal Running Timeindicates the cumulative hours spent on all experiments conducted for a given\nenvironment.\nEach experiment was run for 4096 total runs. This results in a total CPU running timer of 10 105.34\nh and GPU running time of 32 588.46 h (including 40.54 h GPU hours for the running time\nexperiments).\n46", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2117, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c7ff93f1-8506-4778-b8ac-ec6bb60716f3": {"__data__": {"id_": "c7ff93f1-8506-4778-b8ac-ec6bb60716f3", "embedding": null, "metadata": {"page_label": "47", "file_name": "ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 18789005, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e2b81dee-580f-459c-93d1-8675ef524f9c", "node_type": "4", "metadata": {"page_label": "47", "file_name": "ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 18789005, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "bdd45160a4b9375562b72d746cdbc3f4178276cfd55ad41733458310d5acbade", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "J Additional HPO Results\nThis section contains optimizer results per environment. In order to compare optimizer performance\nacross environments, we apply normalization, as done in subset selection. However, to incorporate the\ndistances between incumbents, we use explicit returns rather than ranks. In particular, we normalize\nusing the minimum and maximum returns obtained during the initial landscaping experiments\ndescribed in Section 4.1. This is done per environment, to allow for a comparison of optimizers across\nenvironments while preserving relative order and distances. This yields optimizer scores between\n0 and 1 for our experiments, since no optimizer was able to find a better incumbent performance\nas encountered through 256 Sobol-sampled configurations. However, due to numerical instabilities\nin the Box2D and Brax environments, we fixed the minimum in these domains to -200 and -2000,\nrespectively, to exclude artificially low values that distort the normalization.\nFigures 37, 38, 39, 40, and 41 show results for PPO. Figures 42, 43, 42, and 44 show results for DQN.\nFigures 42, 45 and 46 show results for SAC.\nWhen comparing the results shown in Figure 16, we can see that SMAC outperforms PBT and RS in\nterms of mean performance. However, if we would benchmark the optimizers on another, arbitrary\nselected set of environments, e.g. QBert-v5 (ALE), Phoenix-v5 (ALE), and MiniGrid-DoorKey-5x5\n(XLand), the results would look very different from the observation on our selected subset. This, in\nfact, emphasizes the importance of the subset of environments on which a HPO method is evaluated.\nThe differences show, that in order to reliable benchmark and compare HPO methods, they need to\nbe evaluated on the same, representative subset of environments.\n47", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1775, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "638cc97e-3205-43c7-9318-35d21326b496": {"__data__": {"id_": "638cc97e-3205-43c7-9318-35d21326b496", "embedding": null, "metadata": {"page_label": "48", "file_name": "ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 18789005, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0915ef3f-e0e8-434c-aaed-6f76249a9327", "node_type": "4", "metadata": {"page_label": "48", "file_name": "ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 18789005, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "87e203125fb92eaae4f7b0f8285339a10a7b011fa380f000740ab0934e605fcf", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Figure 37: Anytime performance of optimizers for PPO: ALE. Figure shows 95%-confidence\nintervals.\n48", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 100, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d09c51d2-a16f-4467-8e52-87f38c5c8a19": {"__data__": {"id_": "d09c51d2-a16f-4467-8e52-87f38c5c8a19", "embedding": null, "metadata": {"page_label": "49", "file_name": "ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 18789005, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7fe2e34a-f0dc-4292-a292-0859f7f1af33", "node_type": "4", "metadata": {"page_label": "49", "file_name": "ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 18789005, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "72ff3bc422ffed25fca706cd8d4cc8e36b1e0ad499ae3404f80165b8d0c33c3e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Figure 38: Anytime performance of optimizers for PPO: Brax. Figure shows 95%-confidence\nintervals.\n49", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 101, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "0ccfc0a3-f0ed-4b5c-8a2c-220beac42363": {"__data__": {"id_": "0ccfc0a3-f0ed-4b5c-8a2c-220beac42363", "embedding": null, "metadata": {"page_label": "50", "file_name": "ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 18789005, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "78e70001-7d64-4d27-96d3-282a8683115f", "node_type": "4", "metadata": {"page_label": "50", "file_name": "ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 18789005, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "769d150c93f6d553d2fb7fe1f94b82e61955b8c7ba29cf50dfcc07ee3b76dcbc", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Figure 39: Anytime performance of optimizers for PPO: Classic Control. Figure shows 95%-\nconfidence intervals.\n50", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 113, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "2d258e3b-4a67-4755-990a-80f4922545ad": {"__data__": {"id_": "2d258e3b-4a67-4755-990a-80f4922545ad", "embedding": null, "metadata": {"page_label": "51", "file_name": "ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 18789005, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "aa4f36f2-41f5-4dba-8939-ca44ceb388fd", "node_type": "4", "metadata": {"page_label": "51", "file_name": "ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 18789005, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "4d6856fe86da81c2d42cd87e678d871dda1ce7b139fe0e65c53305599d8174fe", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Figure 40: Anytime performance of optimizers for PPO: Box2D. Figure shows 95%-confidence\nintervals.\nFigure 41: Anytime performance of optimizers for PPO: XLand. Figure shows 95%-confidence\nintervals.\n51", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 202, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "93a1b5d1-ca47-4add-ae2e-7125d83c0346": {"__data__": {"id_": "93a1b5d1-ca47-4add-ae2e-7125d83c0346", "embedding": null, "metadata": {"page_label": "52", "file_name": "ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 18789005, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d5581b74-0a29-461b-bf09-713c5c91b2fa", "node_type": "4", "metadata": {"page_label": "52", "file_name": "ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 18789005, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "914b707a8509aa797d11b30e9b4cd39205371e53e8eea0204a2910688d9203ce", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Figure 42: Anytime performance of optimizers for DQN: Classic Control and Box2D. Figure shows\n95%-confidence intervals.\n52", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 122, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a4d2e71b-c680-4609-bc9b-41fe242ea5c9": {"__data__": {"id_": "a4d2e71b-c680-4609-bc9b-41fe242ea5c9", "embedding": null, "metadata": {"page_label": "53", "file_name": "ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 18789005, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9fd6f04d-b140-46e8-995e-5cecd157e86f", "node_type": "4", "metadata": {"page_label": "53", "file_name": "ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 18789005, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "8333c9038e20fe358e14effeea599b0706c7f982c7fe6d89e10e43f04d619d3a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Figure 43: Anytime performance of optimizers for DQN: ALE. Figure shows 95%-confidence\nintervals.\n53", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 100, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5700acd5-cb6b-4fd6-988a-ebaf5cb8f7d8": {"__data__": {"id_": "5700acd5-cb6b-4fd6-988a-ebaf5cb8f7d8", "embedding": null, "metadata": {"page_label": "54", "file_name": "ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 18789005, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "acf5c0ed-ee63-4cb0-8cfa-6cc1f117ce51", "node_type": "4", "metadata": {"page_label": "54", "file_name": "ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 18789005, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "d9a25087cc9bf1e8ad8e8a0699a8b192e73a6c7c7d15b76de5f99d8e4a7e0a84", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Figure 44: Anytime performance of optimizers for DQN: XLand. Figure shows 95%-confidence\nintervals.\nFigure 45: Anytime performance of optimizers for SAC: Brax. Figure shows 95%-confidence\nintervals.\n54", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 201, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "0aac1b45-9757-4fc1-991f-1de25e023c02": {"__data__": {"id_": "0aac1b45-9757-4fc1-991f-1de25e023c02", "embedding": null, "metadata": {"page_label": "55", "file_name": "ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 18789005, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f5a327cb-be6e-40af-9f97-03cf8d03f531", "node_type": "4", "metadata": {"page_label": "55", "file_name": "ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\ARLBench Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 18789005, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "b3f1c5df0c47f63669fc6deb46e7c603f702d7590b3ca70471386ebd63d7ed4e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Figure 46: Anytime performance of optimizers for SAC: Classic Control and Box2D. Figure shows\n95%-confidence intervals.\n55", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 122, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "840afaa9-710e-4d94-832e-fd8112d90102": {"__data__": {"id_": "840afaa9-710e-4d94-832e-fd8112d90102", "embedding": null, "metadata": {"page_label": "1", "file_name": "Causal-Paced Deep Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Causal-Paced Deep Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 741276, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e5956047-3129-4ab4-957d-3b78d6caa8fd", "node_type": "4", "metadata": {"page_label": "1", "file_name": "Causal-Paced Deep Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Causal-Paced Deep Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 741276, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "aed9c451cb2d6633bcee70fd8d5cd7d3e59594170df2c3abdce721d0889f39c3", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "arXiv:2507.02910v1  [cs.LG]  24 Jun 2025\nCausal-Paced Deep Reinforcement Learning\nCausal-Paced Deep Reinforcement Learning\nGeonwoo Cho Jaegyun Im Doyoon Kim Sundong Kim\n{gwcho.public, jaegyun.public, dykim.research}@gmail.com,\nsundong@gist.ac.kr\nGwangju Intsitute of Science and Technology\nAbstract\nDesigning effective task sequences is crucial for curriculum reinforcement learning\n(CRL), where agents must gradually acquire skills by training on intermediate tasks.\nA key challenge in CRL is to identify tasks that promote exploration, yet are sim-\nilar enough to support effective transfer. While recent approach suggests compar-\ning tasks via their Structural Causal Models (SCMs), the method requires access to\nground-truth causal structures, an unrealistic assumption in most RL settings. In this\nwork, we propose Causal-Paced Deep Reinforcement Learning (CP-DRL), a curricu-\nlum learning framework aware of SCM differences between tasks based on interac-\ntion data approximation. This signal captures task novelty, which we combine with\nthe agent\u2019s learnability, measured by reward gain, to form a unified objective. Empir-\nically, CP-DRL outperforms existing curriculum methods on the Point Mass bench-\nmark, achieving faster convergence and higher returns. CP-DRL demonstrates re-\nduced variance with comparable final returns in the Bipedal Walker-Trivial setting,\nand achieves the highest average performance in the Infeasible variant. These results\nindicate that leveraging causal relationships between tasks can improve the structure-\nawareness and sample efficiency of curriculum reinforcement learning. We provide the\nfull implementation of CP-DRL to facilitate the reproduction of our main results at\nhttps://github.com/Cho-Geonwoo/CP-DRL.\n1 Introduction\nJust as a child first learns to crawl before walking and running, intelligent behavior in complex\nenvironments is rarely acquired in a single leap. Instead, learning unfolds through a gradual ac-\ncumulation of simpler skills that scaffold more advanced capabilities. This principle underlies the\nidea of curriculum learning in reinforcement learning (RL), where agents are trained on a structured\nsequence of tasks that progressively increase in complexity (Narvekar et al., 2020; Florensa et al.,\n2018; Klink et al., 2022). By mastering simpler subtasks before facing the target task, the agent can\navoid inefficient exploration and accelerate learning.\nA core challenge in curriculum reinforcement learning (CRL) is to measure how tasks differ, iden-\ntify what the agent has not yet learned, and expose it to novel yet transferable tasks (Hughes et al.,\n2024). Existing approaches typically approximate task differences using agent-centric signals, such\nas regret (Jiang et al., 2021) or value disagreement (Zhang et al., 2020), but these are inherently\npolicy-dependent and sensitive to noise. As a more principled alternative, Li et al. (2024) pro-\npose comparing tasks via differences in their Structural Causal Models (SCMs), enabling a policy-\nindependent and structure-aware measure of transferability.\nWhile SCM-based comparison provides a principled way to quantify task differences, it relies on\naccess to the true causal structure, which is rarely available in realistic RL environments (Zanga\net al., 2022). In this work, we proposeCausal-Paced Deep Reinforcement Learning (CP-DRL), a\n1", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3369, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "33e73d7f-8b91-45c8-9f38-1c676279eb82": {"__data__": {"id_": "33e73d7f-8b91-45c8-9f38-1c676279eb82", "embedding": null, "metadata": {"page_label": "2", "file_name": "Causal-Paced Deep Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Causal-Paced Deep Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 741276, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d4ffc73a-eef6-4665-af8c-3dcaaa015fa9", "node_type": "4", "metadata": {"page_label": "2", "file_name": "Causal-Paced Deep Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Causal-Paced Deep Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 741276, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "5d673a8b52c88b8b5af6d559bd0bceffb686bed37fe3d05663a3b4c79eb550a2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Causal-Paced Deep Reinforcement Learning\nTeacherStudentCurriculum\nTask ATrajectory 1\nTrajectory N\nTaskBTrajectory 1\nTrajectory KCausalDifference\nFigure 1: An overview of the CP-DRL. We estimate the causal difference between tasks based on\nthe observed trajectories in each task. This structural signal is used by the teacher to construct a\ncurriculum that gradually exposes the student to novel tasks.\nframework that addresses this limitation by approximating the SCM difference between tasks using\nonly interaction data. Specifically, we estimate the differences in state, action, transition, and re-\nward components, the key constituents of SCM difference in deterministic reinforcement learning,\nby measuring the disagreement across modular ensemble models. These component-wise disagree-\nments are then aggregated into a causal misalignment score, which serves to identify tasks whose\nunderlying causal structures remain unfamiliar to the agent. To balance novelty with learnability,\nwe combine the causal misalignment score with the agent\u2019s reward improvement, yielding a unified\nsignal that characterizes both structural novelty and learning potential. This unified signal is then in-\ncorporated into an optimal transport-based curriculum optimization framework (Klink et al., 2022),\nenabling the curriculum to prioritize tasks that are structurally informative yet learnable, and guide\ntraining smoothly toward the target distribution. This process is conceptually illustrated in Figure 1.\nWe conduct experiments on two RL benchmarks, Point Mass (PM) and Bipedal Walker (BW),\ndemonstrating that CP-DRL effectively balances exploration and transfer through causal reason-\ning. In the PM environment, CP-DRL achieves the highest return and converges faster than baseline\nmethods. In the Bipedal Walker-Trivial setting, it achieves comparable final performance with re-\nduced variance, and in the Infeasible setting, it achieves the highest mean return among all methods.\nThese results highlight CP-DRL\u2019s potential to generate structure-aware curricula that enhance gen-\neralization and sample efficiency in complex reinforcement learning environments.\n2 Preliminary\n2.1 Markov Decision Process and Contextual RL for curriculum learning\nWe assume each task or environment can be modeled as a Markov Decision Process (MDP) M,\ndefined by the tuple \u27e8S, A, p, r, \u03b3\u27e9, where S is the state space, A the action space, p(s\u2032 | s, a)\nwith s, s\u2032 \u2208 Sand a \u2208 Athe transition probability function, r(s, a) the reward function, and\n\u03b3 \u2208 [0, 1) the discount factor. The solution to an MDP is an optimal policy \u03c0(a|s) that maximizes\nthe expected return J(\u03c0) = E\u03c0 [P\u221e\nt=0 \u03b3tr(st, at)]. In some curriculum learning scenarios, tasks are\nparameterized or varied by some additional context c \u2208 C, where C denotes the space of possible\ncontexts (Hallak et al., 2015). For example, c could represent the goal location in a navigation task,\nthe layout of a maze, or physical properties of the environment. We can formalize a family of tasks\nas a Contextual MDP M(c) = \u27e8S, A, pc, rc, \u03b3\u27e9 where c influences the transition dynamics pc and\nreward function rc. Each value of c corresponds to a different MDP, but typically there is structure\nshared across contexts (e.g., common state and action spaces, and some common dynamics).\nThe curriculum learning problem can then be formulated as,\n\u27e8q\u2217\n0 , q\u2217\n1 , . . . , q\u2217\nT \u27e9 = arg max\n\u27e8q0,q1,...,qT \u27e9\nJ\n\u0000\n\u03c0T\n\u0001\nwhere \u03c0t+1 = Update(\u03c0t, qt) and qT = \u00b5. (1)\n2", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3473, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e0f8c644-ade6-4e71-b0af-6df46df74bcc": {"__data__": {"id_": "e0f8c644-ade6-4e71-b0af-6df46df74bcc", "embedding": null, "metadata": {"page_label": "3", "file_name": "Causal-Paced Deep Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Causal-Paced Deep Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 741276, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "00d68343-4b5d-4cf2-9fdf-009d363b8f77", "node_type": "4", "metadata": {"page_label": "3", "file_name": "Causal-Paced Deep Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Causal-Paced Deep Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 741276, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "a7191460b8e58b2cc19a740cbbf508c8c5a1a554292e3446afe1527d38c743cd", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Causal-Paced Deep Reinforcement Learning\nwhere T is a total number of curriculum steps, qt is the context distribution used at training step t,\n\u03c0t is the agent\u2019s policy after t updates, \u00b5 is a target task distribution and Update(\u03c0t, qt) denotes\nthe learning step to get next policy \u03c0t+1 using current policy \u03c0t and the data sampled from qt. The\nfinal policy \u03c0T is trained via the sequence (q0, . . . , qT ) and is evaluated on \u00b5. The goal is to find a\ncurriculum (q0, . . . , qT ) that yields a policy \u03c0T performing well on the target distribution.\n2.2 Structural Causal Model in RL\nFigure 2: Causal graph in our RL setting.\nSolid arrows: environment-induced transi-\ntions and reward generation, Dotted arrows:\ndenote policy-induced dependencies.\nA Structural Causal Model (SCM) describes causal\nrelationships between variables (Pearl, 2009; Pearl\n& Bareinboim, 2011). An SCM M is defined as a\ntuple (U, V, F, P), where U is a set of exogenous\n(unobserved) variables, V is a set of endogenous\n(observed) variables, F = {fV : Pa(V ) \u222a UV \u2192\nV }V \u2208V is a set of structural functions, where each\nfV determines V based on its endogenous parents\nPa(V ) \u2286 V (endogenous variables that directly in-\nfluence V ) and exogenous inputs UV \u2286 U (exoge-\nnous variables that directly influence V ), and P(U)\nis the distribution over U, from which the exogenous variables are sampled.\nFrom an RL perspective, MDP can be interpreted through the lens of SCM, where the state spaceS,\nthe action space X, and the reward space Y are subsets of the set of endogenous variables V, and\nthe randomness in the reward and transition functions is captured by the set of exogenous variables\nU (Li et al., 2024). In this paper, we assume a deterministic setting without exogenous variables, i.e.,\nthere is no underlying randomness in the environment. The resulting causal structure is illustrated\nin Figure 2. Here, st, st+1 \u2208 S, at, at+1 \u2208 X, and rt, rt+1 \u2208 Y, and the structural functions in F\nmap (st, at) to both st+1 and rt.\n2.3 Curriculum reinforcement learning via constrained optimal transport\nOur method is built upon CURROT (Klink et al., 2022), which makes a curriculum using the optimal\ntransport problem with constraints.\npW (c) = arg min\np\nW2(p(c), \u00b5(c)) s.t.\n(\np(c) > 0 \u21d2 J(\u03c0, c) \u2265 \u03b4 \u2200c \u2208 C\nW2(p(c), q(c)) \u2264 \u03f5 (2)\nIt tries to minimize the 2-Wasserstein distance W2, the distance between the current source task\ndistribution p(c) and the target task distribution\u00b5(c), with two constraints: (1) among all tasks, only\nthose with nonzero probability under p(c)-i.e., tasks actually selected for training-must yield an\nexpected return above a threshold \u03b4 (which decreases over time), and (2) the Wasserstein distance\nbetween consecutive source task distributions p(c) and q(c) should be lower than the threshold\n\u03f5. This optimization enables the construction of a curriculum that smoothly approaches the target\ndistribution, while maintaining per-task expected return J(\u03c0, c) above a decreasing threshold \u03b4 and\nbounding the distributional shift between successive task distributions.\n3 Method\nWe propose a curriculum learning method that encourages the agent to explore tasks with underex-\nplored causal structures. To achieve this, we introduce the Causal Misalignment (CM) score, which\nquantifies how unfamiliar a task\u2019s causal structure is relative to the agent\u2019s previously learned expe-\nrience. We empirically analyze the relationship between the CM score and underlying causal differ-\nences in the CausalWorld environment (Ahmed et al., 2021). We incorporate this score into CUR-\nROT to iteratively update the task distribution. By guiding the agent toward tasks with high causal\nmisalignment, our method promotes more efficient exploration and fosters generalizable learning.\n3", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3762, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "4dd8fdc8-c9ba-4673-868c-62536cd5ae35": {"__data__": {"id_": "4dd8fdc8-c9ba-4673-868c-62536cd5ae35", "embedding": null, "metadata": {"page_label": "4", "file_name": "Causal-Paced Deep Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Causal-Paced Deep Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 741276, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "42ea8556-166d-4f2b-9fbf-1ad18a2a669e", "node_type": "4", "metadata": {"page_label": "4", "file_name": "Causal-Paced Deep Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Causal-Paced Deep Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 741276, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "69ccc5980fca7747dda6eb6d773e23eae93cc6b5676d0b0f48651c12f2bd8e9e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8497417e-65b9-439c-8c6e-e223e194a739", "node_type": "1", "metadata": {}, "hash": "cc0ee920488a78b4d5ff7037b8571e88173bd640ff5df17f0adcc23c718c1604", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Causal-Paced Deep Reinforcement Learning\n3.1 Prioritizing Causally underexplored Tasks in Curriculum Learning\nTo construct an effective curriculum, it is essential to accurately quantify how a new task differs from\npreviously encountered ones in terms of learning dynamics and structural complexity (Hughes et al.,\n2024). If the SCMs of tasks were available, they could serve as a principled basis for comparing tasks\nin terms of their underlying causal mechanisms (Li et al., 2024). However, direct access to SCMs\nis rarely feasible in realistic reinforcement learning environments, and uncovering such structures\nfrom purely observational data remains a challenging problem (Zanga et al., 2022; Zeng et al.,\n2024). In deterministic settings, the interplay among state, action, transition, and reward defines a\nstructured dependency that implicitly captures the task\u2019s causal structure. Thus, the causal difference\nbetween two tasks can be approximated by aggregating the differences in these four components.\nTo estimate each component-wise difference, we train dedicated models using trajectories collected\nfrom previously experienced tasks and compute disagreement among ensemble predictions.\nState encoder: zs\nt \u223c q\u03d5s(zs\nt | st)\nState decoder: \u02c6st \u223c p\u03b8s(\u02c6st | zs\nt )\nAction encoder: za\nt \u223c q\u03d5a(za\nt | at)\nAction decoder: \u02c6at \u223c p\u03b8a(\u02c6at | za\nt )\nTransition predictor: \u02c6st+1 \u223c p\u03d5\u03c4 (\u02c6st+1 | st, at)\nReward predictor: \u02c6rt \u223c p\u03d5r (\u02c6rt | st, at)\nWe train six separate neural networks. (\u02c6s, \u02c6a, \u02c6st+1, \u02c6rt) denote the model\u2019s reconstruction or prediction\nof a corresponding variable. The state encoderq\u03d5s(zs\nt | st) and the state decoderp\u03b8s(\u02c6st | zs\nt ) encode\nthe current statest into a latent representationzs\nt and reconstruct it to the state\u02c6st. The action encoder\nq\u03d5a(za\nt | at) and the action decoder p\u03b8a(\u02c6at | za\nt ) encode the action at into a latent representation\nza\nt and reconstruct it to the action \u02c6at. The transition predictor p\u03d5\u03c4 (\u02c6st+1 | st, at) predicts the next\nstate based on the current state and action, approximating the environment\u2019s transition function. The\nreward predictor p\u03d5r (\u02c6rt | st, at) predicts rewards for the current state and action, modeling the\nenvironment\u2019s reward function. The detailed architecture of these neural networks is provided in\nAppendix B.3.\nLstate = \u2225\u02c6st \u2212 st\u22252\n2 + \u03b21 \u00b7 KL(q\u03d5s(zs\nt | st) | N(0, I))\nLaction = \u2225\u02c6at \u2212 at\u22252\n2 + \u03b22 \u00b7 KL(q\u03d5a(za\nt | at) | N(0, I))\nLtransition = \u2225\u02c6st+1 \u2212 st+1\u22252\n2\nLreward = \u2225\u02c6rt \u2212 rt\u22252\n2\n(3)\nThese models are trained using separate loss functions, each guiding a distinct component of the\ncausal structure. The state and action losses, Lstate and Laction, guide the model to accurately re-\nconstruct the state and action, respectively, ensuring that the latent representations capture sufficient\ninformation to regenerate the original inputs. These losses also regularize the latent distributions\ntoward a standard Gaussian prior via KL divergence as in Higgins et al. (2017). The transition loss,\nLtransition, guides the model to approximate the transition dynamics by predicting the next state given\nthe current state and action. Similarly, the reward loss, Lreward, promotes accurate modeling of the\nreward function by supervising reward prediction from the current state and action.\nWhile component-wise neural networks enable the separate modeling of individual environment\ncomponents, simply training these models is insufficient to assess the causal difference between a\nnew task and previously learned tasks. To quantify this difference, we train an ensemble of K net-\nworks for each of the four components-state, action, transition, and reward-and measure the struc-\ntural unawareness of each component through ensemble disagreement, computed as the standard\ndeviation of predictions across ensemble members as in Zhang et al. (2020).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3820, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8497417e-65b9-439c-8c6e-e223e194a739": {"__data__": {"id_": "8497417e-65b9-439c-8c6e-e223e194a739", "embedding": null, "metadata": {"page_label": "4", "file_name": "Causal-Paced Deep Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Causal-Paced Deep Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 741276, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "42ea8556-166d-4f2b-9fbf-1ad18a2a669e", "node_type": "4", "metadata": {"page_label": "4", "file_name": "Causal-Paced Deep Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Causal-Paced Deep Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 741276, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "69ccc5980fca7747dda6eb6d773e23eae93cc6b5676d0b0f48651c12f2bd8e9e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4dd8fdc8-c9ba-4673-868c-62536cd5ae35", "node_type": "1", "metadata": {"page_label": "4", "file_name": "Causal-Paced Deep Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Causal-Paced Deep Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 741276, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "3c940b41264069940f83281a1a72673684c644c9cf084f637d27f3fc2f7fd139", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "These losses also regularize the latent distributions\ntoward a standard Gaussian prior via KL divergence as in Higgins et al. (2017). The transition loss,\nLtransition, guides the model to approximate the transition dynamics by predicting the next state given\nthe current state and action. Similarly, the reward loss, Lreward, promotes accurate modeling of the\nreward function by supervising reward prediction from the current state and action.\nWhile component-wise neural networks enable the separate modeling of individual environment\ncomponents, simply training these models is insufficient to assess the causal difference between a\nnew task and previously learned tasks. To quantify this difference, we train an ensemble of K net-\nworks for each of the four components-state, action, transition, and reward-and measure the struc-\ntural unawareness of each component through ensemble disagreement, computed as the standard\ndeviation of predictions across ensemble members as in Zhang et al. (2020).\nDisagreementi = std\n\u0012n\n\u02c6y(k)\ni (\u00b7)\noK\nk=1\n\u0013\n, i \u2208 {state, action, transition, reward} (4)\n4", "mimetype": "text/plain", "start_char_idx": 2820, "end_char_idx": 3912, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "304af189-e79f-41c3-ba62-c9005b3f9a55": {"__data__": {"id_": "304af189-e79f-41c3-ba62-c9005b3f9a55", "embedding": null, "metadata": {"page_label": "5", "file_name": "Causal-Paced Deep Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Causal-Paced Deep Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 741276, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e578675a-57f3-4346-a9cf-63d534aeaf60", "node_type": "4", "metadata": {"page_label": "5", "file_name": "Causal-Paced Deep Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Causal-Paced Deep Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 741276, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "254b534d133afa0bd32e43232fc0d8a44702398b6c5a84c22a750fe186fc7ed1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Causal-Paced Deep Reinforcement Learning\nHere, \u02c6y(k)\ni denotes the prediction of thek-th ensemble member for componenti. A high level of dis-\nagreement indicates that the task\u2019s causal structure significantly differs from previously encountered\ntasks, which we interpret as causal misalignment, suggesting that the agent has not yet confidently\ncaptured the underlying structure. In contrast, low disagreement implies that the agent has developed\na reliable understanding of the corresponding environment component.\nWe aggregate these component-wise disagreements into a CM score, which quantifies the overall\ncausal structure unawareness associated with a given context. The CM score is computed as a\nweighted sum of the individual component disagreements.\nCM(c) =\nX\ni\nwi \u00b7 Disagreementi i \u2208 {state, action, transition, reward} (5)\nThis score quantifies the extent to which the structural properties of a given task deviate from those\ncaptured by the agent\u2019s current model. Tasks with higher misalignment scores are prioritized during\ncurriculum sampling, guiding the agent toward regions of the task space where its understanding of\nthe causal structure remains underdeveloped. Details of the weighting hyperparameters are provided\nin Appendix C.\nWe extend CURROT by incorporating causal misalignment into the context distribution update. At\neach iteration, the agent samples a batch of tasks from the current context distribution, collects inter-\naction trajectories, and updates its policy. For each task, component-wise ensemble disagreements\nare computed and aggregated into a CM score via a weighted sum. This score, combined with\nepisodic rewards, captures both structural novelty and learnability. These signals are stored and\nused to update the context distribution through an optimal transport-based objective (Eq. 2). The\nfull procedure is described in Appendix D.\n3.2 Disagreement as a Proxy for Causal Difference: A Toy Example in CausalWorld\nT1\n T2\n T3\n T4\nFigure 3: Visualization of tasks T1\u2013T4 from the CausalWorld General environment. In this\nsetup, agents receive rewards proportional to the intersection ratio between each block and the goal\nconfiguration. From T1 to T4, we progressively increase the block size, action magnitude, and\nreward scale, inducing increasing causal differences between tasks.\nTo assess the effectiveness of the CM score in capturing task-level causal differences, we conduct\na controlled experiment using the CausalWorld General environment (Ahmed et al., 2021). Causal-\nWorld provides a realistic and structured environment suitable for analyzing how modular disagree-\nment relates to underlying causal changes.\nWe construct four tasks, T1 through T4 (Figure 3), by progressively increasing the block size, action\nmagnitude, and reward scale, thereby inducing increasing levels of causal variation across tasks.\nThese modifications induce monotonic increases in causal differences relative to T1. Detailed task\nconfigurations are provided in Appendix E. We use the same model architecture described in Ap-\npendix B.3, except that, for this experiment, the hidden dimension of the transition and reward\npredictors is fixed to 64 and the ensemble size is set to 5. We designate T1 as the base task and pre-\ntrain on it for 10 episodes using a random policy. Then, we collect 5 episodes of random trajectories\nfrom each of the four tasks and evaluate the disagreement scores across state, action, transition, and\nreward models.\n5", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3475, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "361a193c-0f63-40e2-9ea9-7dc219c5cbc4": {"__data__": {"id_": "361a193c-0f63-40e2-9ea9-7dc219c5cbc4", "embedding": null, "metadata": {"page_label": "6", "file_name": "Causal-Paced Deep Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Causal-Paced Deep Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 741276, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4cf34b23-5a92-4095-96d3-94605545a6aa", "node_type": "4", "metadata": {"page_label": "6", "file_name": "Causal-Paced Deep Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Causal-Paced Deep Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 741276, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "55841b6733bff6d4e053aa799bf7e93cf734e231c04454e9e776bb5f221184b3", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Causal-Paced Deep Reinforcement Learning\nT1 T2 T3 T4\n0.530\n0.535\n0.540\n0.545\n0.550\n0.555\nTransition\nT1 T2 T3 T4\n0.20\n0.21\n0.22\n0.23\n0.24\n0.25\n0.26\nReward\nT1 T2 T3 T4\n1.28\n1.30\n1.32\n1.34\n1.36\nState\nT1 T2 T3 T4\n0.6\n0.7\n0.8\n0.9\n1.0\nAction\nFigure 4: Disagreement metrics across tasks T1\u2013T4. Each metric is computed after training on T1\nfor 10 episodes and sampling 5 episodes from each task. Transition, state, and action disagreements\nincrease monotonically, suggesting they effectively reflect causal differences. Reward disagreement\nshows no clear trend due to reward sparsity and random trajectory collection.\nFigure 4 presents the averaged disagreement metrics over three random seeds. Transition, state, and\naction disagreements increase monotonically from T1 to T4, aligning well with the intended causal\ndifferences induced by task variations. In contrast, reward disagreement shows no consistent trend,\nlikely due to sparse reward signals and the use of random policies during data collection. These\nresults validate that the proposed component-wise disagreement metrics-especially for state, action,\nand transition-serve as effective proxies for capturing structural differences across tasks.\nMoreover, further analysis in Appendix E reveals that transition disagreement is primarily influenced\nby both block size and action magnitude, action disagreement correlates with action scale, and state\ndisagreement increases with block size. These findings reinforce our interpretation that modular\ndisagreement meaningfully reflects the causal components underlying task variations.\n4 Experiments\n(a) Point Mass\n (b) BipedalWalker\nFigure 5: Illustrations of the environments used in our experiments.(a) Point Mass. (b) Bipedal-\nWalker.\nWe evaluated our method, CP-DRL, on two benchmark environments: Point Mass (PM) and Bipedal\nWalker (BW). For the PM environment, we used Proximal Policy Optimization (PPO) as the student\nalgorithm, while for BW, we employed Soft Actor-Critic (SAC) as in Klink et al. (2022). Visualiza-\ntions of each environment are provided in Figure 5.\nFor comparison, we evaluated several curriculum learning baselines, including CURROT, SPRL,\nGoalGAN, ALPGMM, ACL, PLR, and VDS (Klink et al., 2022; 2021a; Florensa et al., 2018; Porte-\nlas et al., 2020; Graves et al., 2017; Jiang et al., 2021; Zhang et al., 2020). All experiments are\nconducted with multiple random seeds to ensure statistical reliability, 10 seeds for PM, 5 seeds for\nBW trivial tasks, and 3 seeds for BW infeasible tasks. We present a comparison with CURROT, the\nstrongest among the baselines, and include a comprehensive evaluation in Appendix A.\n4.1 Point Mass\nThe PM environment requires controlling a point agent to navigate through a narrow gate in order\nto reach a target location on the opposite side of a wall (Klink et al., 2020a;b; 2021a). The task\ndifficulty is modulated by the gate\u2019s width and position. The target context distribution \u00b5(c) is\n6", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2952, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5752b15b-86c9-4a3f-8997-312b7afdc63c": {"__data__": {"id_": "5752b15b-86c9-4a3f-8997-312b7afdc63c", "embedding": null, "metadata": {"page_label": "7", "file_name": "Causal-Paced Deep Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Causal-Paced Deep Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 741276, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3024380c-80b7-4d35-8d81-71c242fc3441", "node_type": "4", "metadata": {"page_label": "7", "file_name": "Causal-Paced Deep Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Causal-Paced Deep Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 741276, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "3d722a965bd67e32a16113f617b8a29c9e0508e70ccf457e3e3ab6aa6a48453d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Causal-Paced Deep Reinforcement Learning\nCP-DRL (Ours) CURROT (best method)\n0 25 50 75 100 125 150 175 200\nEpoch\n3\n6Cum. Disc. Ret.\n(a) Cumulative discounted return\n0 50 100 150 200\nEpoch\n0.00\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\n1.75Gate Position (b) Gate position\n0 50 100 150 200\nEpoch\n0\n1\n2\n3\n4Gate Width (c) Gate width\nFigure 6: Performance comparison between CP-DRL and CURROT in Point Mass environ-\nment. (a) Cumulative discounted return. (b) Median distance to the target gate position. (c) Median\ndistance to the target gate width. All curves show the mean with 95% confidence intervals.\nbimodal, corresponding to two gate positions on opposite sides. Each training epoch consists of\n4,096 rollouts, and all methods are trained for 200 epochs to ensure convergence. For a detailed\ndescription of the environment, please refer to Appendix B.1.\nCP-DRL demonstrates superior performance throughout training, reaching a return of 6.17 \u00b1 0.08\nat epoch 195. This substantially outperforms the second-best method, CURROT ( 5.6 \u00b1 0.34), with\nan improvement of approximately 10.2%. Moreover, CP-DRL maintains consistently low standard\nerrors during training, indicating more stable learning compared to CURROT, which exhibits in-\ncreasing variance over time. The overall performance trend is illustrated in Figure 6a.\nWe also examine how CP-DRL adapts the curriculum over time by visualizing the evolution of gate\npositions and gate width precision. The agent quickly converges to tasks aligned with the target\ngate position and progressively focuses on narrower gate widths, demonstrating that the curriculum\ndrives the agent toward more precise and challenging contexts. These patterns reflect CP-DRL\u2019s\nability to facilitate efficient exploration by prioritizing causally unfamiliar tasks, thereby accelerating\nconvergence and enhancing adaptation. Visualizations are provided in Figure 6b and Figure 6c.\n4.2 Bipedal Walker\nCP-DRL (Ours) CURROT (best method)\n0 5 10 15 20 25 30 35 40\nEpoch\n150\n100\n50\n0\n50\n100\n150\nPerformance\n(a) Performance in Bipedal Walker (Trivial)\n0 5 10 15 20 25 30 35 40\nEpoch\n150\n100\n50\n0\n50\n100\n150\nPerformance (b) Performance in Bipedal Walker (Infeasible)\nFigure 7: Performance comparison between CP-DRL and CURROT in BipedalWalker. (a)\nTrivial. (b) Infeasible. All curves show the mean with 95% confidence intervals.\nThe BW environment is a continuous control benchmark where a bipedal agent must traverse un-\neven terrain consisting of randomly spaced stumps. Each task is parameterized by two contextual\nvariables: stump height and stump spacing, which respectively control the vertical and horizontal\ndifficulty of locomotion. In the Trivial setting, stump height is sampled from [\u22123, 3] and clipped\nat zero, resulting in approximately half of the tasks being flat and easy. In contrast, the Infeasible\nsetting uses a wider range [0, 9] for stump height, leading to many tasks that are physically impos-\n7", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2932, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "0b726b32-b5fb-48fa-953d-1f386693c4d8": {"__data__": {"id_": "0b726b32-b5fb-48fa-953d-1f386693c4d8", "embedding": null, "metadata": {"page_label": "8", "file_name": "Causal-Paced Deep Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Causal-Paced Deep Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 741276, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d74fee4b-74a0-40f0-8eeb-f876277c7fca", "node_type": "4", "metadata": {"page_label": "8", "file_name": "Causal-Paced Deep Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Causal-Paced Deep Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 741276, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "0b5a02750d5a19d812ee3f7c9b8abaa01a7495f842a633cf4ff5c92b1b354242", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Causal-Paced Deep Reinforcement Learning\nsible to solve. Stump spacing is sampled from [0, 6] in both settings. Each training epoch consists\nof 5 \u00d7 104 environment steps, and all methods are trained for 40 epochs to ensure convergence. For\nfurther environment details, refer to Appendix B.2.\nIn the Trivial setting (Figure 7a), CP-DRL achieves the fastest convergence among all methods,\nreaching a return of 93.82 \u00b18.12 at 20k steps and maintaining stable performance thereafter. While\nCURROT attains slightly higher final returns, CP-DRL exhibits a lower standard error for most of\nthe training period. Other methods, such as ALP-GMM and GOALGAN, exhibit moderate success,\nwhile SPRL and Random sampling fail to achieve meaningful progress.\nIn the Infeasible setting (Figure 7b), CP-DRL outperforms all baselines during the mid-training\nphase, achieving the highest mean return of130.61\u00b19.32 at 30k steps. Although CURROT achieves\na higher final return ( 123.58 \u00b1 5.72 vs. 101.85 \u00b1 13.87), CP-DRL reaches superior performance\nearlier in training. In contrast, several baselines, such as SPRL, RIAC, and Random, show severe\ninstability or collapse entirely.\nTogether, these results confirm that CP-DRL not only achieves competitive performance but also ex-\ncels in stability and convergence speed, especially in structurally diverse or difficult environments.\n5 Limitations\nOur approach, while effective, has several limitations that highlight opportunities for future im-\nprovement. The method depends on approximating SCMs to estimate differences between tasks,\nbut the quality of this approximation is inherently limited by the choice of model architecture and\ntraining strategy. Incorporating more expressive or causally grounded models, such as those based\non counterfactual reasoning or latent causal representations, may lead to more accurate consistency\nestimation and improved curriculum design.\nThe current evaluation is confined to a small set of benchmark environments, leaving open the\nquestion of how well the method generalizes to more complex or diverse settings. Environments\nwith sparse rewards or unobserved confounders may present different challenges, and extending our\nexperiments to these domains would help validate the robustness and scalability of the approach.\nAlthough the framework integrates naturally with CURROT, its applicability to other curriculum\nlearning paradigms has not been explored. Understanding whether the proposed causal-guided strat-\negy can be transferred to alternative frameworks would clarify its general utility and flexibility.\nFinally, in its current form, the method uses heuristic rules, such as thresholds or fixed weightings.\nDeveloping more principled weighting mechanisms, potentially through meta-learning or causal\nscore matching, could improve the adaptivity and performance of the curriculum.\n6 Conclusion\nWe presented CP-DRL, a curriculum learning framework that leverages causal reasoning to guide\ntask selection. Instead of relying on access to true SCMs, CP-DRL quantifies the agent\u2019s unfamil-\niarity with the causal structure of a task through the disagreement of the state, action, transition,\nand reward predictors. These structural signals are integrated into a transport-based curriculum op-\ntimization scheme, enabling the agent to explore causally underexplored regions while gradually\naligning with the target task distribution.\nThrough experiments on two reinforcement learning benchmarks, PM and BW, we demonstrate the\neffectiveness of CP-DRL in improving curriculum learning. Our findings demonstrate the promise\nof causal signals as a general tool for curriculum design in RL. Future work may explore tighter in-\ntegration with causal representation methods, application to real-world robotic systems, or extension\nto partially observable and sparse reward settings. We hope that this work inspires further research\ninto causally grounded learning and structure-aware exploration for adaptive agents in open-ended\nenvironments.\n8", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4009, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8d0560b4-4290-4e19-8566-4c23d608c96b": {"__data__": {"id_": "8d0560b4-4290-4e19-8566-4c23d608c96b", "embedding": null, "metadata": {"page_label": "9", "file_name": "Causal-Paced Deep Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Causal-Paced Deep Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 741276, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "20b3b4bc-a2d9-469d-b6eb-3e4440044a36", "node_type": "4", "metadata": {"page_label": "9", "file_name": "Causal-Paced Deep Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Causal-Paced Deep Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 741276, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "4434a62c57bfd46b68b25af2385cd5a2f87cf7a05917eeca408004ac3a66fdc5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Causal-Paced Deep Reinforcement Learning\nReferences\nOssama Ahmed, Frederik Tr\u00e4uble, Anirudh Goyal, Alexander Neitz, Manuel W\u00fcthrich, Yoshua\nBengio, Bernhard Sch\u00f6lkopf, and Stefan Bauer. Causalworld: A robotic manipulation benchmark\nfor causal structure and transfer learning. In International Conference on Learning Representa-\ntions, 2021.\nLars Buesing, Theophane Weber, Yori Zwols, Nicolas Heess, Sebastien Racaniere, Arthur Guez,\nand Jean-Baptiste Lespiau. Woulda, coulda, shoulda: Counterfactually-guided policy search. In\nInternational Conference on Learning Representations, 2018.\nMaxime Chevalier-Boisvert, Lucas Willems, and Suman Pal. Minimalistic gridworld environment\nfor gymnasium. https://github.com/Farama-Foundation/Minigrid, 2018. Ac-\ncessed: 2025-06-01.\nDaesol Cho, Seungjae Lee, and H Jin Kim. Diversify & Conquer: Outcome-directed curriculum\nrl via out-of-distribution disagreement. Advances in Neural Information Processing Systems, 36:\n53593\u201353623, 2023a.\nDaesol Cho, Seungjae Lee, and H Jin Kim. Outcome-directed reinforcement learning by uncertainty\n& temporal distance-aware curriculum goal generation. arXiv preprint arXiv:2301.11741, 2023b.\nCarlos Florensa, David Held, Markus Wulfmeier, Michael Zhang, and Pieter Abbeel. Reverse cur-\nriculum generation for reinforcement learning. In Conference on Robot Learning, pp. 482\u2013495.\nPMLR, 2017.\nCarlos Florensa, David Held, Xinyang Geng, and Pieter Abbeel. Automatic goal generation for rein-\nforcement learning agents. In Proceedings of the International Conference on Machine Learning,\npp. 1515\u20131528. PMLR, 2018.\nAlex Graves, Marc G Bellemare, Jacob Menick, Remi Munos, and Koray Kavukcuoglu. Auto-\nmated curriculum learning for neural networks. In Proceedings of the International Conference\non Machine Learning, pp. 1311\u20131320. PMLR, 2017.\nAssaf Hallak, Dotan Di Castro, and Shie Mannor. Contextual markov decision processes. arXiv\npreprint arXiv:1502.02259, 2015.\nIrina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick,\nShakir Mohamed, and Alexander Lerchner. beta-vae: Learning basic visual concepts with a\nconstrained variational framework. In International Conference on Learning Representations ,\n2017.\nPeide Huang, Mengdi Xu, Jiacheng Zhu, Laixi Shi, Fei Fang, and Ding Zhao. Curriculum rein-\nforcement learning using optimal transport via gradual domain adaptation. Advances in Neural\nInformation Processing Systems, 35:10656\u201310670, 2022.\nEdward Hughes, Michael Dennis, Jack Parker-Holder, Feryal Behbahani, Aditi Mavalankar, Yuge\nShi, Tom Schaul, and Tim Rockt\u00e4schel. Open-endedness is essential for artificial superhuman\nintelligence. arXiv preprint arXiv:2406.04268, 2024.\nMinqi Jiang, Edward Grefenstette, and Tim Rockt\u00e4schel. Prioritized level replay. In Proceedings of\nthe International Conference on Machine Learning, pp. 4940\u20134950. PMLR, 2021.\nPascal Klink, Hany Abdulsamad, Boris Belousov, and Jan Peters. Self-paced contextual reinforce-\nment learning. In Conference on Robot Learning, pp. 513\u2013529. PMLR, 2020a.\nPascal Klink, Carlo D\u2019Eramo, Jan R Peters, and Joni Pajarinen. Self-paced deep reinforcement\nlearning. Advances in Neural Information Processing Systems, 33:9216\u20139227, 2020b.\n9", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3217, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "88c42e7f-a908-4cb7-8026-0356b60bd717": {"__data__": {"id_": "88c42e7f-a908-4cb7-8026-0356b60bd717", "embedding": null, "metadata": {"page_label": "10", "file_name": "Causal-Paced Deep Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Causal-Paced Deep Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 741276, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2324ca46-bbb4-4a98-9c89-299fb48e765d", "node_type": "4", "metadata": {"page_label": "10", "file_name": "Causal-Paced Deep Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Causal-Paced Deep Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 741276, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "27c6362f362215deeb4449f8986db7475e3b3807b1ba0eddc4a5e5e1384f60fd", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Causal-Paced Deep Reinforcement Learning\nPascal Klink, Hany Abdulsamad, Boris Belousov, Carlo D\u2019Eramo, Jan Peters, and Joni Pajarinen.\nA probabilistic interpretation of self-paced learning with applications to reinforcement learning.\nJournal of Machine Learning Research, 22(182):1\u201352, 2021a.\nPascal Klink, Carlo D\u2019Eramo, Jan Peters, and Joni Pajarinen. Boosted curriculum reinforcement\nlearning. In International Conference on Learning Representations, 2021b.\nPascal Klink, Haoyi Yang, Carlo D\u2019Eramo, Jan Peters, and Joni Pajarinen. Curriculum reinforce-\nment learning via constrained optimal transport. In Proceedings of the International Conference\non Machine Learning, pp. 11341\u201311358. PMLR, 2022.\nPascal Klink, Carlo D\u2019Eramo, Jan Peters, and Joni Pajarinen. On the benefit of optimal transport for\ncurriculum reinforcement learning. IEEE Transactions on Pattern Analysis and Machine Intelli-\ngence, 2024.\nMingxuan Li, Junzhe Zhang, and Elias Bareinboim. Causally aligned curriculum learning. In\nInternational Conference on Learning Representations, 2024. URL https://openreview.\nnet/forum?id=hp4yOjhwTs.\nSanmit Narvekar, Bei Peng, Matteo Leonetti, Jivko Sinapov, Matthew E Taylor, and Peter Stone.\nCurriculum learning for reinforcement learning domains: A framework and survey. Journal of\nMachine Learning Research, 21(181):1\u201350, 2020.\nJack Parker-Holder, Minqi Jiang, Michael Dennis, Mikayel Samvelyan, Jakob Foerster, Edward\nGrefenstette, and Tim Rockt\u00e4schel. Evolving curricula with regret-based environment design. In\nProceedings of the International Conference on Machine Learning , pp. 17473\u201317498. PMLR,\n2022.\nJudea Pearl. Causality: Models, Reasoning, and Inference. Cambridge University Press, 2 edition,\n2009. DOI: 10.1017/CBO9780511803161.\nJudea Pearl and Elias Bareinboim. Transportability of causal and statistical relations: A formal\napproach. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 25, pp. 247\u2013\n254, 2011.\nR\u00e9my Portelas, C\u00e9dric Colas, Katja Hofmann, and Pierre-Yves Oudeyer. Teacher algorithms for\ncurriculum learning of deep rl in continuously parameterized environments. In Conference on\nRobot Learning, pp. 835\u2013853. PMLR, 2020.\nCl\u00e9ment Romac, R\u00e9my Portelas, Katja Hofmann, and Pierre-Yves Oudeyer. Teachmyagent: a\nbenchmark for automatic curriculum learning in deep rl. In Proceedings of the International\nConference on Machine Learning, pp. 9052\u20139063. PMLR, 2021.\nErdi Sayar, Giovanni Iacca, Ozgur S Oguz, and Alois Knoll. Diffusion-based curriculum reinforce-\nment learning. Advances in Neural Information Processing Systems, 37:97587\u201397617, 2024.\nMax-Philipp B. Schrader. Gym-sokoban. https://github.com/mpSchrader/\ngym-sokoban, 2018. Accessed: 2025-06-04.\nMaximilian Seitzer, Bernhard Sch\u00f6lkopf, and Georg Martius. Causal influence detection for improv-\ning efficiency in reinforcement learning.Advances in Neural Information Processing Systems, 34:\n22905\u201322918, 2021.\nSumedh A Sontakke, Arash Mehrjou, Laurent Itti, and Bernhard Sch\u00f6lkopf. Causal curiosity: Rl\nagents discovering self-supervised experiments for causal representation learning. InProceedings\nof the International Conference on Machine Learning, pp. 9848\u20139858. PMLR, 2021.\nYuewen Sun, Kun Zhang, and Changyin Sun. Model-based transfer reinforcement learning based on\ngraphical model representations. IEEE Transactions on Neural Networks and Learning Systems,\n2021.\n10", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3394, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f82c81f7-a44e-431a-93fc-0f4b7493d9f1": {"__data__": {"id_": "f82c81f7-a44e-431a-93fc-0f4b7493d9f1", "embedding": null, "metadata": {"page_label": "11", "file_name": "Causal-Paced Deep Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Causal-Paced Deep Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 741276, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "54d5c5c8-3556-48a1-9e79-2e3cb7c99224", "node_type": "4", "metadata": {"page_label": "11", "file_name": "Causal-Paced Deep Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Causal-Paced Deep Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 741276, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "69c11401f33a8d9f453058bbb6d964cc95958dc154149d5030e1eed37377615b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Causal-Paced Deep Reinforcement Learning\nGeorgios Tzannetos, B\u00e1rbara Gomes Ribeiro, Parameswaran Kamalaruban, and Adish Singla. Prox-\nimal curriculum for reinforcement learning agents. Transactions on Machine Learning Research,\n2023.\nRundong Wang, Longtao Zheng, Wei Qiu, Bowei He, Bo An, Zinovi Rabinovich, Yujing Hu,\nYingfeng Chen, Tangjie Lv, and Changjie Fan. Towards skilled population curriculum for multi-\nagent reinforcement learning. arXiv preprint arXiv:2302.03429, 2023.\nJunlin Wu and Yevgeniy V orobeychik. Robust deep reinforcement learning through bootstrapped\nopportunistic curriculum. In Proceedings of the International Conference on Machine Learning,\npp. 24177\u201324211. PMLR, 2022.\nAlessio Zanga, Elif Ozkirimli, and Fabio Stella. A survey on causal discovery: Theory and practice.\nInternational Journal of Approximate Reasoning, 151:101\u2013129, 2022.\nYan Zeng, Ruichu Cai, Fuchun Sun, Libo Huang, and Zhifeng Hao. A survey on causal reinforce-\nment learning. IEEE Transactions on Neural Networks and Learning Systems, 2024.\nYunzhi Zhang, Pieter Abbeel, and Lerrel Pinto. Automatic curriculum learning through value dis-\nagreement. Advances in Neural Information Processing Systems, 33:7648\u20137659, 2020.\nZheng-Mao Zhu, Shengyi Jiang, Yu-Ren Liu, Yang Yu, and Kun Zhang. Invariant action effect\nmodel for reinforcement learning. In Proceedings of the AAAI Conference on Artificial Intelli-\ngence, volume 36, pp. 9260\u20139268, 2022.\n11", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1442, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "369136cc-11f6-42b5-9138-2393f21e36cd": {"__data__": {"id_": "369136cc-11f6-42b5-9138-2393f21e36cd", "embedding": null, "metadata": {"page_label": "12", "file_name": "Causal-Paced Deep Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Causal-Paced Deep Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 741276, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6e777095-602f-4dbb-8308-a3de47f1f1ca", "node_type": "4", "metadata": {"page_label": "12", "file_name": "Causal-Paced Deep Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Causal-Paced Deep Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 741276, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "9ffec42c177ac7beeef1e9cd26a6311e88841307897cad45f1cb0ceedbe5c74c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Causal-Paced Deep Reinforcement Learning\nSupplementary Materials\nThe following content was not necessarily subject to peer review.\nA Comprehensive Experiments Results\nCP-DRL (Ours)\nCURROT\nSPRL\nRandom\nGoalGAN\nALP-GMM\nACL\nPLR\nVDS\n0 25 50 75 100 125 150 175 200\nEpoch\n3\n6Cum. Disc. Ret.\n(a) Cumulative discounted return\n0 50 100 150 200\nEpoch\n0.0\n0.5\n1.0\n1.5\n2.0Gate Position (b) Gate position\n0 50 100 150 200\nEpoch\n0\n1\n2\n3\n4\n5\n6Gate Width (c) Gate width\nFigure 8: Performance comparison in Point Mass environment under different curriculum\nmethods. (a) Cumulative discounted return over 200 epochs. (b) Median distance to the target gate\nposition (c) Median distance to the target gate width. All curves show the mean with 95% confidence\nintervals.\nCP-DRL (Ours)\nCURROT\nSPRL\nRandom\nGoalGAN\nALP-GMM\nRIAC\n0 5 10 15 20 25 30 35 40\nEpoch\n150\n100\n50\n0\n50\n100\n150\nPerformance\n(a) Performance in Bipedal Walker (Trivial)\n0 5 10 15 20 25 30 35 40\nEpoch\n150\n100\n50\n0\n50\n100\n150\nPerformance (b) Performance in Bipedal Walker (Infeasible)\nFigure 9: Performance comparison of curriculum methods in BipedalWalker. (a) Trivial, (b)\nInfeasible. All curves show the mean with 95% confidence intervals.\nB Implementation Details\nB.1 Point Mass Environment\nThe Point Mass environment presents a continuous 2-dimensional task where the agent, starting\nfrom [x = 0, y= 0], must pass through the narrow gate at y = 0 and reach the goal at [0, \u22123]. The\nagent\u2019s state is represented as[x, vx, y, vy] where [x, y] is a position and the[vx, vy] is a speed (Klink\net al., 2020a;b; 2021a). The action space is a 2-dimensional continuous vector within[\u221210.0, 10.0]2,\nso the action [ax, ay] is clipped into [\u221210.0, 10.0]2. The agent starts from the position [0.0, 3.0] with\nno velocity. The agent\u2019s state is updated 10 times per step with a time interval of 0.01 using Euler\nintegration to approximate the differential equation. The update rule for the x-axis is given by\nxt+1 = xt + \u2206t \u00b7 vx,t\nvx,t+1 = vx,t + \u2206t \u00b7 (1.5 \u00b7 ax,t \u2212 f \u00b7 vx,t + \u03f5) , \u03f5 \u223c N(0, 0.052)\n12", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2032, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "73fbaaef-fe9a-437a-9ba4-0f917b38ce34": {"__data__": {"id_": "73fbaaef-fe9a-437a-9ba4-0f917b38ce34", "embedding": null, "metadata": {"page_label": "13", "file_name": "Causal-Paced Deep Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Causal-Paced Deep Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 741276, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4f70a28c-6500-4f4d-a9e9-62ac94100ecf", "node_type": "4", "metadata": {"page_label": "13", "file_name": "Causal-Paced Deep Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Causal-Paced Deep Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 741276, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "2aaed818b57c9cfe3c0c31c34e433186b0f12b66ab9ab24c1704e00414ec1300", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Causal-Paced Deep Reinforcement Learning\nwhere xt is an x coordinate at timestep t, vx,t the velocity at x-axis, f the fraction constant which is\nset to 0 in our setting, and \u03f5 is the Gaussian noise with mean zero and standard deviation 0.05. The\nsame rule is applied to the y-axis. The reward is defined as r = exp (\u22120.6 \u00b7 |[x, y] \u2212 [0.0, \u22123.0]|2)\nwhere the reward gradually diminishes based on the Euclidean distance from the goal position. If\nthe distance becomes less than 0.25, the episode is considered successful. Each task is defined by\nthe context vector c = [xgate, wgate] where xgate is the center of the door located at y = 0 and wgate\nthe width of the door. On the timestep that the agent passes the y = 0 and its x-coordinate is out\nof\n\u0002\nxgate \u2212 wgate\n2 , xgate + wgate\n2\n\u0003\n, it is considered a collision with the wall. In this case, the agent\u2019s\nposition is reset to the collision point, and its velocity is reset to 0.\nB.2 BipedalWalker Environment\nThe BipedalWalker environment is based on the environment BipedalWalkerHardcore provided\nby OpenAI Gym, as implemented in Romac et al. (2021). In this environment, the agent is a\nbipedal robot that walks forward on terrain with evenly spaced stumps of uniform height, using\na 4-dimensional continuous action vector to control its left and right hip and knee joints. The agent\nreceives different types of reward and penalty which are (1) forward reward: based on the increase\nin the agent\u2019s horizontal position (2) torque penalty: proportional to the magnitude of the joint\ntorques (3) stability reward: given when the agent maintains a stable hull posture while walking (4)\ntermination penalty: applied when the agent\u2019s head touches the ground. The episode ends when\nthe agent reaches the end of the track, exceeds 2000 steps, or the agent\u2019s head touches the ground.\nObservation space is a 24-dimensional continuous vector consisting of 10-dimensional lidar sensor\ndetecting front stumps, 4-dimensional hull state containing information such as angle, angular veloc-\nity and horizontal/vertical velocity, 4-dimensional joint angle and 4-dimensional velocity containing\nleft and right hip and knee angle or velocity, and information of whether each leg is contacting the\nground (2-dimensional). Each context (task) is defined by the context vector c = [ hstump, sstump]\nwhere hstump is the height of the stump, sstump is the interval between two stumps.\nB.3 Network Structures\nOur framework employs modular neural networks to approximate each component of the underlying\ncausal structure\u2014state, action, transition, and reward. Each model is instantiated as an ensemble to\nenable SCM difference approximation through disagreement.\nTransition Prediction. The transition model approximates the environment dynamics by predict-\ning the next state \u02c6st+1 from the current state st and action at. Each ensemble member is a two-layer\nMLP: the input [st; at] is passed through a hidden layer of dimension 32, followed by a ReLU acti-\nvation and a final linear layer outputting a vector of the same dimension as the state space. The loss\nis defined as the mean squared error between predicted and actual next states:\nLtransition = \u2225\u02c6st+1 \u2212 st+1\u22252\n2.\nReward Prediction. The reward model learns to approximate the reward function r(st, at) using\na similar architecture to the transition model. It takes [st; at] as input, processes it through a single\nhidden layer with ReLU activation, and outputs a scalar reward prediction. The loss is again the\nmean squared error:\nLreward = \u2225\u02c6rt \u2212 rt\u22252\n2.\nState and Action Reconstruction. To model state and action representations, we employ a\u03b2-V AE\nfor each modality. Each encoder maps the input (state or action) to a Gaussian latent distribution\nparameterized by mean and log-variance vectors. A latent code is sampled via the reparameteri-\nzation trick and decoded to reconstruct the input. The loss combines reconstruction error and KL\ndivergence:\nLV AE= \u2225\u02c6x \u2212 x\u22252\n2 + \u03b2 \u00b7 KL (q(z|x) | N(0, I)) ,\n13", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3993, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5994d7d9-bd15-4874-ab95-8b93f9702b4a": {"__data__": {"id_": "5994d7d9-bd15-4874-ab95-8b93f9702b4a", "embedding": null, "metadata": {"page_label": "14", "file_name": "Causal-Paced Deep Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Causal-Paced Deep Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 741276, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "90663b36-1b16-4f13-9ceb-7f26ae82fb9e", "node_type": "4", "metadata": {"page_label": "14", "file_name": "Causal-Paced Deep Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Causal-Paced Deep Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 741276, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "e323bd567d8f006ca8567cba1e1e6c7d748b96ea84b76cd2a05e2b7e13fc4cdd", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Causal-Paced Deep Reinforcement Learning\nwhere x is either a state or an action vector. The latent dimensions for state and action V AEs are 32\nand 16, respectively. We use a weighting coefficient of \u03b2=4 to balance reconstruction fidelity and\nlatent space regularization.\nEnsemble and Uncertainty Estimation. All four model types (transition, reward, state V AE, and\naction V AE) are trained as ensembles of size 10. Disagreement is quantified as the standard deviation\nof predictions across ensemble members, averaged over a batch of samples. This disagreement\nserves as a proxy for causal misalignment and is integrated into the task selection criterion.\nOptimization. Each model in the ensemble is optimized independently using the Adam optimizer\nwith a shared learning rate. During training, the reconstruction or prediction losses are computed\nand backpropagated per model.\nB.4 Reproducing Baselines\nAll baseline methods were integrated from their respective open-source implementations and eval-\nuated under our unified experimental settings. For the PM baselines, we used the implementation\navailable at https://github.com/psclklnk/currot, and for BW, we adopted the imple-\nmentation from https://github.com/flowersteam/TeachMyAgent.\nC Hyperparameters\nTable 1: Hyperparameters used for training each method in each environment.\nHyperParameter Point Mass BipedalWalker\nPPO/SAC\n\u03b3 0.95 0.99\n\u03bb 0.99 0.95\nPPO rollout length 4096 -\nPPO epochs 10 -\nPPO minibatches size 128 -\nPPO clip range 0.2 -\nPPO number of workers 1 -\nPPO max gradient norm 0.5 -\nPPO value clipping False -\nPPO Entropy coefficient 0.0 -\nSAC batch size - 1000\nSAC replay buffer size - 2e6\nSAC polyak,\u03c1 - 0.995\nSAC start steps - 1e4\nSAC Entropy coefficient\u03b1 - 5e-3\nAdam learning rate 3e-4 1e-3\nAdam, \u03f5 1e-5 1e-8\nCP-DRL\nstate weight,wstate 0 100\naction weight,waction 0 20\ntransition probability weight,wtransition 10 100\nreward weight,wreward 0 100\nEnsemble size,K 10 10\n14", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1942, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "fafd2570-3a4d-4983-9123-022aa5d415ef": {"__data__": {"id_": "fafd2570-3a4d-4983-9123-022aa5d415ef", "embedding": null, "metadata": {"page_label": "15", "file_name": "Causal-Paced Deep Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Causal-Paced Deep Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 741276, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6cbb9f05-e75a-471d-addd-56f8c12cae6c", "node_type": "4", "metadata": {"page_label": "15", "file_name": "Causal-Paced Deep Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Causal-Paced Deep Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 741276, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "1a7d7a8b9cc55f415eac86a6d7593f05cb7fb8969f03a82851fe8c1dd7e4b72a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Causal-Paced Deep Reinforcement Learning\nD Overall Workflow\nThis section outlines the overall workflow of CP-DRL. As illustrated in Algorithm 1, the method\niteratively updates a context distribution to guide curriculum learning. At each iteration, the agent\nsamples a batch of tasks, trains its policy, and collects the corresponding trajectories. The disagree-\nment scores for the sampled tasks are computed as defined in Eq. 4. This score is combined with\nepisodic return and is used as a cost in CURROT\u2019s optimal transport framework to update the con-\ntext distribution. This procedure encourages the agent to prioritize causally informative tasks and\nfacilitates efficient learning progression.\nAlgorithm 1 Causally-Paced Deep Reinforcement Learning (CP-DRL)\n1: Input: Initial context distribution \u02c6pW,0(c), ensemble sizeK, component weights{wj}, distance\nbound \u03f5, number of curriculum iterations L, number of sampled tasks per iteration M, episode\nlength T\n2: for l = 0 to L \u2212 1 do\n3: Agent Improvement:\n4: for j = 1 to M do\n5: Sample context cj \u223c \u02c6pW,l(c)\n6: Train policy \u03c0 on cj and collect trajectory \u03c4cj = {(st, at, rt, st+1)}T\nt=1\n7: for each causal factor i \u2208 {state, action, transition, reward} do\n8: Compute disagreement for component i using Eq. 4\n9: Train an ensemble of K neural networks using loss Li (Eq. 3)\n10: end for\n11: Compute episodic return Rj = PT\nt=1 rcj (st, at)\n12: Compute causal misalignment score CM(cj) using Eq. 5\n13: end for\n14: Context Distribution Update:\n15: Update context distribution \u02c6pW,l(c) using CURROT with cost Rj + CM(cj)\n16: end for\nE Component-wise Sensitivity Analysis of Disagreement Metrics\nTo further validate that each disagreement score captures the intended component of causal variation,\nwe conduct a controlled analysis by independently varying block size, action scale, and reward scale,\nwhile keeping the other two factors fixed. Specifically, we define four configurations for each factor,\nresulting in three experimental groups: B1-B4 for block size, A1-A4 for action scale, and R1-R4\nfor reward scale. All configurations are derived from a base environment corresponding to Task\nT1, which uses a block size of 0.05, an action scale of 1.0, and a reward weight of 1.0. For each\nsubsequent task (T2 through T4), these parameters are progressively increased, with T4 using a\nblock size of 0.1, an action scale of 4.0, and a reward weight of 4.0.\nTo isolate the effect of each component, we modify one factor at a time: in the block size group (B1-\nB4), we vary block size from 0.05 to 0.1 while fixing action scale and reward weight; in the action\nscale group (A1-A4), we vary action scale while holding block size and reward weight constant;\nand in the reward scale group (R1-R4), we vary only the reward weight. Figure 10 reports the\ndisagreement scores obtained from each setting.\nWe observe that transition disagreement increases consistently with both block size and action scale,\naligning with the intuition that changes in these factors directly affect the environment\u2019s transition\ndynamics. Similarly, action disagreement steadily grows with increasing action scale, which is ex-\npected since the range of agent actions expands. State disagreement is primarily influenced by block\nsize, suggesting that larger blocks alter the spatial configuration of the environment and thereby af-\nfect the agent\u2019s state representation. In contrast, reward disagreement exhibits no clear pattern under\n15", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3458, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "fdf80ab7-8d16-474d-8843-06066360b0ab": {"__data__": {"id_": "fdf80ab7-8d16-474d-8843-06066360b0ab", "embedding": null, "metadata": {"page_label": "16", "file_name": "Causal-Paced Deep Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Causal-Paced Deep Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 741276, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "218daabb-d00f-47ce-b417-5953b7da3bdf", "node_type": "4", "metadata": {"page_label": "16", "file_name": "Causal-Paced Deep Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Causal-Paced Deep Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 741276, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "01769b712a5c64ce17ccdc153ba38d6b4f4fe25a4b62d478b6bcc8efa60ac159", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Causal-Paced Deep Reinforcement Learning\nB1 B2 B3 B4\n0.530\n0.535\n0.540\nTransition\nB1 B2 B3 B4\n0.18\n0.19\n0.20\n0.21\n0.22\n0.23\n0.24\nReward\nB1 B2 B3 B4\n1.28\n1.30\n1.32\n1.34\n1.36\nState\nB1 B2 B3 B4\n0.6020\n0.6022\n0.6024\n0.6026\n0.6028\n0.6030\nAction\n(a) Disagreement trends with increasing block size.\nA1 A2 A3 A4\n0.5250\n0.5275\n0.5300\n0.5325\n0.5350\n0.5375\nTransition\nA1 A2 A3 A4\n0.220\n0.225\n0.230\n0.235\nReward\nA1 A2 A3 A4\n1.2950\n1.2951\n1.2952\n1.2953\n1.2954\n1.2955\n1.2956\nState\nA1 A2 A3 A4\n0.6\n0.7\n0.8\n0.9\n1.0\nAction\n(b) Disagreement trends with increasing action scale.\nR1 R2 R3 R4\n0.5237\n0.5238\n0.5239\n0.5240\nTransition\nR1 R2 R3 R4\n0.2205\n0.2210\n0.2215\nReward\nR1 R2 R3 R4\n1.29475\n1.29500\n1.29525\n1.29550\n1.29575\n1.29600\nState\nR1 R2 R3 R4\n0.60200\n0.60225\n0.60250\n0.60275\n0.60300\n0.60325\nAction\n(c) Disagreement trends with increasing reward scale.\nFigure 10: Component-wise disagreement under isolated changes to block size, action scale,\nand reward scale.\nany of the settings, which we attribute to the sparsity of the reward signal and the use of random\ntrajectories during data collection.\nTogether, these observations reinforce the interpretation that the proposed modular disagreement\nmetrics serve as meaningful proxies for the underlying causal differences introduced by structural\nvariations in the environment.\nF When Causal Signals Fail: A Case Study in SGR\nFigure 11: Illustration of the Sparse Goal Reaching environment.\nTo further investigate the limitations of CP-DRL, we conducted an additional experiment in the\nSparse Goal Reaching (SGR) environment. SGR is a 2D continuous maze-like setting adapted\nfrom Florensa et al. (2018), where the agent must navigate to a specified goal location. The envi-\nronment is bounded within a square region [\u22127, 7]2, with an impassable square wall occupying the\ncentral area [\u22125, 5]2. The agent\u2019s state is represented by a 2D position vector [x, y], and its action is\na 2D vector [ax, ay] clipped to [\u22121, 1]2, scaled by 0.3. If the action\u2019s norm exceeds 1, it is normal-\nized to unit length before scaling. Collisions with the wall prevent movement, resulting in the agent\nremaining at its current position.\n16", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2151, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a3dd4d5d-f172-46e6-b86b-7ad9d5abd1df": {"__data__": {"id_": "a3dd4d5d-f172-46e6-b86b-7ad9d5abd1df", "embedding": null, "metadata": {"page_label": "17", "file_name": "Causal-Paced Deep Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Causal-Paced Deep Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 741276, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c7426651-9112-4859-bd12-e48f074a35c8", "node_type": "4", "metadata": {"page_label": "17", "file_name": "Causal-Paced Deep Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Causal-Paced Deep Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 741276, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "083b3e0bfe45f6ad637f62259969313c0efd70506a6eb03f550d406ecf34b9f9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Causal-Paced Deep Reinforcement Learning\nCP-DRL (Ours)\nCURROT\nSPRL\nRandom\nGoalGAN\nALP-GMM\nACL\nPLR\nVDS\n0 100 200 300 400\nEpoch\n0\n0.25\n0.5\n0.75\n1\nSuccess Rate\n(a) Success Rate\n0 100 200 300 400\nEpoch\n10 1\n100\n101\nTolerance (b) Tolerance\nFigure 12: Performance comparison of curriculum methods in Sparse Goal Reaching. (a)\nSuccess rate (b) Median tolerance. All curves show the mean with 95% confidence intervals over 10\nrandom seeds.\nEach task in this environment is defined by a context vector c = [ xgoal, ygoal, \u03f5] \u2208 R3, where\n(xgoal, ygoal) specifies the goal location and \u03f5 is a fixed tolerance threshold, set to 0.05 in our ex-\nperiments. The agent receives a reward of 1.0 upon reaching the goal within the \u03f5-radius and a step\npenalty of \u22121.0 otherwise. Episodes terminate upon successful goal reaching. The agent\u2019s initial\nposition is uniformly sampled from [\u22127.0, \u22125.0]2, corresponding to the bottom-left region of the\nmaze. To ensure convergence, we trained all baselines for 400 epochs and report results averaged\nover 10 random seeds.\nAs shown in Figure 12, CP-DRL underperforms compared to CURROT. This performance drop is\nattributed to the absence of meaningful structural differences between tasks. In the SGR environ-\nment, tasks differ only by the goal position, while the underlying state, action, transition dynamics,\nand reward mechanisms remain invariant. As detailed in Appendix E, when causal structure re-\nmains unchanged, the disagreement metrics are primarily driven by noise. Consequently, CP-DRL\ndegenerates into a noisier variant of CURROT. These results highlight a limitation of CP-DRL:\nwhen structural variation is minimal or absent, causality-driven curriculum shaping can introduce\ndetrimental stochasticity.\nG Related Works\nCurriculum learning aims to enhance the efficiency of reinforcement learning (RL) and improve\nperformance on the target task by training on a sequence of intermediate tasks guided by various\nsurrogate objectives (Narvekar et al., 2020; Klink et al., 2022). The learning progress perspective\nprioritizes tasks that maximize a model\u2019s performance gain (Portelas et al., 2020; Wu & V orob-\neychik, 2022). The task difficulty perspective focuses on employing tasks of moderate difficulty\nrelative to the current learning state (Florensa et al., 2017; 2018; Huang et al., 2022; Tzannetos\net al., 2023; Cho et al., 2023b; Sayar et al., 2024). The regret perspective selects tasks based on their\nlearning potential, quantified by the difference between optimal and current policy rewards (Parker-\nHolder et al., 2022; Wang et al., 2023). The disagreement perspective estimates uncertainty by\nusing multiple Q-functions to measure the variance or mean distance across state-action pairs, and\nconstructs a curriculum that progresses from high to low disagreement to promote exploration and\nreduce epistemic uncertainty (Zhang et al., 2020; Cho et al., 2023a).\nSelf-paced curriculum RL (Klink et al., 2020b) automatically generates and selects tasks based on\nthe agent\u2019s learning state without relying on external models. Interpolation-based approaches im-\nplement this by gradually shifting the task sampling distribution from an initial to a target distribu-\ntion (Klink et al., 2020a;b; 2021a;b; 2022; Huang et al., 2022; Tzannetos et al., 2023; Klink et al.,\n17", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3312, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "11e154ef-0913-409f-af3a-b0740c10a48e": {"__data__": {"id_": "11e154ef-0913-409f-af3a-b0740c10a48e", "embedding": null, "metadata": {"page_label": "18", "file_name": "Causal-Paced Deep Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Causal-Paced Deep Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 741276, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d3c910f2-7973-491a-93b7-66c2551f9c71", "node_type": "4", "metadata": {"page_label": "18", "file_name": "Causal-Paced Deep Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Causal-Paced Deep Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 741276, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "276e3233b3ced9b0b2ccc60ea33e75172d2cded4531cb59fad73fd6c6eefae65", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Causal-Paced Deep Reinforcement Learning\n2024). SPRL (Klink et al., 2020b) achieves this by using KL Divergence between two task distribu-\ntions and uses the expected performance constraint. However, it tends to ignore intermediate-level\ntasks and lacks flexibility in that the task distribution is constrained to be Gaussian or uniform (Klink\net al., 2022; 2024). CURROT (Klink et al., 2022) resolved this problem by viewing the interpolation-\nbased curriculum as an optimal transport problem.\nResearch efforts that focus on utilizing causal inference in RL consider the causal relationship be-\ntween variables (Buesing et al., 2018; Sontakke et al., 2021; Seitzer et al., 2021; Sun et al., 2021; Zhu\net al., 2022). These works can be divided into two categories, whether it consider structural causal\nmodels (SCM), which are Directed Acyclic Graphs that represent causal relationships between vari-\nables, as given or not (Zeng et al., 2024). Li et al. (2024) applied causal inference to curriculum\nlearning, extracting causally aligned states via conditional independence tests and constructing a\ncurriculum by editing them in reverse topological order of actions. Although this approach has\ndemonstrated significant performance gains on specific benchmarks such as Colored Sokoban and\nButton Maze (Schrader, 2018; Li et al., 2024; Chevalier-Boisvert et al., 2018), it still relies on the as-\nsumption that the underlying SCM is known in advance, which is often not the case in environments\nwith complex or unknown causal structures (Zeng et al., 2024).\nUnsupervised Environment Design (UED) approaches utilize an external model to generate suitable\nenvironments that help agents acquire the ability to handle diverse situations. It differs from our\ncurriculum learning setting in that it generates the entire environment and focuses on robustness of\nthe policy (Parker-Holder et al., 2022).\nH Experimental Setup and Reproducibility\nALL experiments were performed on a server running Ubuntu 22.04.4 LTS, equipped with an\nAMD EPYC 7543 32-core CPU, 885GB of RAM, and an 8 NVIDIA A100 GPU. For the Bipedal-\nWalker experiments, each run was assigned GPU using Python 3.7.12, PyTorch (v1.13.1) and Gym\n(v0.17.3). For the Point Mass experiments, only CPU resources were used with Python 3.8.0, Py-\nTorch (v2.4.1), and Gym (v0.17.3).\n18", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2333, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "2a791035-2164-44e6-a8ac-63f959bab642": {"__data__": {"id_": "2a791035-2164-44e6-a8ac-63f959bab642", "embedding": null, "metadata": {"page_label": "1", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ab6e80be-7b67-4943-ad37-ae28fcef98f4", "node_type": "4", "metadata": {"page_label": "1", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "62555ae63a013f82f1a2d2edb68bdcfea29348a78459b39edb426779fd909f99", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Deep Learning and Computational Physics\n(Lecture Notes)\nDeep Ray, Orazio Pinti and Assad A. Oberai1\n1Department of Aerospace and Mechanical Engineering, University of Southern California,\nLos Angeles, California, USA\narXiv:2301.00942v1  [cs.LG]  3 Jan 2023", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 256, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "536f8afd-cc56-496e-a997-f1eccd13d1eb": {"__data__": {"id_": "536f8afd-cc56-496e-a997-f1eccd13d1eb", "embedding": null, "metadata": {"page_label": "1", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d8b67d49-0086-4fb6-8c74-d1f947b98013", "node_type": "4", "metadata": {"page_label": "1", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "08d418811c5886f3bc3f680cceda7e7a0a29d83bac0b45ce4539b31cfc9073d4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bd4a7088-2165-4501-8e78-bb39ed23b385", "node_type": "1", "metadata": {}, "hash": "79dbe415e4ca5dd52cb97db3894465113556e0f565413b4adce2fc14c41e82e6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Contents\nPreface 3\n1 Introduction 4\n1.1 Computational physics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4\n1.2 Machine learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5\n1.2.1 Examples of ML . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5\n1.2.2 Types of ML algorithms based leaning task . . . . . . . . . . . . . . . . . 5\n1.3 Arti\ufb01cial Intelligence, Machine Learning and Deep Learning . . . . . . . . . . . . 6\n1.4 Machine learning and computational physics . . . . . . . . . . . . . . . . . . . . . 7\n2 Introduction to deep neural networks 9\n2.1 MLP architecture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9\n2.2 Activation functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10\n2.2.1 Linear activation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12\n2.2.2 Recti\ufb01ed linear unit (ReLU) . . . . . . . . . . . . . . . . . . . . . . . . . . 12\n2.2.3 Leaky ReLU . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12\n2.2.4 Logistic function . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12\n2.2.5 Tanh . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13\n2.2.6 Sine . . . . . . . . . . . . . . . . . . . . . . . . . . . . .", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1327, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "bd4a7088-2165-4501-8e78-bb39ed23b385": {"__data__": {"id_": "bd4a7088-2165-4501-8e78-bb39ed23b385", "embedding": null, "metadata": {"page_label": "1", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d8b67d49-0086-4fb6-8c74-d1f947b98013", "node_type": "4", "metadata": {"page_label": "1", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "08d418811c5886f3bc3f680cceda7e7a0a29d83bac0b45ce4539b31cfc9073d4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "536f8afd-cc56-496e-a997-f1eccd13d1eb", "node_type": "1", "metadata": {"page_label": "1", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "10853eab443211187999d690fcf72bfc3b1f09537dcc105a7bbfca0141be5b7a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8ebc271f-4e93-404e-a65f-827cbebd235b", "node_type": "1", "metadata": {}, "hash": "4a75866052a6b96b2483070e0c85094b9e8483e10ca52f2971df54aceee805df", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": ". . . . . . . . . . . . . . . . . . . . . . . 12\n2.2.5 Tanh . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13\n2.2.6 Sine . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13\n2.3 Expressivity of a network . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13\n2.3.1 Universal approximation results . . . . . . . . . . . . . . . . . . . . . . . . 15\n2.4 Training, validation and testing of neural networks . . . . . . . . . . . . . . . . . 15\n2.5 Generalizability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17\n2.5.1 Regularization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17\n2.6 Gradient descent . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18\n2.7 Some advanced optimization algorithms . . . . . . . . . . . . . . . . . . . . . . . 20\n2.7.1 Momentum methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20\n2.7.2 Adam . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21\n2.7.3 Stochastic optimization . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21\n2.8 Calculating gradients using back-propagation . . . . . . . . . . . . . . . . . . . . 23\n2.9 Regression versus classi\ufb01cation . . . . . . . . .", "mimetype": "text/plain", "start_char_idx": 1118, "end_char_idx": 2413, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8ebc271f-4e93-404e-a65f-827cbebd235b": {"__data__": {"id_": "8ebc271f-4e93-404e-a65f-827cbebd235b", "embedding": null, "metadata": {"page_label": "1", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d8b67d49-0086-4fb6-8c74-d1f947b98013", "node_type": "4", "metadata": {"page_label": "1", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "08d418811c5886f3bc3f680cceda7e7a0a29d83bac0b45ce4539b31cfc9073d4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bd4a7088-2165-4501-8e78-bb39ed23b385", "node_type": "1", "metadata": {"page_label": "1", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "767ce41c1f9e0d2c03b34aff8cd2c95aaad85f98f6a6f7327274724465175b97", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": ". . . . . . . . . . . . . . . . . . . . . . . . . . 21\n2.7.3 Stochastic optimization . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21\n2.8 Calculating gradients using back-propagation . . . . . . . . . . . . . . . . . . . . 23\n2.9 Regression versus classi\ufb01cation . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25\n3 Residual neural networks 27\n3.1 Vanishing gradients in deep networks . . . . . . . . . . . . . . . . . . . . . . . . . 27\n3.2 ResNets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28\n3.3 Connections with ODEs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29\n3.4 Neural ODEs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30\n1", "mimetype": "text/plain", "start_char_idx": 2124, "end_char_idx": 2865, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "25bfcfa7-6c79-419e-9d1b-41c88f5723b6": {"__data__": {"id_": "25bfcfa7-6c79-419e-9d1b-41c88f5723b6", "embedding": null, "metadata": {"page_label": "2", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7b547050-93f2-4e56-87a1-d88f740d3255", "node_type": "4", "metadata": {"page_label": "2", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "5d91c843ba01f934ae809f887558d59b175611741bf886f3439eaee43636d813", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8216b001-23d0-494c-a426-3dcbe88cc54f", "node_type": "1", "metadata": {}, "hash": "8e21f0d7b12c87a9bfb9b4e35795a32acde6f64c996ae606042645f7718f39d0", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "4 Solving PDEs with MLPs 33\n4.1 Finite di\ufb00erence method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34\n4.2 Spectral collocation method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35\n4.3 Physics-informed neural networks (PINNs) . . . . . . . . . . . . . . . . . . . . . . 38\n4.4 Extending PINNs to a more general PDE . . . . . . . . . . . . . . . . . . . . . . 40\n4.5 Error analysis for PINNs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41\n4.6 Data assimilation using PINNs . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42\n5 Convolutional Neural Networks 44\n5.1 Functions and images . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44\n5.2 Convolutions of functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44\n5.2.1 Example 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45\n5.2.2 Example 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46\n5.3 Discrete convolutions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46\n5.4 Connection to \ufb01nite di\ufb00erences . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48\n5.5 Convolution layers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48\n5.5.1 Average and Max Pooling . . . . . . . .", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1326, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8216b001-23d0-494c-a426-3dcbe88cc54f": {"__data__": {"id_": "8216b001-23d0-494c-a426-3dcbe88cc54f", "embedding": null, "metadata": {"page_label": "2", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7b547050-93f2-4e56-87a1-d88f740d3255", "node_type": "4", "metadata": {"page_label": "2", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "5d91c843ba01f934ae809f887558d59b175611741bf886f3439eaee43636d813", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "25bfcfa7-6c79-419e-9d1b-41c88f5723b6", "node_type": "1", "metadata": {"page_label": "2", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "81701dbdb5c433f41159ad0533f189e5c149fe58bd31f42bec635983524bcf51", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e9e45378-07d2-41da-bfcb-4a5d282c4ebf", "node_type": "1", "metadata": {}, "hash": "82f286d6f7635d2bf7eac4c3e7da0e1db6f7cbe7f8cc644eacbdc5de80b4d3d1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": ". . . . . . . . . . . 46\n5.4 Connection to \ufb01nite di\ufb00erences . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48\n5.5 Convolution layers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48\n5.5.1 Average and Max Pooling . . . . . . . . . . . . . . . . . . . . . . . . . . . 50\n5.5.2 Convolution for inputs with multiple channels . . . . . . . . . . . . . . . . 51\n5.6 Convolution Neural Network (CNN) . . . . . . . . . . . . . . . . . . . . . . . . . 51\n5.7 Transpose convolution layers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53\n5.8 Image-to-image transformations . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56\n6 Operator Networks 57\n6.1 The problem with PINNs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 57\n6.2 Parametrized PDEs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 57\n6.3 Operators . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58\n6.4 Deep Operator Network (DeepONet) Architecture . . . . . . . . . . . . . . . . . 59\n6.5 Training DeepONets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61\n6.6 Error Analysis for DeepONets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 62\n6.7 Physics-Informed DeepONets . . . . . . . . . . . . . . . . . . . . . . . . . . . . .", "mimetype": "text/plain", "start_char_idx": 1066, "end_char_idx": 2405, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e9e45378-07d2-41da-bfcb-4a5d282c4ebf": {"__data__": {"id_": "e9e45378-07d2-41da-bfcb-4a5d282c4ebf", "embedding": null, "metadata": {"page_label": "2", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7b547050-93f2-4e56-87a1-d88f740d3255", "node_type": "4", "metadata": {"page_label": "2", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "5d91c843ba01f934ae809f887558d59b175611741bf886f3439eaee43636d813", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8216b001-23d0-494c-a426-3dcbe88cc54f", "node_type": "1", "metadata": {"page_label": "2", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "9a0eb32b6668b9aafd50bdf83ced1b9b7e38e3f83545b6ccf7440160d6176096", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "361c2b8e-de61-4f90-8a25-83c6ebe308c3", "node_type": "1", "metadata": {}, "hash": "be3cae5f6946079cec321a2206e80581cdefdb5c2f993f468febafe77ca332b6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61\n6.6 Error Analysis for DeepONets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 62\n6.7 Physics-Informed DeepONets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63\n6.8 Fourier Neural Operators - Architecture . . . . . . . . . . . . . . . . . . . . . . . 64\n6.9 Discretization of the Fourier Neural Operator . . . . . . . . . . . . . . . . . . . . 66\n6.10 The Use of Fourier Transforms . . . . . . . . . . . . . . . . . . . . . . . . . . . . 67\n7 Probabilistic Deep Learning 70\n7.1 Key elements of Probability Theory . . . . . . . . . . . . . . . . . . . . . . . . . . 70\n7.2 Random Variables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 72\n7.2.1 Cumulative distribution function . . . . . . . . . . . . . . . . . . . . . . . 72\n7.2.2 Probability density function . . . . . . . . . . . . . . . . . . . . . . . . . . 74\n7.2.3 Examples of Important RVs . . . . . . . . . . . . . . . . . . . . . . . . . . 75\n7.2.4 Expectation and variance of RVs . . . . . . . . . . . . . . . . . . . . . . . 76\n7.2.5 Pair of RVs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 77\n7.3 Unsupervised probabilistic deep learning algorithms . . . . . . . . . . . . . . . . . 79\n7.3.1 GANs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .", "mimetype": "text/plain", "start_char_idx": 2158, "end_char_idx": 3532, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "361c2b8e-de61-4f90-8a25-83c6ebe308c3": {"__data__": {"id_": "361c2b8e-de61-4f90-8a25-83c6ebe308c3", "embedding": null, "metadata": {"page_label": "2", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7b547050-93f2-4e56-87a1-d88f740d3255", "node_type": "4", "metadata": {"page_label": "2", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "5d91c843ba01f934ae809f887558d59b175611741bf886f3439eaee43636d813", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e9e45378-07d2-41da-bfcb-4a5d282c4ebf", "node_type": "1", "metadata": {"page_label": "2", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "7e89968d242cb176af7990f26cd0d730c95c80634acbf0697da636d56d74ac58", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 77\n7.3 Unsupervised probabilistic deep learning algorithms . . . . . . . . . . . . . . . . . 79\n7.3.1 GANs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 79\n7.4 Supervised probabilistic deep learning algorithms . . . . . . . . . . . . . . . . . . 82\n7.4.1 Conditional GANs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 83\n2", "mimetype": "text/plain", "start_char_idx": 3284, "end_char_idx": 3718, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ae8c764a-8be7-4380-883c-3e8105993751": {"__data__": {"id_": "ae8c764a-8be7-4380-883c-3e8105993751", "embedding": null, "metadata": {"page_label": "3", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "39681515-8c85-4abb-ab62-c4d0f0977426", "node_type": "4", "metadata": {"page_label": "3", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "2d91b0114ec7dd6f9042b946aedd07c0e70a6da4c4a4648a9e01345aef857308", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Preface\nThese notes were compiled as lecture notes for a course developed and taught at the University\nof the Southern California. They should be accessible to a typical engineering graduate student\nwith a strong background in Applied Mathematics.\nThe main objective of these notes is to introduce a student who is familiar with concepts\nin linear algebra and partial di\ufb00erential equations to select topics in deep learning. These\nlecture notes exploit the strong connections between deep learning algorithms and the more\nconventional techniques of computational physics to achieve two goals. First, they use concepts\nfrom computational physics to develop an understanding of deep learning algorithms. Not\nsurprisingly, many concepts in deep learning can be connected to similar concepts in computational\nphysics, and one can utilize this connection to better understand these algorithms. Second,\nseveral novel deep learning algorithms can be used to solve challenging problems in computational\nphysics. Thus, they o\ufb00er someone who is interested in modeling a physical phenomena with a\ncomplementary set of tools.\n3", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1115, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d7e9081e-e29d-441f-b2a9-aa6509a8db9a": {"__data__": {"id_": "d7e9081e-e29d-441f-b2a9-aa6509a8db9a", "embedding": null, "metadata": {"page_label": "4", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "86094565-3380-4e17-9a7a-493cbe5da686", "node_type": "4", "metadata": {"page_label": "4", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "f6adf67971b79fc82346ad6afd9e3ec83d92d9aa9f22fd50eeaefd1cf187f460", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Chapter 1\nIntroduction\nThis course deals with topics that lie at the interface ofcomputational physics and machine\nlearning. Before we can appreciate the need to combine both these important concepts, we need\nto understand what each of them mean on their own.\n1.1 Computational physics\nComputational physics plays a fundamental role in solving many problems in \ufb01elds of science and\nengineering. To gain an understanding of this concept, we brie\ufb02y outline the key steps involved\nin solving a physical problem:\n1. Consider a physical phenomena and collect measurements of some observable of interest.\nFor example, the measurements of the water height and wave direction obtain from ocean\nbouys, when studying oceanic waves.\n2. Based on the observations, postulate aphysical law. For instance, you observe that the\nmass of \ufb02uid is a closed-system is conserved for all time.\n3. Write down a mathematical description of the law. This could make use of ordinary\ndi\ufb00erential equations (ODEs), partial di\ufb00erential equations (PDEs), integral equations, etc.\n4. Once the mathematical model is framed, solve for the solution of the system. There are\ntwo ways to obatin this:\n(a) In certain situations an exact analytical form of the solution can be obtained. For\ninstance one could solve ODEs/PDEs using separation of variables, Laplace transforms,\nFourier transforms or integration factors.\n(b) In most scenarios, exact expressions of the solution cannot be obtained and must\nbe suitable approximated using a numerical algorithm. For instance, one could use\nforward or backward Euler, mid-point rule, or Runge-Kutta schemes for solving\nsystems of ODEs; or one could use \ufb01nite di\ufb00erence/volume/element methods methods\nfor solving PDEs.\n5. Once the algorithm to evaluate the solution (exactly or approximately) is designed, use it\nto validate the mathematical model, i.e., see if the predictions are in tune with the data\ncollected.\nAll these steps broadly describe what computational physics entails.\n4", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1991, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "2a3d6128-83ba-4206-b97b-0596ab3d51b3": {"__data__": {"id_": "2a3d6128-83ba-4206-b97b-0596ab3d51b3", "embedding": null, "metadata": {"page_label": "5", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7447810b-a508-49de-b11a-06cce427985d", "node_type": "4", "metadata": {"page_label": "5", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "8dc1bad59e0da325f8fb225d6e87ecfcdc2a1d7fdbe51f1243e056c65a22e56d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "1.2 Machine learning\nUnlike computational physics, machine learning (ML) does not require the postulation of a\nphysical law. The general steps involved are:\n1. Collect data by observing the physical phenomena, by real-time measurements of some\nobservable or by using an numerical solver approximating the phenomenon.\n2. Train a suitable algorithm using the collected data, with the aim of discovering a pattern\nor relation between the various samples. See Section 1.2.1 for some concrete examples.\n3. Once trained, use the ML algorithm to make future predictions, and validate it with\nadditional collected data.\n1.2.1 Examples of ML\n1. Regression algorithms: Given the set of pairwise data{(xi,yi) : 1 \u2264i \u2264N}which\ncorresponds to some unknown functiony= f(x), \ufb01t a polynomial (or any other basis) to\nthis data set in order to approximatef. For instance, \ufb01nd the coe\ufb03cientsa,b of the linear\n\ufb01t \u02dcf(x; a,b) = ax+ b to minimize the error\nE(a,b) =\nN\u2211\ni=1\n|yi \u2212\u02dcf(xi)|2.\nIf (a\u2217,b\u2217) = arg mina,bE(a,b), then we can consider\u02dcf\u2217(x) := \u02dcf(x; a\u2217,b\u2217) to be the approxi-\nmation off(x) (see Figure 1.1(a)).\n2. Decision trees:We are given a dataset from a sample population, containing the features:\nage, income. Furthermore, the data is divided into two groups; an individual in Group A\nowns a house while an individual in Group B does not. Then, given a features of a new\ndata point, we would like to predict the probability of this new individual owning a house.\nDecision trees can be used to solve thisclassi\ufb01cation problem. The way a typical decision\ntree works is by making cuts the maximize the group-based separation for the samples in\nthe dataset (see Figure 1.1(b)). Then, based on these cuts, the algorithm determines the\nprobability of belonging to a particular class/group for a new point.\n3. Clustering algorithms:Given a set of data with a number of features per sample, \ufb01nd\ncluster/patterns in the data (see Figure 1.1(c)).\n1.2.2 Types of ML algorithms based leaning task\nBroadly speaking, there are four types of ML algorithms:\n1. Supervised learning:Given the dataS= {(xi,yi) : 1 \u2264i\u2264N}, predict \u02c6yfor some new\n\u02c6x, such that(\u02c6x,\u02c6y) /\u2208S. For instance, given a set of images and image labels (e.g. dog,\ncat, cow, etc), train a classi\ufb01cation ML algorithm to learn the relation between images and\nlabels, and use it to predict the label of a new image.\n2. Unsupervised learning: Given the dataS= {xi \u2208\u2126x : 1 \u2264i \u2264N}, \ufb01nd a relation\namong di\ufb00erent regions of\u2126x. For instance, \ufb01nd clusters in the dataset, or \ufb01nd an expression\nfor the probability distributionpx(x) governing the spread of this data and generate new\nsamples from it.\n5", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2628, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "4e7a4ac9-8f75-4056-897e-35cfc16e073a": {"__data__": {"id_": "4e7a4ac9-8f75-4056-897e-35cfc16e073a", "embedding": null, "metadata": {"page_label": "6", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "63eea7af-fb3c-447e-a615-96c7fa6bd98e", "node_type": "4", "metadata": {"page_label": "6", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "119897b3a600771a9f0da6638d3026f3f00d327859812b3c234214f41b7780aa", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "(a) Linear regression\n (b) Decision tree\n (c) Clustering\nFigure 1.1: Examples of ML\n3. Semi-supervised learning: This family of methods falls between the supervised and\nunsupervised learning families. They typically make use of a combination of labelled and\nunlabelled data for training. For example let\u2019s say we are given 10,000 images that are\nunlabeled and only 50 images that are labeled. Can we use this dataset to develop an\nimage classi\ufb01cation algorithm?\n4. Re-inforcement learning:The methods belonging to this family learn driven by rewards\nor penalties for decisions taken. Thus, a suitable path/policy is learned to maximize the\nreward. These kinds of methods are used to train algorithms to play chess or Go.\nIn this course, we will primarily focus on the \ufb01rst two types of ML algorithms.\n1.3 Arti\ufb01cial Intelligence, Machine Learning and Deep Learning\nAt times, the terms Arti\ufb01cial Intelligence (AI), ML and Deep Learning (DL) are used inter-\nchangeably. In reality, these are three related but di\ufb00erent concepts. This can be understood by\nlooking at the Venn diagram in Figure 1.2.\nFigure 1.2: The relation between AI, ML and DL\nAI refers to a system with human-like intelligence. While ML is a key component of an AI\nsystem, there are other ingredients involved. A self-driving car is a prototypical example of AI.\n6", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1330, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "100c7829-edde-4cb0-bbd2-03324a0100c6": {"__data__": {"id_": "100c7829-edde-4cb0-bbd2-03324a0100c6", "embedding": null, "metadata": {"page_label": "7", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "12e1ca77-8426-4d3d-ab10-3879b17971e5", "node_type": "4", "metadata": {"page_label": "7", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "e3ed043eda8378b90f8f3c14bb9228b12c43872b9c34251106304e07afbe5abe", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Let\u2019s take a closer look at the design of such a system (see Figure 1.3). A car is mounted with a\ncamera which takes lives images/video of the road ahead. These frames are then passed to an\nML algorithm which performs a semantic segmentation, i.e., segments out di\ufb00erent regions of\nthe frame and classi\ufb01es the type of object (car, tree, road, sky, etc) in each segment. Once this\nsegmentation is done, it is passed to adecision systemthat decides the next action of the car\nshould be based on this segmented image. This information then passes though a control module\nthat actually controls the mechanical actions of the car. This entire process is mimics what a\nreal driver would do, and is thus arti\ufb01cial intelligence.\nOn the other hand, machine learning (ML) are the components of this system that are trained\nusing data. That is they learn through data. In the example above, the Semantic Segmenter is\none such system. There are many ML algorithms that can perform this task using data, and we\nwill learn some in this course. The Decision System could also be an ML component - where the\nappropriate decision to be made is learnt from prior data. However, it could be non-ML. Perhaps\na rules based expert system.\nFigure 1.3: Schematic of AI system for a self-driving car. Some illustration taken from [32].\nDL is a subset of ML algorithms. The simplest form of a DL architecture, known as a feed-\nforward network. It comprises a number of layers of non-linear transformations. The non-linear\ntransformations are applied (component-wise) to an a\ufb03ne transformation of an intermediate\noutput. This architecture is loosely motivated by how signals are transmitted by the central\nnervous in living organisms. We will study the DL architecture in greater detail in Chapter 2.\n1.4 Machine learning and computational physics\nNow that we have a better understanding of computational physics and ML, the next obvious\nquestion would be \u201cwhy do we need to look at a combination of the two?\u201d We list down a few\nmotivations below:\n\u2022 For complex patterns of \u201cphysical\u201d data, ML provides an alternate route to representing\nmathematical laws. Consider a physical process that contains two important components.\nOf these, one is well understood and has a trusted mathematical model, and the other is\npoorly understood and does not have a mathematical description. In this scenario, one\nmay use computational physics for the \ufb01rst component and ML for the second. A concrete\nexample of this would be a system governed by conservation of energy and a complex\nconstitutive model. For the former we may have a well understood mathematical model,\nwhile for the latter we may have to rely on ML to develope a model.\n\u2022 ML in general is very data hungry. But the knowledge of physics can help restrict the\n7", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2784, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c637d880-3c53-44a8-87f3-ab72b6c2d5be": {"__data__": {"id_": "c637d880-3c53-44a8-87f3-ab72b6c2d5be", "embedding": null, "metadata": {"page_label": "8", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0d854286-079e-4779-bef3-d571491f4ce4", "node_type": "4", "metadata": {"page_label": "8", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "c0661303fb6383fb2588abe98a957e43edff15fdb5a16198f6663357fb1986e8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "manifold on which the input and solution/predictions lie. With such constraints, we can\nreduce the amount of data required to train the ML algorithm.\n\u2022 Tools for analyzing computational physics (functional analysis, numerical analysis, notions\nof convergence to exact solutions, probabilistic frameworks) carry over to ML. Applying\nthese tools to ML helps us better understand and design better ML algorithms.\nWe brie\ufb02y summarize the various topics that will be covered in this course:\n\u2022 Deep Neural Networks (MLPs) and their convergence.\n\u2022 Resnets and their connections with non-linear ODEs (Neural ODEs).\n\u2022 Recurnets and their connections with nonlinear ODEs.\n\u2022 Convolutional neural networks and their connection to PDEs.\n\u2022 Stochastic gradient descent and how it is related to ODEs.\n\u2022 Deep Learning algorithms for solving PDEs.\n\u2022 Deep Learning algorithms for approximation operators.\n\u2022 Generative adversarial algorithms and their connection to computational physics.\n8", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 970, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1ddcc79c-d999-4448-b698-49fd13bf9bbb": {"__data__": {"id_": "1ddcc79c-d999-4448-b698-49fd13bf9bbb", "embedding": null, "metadata": {"page_label": "9", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d8e6f09a-1c6f-4d08-a08e-608525b982f1", "node_type": "4", "metadata": {"page_label": "9", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "578576b4ec68c3caabcdebcbdf505f2049d51c748cd10a2f7b30700df521d605", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Chapter 2\nIntroduction to deep neural networks\nIn this chapter, we will take a closer look at the simplest network architecture that is available\nknown asmultilayer perceptron(MLP).\n2.1 MLP architecture\nLet us de\ufb01ne our objective as the approximation of a functionf : x\u2208Rd \u21a6\u2192y\u2208RD using an\nMLP, which we denote asF. Computing units of an MLP, calledarti\ufb01cial neurons, are stacked\nin a number of consecutive layers. The zeroth layer ofF is called thesource layer, which is not\na computing layer but is only responsible for providing an input (of dimensiond) to the network.\nThe last layer ofF is known as theoutput layer, which outputs the network\u2019s prediction (of\ndimension D). Every other layer in between is known as ahidden layer. The number of neurons\nin a layer de\ufb01nes the width of that layer. A schematic of an MLP with 2 hidden layers is shown\nin Figure 2.1.\nx(l)\ni = \u03c3(W (l)\ni j x(l\u22121)\nj +b(l)\ni )\nx(0) x(1) x(2) x(3)\nAct. func.\nAct. func.\nOut. func.\nSource Layer\nHidden Layer 1 Hidden Layer 2\nOutput Layer\nFigure 2.1: MLP with 2 hidden layers\nTo understand the operations occurring inside an MLP, let us de\ufb01ne some notations. We\nconsider a network withL hidden layers, with the width of layer(l) denoted as Hl for l =\n0,1,...,L +1. Note that for consistency with the functionf that we are trying to approximate, we\nmust haveH0 = d and HL+1 = D. Let us denote the output vector forl-th layer byx(l) \u2208RHl,\nwhich will serve as the input to the next layer. We setx(0) = x\u2208Rd which will be the input\nsignal provided by the input layer. In each layerl, 1 \u2264l\u2264L+ 1, thei-th neuron performs an\n9", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1594, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f27f5465-af2e-447d-a6a8-bea50657e0b6": {"__data__": {"id_": "f27f5465-af2e-447d-a6a8-bea50657e0b6", "embedding": null, "metadata": {"page_label": "10", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1e08d2fe-8ac0-4f64-a56b-0decdb99d71c", "node_type": "4", "metadata": {"page_label": "10", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "1709d83a3bb97eda58162e9ca31ab2f588123a3c53a07e60c43fa8241c159d15", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "a\ufb03ne transformation on that layers inputx(l\u22121) followed by a non-linear transformation\nx(l)\ni = \u03c3\n(\nW(l)\nij x(l\u22121)\nj\ued19 \ued18\ued17 \ued1a\nEinstein sum\n+b(l)\ni\n)\n, 1 \u2264i\u2264Hl, 1 \u2264j \u2264Hl\u22121 (2.1)\nwhere W(l)\nij and b(l)\ni are respectively known as the weights and bias associated withi-th neuron\nof layerl, while the function\u03c3(.) is known as theactivation function, and plays a pivotal role in\nhelping the network to represent non-linear complex functions. If we setW(l) \u2208RHl\u22121\u00d7Hl to be\nthe weight matrix for layerl and b(l) \u2208RHl to be the bias vector for layerl, then we can re-write\nthe action of the whole layer as\nx(l) = \u03c3\n(\nA(l)(x(l\u22121))\n)\n, A(l)(x(l\u22121)) = W(l)x(l\u22121) + b(l) (2.2)\nwhere the activation function is applied component-wise. Thus, the action of the whole network\nF : Rd \u21a6\u2192RD can be mathematically seen as a composition of alternating a\ufb03ne transformations\nand component-wise activations\nF(x) = A(L+1) \u25e6\u03c3\u25e6A(L) \u25e6\u03c3\u25e6A(L\u22121) \u25e6\u00b7\u00b7\u00b7\u25e6 \u03c3\u25e6A(1)(x). (2.3)\nWe make a few remarks here:\n1. For simplicity of the representation, we assume that the same activation function is used\nacross all layers of the network. However, this is not a strict rule. In fact, there is recent\nevidence that suggests that alternating activation function from layer to layer leads to\nbetter neural networks [33].\n2. At times, there might be an output functionO instead of an activation function at the end\nof the output layer, which is typically used to reformulate the output into a suitable form.\nWe will see examples of such functions later in the course.\n3. We will use the termdepth of the network to denote the number of computing layers in\nthe MLP, i.e. the number of hidden layers and the output layer, which would beL+ 1 as\nper the notations used above.\nThe parameters of the network is all the weights and biases, which we will represent as\n\u03b8= {W(l),b(l)}L+1\nl=1 \u2208RN\u03b8\nwhere N\u03b8 denotes the total number of parameters of the network. The networkF(x; \u03b8) represents\na family of parameterized functions, where\u03b8 needs to suitably chosen such that the network\napproximates the target functionf(x) at the inputx.\nQuestion 2.1.1.Prove thatN\u03b8 = \u2211L+1\nl=1 (Hl\u22121 + 1)Hl.\n2.2 Activation functions\nThe activation function is perhaps the most important component of an MLP. A large number of\nactivations are available in literature, each with its own advantages and disadvantages. Let us\ntake a look at a few of these options (also see Figure 2.2).\n10", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2401, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "9645e3c7-3ed6-46dc-b693-c114f5ddad7a": {"__data__": {"id_": "9645e3c7-3ed6-46dc-b693-c114f5ddad7a", "embedding": null, "metadata": {"page_label": "11", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a63198f0-3d82-4093-9c75-253d3fa4b043", "node_type": "4", "metadata": {"page_label": "11", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "5e4c952cf82d9d284d3a1e3c570e9b0a84e89ed480a3771a38e30e498657ee6e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "\u03be\n\u03c3(\u03be)\n(a) Linear\n\u03be\n\u03c3(\u03be) (b) ReLU\n\u03b1 = 0.1 \u03be\n\u03c3(\u03be) (c) Leaky ReLU\n1\n\u03be\n\u03c3(\u03be)\n(d) Logistic\n1\n\u22121\n\u03be\n\u03c3(\u03be) (e) Tanh\n1\n\u22121\n\u03be\n\u03c3(\u03be) (f) Sine\nFigure 2.2: Examples of activation functions\n11", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 175, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "19b0c44c-2fc0-409c-a866-743222f5eea9": {"__data__": {"id_": "19b0c44c-2fc0-409c-a866-743222f5eea9", "embedding": null, "metadata": {"page_label": "12", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "102c4efc-eeb7-41c8-b875-39ba595955a3", "node_type": "4", "metadata": {"page_label": "12", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "8e46ba9dc85c2adf0df7314a3b2152ab70e3e636b152e7e7fdc7ea7ee491e106", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "2.2.1 Linear activation\nThe simplest activation corresponds to\u03c3(\u03be) = \u03be. Some features of this function are\n\u2022 The function is in\ufb01nitely smooth, but all derivatives beyond the second derivative are zero.\n\u2022 The range of the function is(\u2212\u221e,\u221e).\n\u2022 Using the linear activation function (in all layers) will reduce the entire network to a single\na\ufb03ne transformation of the inputx. In other words, the network will be nothing more that\na linear approximation of the target functionf, which is not useful iff is highly non-linear.\n2.2.2 Recti\ufb01ed linear unit (ReLU)\nThis function is piecewise linear and de\ufb01ned as\n\u03c3(\u03be) = max{0,\u03be}=\n{\n\u03be, if \u03be\u22650\n0, if \u03be <0 (2.4)\nThis is one of the most popular activation functions used in practice. Some features of this\nfunction are:\n\u2022 The function is continuous, while its derivative will be piecewise constant with a jump\n\u03be= 0. The second derivative will be a dirac function concentrated at\u03be= 0. In other words,\nthe higher-order derivates (greater than 1) are not well-de\ufb01ned.\n\u2022 The range of the function is[0,\u221e).\n2.2.3 Leaky ReLU\nThe ReLU activation leads to a null output from a neuron if the a\ufb03ne transformation of the\nneuron is negative. This can lead to the phenomena ofdying neurons[15] while training a neural\nnetwork, where neurons drops out completely from the network and no longer contribute to the\n\ufb01nal prediction. To overcome this challenge, a leaky version ReLU was designed\n\u03c3(\u03be; \u03b1) =\n{\n\u03be, if \u03be\u22650\n\u03b1\u03be, if \u03be <0 (2.5)\nwhere \u03b1 becomes a networkhyper-parameter. Some features of this function are:\n\u2022 The derivates of Leaky ReLU behave in the same way as those for ReLU.\n\u2022 The range of the function is(\u2212\u221e,\u221e).\n2.2.4 Logistic function\nThe Logistic or Sigmoid activation function is given by\n\u03c3(\u03be) = 1\n1 + e\u2212\u03be (2.6)\nand has the following properties\n\u2022 The function is in\ufb01nitely smooth and monotonic.\n\u2022 The range of the function is(0,1), i.e., the function is bounded. Such a function is useful\nin representing probabilities.\n\u2022 Since the derivative quickly decays to zero away from\u03be= 0, this activation function can\nlead to slow convergence of the network while training.\n12", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2100, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "bb111975-0519-46e5-bb2a-d56d98ba2bf7": {"__data__": {"id_": "bb111975-0519-46e5-bb2a-d56d98ba2bf7", "embedding": null, "metadata": {"page_label": "13", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "36f33a13-8462-4ceb-8ee2-4c23704a09ff", "node_type": "4", "metadata": {"page_label": "13", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "99683f125e386de9dea4bdcb7412d0e52ad25afa0be7ff59a7ce6cacda4cf5ea", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "2.2.5 Tanh\nThe tanh function is can be seen as a symmetric extension of the logistic function\n\u03c3(\u03be) = e\u03be \u2212e\u2212\u03be\ne\u03be + e\u2212\u03be (2.7)\nand has the following properties\n\u2022 The function is in\ufb01nitely smooth and monotonic.\n\u2022 The range of the function is(\u22121,1), i.e., the function is bounded. Note that it maps zeros\ninput to zero, while pushing positive (negative) inputs to +1 (-1).\n\u2022 Similar to the logistic function, the derivative of tanh quickly decays to zero away from\n\u03be= 0 and can thus lead to slow convergence while training networks.\n2.2.6 Sine\nRecently, the sine function, i.e.,\u03c3(\u03be) = sin(\u03be) has been proposed as an e\ufb03cient activation function\n[27]. It has the best features of all the activation function discussed above:\n\u2022 The function is in\ufb01nitely smooth.\n\u2022 The range of the function is(\u22121,1), i.e., the function is bounded.\n\u2022 None of the derivatives of this function decay to zero.\nQuestion 2.2.1.Can you think of an MLP architecture with the sine activation function, which\nleads to an approximation very similar to a Fourier series expansion?\n2.3 Expressivity of a network\nLet us try to understand the e\ufb00ects ofN\u03b8 increases. To see this, let us consider a simple example\nusing the ReLU activation function, i.e.,\u03c3(\u03be) = max{\u03be,0}. We setd= D= 1, L= 1 and the\nparameters\nW(1) =\n[2\n1\n]\n, b(1) =\n[\u22122\n0\n]\n, W(2) =\n[\n1 1\n]\n, b (2) = 0.\nas shown in Figure 2.3(a). Then the various layer outputs are\nx(1)\n1 = max{2x(0)\n1 \u22122,0}, x (1)\n2 = max{x(0)\n1 ,0}, x (2)\n1 = max{2x(0)\n1 \u22122,0}+ max{x(0)\n1 ,0}.\nNotice that while the the outputx(1) of the hidden layer (see Figures 2.3(b) and (c)) have only\none corner/kink, the \ufb01nal output ends up having two kinks (see Figures 2.3(d)).\nWe generalize this formulation to a bigger network withL hidden layers each of widthH.\nThen one can expect thatx(1)\ni , 1 \u2264i\u2264H will have a single kink, with the location and angle of\nthe kink depending on the weights and bias associated with each neuron of the hidden layer. The\nvector x(1) is passed to the next hidden layer, where each neuron will combine the single kinks\nand give an output with possiblyH kinks. Once again, the location and angles of theH kinks in\nthe output from each neuron of the second hidden layer will be di\ufb00erent. The location of the\nkinks will be di\ufb00erent because each neuron is allowed a di\ufb00erent bias, and therefore can induce\na di\ufb00erent shift. Continuing this argument, one can expect the number of kinks to increase as\nH, H2, H3 as it passes through the various hidden layers with widthH. In general the total\nnumber of kinks can grow asHL. In other words, the networks have the ability to become more\nexpressive as the depth (and width) of the network is increased.\n13", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2669, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f0789c36-bc8b-4ffa-8f2b-da3c70ab7a1b": {"__data__": {"id_": "f0789c36-bc8b-4ffa-8f2b-da3c70ab7a1b", "embedding": null, "metadata": {"page_label": "14", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f450d412-a1d5-4f30-a705-4d92ca30de83", "node_type": "4", "metadata": {"page_label": "14", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "9d8d40a96d7e21662352da5dd92ec554dcc704b3c2299cdc71e6919dd01eac67", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "w = 2 b = \u22122\nw = 1 b = 0\nw = 1\nb = 0\nw = 1\n(0) (1) (2)\n(a) MLP withL= 1,W = 2\nx(0)\n1\nx(1)\n1\n0\n4\n4\nOne kink (b) x(1)\n1 vs x(0)\n1\nx(0)\n1\nx(1)\n2\n0\n4\n4\nOne kink\n(c) x(1)\n2 vs x(0)\n1\nx(0)\n1\nx(2)\n1\n0\n4\n4\nTwo kinks (d) x(2)\n1 vs x(0)\n1\nFigure 2.3: Examples to understand the expressivity of neural networks\n14", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 302, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "80235b9c-74bb-40af-a3b7-33f3c6327b85": {"__data__": {"id_": "80235b9c-74bb-40af-a3b7-33f3c6327b85", "embedding": null, "metadata": {"page_label": "15", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7db9e367-ee68-4ea4-82f9-d99b87cef3c1", "node_type": "4", "metadata": {"page_label": "15", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "d53e239866f0885d45ded546fab80230070fa397e2f21390f000627033d9b809", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "2.3.1 Universal approximation results\nTo quantify the expressivity of networks in a mathematically rigorous manner, we look at some\nresults about the approximation properties of MLPs. For these results, we assumeK \u2282Rd is a\nclosed and bounded set.\nTheorem 2.3.1 (Pinkus, 1999 [23]). Let f : K \u2192R, i.e., D = 1, be a continuous function.\nThen given an\u03f5> 0, there exists an MLP with a single hidden layer (L= 1), arbitrary widthH\nand a non-polynomial continuous activation\u03c3 such that\nmax\nx\u2208K\n|F(x; \u03b8) \u2212f(x)|\u2264 \u03f5.\nTheorem 2.3.2(Kidger, 2020 [9]). Let f : K \u2192RD be a continuous vector-valued function.\nThen given an\u03f5> 0, there exists an MLP with arbitrary number of hidden layersL, each having\nwidth H \u2265d+ D+ 2, a continuous activation\u03c3 (with some additional mild conditions), such that\nmax\nx\u2208K\n\u2225F(x; \u03b8) \u2212f(x)\u2225\u2264 \u03f5.\nTheorem 2.3.3 (Yarotsky, 2021 [33]). Let f : K \u2192R be a function with two continuous\nderivates, i.e.,f \u2208C2(K). Consider an MLP with ReLU activations andH \u22652d+ 10. Then\nthere exists a network with this con\ufb01guration such that the error converges as\nmax\nx\u2208K\n|F(x; \u03b8) \u2212f(x)|\u2264 C(N\u03b8)\u22124\nwhere C is a constant depending on the number of network parameters.\nNumerical results like those mentioned above help demystify the \u201cblack-box\u201d nature of neural\nnetwork, and serve as useful practical guidelines when designing network architectures.\n2.4 Training, validation and testing of neural networks\nNow that we have a better understanding of the architecture of MLPs, we would now like to\ndiscuss how the parameters of these networks are set to approximate some target function. We\nrestrict our discussions to the framework of supervised learning.\nLet us assume that we are given a dataset of pairwise samplesS= {(xi,yi) : 1 \u2264i\u2264N}\ncorresponding to a target functionf : x\u21a6\u2192y. We wish to approximate this function using the\nneural network\nF(x; \u03b8,\u0398)\nwhere \u03b8are the network parameters de\ufb01ned before, while\u0398 corresponds to thehyper-parameters\nof the network such as the depthL+ 1, widthH, type of activation function\u03c3, etc. The strategy\nto design a robust network involves three steps:\n1. Find the optimal values of\u03b8(for a \ufb01xed\u0398) in thetraining phase.\n2. Find the optimal values of\u0398 in thevalidation phase.\n3. Test the performance of the network on unseen data on thetesting phase.\nTo accomplish these three tasks, it is \ufb01rst customary to split the datasetSinto three distinct\nparts: a training setwith Ntrain samples, avalidation set with Nval samples andtest set with\nNtest samples, withN = Ntrain + Nval + Ntest. Typically, one uses around 60% of the samples as\ntraining samples, 20% as validation samples and the remaining 20% for testing.\n15", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2635, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a48afe87-044f-4bba-926a-1f523e338f27": {"__data__": {"id_": "a48afe87-044f-4bba-926a-1f523e338f27", "embedding": null, "metadata": {"page_label": "16", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "923e5c9b-accb-4571-8cd8-11d062542bd3", "node_type": "4", "metadata": {"page_label": "16", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "8b0f24c5bb3505a7c0cd8a6aaad625ec050268e8a122aad695cd4990fc39390f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Splitting the dataset is necessary as neural networks are heavily over-parameterized functions.\nThe large number of degrees of freedom available to model the data can lead to over-\ufb01tting\nthe data. This happens when the error or noise present in the data drives the behavior of the\nnetwork more than the underlying input-output relation itself. Thus, a part of the data is used\nto determine\u03b8, and another part to determine the hyper-parameters\u0398. The remainder of the\ndata is kept aside for testing the performance of the trained network on unseen data, i.e., the\nnetwork\u2019s ability togeneralize well.\nNow let us discuss how this split is used during the three phases in further details:\nTraining: Training the network makes use of the training setStrain to solve the following\noptimization problem: Find\n\u03b8\u2217= arg min\n\u03b8\n\u03a0train(\u03b8), where \u03a0train(\u03b8) = 1\nNtrain\nNtrain\u2211\ni=1\n(xi,yi)\u2208Strain\n\u2225yi \u2212F(xi; \u03b8,\u0398)\u22252\nfor some \ufb01xed\u0398. The optimal\u03b8\u2217is obtained using a suitable gradient based algorithm (will be\ndiscussed later). The function\u03a0train is referred to as the loss function. In the example above we\nhave used the mean-squared loss function. Later we will consider other types of loss functions.\nValidation: Validation of the network involves using the validation setSval to solve the\nfollowing optimization problem: Find\n\u0398\u2217= arg min\n\u0398\n\u03a0val(\u0398), where \u03a0val(\u0398) = 1\nNval\nNval\u2211\ni=1\n(xi,yi)\u2208Sval\n\u2225yi \u2212F(xi; \u03b8\u2217,\u0398)\u22252.\nThe optimal\u0398\u2217is obtained using a techniques such as (random or tensor) grid search.\nTesting: Once the \"best\" network is obtained, characterized by\u03b8\u2217and \u0398\u2217, it is evaluated on\nthe test setStest to estimate the networks performance on data not used during the \ufb01rst two\nphases.\n\u03a0test = 1\nNtest\nNtest\u2211\ni=1\n(xi,yi)\u2208Stest\n\u2225yi \u2212F(xi; \u03b8\u2217,\u0398\u2217)\u22252.\nThis testing error is also known as the (approximate)generalizing errorof the network.\nLet\u2019s see an example to better understand how such a network is obtained\nExample 2.4.1. Let us consider an MLP where all hyper-parameters are \ufb01xed except for the\nfollowing \ufb02exible choices\n\u03c3\u2208{ReLU, tanh}, L \u2208{10,20}.\nWe use the following algorithm\n1. For each possible\u03c3,L pair:\n(a) Find \u03b8\u2217= arg min\u03b8\u03a0train(\u03b8)\n(b) With this\u03b8\u2217, evaluate\u03a0val(\u0398)\n2. Select \u0398\u2217 to be the one that gave the smallest value of\u03a0val(\u0398).\n3. Finally, report\u03a0test for this\u0398\u2217 and the corresponding\u03b8\u2217.\n16", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2294, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "34502f2c-174f-4b24-9295-7467d177b6cd": {"__data__": {"id_": "34502f2c-174f-4b24-9295-7467d177b6cd", "embedding": null, "metadata": {"page_label": "17", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "dc0734d4-8eb5-4574-a651-0aede047b15a", "node_type": "4", "metadata": {"page_label": "17", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "290fff0a42ed70315ebaa53572f0d21151fec6d88970d5ed5fb0435fb1f08bfa", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "2.5 Generalizability\nIf we train a network that has a small value of\u03a0train and \u03a0val, does it ensure that\u03a0test will be\nsmall? This question is addressed by studying thegeneralizability of the trained network, i.e., it\ncapability to perform well on data not seen while training/validating the network. If the network\nis trained to over\ufb01t the training data, the network will typically lead to poor predictions on test\ndata. Typically, ifStrain, Sval and Stest are chosen from the same distribution of data, then a\nsmall value of\u03a0train,\u03a0val can lead to small values of\u03a0test. Let us look at the commonly used\ntechnique to avoid data over\ufb01tting, calledregularization.\n2.5.1 Regularization\nNeural networks, especially MLPs, are almost alwaysover-parametrized, i.e.,N\u03b8 \u226bN where N is\nthe number of training samples. This would lead to a highly non-linear network model, for which\nthe loss function\u03a0(\u03b8) (where we omit the subscript \"train\" for brevity) can have a landscape\nwith many local minimas (see Figure 2.4(a)). Then how do we determine which minima leads to\na better generalization? To nudge the choice of\u03b8\u2217in a more favorable direction, a regularization\ntechnique can be employed.\n(a) Loss function landscape\n (b) Network sensitivity\nFigure 2.4: The e\ufb00ect of regularization on the loss function. We have assumed a scalar\u03b8 for\neasier illustration.\nThe simplest method of regularization involves augmenting a penalty term to the loss function:\n\u03a0(\u03b8) \u2212\u2192\u03a0(\u03b8) + \u03b1\u2225\u03b8\u2225, \u03b1 \u22650\nwhere \u03b1is a regularization hyper-parameter, and\u2225\u03b8\u2225is a suitable norm of the network parameters\n\u03b8. This augmentation can change the landscape of\u03a0(\u03b8) as illustrated in Figure 2.4(a). In other\nwords, such a regularization encourages the selection of a minima corresponding to smaller values\nof the parameters\u03b8.\nIt is not obvious why a smaller value of\u03b8would be a better choice. To see why this is better,\nconsider the intermediate network output\nx(1)\n1 = \u03c3(W(1)\n1j x(0)\nj + b(1)\n1 ),\nwhich gives\n\u2202x(1)\n1\n\u2202x(0)\n1\n= \u03c3\u2032(W(1)\n1j x(0)\nj + b(1)\n1 )W(1)\n11 \u221dW(1)\n11 .\n17", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2026, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5bec7abe-352f-4107-8b8f-ff3c752f2370": {"__data__": {"id_": "5bec7abe-352f-4107-8b8f-ff3c752f2370", "embedding": null, "metadata": {"page_label": "18", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9bb82254-c236-482c-94b0-1bd5f94184c2", "node_type": "4", "metadata": {"page_label": "18", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "4b6b467f3ba96676a7c3b151788d624f259f9c6ca66f77b456739e689299cbdf", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Since this derivate scales withW(1)\n11 , this implies that |\u2202F(x)\n\u2202x(0)\n1\n|scales with W(1)\n11 as well. If\n|W(1)\n11 |\u226b 1, then network would be very sensitive to even small changes in the inputx(0)\n1 , i.e.,\nthe network would be ill-posed. As illustrated in Figure 2.4(b), using a proper regularization\nwould help avoid over \ufb01tting.\nLet us consider some common types of regularization:\n\u2022 l2 regularization: Here we use thel2 norm in the regularization term\n\u2225\u03b8\u2225= \u2225\u03b8\u22252 =\n(N\u03b8\u2211\ni=1\n\u03b82\ni\n)1/2\n.\n\u2022 l1 regularization: Here we use thel1 norm in the regularization term\n\u2225\u03b8\u2225= \u2225\u03b8\u22251 =\nN\u03b8\u2211\ni=1\n|\u03b8i|,\nwhich promotes the sparsity of\u03b8.\n2.6 Gradient descent\nRecall that we wish to solve the minimization problem\u03b8\u2217= arg min\u03a0(\u03b8) in the training phase.\nThis minimization problem can be solved using gradient descent (GD), also known as steepest\ndescent. Consider the Taylor expansion about\u03b80\n\u03a0(\u03b80 + \u2206\u03b8) = \u03a0(\u03b80) + \u2202\u03a0\n\u2202\u03b8(\u03b80) \u00b7\u2206\u03b8+ \u22022\u03a0\n\u2202\u03b8i\u03b8j\n( \u02c6\u03b8)\u2206\u03b8i\u2206\u03b8j\nfor some \u02c6\u03b8in a small neighbourhood of\u03b80. When |\u2206\u03b8|is small and assuming\u22022\u03a0\n\u2202\u03b8i\u03b8j\nis bounded,\nwe can neglect the second order term and just consider the approximation\n\u03a0(\u03b80 + \u2206\u03b8) \u2248\u03a0(\u03b80) + \u2202\u03a0\n\u2202\u03b8(\u03b80) \u00b7\u2206\u03b8.\nIn order to lower the value of the loss function as much as possible compared to its evaluation at\n\u03b80, i.e. minimize\u2206\u03a0 = \u03a0(\u03b80 + \u2206\u03b8) \u2212\u03a0(\u03b80), we need to choose the step\u2206\u03b8 in the opposite\ndirection of the gradient, i.e.:\n\u2206\u03b8= \u2212\u03b7\u2202\u03a0\n\u2202\u03b8(\u03b80)\nwith the step-size\u03b7\u22650, also known as thelearning-rate. This is yet another hyper-parameter\nthat we need to tune during the validation phase. This is the crux of the GD algorithm, and can\nbe summarized as follows:\n1. Initialize k= 0 and \u03b80\n2. While |\u03a0(\u03b8k)|>\u03f51, do\n(a) Evaluate \u2202\u03a0\n\u2202\u03b8(\u03b8k)\n(b) Update \u03b8k+1 = \u03b8k \u2212\u03b7\u2202\u03a0\n\u2202\u03b8(\u03b8k)\n(c) Increment k= k+ 1\n18", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1709, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "831b9a68-e85f-4bd8-9fd7-8bbfa093bb43": {"__data__": {"id_": "831b9a68-e85f-4bd8-9fd7-8bbfa093bb43", "embedding": null, "metadata": {"page_label": "19", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8b0e73f7-027b-4e4e-92b7-20031e68e0cb", "node_type": "4", "metadata": {"page_label": "19", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "cb6c20100cb95fc3393cca81121278a3b07dac52de4fd12e18f7fa5397f61d13", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Convergence: Assume that\u03a0(\u03b8) is convex and di\ufb00erentiable, and its gradient is Lipschitz\ncontinuous with Lipschitz constantK. Then for a\u03b7\u22641/K, the GD updates converges as\n\u2225\u03b8\u2217\u2212\u03b8k\u22252 \u2264C\nk.\nHowever, in most scenarios\u03a0(\u03b8) is not convex. If there is more than one minima, then what\nkind of minima does GD like to pick? To answer this, consider the loss function for a scalar\u03b8 as\nshown in Figure 2.5, which has two valleys. Let\u2019s assume that the pro\ufb01le of\u03a0(\u03b8) in the each\nvalley can be approximated by a (centered) parabola\n\u03a0(\u03b8) \u22481\n2a\u03b82\nwhere a> 0 is the curvature of each valley. Note that the curvature of the left valley is much\nsmaller than the curvature of the right valley. Let\u2019s pick a constant learning rate\u03b7 and a starting\nvalue \u03b80 in either of the valleys. Then,\n\u2202\u03a0\n\u2202\u03b8(\u03b80) = a\u03b80\nand the new point after a GD update will be\u03b81 = \u03b80(1 \u2212a\u03b7). Similarly, it is easy to see that all\nsubsequent iterates write\u03b8k+1 = \u03b8k(1 \u2212a\u03b7). For convergence, we need\n\u23d0\u23d0\u23d0\u23d0\n\u03b8k+1\n\u03b8k\n\u23d0\u23d0\u23d0\u23d0<1 = \u21d2 |1 \u2212a\u03b7|<1.\nSince a> 0 in the valleys, we will need the following condition on the learning rate\n\u22121 <1 \u2212a\u03b7 =\u21d2 a\u03b7 <2.\nIf we \ufb01x\u03b7, then for convergence we need the local curvature to satisfya< 2/\u03b7. In other words,\nGD will prefer to converge to a minima with a \ufb02at/small curvature, i.e., it will prefer the minima\nin the left valley. If the starting point is in the right valley, there is a chance that we will keep\novershooting the right minima and bounce o\ufb00 the opposite wall till the GD algorithm slingshots\n\u03b8k outside the valley. After this it will enter the left valley with a smaller curvature and gradually\nmove towards its minima.\nFigure 2.5: GD prefers \ufb02atter minimas.\nWhile it is clear that GD prefers \ufb02at minima, what is not clear is why are \ufb02at minima better.\nThere is empirical evidence that the parameter values obtained at \ufb02at minima tend to generalize\nbetter, and therefore are to be preferred.\n19", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1878, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "482fe824-8664-48ec-8fca-96ea7735c5eb": {"__data__": {"id_": "482fe824-8664-48ec-8fca-96ea7735c5eb", "embedding": null, "metadata": {"page_label": "20", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "fd325f97-52db-439d-8df0-2598c173c248", "node_type": "4", "metadata": {"page_label": "20", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "5e0cdeb86ef84f2b62cb51097e8ddb2e24994d9087d384d261784969eb5cc15d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "2.7 Some advanced optimization algorithms\nWe discussed how GD can be used to solve the optimization problem involved in training neural\nnetworks. Let us look at a few advanced and popular optimization techniques motivated by GD.\nIn general, the update formula for most optimization algorithms make use of the following\nformula\n[\u03b8k+1]i = [\u03b8k]i \u2212[\u03b7k]i[gk]i, 1 \u2264i\u2264N\u03b8, (2.8)\nwhere[\u03b7k]i isthecomponent-wiselearningrateandthevector-valuedfunction gdepends/approximates\nthe gradient. Note that the notation[.]i is used to denote thei-th component of the vector. Also\nnote that the learning rate is allowed to depend on the iteration numberk. The GD method\nmakes use of\n[\u03b7k]i = \u03b7, gk = \u2202\u03a0\n\u2202\u03b8(\u03b8k).\nAn issue with the GD method is that the convergence to the minima can be quite slow if\u03b7 is\nnot suitably chosen. For instance, consider the objective function landscape shown in Figure\n2.6, which has sharper gradients along the[\u03b8]2 direction compared to the[\u03b8]1 direction. If we\nstart from a point, such as the one shown in the \ufb01gure, then if\u03b7 is too large (but still within the\nstable bounds) the updates will keep zig-zagging its way towards the minima. Ideally, for the\nparticular situation shown in Figure 2.6, we would like the steps to take longer strides along the\n[\u03b8]1 compared to the[\u03b8]2 direction, thus reaching the minima faster.\nFigure 2.6: Zig-zagging updates with GD.\nLet us look at two popular methods that are able to overcome some of the issues faced by\nGD.\n2.7.1 Momentum methods\nMomentum methods make use of the history of the gradient, instead of just the gradient at the\nprevious step. The formula for the update is given by\n[\u03b7k]i = \u03b7, gk = \u03b21gk\u22121 + (1 \u2212\u03b21)\u2202\u03a0\n\u2202\u03b8(\u03b8k), g\u22121 = 0\n20", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1687, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "abcd1762-d6e1-4d55-a86c-34290d7f1b8c": {"__data__": {"id_": "abcd1762-d6e1-4d55-a86c-34290d7f1b8c", "embedding": null, "metadata": {"page_label": "21", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "00e6d59c-b9ce-4256-a537-4c7c3657ef7d", "node_type": "4", "metadata": {"page_label": "21", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "a2bc7d27bdb1d29e0953bfd39d50f9bce9ed03409f488536b52d94f07194a21e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "where gk is a weighted moving average of the gradient. This weighting is expected to smoothen\nout the zig-zagging seen in Figure 2.6 by cancelling out the components of gradient along the[\u03b8]2\ndirection and move more smoothly towards the minima. A commonly used value for\u03b21 is 0.9.\n2.7.2 Adam\nThe Adam optimization was introduced by Kingma and Ba [10], which makes use of the history\nof the gradient as well the second moment (which is a measure of the magnitude) of the gradient.\nFor an initial learning rate\u03b7, the updates are given by\ngk = \u03b21gk\u22121 + (1 \u2212\u03b21)\u2202\u03a0\n\u2202\u03b8(\u03b8k)\n[Gk]i = \u03b22[Gk\u22121]i + (1 \u2212\u03b22)\n(\u2202\u03a0\n\u2202\u03b8i\n(\u03b8k)\n)2\n[\u03b7k]i = \u03b7\u221a\n[Gk]i + \u03f5\n(2.9)\nwheregk andGk aretheweightedrunningaveragesofthegradientsandthesquareofthegradients,\nrespectively. The recommended values for the hyper-parameters are\u03b21 = 0.9, \u03b22 = 0.999 and\n\u03f5= 10\u22128. Note that the learning rate for each component is di\ufb00erent. In particular, the larger\nthe magnitude of the gradient for a component the smaller is its learning rate. Referring back to\nthe example in Figure 2.6, this would mean a smaller learning rate for\u03b82 in comparison to\u03b81,\nand therefore will help alleviate the zig-zag path of the optimization algorithm.\nRemark 2.7.1. The Adam algorithm also has additional correction steps forgk and Gk to\nimprove the e\ufb03ciency of the algorithm. See [10] for details.\n2.7.3 Stochastic optimization\nWe note that the training loss can be rewritten as\n\u03a0(\u03b8) = 1\nNtrain\nNtrain\u2211\ni=1\n\u03a0i(\u03b8), \u03a0i(\u03b8) = \u2225yi \u2212F(xi; \u03b8,\u0398)\u22252\nThus, the gradient of the loss function is\n\u2202\u03a0\n\u2202\u03b8(\u03b8) = 1\nNtrain\nNtrain\u2211\ni=1\n\u2202\u03a0i\n\u2202\u03b8(\u03b8)\nHowever, taking the summation of gradients can be very expensive sinceNtrain is typically very\nlarge, Ntrain \u223c106. One easy way to circumvent this problem is to use the following update\nformula (shown here for the GD method)\n\u03b8k+1 = \u03b8k \u2212\u03b7k\n\u2202\u03a0i\n\u2202\u03b8(\u03b8k), (2.10)\nwhere i is randomly chosen for each update step k. This is known as stochastic gradient\ndescent. Remarkably, this modi\ufb01ed algorithm does converge assuming that\u03a0i(\u03b8) is convex\nand di\ufb00erentiable, and\u03b7k \u223c1/\n\u221a\nk [19]. To illustrate why\u03b7k needs to decay, consider the toy\nfunction(s) for\u03b8\u2208R2\n\u03a01(\u03b8) = ([\u03b8]1 \u22121)2 + ([\u03b8]2 \u22121)2, \u03a02(\u03b8) = ([\u03b8]1 + 1)2 + 0.5([\u03b8]2 \u22121)2,\n\u03a03(\u03b8) = 0.7([\u03b8]1 + 1)2 + 0.5([\u03b8]2 + 1)2, \u03a04(\u03b8) = 0.7([\u03b8]1 \u22121)2 + 1\n2([\u03b8]2 + 1)2,\n\u03a0(\u03b8) = 1\n4 (\u03a01(\u03b8) + \u03a02(\u03b8) + \u03a03(\u03b8) + \u03a04(\u03b8)) .\n(2.11)\n21", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2305, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a8ae107b-a548-4de3-a049-7ead7caa8ed7": {"__data__": {"id_": "a8ae107b-a548-4de3-a049-7ead7caa8ed7", "embedding": null, "metadata": {"page_label": "22", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "cdff2e2b-da73-41cf-bbb7-68fb83ce9f53", "node_type": "4", "metadata": {"page_label": "22", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "6afbb34906eb45d216a286cc1144999860d36546bebc7de5076c36d0a3cc1b6d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The contour plots of these functions in shown in Figure 2.7(a), where the black contour plots\ncorresponds to \u03a0(\u03b8). Note that the\u03b8\u2217 = (0,0) is the unique minima for\u03a0(\u03b8). We consider\nsolving with the SGD algorithm with a constant learning rate\u03b7k = 0.4 and a decaying learning\nrate \u03b7k = 0.4/\n\u221a\nk. Starting with\u03b80 = (\u22121.0,2.0) and randomly selectingi\u22081,2,3,4 for each\nstep k, we run the algorithm for 10,000 iterations. The \ufb01rst 10 steps with each learning rate is\nplotted in Figure 2.7(a). We can clearly see that without any decay in the learning rate, the\nSGD algorithm keeps overshooting the minima. In fact, this behaviour continues for all future\niterations as can be seen in Figure 2.7(b) where the norm of the updates does not decay (we\nexpect it to decay to|\u03b8\u2217|= 0). On the other hand, we quickly move closer to\u03b8\u2217if the learning\nrate decays as1/\n\u221a\nk.\nThe reason for reducing the step size as we approach closer to the minima is that far away\nfrom the minima for\u03a0 the gradient vector for\u03a0 and all the individual\u03a0i\u2019s align quite well.\nHowever, as we approach closer to the minima for\u03a0 this is not the case and therefore one is\nrequired to take smaller steps so as not be thrown o\ufb00 to a region far away from the minima.\n(a) Function contours and paths\n (b) Norm of updates\nFigure 2.7: SGD algorithm with and without a decay in the learning rate.\nIn practice, stochastic optimization algorithms are not used for the following reasons:\n1. Although the loss function decays with the number of iterations, it \ufb02uctuates in a chaotic\nmanner close the the minima and never manages to reach the minima.\n2. While handling all samples at once can be computationally expensive, handling a single\nsample at a time severly under-utilizes the computational and memory resources.\nHowever, a compromise can be made by usingmini-batch optimization. In this strategy, the\ndataset of Ntrain samples is split intoNbatch disjoint subsets known as mini-batches. Each\nmini-batch containsNtrain = Ntrain/Nbatch samples, which also refered to as the batch-size. Thus,\nthe gradient of the loss function can be approximated by\n\u2202\u03a0\n\u2202\u03b8(\u03b8) = 1\nNtrain\nNtrain\u2211\ni=1\n\u2202\u03a0i\n\u2202\u03b8(\u03b8) \u2248 1\nNtrain\n\u2211\ni\u2208batch(j)\n\u2202\u03a0i\n\u2202\u03b8(\u03b8). (2.12)\n22", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2188, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d6bfd81a-c863-4b8e-af80-40c16879f183": {"__data__": {"id_": "d6bfd81a-c863-4b8e-af80-40c16879f183", "embedding": null, "metadata": {"page_label": "23", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c4355eb6-0952-4918-b144-08ff2a7ea3a9", "node_type": "4", "metadata": {"page_label": "23", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "c013745e67774de237b6cdc965e07216290111e88185738c31b12d1abd8a6822", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Note that takingNbatch = 1 leads to the original optimization algorithms, while takeNbatch =\nNtrain gives the stochastic gradient descent algorithm. One typically chooses a batch-size to\nmaximize the amount of data that can be loaded into the RAM at one time. We de\ufb01ne anepoch\nas one full pass through all samples (or mini-batches) of the full training set. The following\ndescribes the mini-batch stochastic optimization algorithm:\n1. For epoch = 1, ..., J\n(a) Randomly shu\ufb04e the full training set\n(b) Create Nbatch mini-batches\n(c) For i= 1,\u00b7\u00b7\u00b7 ,Nbatch\ni. Evaluate the batch gradient using (2.12).\nii. Update \u03b8using this gradient and your favorite optimization algorithm (gradient\ndescent, momentum, or Adam).\nRemark 2.7.2.There is an interesting study [31] that suggests that stochastic gradient descent\nmight actually help in selecting minima that generalize better. In that study the authors prove\nthat SGD prefers minima whose curvature is more homogeneous. That is, the distribution of the\ncurvature of each of the components of the loss function is sharp and centered about a small value.\nThis is contrast to minima where the overall curvature might be small; however the distribution\nof the curvature of each component of loss function is more spread out. Then they go on to show\n(empirically) that the more homogeneous minima tend to generalize better than their heterogeneous\ncounterparts.\n2.8 Calculating gradients using back-propagation\nThe \ufb01nal piece of the training algorithm that we need to understand is how the gradients are\nactually evaluated while training the network. Recall the outputx(l+1) of layerl+ 1 is given by\nA\ufb03ne transform: \u03be(l+1)\ni = W(l+1)\nij x(l)\nj + b(l+1)\ni , 1 \u2264i\u2264Hl+1 (2.13)\nNon-linear transform: x(l+1)\ni = \u03c3\n(\n\u03be(l+1)\ni\n)\n, 1 \u2264i\u2264Hl+1. (2.14)\nGiven a training sample(x,y), setx(0) = x. The value of the loss/objective function (for this\nparticular sample) can be evaluated using the forward pass:\n1. For l= 1,...,L + 1\n(a) Evaluate \u03be(l) using (2.13).\n(b) Evaluate x(l) using (2.14).\n2. Evaluate the loss function for the given sample\n\u03a0(\u03b8) = \u2225y\u2212F(x; \u03b8,\u0398)\u22252.\nThis operation can be written succinctly in the form of a computational graph as shown in Figure\n2.8. In this \ufb01gure, the lower portion of the graph represents the evaluation of the loss function\u03a0.\nWe would of course need to repeat this step for all samples in the training set (or a mini-batch\nfor stochastic optimization). For simplicity, we restrict the discussion to the evaluation of the\nloss and its gradient for a single sample.\n23", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2531, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "022752de-e320-470b-a41b-0dc4b8cf0b97": {"__data__": {"id_": "022752de-e320-470b-a41b-0dc4b8cf0b97", "embedding": null, "metadata": {"page_label": "24", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ef9e6651-081a-419e-86b0-c3051cce4eb3", "node_type": "4", "metadata": {"page_label": "24", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "22ff91e84678ea2b8aabdb2b404a4270dde7d3c84fd755f8af149e4b8409e29d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2b1781e4-210a-4576-ad01-36c934cc55b7", "node_type": "1", "metadata": {}, "hash": "ec2400bc2f511d24482222691ac18771d4e6570c608614eb4ab5652f94a3f40c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In order to update the network parameters, we need\u2202\u03a0\n\u2202\u03b8, or more precisely \u2202\u03a0\n\u2202W(l) , \u2202\u03a0\n\u2202b(l) for\n1 \u2264l\u2264L+ 1. We will derive expressions for these derivatives by \ufb01rst deriving expressions for\n\u2202\u03a0\n\u2202\u03be(l) and \u2202\u03a0\n\u2202x(l) .\nFrom the computational graph it is easy to see how each hidden variable in the network is\ntransformed to the next. Recognizing this, and applying the chain rule repeatedly yields\n\u2202\u03a0\n\u2202\u03be(l) = \u2202\u03a0\n\u2202x(L+1) \u00b7\u2202x(L+1)\n\u2202\u03be(L+1) \u00b7\u2202\u03be(L+1)\n\u2202x(L) \u00b7\u00b7\u00b7 \u2202x(l+1)\n\u2202\u03be(l+1) \u00b7\u2202\u03be(l+1)\n\u2202x(l) \u00b7\u2202x(l)\n\u2202\u03be(l) . (2.15)\nIn order to evaluate this expression we need to evaluate the following terms:\n\u2202\u03a0\n\u2202x(L+1) = \u22122(y\u2212x(L+1))T (2.16)\n\u2202\u03be(l+1)\n\u2202x(l) = W(l+1) (2.17)\n\u2202x(l)\n\u2202\u03be(l) = S(l) \u2261diag[\u03c3\u2032(\u03be(l)\n1 ),\u00b7\u00b7\u00b7 ,\u03c3\u2032(\u03be(l)\nHl\n)], (2.18)\nwhere the last two relations hold for any network layerl, Hl is the width of that particular layer,\nand \u03c3\u2032denotes the derivative of the activation with respect to its argument. Using these relations\nin (2.15), we arrive at,\n\u2202\u03a0\n\u2202\u03be(l) = \u2202\u03a0\n\u2202x(L+1) \u00b7S(L+1) \u00b7W(L+1) \u00b7\u00b7\u00b7S(l+1) \u00b7W(l+1) \u00b7S(l). (2.19)\nTaking the transpose, and recognizing that\u03a3(l) is diagonal and therefore symmetric, we \ufb01nally\narrive at\n\u2202\u03a0\n\u2202\u03be(l) = S(l)W(l+1)TS(l+1) \u00b7\u00b7\u00b7W(L+1)TS(L+1)[\u22122(y\u2212x(L+1))]. (2.20)\nThis evaluation can also be represented as a computational graph. In fact, as shown in Figure\n2.8, it can be appended to the original graph, where this part of the computation appear in the\nupper row of the graph. Note that we are now traversing in the backward direction. Hence the\nname back propagation.\nPA(1)s A(l+1)ss\nW(1) W(l+1) S(L+1)\n\ud835\udc99(\ud835\udfce) \ud835\udf43(\ud835\udfcf) \ud835\udc99(\ud835\udfcf) \ud835\udc99(\ud835\udc8d) \ud835\udc99(\ud835\udc8d&\ud835\udfcf) \ud835\udc99(\ud835\udc73&\ud835\udfcf)\ud835\udf43(\ud835\udc8d) \ud835\udf43(\ud835\udc8d&\ud835\udfcf) \ud835\udf43(\ud835\udc73&\ud835\udfcf)\n\ud835\udf4f\ud835\udeb7\ud835\udf4f\ud835\udf43(\ud835\udfcf)\ud835\udf4f\ud835\udeb7\ud835\udf4f\ud835\udc99(\ud835\udfce) \ud835\udf4f\ud835\udeb7\ud835\udf4f\ud835\udc99(\ud835\udc8d) \ud835\udf4f\ud835\udeb7\ud835\udf4f\ud835\udc99(\ud835\udc8d&\ud835\udfcf) \ud835\udf4f\ud835\udeb7\ud835\udf4f\ud835\udc99(\ud835\udc73&\ud835\udfcf)\ud835\udf4f\ud835\udeb7\ud835\udf4f\ud835\udf43(\ud835\udc8d) \ud835\udf4f\ud835\udeb7\ud835\udf4f\ud835\udf43(\ud835\udc8d&\ud835\udfcf) \ud835\udf4f\ud835\udeb7\ud835\udf4f\ud835\udf43(\ud835\udc73&\ud835\udfcf)\ud835\udf4f\ud835\udeb7\ud835\udf4f\ud835\udc99(\ud835\udfcf)\nS(1) S(l) S(l+1)\ns\nFigure 2.8: Computational graph for computing the loss function and its derivatives with respect\nto hidden/latent vectors.\nThe \ufb01nal step is to evaluate an explicit expression for\u2202\u03a0\n\u2202W(l) .", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1855, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "2b1781e4-210a-4576-ad01-36c934cc55b7": {"__data__": {"id_": "2b1781e4-210a-4576-ad01-36c934cc55b7", "embedding": null, "metadata": {"page_label": "24", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ef9e6651-081a-419e-86b0-c3051cce4eb3", "node_type": "4", "metadata": {"page_label": "24", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "22ff91e84678ea2b8aabdb2b404a4270dde7d3c84fd755f8af149e4b8409e29d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "022752de-e320-470b-a41b-0dc4b8cf0b97", "node_type": "1", "metadata": {"page_label": "24", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "3ee22d7e0149a031db7a8e52b2dd861fb6ba7a537dbfee30e588be56c1630f5d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The \ufb01nal step is to evaluate an explicit expression for\u2202\u03a0\n\u2202W(l) . This can be done by recognizing,\n\u2202\u03a0\n\u2202W(l) = \u2202\u03a0\n\u2202\u03be(l) \u00b7 \u2202\u03be(l)\n\u2202W(l) = \u2202\u03a0\n\u2202\u03be(l) \u2297x(l\u22121), (2.21)\n24", "mimetype": "text/plain", "start_char_idx": 1790, "end_char_idx": 1952, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c777173e-4af4-4e3a-87da-c28860e2d567": {"__data__": {"id_": "c777173e-4af4-4e3a-87da-c28860e2d567", "embedding": null, "metadata": {"page_label": "25", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7f699359-86b4-4892-944d-e1fda009bbaa", "node_type": "4", "metadata": {"page_label": "25", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "542b4f962a0dff056ebe0b522f2db387587a057c70358aeb5274b1e22864fd14", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "where [x\u2297y]ij = xiyj is the outer product. Thus, in order to evaluate\u2202\u03a0\n\u2202W(l) we needx(l\u22121)\nwhich is evaluted during the forward phase and\u2202\u03a0\n\u2202\u03be(l) which is evaluated during back propagation.\nQuestion 2.8.1.Can you derive a similar set of expressions and the corresponding algorithm to\nevaluate \u2202\u03a0\n\u2202b(l) ?\nQuestion 2.8.2.Can you derive an explicit expression for\u2202x(L+1)\n\u2202x(0) . That is the an expression for\nthe derivative of the output of the network with respect to its input? This is a very useful quantity\nthat \ufb01nds use in algorithms like physics informed neural networks and Wasserstein generative\nadversarial networks.\n2.9 Regression versus classi\ufb01cation\nTill now, given the labelled datasetS= {(xi,yi) : 1 \u2264i\u2264N}, we have considered two types of\nlosses\n\u2022 The mean square error (MSE)\n\u03a0(\u03b8) = 1\nNtrain\nNtrain\u2211\ni=1\n\u2225yi \u2212F(xi; \u03b8,\u0398)\u22252.\n\u2022 The mean absolute error (MAE)\n\u03a0(\u03b8) = 1\nNtrain\nNtrain\u2211\ni=1\n\u2225yi \u2212F(xi; \u03b8,\u0398)\u2225.\nNeural networks with the above losses can be used to solve various regression problems where\nthe underlying function is highly nonlinear and the inputs/outputs are multi-dimensional.\nExample 2.9.1. Given the house/apartment features such as the zip code, the number of\nbedrooms/bathrooms, carpet area, age of construction, etc, predict the outcomes such as the\nmarket selling price, or the number of days on the market.\nNow let us consider some examples of classi\ufb01cation problems, where the output of the network\ntypically lies in a discrete \ufb01nite set.\nExample 2.9.2. Given the symptoms and blood markers of patients with COVID-19, predict\nwhether they will need to be admitted to ICU. So the input and output for this problem would be\nx= [pulse rate,temperature,SPO2,procalcitonin,...]\ny= [p1,p2]\nwhere p1 is the probability of being admitted to the ICU, whilep2 is the probability of not being\nadmitted. Note that0 \u2264p1,p2 \u22641 and p1 + p2 = 1.\nExample 2.9.3.Given a set of images of animals, predict whether the animal is a dog, cat or\nbird. In this case, the input and output should be\nx= the image\ny= [p1,p2,p2]\nwhere p1,p2,p3 is the probability of being a dog, cat or bird, respectively.\n25", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2106, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "55618c4c-92da-41a1-9d8e-54120568517f": {"__data__": {"id_": "55618c4c-92da-41a1-9d8e-54120568517f", "embedding": null, "metadata": {"page_label": "26", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "62d28a5d-4e62-41fa-9a37-8bc1fc70066c", "node_type": "4", "metadata": {"page_label": "26", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "d0575a415d28bf715291c9a8e8000b0e4f0956c0fc9b19040a0d529c5497344d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Since the output for the classi\ufb01cation problem corresponds to probabilities, we need to make\na few changes to the network\n1. Make use of an output function at the end of the output layer that suitably transforms the\noutput vector into the desired form, i.e, a vector of probabilities. This is typically done\nusing thesoftmax function\nx(L+1)\ni = exp (\u03be(L+1)\ni )\n\u2211C\nj=1 exp (\u03be(L+1)\nj )\nwhere C is the number of classes (and also the output dimension). Verify that with this\ntransformation, the components of thex(L+1) form a convex combination, i.e.,x(L+1)\ni \u2208[0,1]\nand \u2211C\ni=1 x(L+1)\ni = 1.\n2. The output labels for the various samples need to be one-hot encoded. In other words, for\nthe sample(x,y), the output labelyshould have dimensionD= C, and whose component\nis 1 only for the component signifying the classxbelongs to, otherwise 0. For instance, in\nExample 2.9.3\ny=\n\uf8f1\n\uf8f4\uf8f2\n\uf8f4\uf8f3\n[1,0,0]\u22a4 if xis a dog,\n[0,1,0]\u22a4 if xis a cat,\n[0,0,1]\u22a4 if xis a pig.\n3. Although the MSE or MSA can still be used as the loss function, it is preferable to use the\ncross-entropy loss function\n\u03a0(\u03b8) = 1\nNtrain\nNtrain\u2211\ni=1\nC\u2211\nc=1\n\u2212ycilog(Fc(xi; \u03b8)), (2.22)\nwhere yci is thec-th component of the true label for thei-th sample. The loss function in\n(2.22) treats yc and Fc as probability distributions and measures the discrepancy between\nthe two. It can be shown to be related to the Kullback-Liebler divergence between the two\ndistributions. Compared to MSE, this loss function severely penalizes strongly con\ufb01dent\nincorrect predictions. This is demonstrated in Example 2.9.4\nExample 2.9.4. Let us consider a binary classi\ufb01cation problem, i.e.,C = 2. For a givenx,\nlet y= [0,1] and let the prediction beF = [p,1 \u2212p]. Clearly, a small value ofp is preferred.\nTherefore any reasonable cost function should penalize large values ofp. Now let us evaluate the\nerror using various loss functions\n\u2022 MSE Loss= (0 \u2212p)2 + (1 \u22121 + p)2 = 2p2.\n\u2022 Cross-entropy Loss= \u2212(0 log(p) + 1 log(1\u2212p) = \u2212log(1 \u2212p).\nNote that both losses penalize large values ofp. Also whenp= 0, both losses are zero. However,\nas p\u21921 (which would lead the wrong prediction), the MSE loss\u21922, while the cross-entropy\nloss \u2192\u221e. That is, it strongly penalizes incorrect con\ufb01dent predictions.\n26", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2224, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1c4569f2-fff6-489f-900b-dc84675ade87": {"__data__": {"id_": "1c4569f2-fff6-489f-900b-dc84675ade87", "embedding": null, "metadata": {"page_label": "27", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "54337252-5c91-4a4a-9966-d6d892be9a9f", "node_type": "4", "metadata": {"page_label": "27", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "eb3e34d46be940b481c583f916d2d72d9a1a92f944cba6e2aa75a4ef7cd70910", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Chapter 3\nResidual neural networks\nResidual networks (or ResNets) were introduced by He et al. [8] in 2015. In this chapter, we will\ndiscuss what these networks are, why they were introduced and their relation to ODEs.\n3.1 Vanishing gradients in deep networks\nWhile training neural networks, the gradients\u2202\u03a0\n\u2202W(l) , \u2202\u03a0\n\u2202b(l) might become very small. For instance,\nconsider a very deep network, sayL\u226520. If\n\u23d0\u23d0\u23d0 \u2202\u03a0\n\u2202W(l)\n\u23d0\u23d0\u23d0\u226a1 for l\u2264\u00afl, then the contribution of \ufb01rst\n\u00afl layers of the network will be negligible, as the in\ufb02uence of their weights on the loss function is\nsmall. Because of this depth cut-o\ufb00, the bene\ufb01t in terms of expressivity of deep networks is lost.\nSo why does this happen? Recall from Section 2.8 that\n\u2202\u03a0\n\u2202W(l) = \u2202\u03a0\n\u2202\u03be(l) \u2297x(l\u22121)\nand\n\u2202\u03a0\n\u2202\u03be(l) = \u03a3(l)\nL+1\u220f\nm=l+1\n(W(m)T\u03a3(m)) \u2202\u03a0\n\u2202\u03be(L+1) . (3.1)\nFor any matrix,A, let\u03c4(A) denote the largest singular value. Then we can bound|\u2202\u03a0\n\u2202\u03be(l) |by\n|\u2202\u03a0\n\u2202\u03be(l) |\u2264 \u03c4(\u03a3(l))\nL+1\u220f\nm=l+1\n(\u03c4(W(m))\u03c4(\u03a3(m)))| \u2202\u03a0\n\u2202\u03be(L+1) |. (3.2)\nRecall that\u03a3(m) \u2261diag[\u03c3\u2032(\u03be(m)\n1 ),\u00b7\u00b7\u00b7 ,\u03c3\u2032(\u03be(m)\nHl\n)], where\u03c3\u2032 denotes the derivative of\u03c3 with\nrespect to its argument. For ReLU its value is either0 or 1. Therefore \u03c4(\u03a3(m))) = 1.\nAlso, for stability we would want\u03c4(W(m)) <1. Otherwise the output of the network can\nbecome unbounded. In practise this is enforced by the regularization term.\nUsing this in the equation above we have\n|\u2202\u03a0\n\u2202\u03be(l) |\u2264\nL+1\u220f\nm=l+1\n(\u03c4(W(m)))| \u2202\u03a0\n\u2202\u03be(L+1) |, (3.3)\nwhere each term in the product is a scalar less than 1. As the number of terms increases, that\nis L\u2212l \u226b1, this product can, and does, become very small. This typically happens when\n27", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1590, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "bb5f773b-29f0-4fc1-a00b-4661f850c476": {"__data__": {"id_": "bb5f773b-29f0-4fc1-a00b-4661f850c476", "embedding": null, "metadata": {"page_label": "28", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "325ed61c-1ee0-4c24-bde5-695dbbf99d96", "node_type": "4", "metadata": {"page_label": "28", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "67a6f04dfe5f950ea989f70214245fd6944d01415c862015ef58755f70eadbd1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "L\u2212l\u224820, in which case|\u2202\u03a0\n\u2202\u03be(l) |, and therefore| \u2202\u03a0\n\u2202W(l) |, become very small. This issue is called\nthe problem of vanishing gradients. It manifests itself in deep networks where the weights in the\ninner layers (sayL\u2212l> 20) do not contribute to the network.\nIn [8], the authors demonstrate that taking a deeper network can actually lead to an increase\nin training and validation error (see Figure 3.1). Thus, beyond a certain point, increasing the\ndepth of a network can be counterproductive. Based on our previous discussion on vanishing\ngradients we know why this is the case. Given this, we would like to come up with a network\narchitecture that addresses the problem of vanishing gradients by ensuring\n\u23d0\u23d0\u23d0 \u2202\u03a0\n\u2202\u03be(L+1)\n\u23d0\u23d0\u23d0\u2248\n\u23d0\u23d0\u23d0 \u2202\u03a0\n\u2202\u03be(1)\n\u23d0\u23d0\u23d0.\nThis means requiring that when the weights of the network approach small values, the network\nshould approach the identity mapping, and not the null mapping. This is the core idea behind a\nResNet architecture.\nFigure 3.1: Training error (left) and test error (right) on CIFAR-10 data with \u201cplain\" deep\nnetworks (taken from [8]).\n3.2 ResNets\nFigure 3.2: ResNet of depth 6 with skip connections.\nConsider an MLP with depth 6 (as shown in Figure 3.2) with a \ufb01xed widthH for each hidden\nlayer. We add skip connections between the hidden layers in the following manner\nx(l)\ni = \u03c3(W(l)\nij x(l\u22121)\nj + b(l)\ni ) + x(l\u22121)\ni , 2 \u2264l\u2264L. (3.4)\nWe can make the following observations:\n1. If all weights (and biases) were null, thenx(5) = x(1), which in turn would imply\n\u2202\u03a0\n\u2202x(1) = \u2202\u03a0\n\u2202x(5) ,\n28", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1523, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ae9994b3-ac8a-4c3f-8751-51346607356c": {"__data__": {"id_": "ae9994b3-ac8a-4c3f-8751-51346607356c", "embedding": null, "metadata": {"page_label": "29", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8254e844-bd0f-47b6-90a8-e361f55c579f", "node_type": "4", "metadata": {"page_label": "29", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "b09cf00d582c5a43dc4346cc4324a5b76fcae201f4c9183aeb00d3f982448842", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "PA(1)s A(l+1)ss\nW(1) W(l+1) S(L+1)\n\ud835\udc99(\ud835\udfce) \ud835\udf43(\ud835\udfcf) \ud835\udc99(\ud835\udfcf) \ud835\udc99(\ud835\udc8d) \ud835\udc99(\ud835\udc8d&\ud835\udfcf) \ud835\udc99(\ud835\udc73&\ud835\udfcf)\ud835\udf43(\ud835\udc8d) \ud835\udf43(\ud835\udc8d&\ud835\udfcf) \ud835\udf43(\ud835\udc73&\ud835\udfcf)\n\ud835\udf4f\ud835\udeb7\ud835\udf4f\ud835\udf43(\ud835\udfcf)\ud835\udf4f\ud835\udeb7\ud835\udf4f\ud835\udc99(\ud835\udfce) \ud835\udf4f\ud835\udeb7\ud835\udf4f\ud835\udc99(\ud835\udc8d) \ud835\udf4f\ud835\udeb7\ud835\udf4f\ud835\udc99(\ud835\udc8d&\ud835\udfcf) \ud835\udf4f\ud835\udeb7\ud835\udf4f\ud835\udc99(\ud835\udc73&\ud835\udfcf)\ud835\udf4f\ud835\udeb7\ud835\udf4f\ud835\udf43(\ud835\udc8d) \ud835\udf4f\ud835\udeb7\ud835\udf4f\ud835\udf43(\ud835\udc8d&\ud835\udfcf) \ud835\udf4f\ud835\udeb7\ud835\udf4f\ud835\udf43(\ud835\udc73&\ud835\udfcf)\ud835\udf4f\ud835\udeb7\ud835\udf4f\ud835\udc99(\ud835\udfcf)\nS(1) S(l) S(l+1)\nI I I\nI II\ns\nFigure 3.3: Computational graph for forward and backpropagation in a Resnet.\ni.e., we will not have the issue of vanishing gradients.\n2. The computational graph for forward and back-propagation of a ResNet is shown in Figure\n3.3. Looking at this graph, it is clear that the expression for\u2202x(l+1)\n\u2202x(l) now involves traversing\ntwo branches and adding their sum. Therefore, we have\n\u2202\u03a0\n\u2202\u03be(l) = \u03a3(l)\nL+1\u220f\nm=l+1\n(I+ W(m)T\u03a3(m)) \u2202\u03a0\n\u2202\u03be(L+1) . (3.5)\nNow, if we assume that|W(m)|\u226a 1 via regularization, we have\n\u2202\u03a0\n\u2202\u03be(l) = \u03a3(l)(\nI+\nL+1\u2211\nm=l+1\nW(m) T\u03a3(m) + higher order terms\n) \u2202\u03a0\n\u2202\u03be(L+1) . (3.6)\nIn the expression above, even if the individual matrices have small entries, their sum need\nnot approach a zero matrix. This implies that we can create a \ufb01nite (and signi\ufb01cant)\nchange between the gradients near the input and output layers, while still requiring the\nweights to be small (via regularization).\nRemark 3.2.1.The above analysis can be extended to cases whenH is not \ufb01xed, but the analysis\nis not as clean. See [8] on how we can do this.\n3.3 Connections with ODEs\nLet us \ufb01rst consider the special case of a ResNet withd = D = H. Recall the relation(3.4),\nwhich we can rewrite as\nx(l) \u2212x(l\u22121)\n\u2206t = 1\n\u2206t\u03c3(W(l)x(l\u22121) + b(l)) = 1\n\u2206t\u03c3(\u03be(l)) (3.7)\n29", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1434, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "aca267f2-a652-45d4-80aa-79cae64b1695": {"__data__": {"id_": "aca267f2-a652-45d4-80aa-79cae64b1695", "embedding": null, "metadata": {"page_label": "30", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "94fb958a-6867-40bb-a62f-e156eb0c0119", "node_type": "4", "metadata": {"page_label": "30", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "87ed08fbf5bc438026a3907e42be92d4c503684dab753a385ab6ece6929c05e5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "for some scalar\u2206t, where we note that\u03be(l) is a function ofx(l\u22121) parameterized by \u03b8(l) =\n[W(l),b(l)]. Thus, we can further rewrite (3.7) as\nx(l) \u2212x(l\u22121)\n\u2206t = V(x(l\u22121); \u03b8(l)). (3.8)\nNow consider a \ufb01rst-order system of (possibly non-linear) ODEs, where givenx(0) and\n\u02d9x\u2261dx\ndt = V(x,t) (3.9)\nwe want to \ufb01ndx(T). In order to solve this numerically, we can uniformly divide the temporal\ndomain with a time-step\u2206tand temporal nodest(l) = l\u2206t, 0 \u2264l\u2264L+ 1, where(L+ 1)\u2206t= T.\nDe\ufb01ne the discrete solution asx(l) = x(l\u2206t). Then, givenx(l\u22121), we can use a time-integrator\nto approximate the solutionx(l). We can consider a method motivated by the forward Euler\nintegrator, where the the LHS of (3.9) is approximated by\nLHS \u2248x(l) \u2212x(l\u22121)\n\u2206t .\nwhile the RHS is approximated using a parameter\u03b8(l) as\nRHS \u2248V(x(l\u22121); t(l)) = V(x(l\u22121); \u03b8(l)).\nwhere we are allowing the parameters to be di\ufb00erent at each time-step. Putting these two\ntogether, we get exactly the relation of the ResNet given in(3.8). In other words, a ResNet is\nnothing but a descritization of a non-linear system of ODEs. We make some comments to further\nstrengthen this connection.\n\u2022 In a fully trained ResNet we are givenx(0) and the weights of a network, and we predict\nx(L+1).\n\u2022 In a system of ODEs, we are givenx(0) and V(x,t), and we predictx(T).\n\u2022 Training the ResNet means determining the parameters\u03b8of the network so thatx(L+1) is\nas close as possible toyi when x(0) = xi, fori= 1,\u00b7\u00b7\u00b7 ,Ntrain.\n\u2022 When viewed from the analogous ODE point of view, training means determining the right\nhand sideV(x,t) by requiringx(T) to be as close as possible toyi when x(0) = xi, for\ni= 1,\u00b7\u00b7\u00b7 ,Ntrain.\n\u2022 In a ResNet we are looking for \"one\"V(x,t) that will mapxi to yi, for all1 \u2264i\u2264Ntrain.\n3.4 Neural ODEs\nMotivated by the connection between ResNets and ODEs, neural ODEs were proposed in [4].\nConsider a system of ODEs given by\ndx\ndt = V(x,t) (3.10)\nGiven x(0), we wish to \ufb01ndx(T). In [4], the RHS, i.e.,V(x,t), is de\ufb01ned using a feed-forward\nneural network with parameters\u03b8(see Figure 3.4). The input to the network is(x,t) while the\noutput isV(x,t) (having the same dimension asx). With this description, the system(3.10) is\nsolved using a suitable time-marching scheme, such as forward Euler, Runge-Kutta, etc.\n30", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2257, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1eff1206-0600-4273-9acd-fbcb5c7e2be9": {"__data__": {"id_": "1eff1206-0600-4273-9acd-fbcb5c7e2be9", "embedding": null, "metadata": {"page_label": "31", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "cf56fe60-877b-4154-9fd0-a6bd4f1231da", "node_type": "4", "metadata": {"page_label": "31", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "3de2d9ba1596c5c067660792a42a7ce437161372e436eb0bf08d550f3dd1643d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "\ud835\udc65!\ud835\udc65\"\n\ud835\udc65#$!\ud835\udc61\nMLP\n\ud835\udc63!\ud835\udc63\"\n\ud835\udc63#$!\nFigure 3.4: Feed-forward neural network used to model the right hand side in a Neural ODE.\nThe number of dependent variables= d\u22121.\nFigure 3.5: Analogy between regression problems and Neural ODEs.\nSo how do we use this network to solve a regression problem? Assume that you are given the\nlabelled training dataS= {(xi,yi) : 1 \u2264i\u2264Ntrain}. Here bothxi and yi are assumed to have\nthe same dimensiond\u22121. The key idea is to think ofxi as points in thed\u22121-dimensional space\nthat represent the initial state of the system, and to think ofyi as points that represent the \ufb01nal\nstate. Then the regression problem becomes \ufb01nding the RHS of(3.10) that will map the initial\npoints to the \ufb01nal points with minimal amount of error. In other words, \ufb01nd the parameters\u03b8\nsuch that\n\u03a0(\u03b8) = 1\nN\nN\u2211\ni=1\n|xi(T; \u03b8) \u2212yi|2\nis minimized. Here,xi(T; \u03b8) denotes the solution (at timet= T) to(3.10) with x(0) = xi and\nthe RHS represented by a feed-forward neural networkV(x,t; \u03b8). Note thatyi is the output\nvalue that is measured. There is a relatively straightforward way of extending this approach to\nthe case whenxi and yi have di\ufb00erent dimensions. In summary, in Neural ODEs one transforms\na regression problem to one of \ufb01nding the nonlinear, time-dependent RHS of a system of ODEs.\nLet us list the advantages and di\ufb00erences when comparing Neural ODEs to ResNets:\n\u2022 If we interpret the number of time-steps in the Neural ODE as the number of hidden\n31", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1449, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3239efe0-81cb-4380-868b-752a7b399e2e": {"__data__": {"id_": "3239efe0-81cb-4380-868b-752a7b399e2e", "embedding": null, "metadata": {"page_label": "32", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ebafa02d-1bdc-4ff3-82eb-e033d21a3137", "node_type": "4", "metadata": {"page_label": "32", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "86890ee8d144de2ac277b234568dce27fe15c7491ba24f2e202b2430012a4380", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "layers L in a ResNet, then the computational cost for both methods isO(L). This is the\ncost associated with performing one forward propagation and one backward propagation.\nHowever the memory cost (the cost associated with storing the weights of each layer), is\ndi\ufb00erent. For the neural ODE all the weights are associated with the feed-forward network\nused to represent the functionV(x,t; \u03b8). Thus the number of weights are independent of\nthe number of time-steps used to solve the ODE. On the other hand, for a ResNet the\nnumber of weights increases linearly with the number of layers, therefore the cost of storing\nthem scales asO(L).\n\u2022 In Neural ODEs, we can take the limit\u2206t \u21920 and study the convergence, since this\nwill not change the size of the network used to represent the RHS. However, this is not\ncomputationally feasible to do for ResNets, where\u2206t \u21920 corresponds to the network\ndepth L\u2192\u221e!\n\u2022 ResNet uses a forward Euler type method, but in a Neural ODE one can use any time-\nintegrator. Especially, other higher-order explicit time-integrator like the Runge-Kutta\nmethods that converge to the \u201cexact\u201d solution at a faster rate.\n32", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1141, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "34694811-b364-4f27-b3c7-205b46b8f85c": {"__data__": {"id_": "34694811-b364-4f27-b3c7-205b46b8f85c", "embedding": null, "metadata": {"page_label": "33", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3db175be-3dcf-416a-afa8-a347105a3aff", "node_type": "4", "metadata": {"page_label": "33", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "94d6bd0668cefdbb2442f9979d5f6cfd65613758ed3574b14e49a6f3ef2bae57", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Chapter 4\nSolving PDEs with MLPs\nA number of numerical methods exist to solve PDEs. Some of these are:\n\u2022 Finite di\ufb00erence methods\n\u2022 Finite volume methods\n\u2022 Finite element methods\n\u2022 Spectral Galerkin and collocation methods\n\u2022 Deep neural networks!\nTo better appreciate some of these methods, especially deep neural networks, let us consider\na simple model problem describing the scalar advection-di\ufb00usion problem in one-dimension:\nFind \ufb01ndu(x) in the intervalx\u2208(0,l) such that\nadu\ndx \u2212\u03bad2u\ndx2 = f(x), x \u2208(0,\u2113)\nu(0) = 0\nu(\u2113) = 1\n(4.1)\nwhere adenotes the advective velocity,\u03bais the di\ufb00usion coe\ufb03cient whilef(x) is the source. Such\nequations are used to model many physical phenomena, such as the transport of pollutant by\n\ufb02uids, or modelling the \ufb02ow of electrons through semiconductors. The multi-dimensional version\nof this problem will take the form\na\u00b7\u2207u(s) \u2212\u03ba\u2206u(s) = f(s), s\u2208\u2126\nu(s) = g(s), s\u2208\u2202\u2126 (4.2)\nNote that the model problem is a linear PDE (ODE in the one-dimensional case). Replacing the\nvelocity a by u leads to the viscous Burgers equation.\nThe solution to (4.1) forf \u22610 can be analytically written as\nu(x) = 1 \u2212exp(ax/\u03ba)\n1 \u2212exp(a\u2113/\u03ba)\nwhere a\u2113/\u03ba is known as the Peclet number (Pe) and measures the ratio of the strength of\nadvection to the strength of di\ufb00usion. We plot the solution for varying values ofa and \u03ba in\nFigure 4.1. Note that for small Pe, the solution is essentially a straight line. But as Pe increases,\nthe solution starts to bend and forming a steeper boundary layer near the right boundary. The\nthickness of this boundary layer is given by\u03b4\u2248Pe \u00d7l.\nWe will now consider a few methods to numerically solve this toy problem.\n33", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1649, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "13902d1f-614e-4535-8fc3-4642c28b0ff9": {"__data__": {"id_": "13902d1f-614e-4535-8fc3-4642c28b0ff9", "embedding": null, "metadata": {"page_label": "34", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d5ace620-0092-4fe9-8466-619627db533f", "node_type": "4", "metadata": {"page_label": "34", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "09fddb7d33589170af91a7b77a29b284f77c60067473efb4224b593866ed9d7c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "(a) Fixed \u03ba\n (b) Fixed a\nFigure 4.1: Exact solution of (4.1) with\u2113= 1.\n4.1 Finite di\ufb00erence method\nThe key steps of a \ufb01nite di\ufb00erence scheme are as follows:\n1. Discretize the domain into a grid of points, with the goal being to \ufb01nd the solution at these\npoints.\n2. Approximate the derivates with \ufb01nite di\ufb00erence approximations at these points. This leads\nto a system of (linear or non-linear) algebraic equations.\n3. Solve this system using a suitable algorithm to \ufb01nd the solution.\nApplying these steps to (4.1) leads to:\n1. Discretize the domain intoN + 1 points, withxi = ih, 0 \u2264i \u2264N where h = \u2113/N. We\nwish to solve foru(xi) = ui. We also know from the boundary conditions thatu0 = 0 and\nuN = 1.\n2. Use the approximations\ndu\ndx(xi) = ui+1 \u2212ui\u22121\n2h + O(h2)\nd2u\ndx2 (xi) = ui+1 \u22122ui + ui\u22121\nh2 + O(h2)\nNote that both the approximation used above are second order accurate. They are \u201ccentral\ndi\ufb00erence\u201d approximations, as they weigh points on either side of thei-th point with\nthe same magnitude. It is worth mentioning that in the limit of large Peclet number,\na central di\ufb00erence approximation of the advective term is not ideal since it leads to\nnumerical instability. In such a case, an \u201cupwind\u201d approximation is preferred. Applying\n34", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1238, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "28a429c9-dfaf-4d34-b3fe-9c19b7a0172d": {"__data__": {"id_": "28a429c9-dfaf-4d34-b3fe-9c19b7a0172d", "embedding": null, "metadata": {"page_label": "35", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9f0dcd29-a46d-4b5d-8484-03029a0dc288", "node_type": "4", "metadata": {"page_label": "35", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "8097dc97c3b9b187d7f5696964c89a6d1dbd90c70796c82e7479038bbcc98537", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "the approximations to the PDE atxi, 1 \u2264i\u2264N \u22121\naui+1 \u2212ui\u22121\n2h \u2212\u03baui+1 \u22122ui + ui\u22121\nh2 = fi\n\u21d0\u21d2ui+1\n(a\n2h \u2212 \u03ba\nh2\n)\n\ued19 \ued18\ued17 \ued1a\n\u03b3\n+ui\n(2\u03ba\nh2\n)\n\ued19 \ued18\ued17 \ued1a\n\u03b2\n+ui\u22121\n(\n\u2212a\n2h \u2212 \u03ba\nh2\n)\n\ued19 \ued18\ued17 \ued1a\n\u03b1\n= fi\nLooking at each node where the solution is unknown (recall thatu0 = 0 and uN = 1 are\nknown),\n\u03b2u1 + \u03b3u2 = \u2212\u03b1u0 + f1\n\u03b1ui\u22121 + \u03b2ui + \u03b3ui+1 = fi, \u22002 \u2264i\u2264N \u22122\n\u03b1uN\u22122 + \u03b2uN\u22121 = \u2212\u03b3uN + fN\u22121\n(4.3)\nCombining all theN \u22121 equations in (4.3), we get the following linear system\nKu= f (4.4)\nwhere the tridiagonal matrixK and the other vectors in (4.4) are de\ufb01ned as\nK=\n\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0\n\u03b2 \u03b3 0\n\u03b1 ... ...\n... ... \u03b3\n0 \u03b1 \u03b2\n\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fa\uf8fb\n\u2208R(N\u22121)\u00d7(N\u22121),\nu=\n[\nu1 u2 \u00b7\u00b7\u00b7 uN\u22122 uN\u22121\n]\u22a4\u2208RN\u22121,\nf =\n[\n\u2212\u03b1u0 + f1 f2 f3 \u00b7\u00b7\u00b7 fN\u22122 fN\u22121 \u2212\u03b3uN + fN\u22121\n]\u22a4\u2208RN\u22121\n3. Solve u= K\u22121f.\nNote that:\n\u2022 In practice, we never actually invertK as it is computationally expensive. We instead\nuse smart numerical algorithms to solve the system(4.4). For instance, one can use the\nThomas tridiagonal algorithm for this particular system, which is a simpli\ufb01ed version of\nGaussian elimination.\n\u2022 We only obtain an approximationui \u2248u(xi). To reduce the approximation error, we\ncould reduce the mesh sizeh. Alternatively, we could use higher-order \ufb01nite di\ufb00erence\napproximations which would lead to a \"wider stencil\" to approximate the derivates at each\npoint.\n\u2022 We can think of each point where we \u201capply\u201d the PDE as a collocation point. This idea of\napplying the PDE at collocation points is shared by the next method we consider. It is\nalso shared by the method with uses MLPs to solve PDEs.\n4.2 Spectral collocation method\nSpectral collocation methods seek a solution written as an expansion in terms of a set of smooth\nand global basis functions. The basis functions are chosen a priori, whereas the coe\ufb03cients of\nthe expansion are unknowns, and are computed by requiring that the numerical solution of the\nPDE is exact at a set of so-called collocation points. More speci\ufb01cally, this approach involves the\nfollowing steps.\n35", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1934, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "32c18a82-0324-44cc-a9a2-aa23bb0c7e6d": {"__data__": {"id_": "32c18a82-0324-44cc-a9a2-aa23bb0c7e6d", "embedding": null, "metadata": {"page_label": "36", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f1c3f67d-9095-40f1-883b-5ae1a91d501f", "node_type": "4", "metadata": {"page_label": "36", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "ebf9448590a721f68b6c6c5410a6020bedb1cd6469260773984231a338ea69b6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "1. Select a set of global basis functions with the following properties:\n(a) It forms complete basis in the space of functions being considered.\n(b) Is smooth enough so that derivatives can be evaluated.\n(c) Easy to evaluate.\n(d) Derivatives that are easy to evaluate.\nFor instance, one can use the Chebyshev polynomials de\ufb01ned on\u03be\u2208(\u22121,1), given by the\nfollowing recurrence relation\nT0(\u03be) = 1, T 1(\u03be) = \u03be, T n+1(\u03be) = 2\u03beTn(\u03be) \u2212Tn\u22121(\u03be)\nThe \ufb01rst few Chebyshev polynomials are shown in Figure 4.2. Note that this basis satis\ufb01es\nall the required properties listed above. It is easy to evaluate at any point because one\ncan use the recurrence relation above and the values of the two lower-order polynomials to\nevaluate the Chebyshev polynomial of the subsequent order. One can also take derivatives\nof the recurrence relation above to evaluate a recurrence relation for derivatives of all\norders.\nFigure 4.2: First few Chebyshev polynomials.\n2. Write the solution as a linear combination of the basis functions{\u03c6n(x)}N\nn=0\nu(x) =\nN\u2211\nn=0\nun\u03c6n(x) (4.5)\nwhere un are the basis coe\ufb03cients. For our toy problem(4.1) (assuming \u2113= 1), we will use\nthe Chebyshev polynomials\u03c6n(x) = Tn(2x\u22121), where the argument is transformed to use\nthese functions on the interval(0,1).\n3. Evaluate the derivates for the PDE, which for our toy problem will be\ndu\ndx(x) =\nN\u2211\nn=0\nun\u03c6\u2032\nn(x) =\nN\u2211\nn=0\nun2T\u2032\nn(2x\u22121)\nd2u\ndx2 (x) =\nN\u2211\nn=0\nun\u03c6\u2032\u2032\nn(x) =\nN\u2211\nn=0\nun4T\u2032\u2032\nn(2x\u22121)\n(4.6)\n36", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1445, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "9b24daae-d643-4a73-87ee-0e1aa7117ef2": {"__data__": {"id_": "9b24daae-d643-4a73-87ee-0e1aa7117ef2", "embedding": null, "metadata": {"page_label": "37", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "861cc119-4f77-4d3b-97e8-4ce7edcbc432", "node_type": "4", "metadata": {"page_label": "37", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "58a887f1ae646c20dcfbed349e66647b19a987694d258bd5178c1d4539996816", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "4. Use the boundary conditions of the PDE. For the speci\ufb01c case of (4.1),\nu(0) = 0 =\u21d2\nN\u2211\nn=0\nun\u03c6n(0) =\nN\u2211\nn=0\nunTn(\u22121) = 0,\nu(1) = 1 =\u21d2\nN\u2211\nn=0\nun\u03c6n(1) =\nN\u2211\nn=0\nunTn(1) = 1\n(4.7)\nwhich leads to 2 linear equations forN+ 1 coe\ufb03cients. We then consider a set of (suitably\nchosen) nodesxi, 1 \u2264i\u2264N \u22121 in the interior of the domain, i.e. the collocation points,\nand use the derivatives found in step 3. in the PDE evaluated at theseN \u22121 nodes\na\nN\u2211\nn=0\nun\u03c6\u2032\nn(xi) \u2212\u03ba\nN\u2211\nn=0\nun\u03c6\u2032\u2032\nn(xi) = f(xi)\n=\u21d2\nN\u2211\nn=0\nun\n(\n2aT\u2032\nn(2xi \u22121) \u22124\u03baT\u2032\u2032\nn(2xi \u22121)\n)\n= f(xi)\n(4.8)\nThis leads to an additionalN \u22121 equations for theN + 1 coe\ufb03cients. Combining (4.7)\nand (4.8) leads to the following linear system\nKu= f (4.9)\nwhere\nK\u2208R(N+1)\u00d7(N+1),\nu=\n[\nu0 u2 \u00b7\u00b7\u00b7 uN\u22121 uN\n]\u22a4\u2208RN+1,\nf =\n[\n0 f(x1) f(x1) \u00b7\u00b7\u00b7 f(xN\u22122) f(xN\u22121) 1\n]\u22a4\u2208RN+1\n5. Solve u= K\u22121f.\nWe need to choose the collocation/quadrature pointsxi properly, so that K has desirable\nproperties that make the linear system(4.9) easier to solve. These include invertibility, positive-\nde\ufb01niteness, sparseness, etc.\nRemark 4.2.1.The method is called a collocation method as the PDE is evaluated at the speci\ufb01c\ncollocation/quadrature pointsxi.\nRemark 4.2.2.When working with a non-linear PDE, we will end up with a non-linear systems\nof algebraic equations for the coe\ufb03cientsu0,...,u N, which is typically solved by Newton\u2019s method.\nLet us look at a least-square variant for \ufb01nding the coe\ufb03cients of the expansion of the spectral\nmethods. As done earlier, we still represent the solution using(4.5) and compute its derivates.\nThen, the coe\ufb03cients uare found by minimizing the following loss function\n\u03a0(u) = \u03a0int(u) + \u03bb\u03a0bc(u)\n\u03a0bc(u) =\n\u23d0\u23d0\u23d0\u23d0\u23d0\nN\u2211\nn=0\nun\u03c6n(0) \u22120\n\u23d0\u23d0\u23d0\u23d0\u23d0\n2\n+\n\u23d0\u23d0\u23d0\u23d0\u23d0\nN\u2211\nn=0\nun\u03c6n(1) \u22121\n\u23d0\u23d0\u23d0\u23d0\u23d0\n2\n\u03a0int(u) = 1\nNtrain\nNtrain\u2211\ni=1\n\u23d0\u23d0\u23d0\u23d0\u23d0a\nN\u2211\nn=0\nun\u03c6\u2032\nn(xi) \u2212\u03ba\nN\u2211\nn=0\nun\u03c6\u2032\u2032\nn(xi) \u2212f(xi)\n\u23d0\u23d0\u23d0\u23d0\u23d0\n2\n(4.10)\n37", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1816, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "cccd916d-b521-40b0-916d-ae9a24877e8b": {"__data__": {"id_": "cccd916d-b521-40b0-916d-ae9a24877e8b", "embedding": null, "metadata": {"page_label": "38", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "569d4a4b-d97b-4a12-9c5d-a21b5c157e37", "node_type": "4", "metadata": {"page_label": "38", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "6aa3d590c7c3cc6fba160944254281ac3c3b1d39f4b7782aaa0bf6dcaa34d093", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This can be solved using any of the gradient-based methods we have seen in Chapter 2. This\napproach is especially useful when treating non-linear PDEs. In fact, in those cases it is not be\npossible to write a linear system in terms of the coe\ufb03cients such as (4.9). A few things to note\nhere\n\u2022 \u03bb is a parameter used to scale the interior loss and boundary loss di\ufb00erently.\n\u2022 The number of interior pointxi can be chosen independently of the number of basis\nfunctions. In other words,Ntrain does not have to be the same asN.\n\u2022 We will see in the next section how this variant of the spectral method is very similar to\nhow deep neural networks are used to solve PDEs.\n4.3 Physics-informed neural networks (PINNs)\nThe idea of using neural networks to solve partial di\ufb00erential equations was introduced in 1990-\n2000s by Lagaris et al. [11]. With the renewed interest in using machine learning tools in solving\nPDEs, this idea was rediscovered in 2019 by Raissi et al. [24], and was given the term PINNs\n(physics-informed neural networks). The basic idea of PINNs is similar to regression, except\nthat the loss function\u03a0(\u03b8) contains derivate operators arising in the PDE being considered. We\noutline the main steps below for a one-dimensional scalar PDE, which can easily be extended to\nmulti-dimensional systems of PDEs. We recommend that the reader thinks about the similarities\nand di\ufb00erences between this method and spectral collocation method described in the previous\nsection.\n1. Select a neural network as a function representation of the PDE solution:\nu= F(x; \u03b8). (4.11)\nSome crucial properties required by this representation are:\n(a) Do we have completeness with the representation, i.e., can we accurately approximate\nthe necessary class of function using the representation? The answer is yes, because\nof the universal approximation theorems of neural networks (see Section 2.3.1).\n(b) Is the representation smooth? The answer is yes if the activation function is smooth,\nsuch as tanh, sin, etc. Note that we cannot use ReLU since it does not enough number\nof smooth derivatives.\n(c) Is it easy to evaluate? The answer is yes, due to a quick forward propagation pass.\n(d) Is it easy to evaluate derivates? The answer is yes, due to back-propagation. This will\nbe discussed in detail below.\n2. Given the representation(4.11), we need to \ufb01nd\u03b8such that the PDE is satis\ufb01ed in some\nsuitable form. Compare this with spectral collocation approximation given by(4.5), where\nwe need to determine the coe\ufb03cientsun. Note that while the dependence on the coe\ufb03cients\nun in (4.5) is linear, the dependence on\u03b8in (4.11) can be highly non-linear.\n3. Nextwewantto\ufb01ndthederivativesoftherepresentation. Considerthecomputationalgraph\nof the network as shown in Figure 4.3. It comprises alternate steps of a\ufb03ne transformations\nand component-wise nonlinear transformation. The derivative of the output with respect\nto the input can be evaluated by back-propagation. The graph in Figure 4.3 is obtained by\n38", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2993, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "918b2ba1-0584-406f-b3b4-381b8fc62a99": {"__data__": {"id_": "918b2ba1-0584-406f-b3b4-381b8fc62a99", "embedding": null, "metadata": {"page_label": "39", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3c742491-f432-4f86-89e9-016bc1550d37", "node_type": "4", "metadata": {"page_label": "39", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "821dcbb4d6e3accf9e43e4c768ca2059435760dc158c7ef9cdbd6ac40e907dba", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8b19bb8a-cc74-48a1-a1d6-c5648cb11db6", "node_type": "1", "metadata": {}, "hash": "c6131777282dc313d08f41aed2db78ee6b1674354735b0a682f759fd073fe686", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "simply setting\u03a0 = x(L+1) in the graph shown in Figure 2.8. Further, once we recognize\nthat \u2202x(L+1)\n\u2202x(L+1) = 1, the identity matrix, we can easily read from this graph that\n\u2202x(L+1)\n\u2202x(0) = W(L+1)S(L+1)W(L)S(L) \u00b7\u00b7\u00b7W(2)S(2)W(1)S(1)\nHence, the evaluation ofdu\ndx requires the extention of the original graph with a backward\nbranch used to evaluate the derivative of the activation function for each component of\nthe vectors\u03be(l) (see Figure 4.3). The second derivatived2u\ndx2 is evaluated by performing\nback-propagation of the extended graph. To evaluate higher order derivatives, the graph\nwill need to be extended further in a similar manner. This is what happens behind the\nscenes in Pytorch when a call to \"autograd\" is made.\nA(1)s A(l+1)ss\nW(1) W(l+1) S(L+1)\n\ud835\udc99(\ud835\udfce) \ud835\udf43(\ud835\udfcf) \ud835\udc99(\ud835\udfcf) \ud835\udc99(\ud835\udc8d) \ud835\udc99(\ud835\udc8d&\ud835\udfcf) \ud835\udc99(\ud835\udc73&\ud835\udfcf)\ud835\udf43(\ud835\udc8d) \ud835\udf43(\ud835\udc8d&\ud835\udfcf) \ud835\udf43(\ud835\udc73&\ud835\udfcf)\n\ud835\udf4f\ud835\udc99(\ud835\udc73#\ud835\udfcf)\ud835\udf4f\ud835\udf43(\ud835\udfce)\ud835\udf4f\ud835\udc99(\ud835\udc73#\ud835\udfcf)\ud835\udf4f\ud835\udc99(\ud835\udfce) \ud835\udf4f\ud835\udc99(\ud835\udc73#\ud835\udfcf)\ud835\udf4f\ud835\udc99(\ud835\udc8d) \ud835\udf4f\ud835\udc99(\ud835\udc73#\ud835\udfcf)\ud835\udf4f\ud835\udc99(\ud835\udc8d#\ud835\udfcf) \ud835\udf4f\ud835\udc99(\ud835\udc73#\ud835\udfcf)\ud835\udf4f\ud835\udc99(\ud835\udc73#\ud835\udfcf)= 1 \ud835\udf4f\ud835\udc99(\ud835\udc73#\ud835\udfcf)\ud835\udf4f\ud835\udf43(\ud835\udc8d) \ud835\udf4f\ud835\udc99(\ud835\udc73#\ud835\udfcf)\ud835\udf4f\ud835\udf43(\ud835\udc8d#\ud835\udfcf) \ud835\udf4f\ud835\udc99(\ud835\udc73#\ud835\udfcf)\ud835\udf4f\ud835\udf43(\ud835\udc73#\ud835\udfcf)\ud835\udf4f\ud835\udc99(\ud835\udc73#\ud835\udfcf)\ud835\udf4f\ud835\udc99(\ud835\udfcf)\nS(1) S(l) S(l+1)\ns\nFigure 4.3: Extended graph to evaluate derivatives with respect to network input.\n4. Insert the functional representation of the solution(4.11) into the PDE to \ufb01nd the parame-\nters \u03b8. To do this, we \ufb01rst de\ufb01ne a set of pointsS= {xi : 1 \u2264i\u2264Ntrain}used to train\nthe network, analogous to the set of collocation points in the spectral collocation methods.\nThereafter, we need to de\ufb01ne the loss function (specialized to our toy problem (4.1))\n\u03a0(\u03b8) = \u03a0int(\u03b8) + \u03bbb\u03a0b(\u03b8),\n\u03a0b(\u03b8) = (F(0; \u03b8) \u22120)2 + (F(1; \u03b8) \u22121)2 ,\n\u03a0int(\u03b8) = 1\nNtrain\nNtrain\u2211\ni=1\n(\naF\u2032(xi; \u03b8) \u2212\u03baF\u2032\u2032(xi; \u03b8)n \u2212f(xi)\n)2 .\n(4.12)\nAfter training the network, i.e. solving the minimization problem\u03b8\u2217 = arg min\n\u03b8\n\u03a0(\u03b8),\nthe solution writesu\u2217(x) = F(x; \u03b8\u2217). Note that this is exactly what is done for the least\nsquares variant of the spectral collocation method, where the coe\ufb03cientsun are solved by\nminimizing a similar loss.\nWe make a few remarks:\n\u2022 When we are able to \ufb01nd\u03b8\u2217for which\u03a0(\u03b8\u2217) = 0, this implies\u03a0int(\u03b8\u2217) = 0 and \u03a0b(\u03b8\u2217) = 0.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1941, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8b19bb8a-cc74-48a1-a1d6-c5648cb11db6": {"__data__": {"id_": "8b19bb8a-cc74-48a1-a1d6-c5648cb11db6", "embedding": null, "metadata": {"page_label": "39", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3c742491-f432-4f86-89e9-016bc1550d37", "node_type": "4", "metadata": {"page_label": "39", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "821dcbb4d6e3accf9e43e4c768ca2059435760dc158c7ef9cdbd6ac40e907dba", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "918b2ba1-0584-406f-b3b4-381b8fc62a99", "node_type": "1", "metadata": {"page_label": "39", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "a0564542c9ffbc50107d2bdf592f728c85bfa52619b98ad542afad329e0d881b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "(4.12)\nAfter training the network, i.e. solving the minimization problem\u03b8\u2217 = arg min\n\u03b8\n\u03a0(\u03b8),\nthe solution writesu\u2217(x) = F(x; \u03b8\u2217). Note that this is exactly what is done for the least\nsquares variant of the spectral collocation method, where the coe\ufb03cientsun are solved by\nminimizing a similar loss.\nWe make a few remarks:\n\u2022 When we are able to \ufb01nd\u03b8\u2217for which\u03a0(\u03b8\u2217) = 0, this implies\u03a0int(\u03b8\u2217) = 0 and \u03a0b(\u03b8\u2217) = 0.\nIn other words, the PDE residuals are zero at the collocation points. This will lead to a\ngood solution as long as the collocation points cover the domain well.\n\u2022 There are various ways to improve the accuracy of PINNs, such as\n39", "mimetype": "text/plain", "start_char_idx": 1532, "end_char_idx": 2172, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "199b9f94-0f7d-4c1b-af7d-546b9729cc3b": {"__data__": {"id_": "199b9f94-0f7d-4c1b-af7d-546b9729cc3b", "embedding": null, "metadata": {"page_label": "40", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1bb1a9bd-d279-4390-94dd-f0097b72bc27", "node_type": "4", "metadata": {"page_label": "40", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "a9ab23990aca70cb93b0a599400d1dcae8974c4d95fef865f70d43320a989e7a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "\u2013 Increasing the number of collocation points.\n\u2013 Changing the hyper-parameter\u03bbb weighting the boundary loss.\n\u2013 Increasing the size of the network. That is, increasingN\u03b8.\n\u2022 The boundary conditions (BCs) of a di\ufb00erential equation carry fundamental physical\nproperties of the phenomena we are trying to describe, and it is paramount that those are\nsatis\ufb01ed by our numerical solution. In the framework of PINNs, BCs are enforced as a\nsoft constrained via the penalization term\u03a0b(\u03b8). Hence, the hyper-parameter\u03bbb plays a\ncrucial role in the training of the network, as it balances the interplay between the two loss\nterms during the minimization process. If the gradients of the di\ufb00erent loss terms are not\nadequately scaled, the convergence to a solution that satis\ufb01es both the BCs and the PDE\nitself can be extremely slow. This is particularly exacerbated for sti\ufb00 PDEs. To address\nthis issue, di\ufb00erent self-adaptive techniques to tune the value of\u03bbb along the training have\nbeen proposed [29, 16, 3].\n4.4 Extending PINNs to a more general PDE\nConsider a general PDE: Find the solutionu: \u2126 \u2282Rd \u2192RD such that\nL(u(x)) = f(x), x\u2208\u2126\nB(u(x)) = g(x), x\u2208\u2202\u2126 (4.13)\nwhere L is the di\ufb00erential operator,f is the known forcing term,B is the boundary operator,\nand gis the non-homogeneous part of boundary condition (also prescribed).\nAs an example, we can consider the three-dimensional incompressible Navier-Stokes equation\nsolving for the velocity \ufb01eldv= [v1,v2,v3] and pressurep on \u2126 = \u2126S \u00d7[0,T]. Here \u2126S is the\nthree dimensions spatial domain and[0,T] is the time interval of interest. The equation is given\nby\n\u2202v\n\u2202t + v\u00b7\u2207v+ \u2207p\u2212\u00b5\u2206v= f, \u2200(s,t) \u2208\u2126\n\u2207\u00b7u= 0, \u2200(s,t) \u2208\u2126\nv= 0, \u2200(s,t) \u2208\u2202\u2126S \u00d7[0,T]\nv(s,0) = v0(s), \u2200s\u2208\u2126S.\n(4.14)\nThe \ufb01rst equation above is the balance of linear moment. The second equation enforces the\nconservation of mass. The third equation is the no-slip boundary condition which is used when\nthe boundary is rigid and \ufb01xed. The fourth equation is the prescription of the initial velocity\n\ufb01eld.\nTo design a PINN for(4.13), the input to the network should be the independent variablesx\nand the output should be the solution vectoru. For the speci\ufb01c case of the Navier-Stokes system\n(4.14), the input to the network would be[s1,s2,s3,t] \u2208R4, while the output vector would be\nu= [v1,v2,v3,p] \u2208R4. The steps would be the following:\n1. Construct the loss functions\n\u2022 De\ufb01ne the interior residualR(u) = L(u) \u2212f.\n\u2022 De\ufb01ne the boundary residualRb(u) = B(u) \u2212g.\n\u2022 Select suitableNv collocation points in the interior of the domain andNb points on\nthe domain boundary to evaluate the residuals. These could be chosen as based on\nquadrature rules, such as Gaussian, Lobatto, Uniform, Random, etc.\n40", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2685, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3348b843-0260-4877-9b39-5e692cc0586c": {"__data__": {"id_": "3348b843-0260-4877-9b39-5e692cc0586c", "embedding": null, "metadata": {"page_label": "41", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ec2c7748-1085-4cb4-820d-ce6271e84277", "node_type": "4", "metadata": {"page_label": "41", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "f53288cd9d0778d1ebe3322dc8b2ac42811f6e6a6251a93f956456908d78bf13", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Then the loss function is\n\u03a0(\u03b8) = \u03a0int(\u03b8) + \u03bbb\u03a0b(\u03b8)\n\u03a0int(\u03b8) = 1\nNv\nNv\u2211\ni=1\n|R(F(xi; \u03b8)|2\n\u03a0b(\u03b8) = 1\nNb\nNb\u2211\ni=1\n|Rb(F(xi; \u03b8)|2\n2. Train the network: \ufb01nd\u03b8\u2217= arg min\n\u03b8\n\u03a0(\u03b8), and set the solution asu\u2217= F(x; \u03b8\u2217)\nWe make some remarks here:\n\u2022 It is implicitly assumed that a weight regularization term is also added to the loss\u03a0(\u03b8).\n\u2022 Is u\u2217(x) the exact solution to the PDE? The answer is No!\n\u2013 Firstly,\u03a0(\u03b8\u2217) may not be zero.\n\u2013 Even if \u03a0(\u03b8\u2217) is identically zero, it only means that the residuals vanishes at the\ncollocation points. However, that does not guarantee that the residuals will vanish\neverywhere in the domain. For thatNv,Nb \u2192\u221e.\n\u2013 Also, with a \ufb01xed network (N\u03b8 \ufb01xed) we cannot represent all functions. For that, we\nwill needN\u03b8 \u2192\u221e.\n\u2022 In practice, we only compute\u03a0(\u03b8\u2217). Is the solution error\u2225e\u2225= \u2225u\u2217\u2212u\u2225related to this\nloss value? And if it is, can we say that this error will be small as long as the loss is small?\nThis is what we try to answer in next section.\n4.5 Error analysis for PINNs\nIn order to evaluate the errore= u\u2217\u2212u, we need to know the exact solutionuwhich is not\navailable in general. We consider a way of overcoming this issue, by restricting our discussion to\nlinear PDEs i.e.,L and B are linear operators.\nNote that ifuis the exact solution, then\nL(e) = L(u\u2217\u2212u) = L(u\u2217) \u2212L(u) = L(u\u2217) \u2212f = R(u\u2217) (4.15)\nand\nB(e) = B(u\u2217\u2212u) = B(u\u2217) \u2212B(u) = B(u\u2217) \u2212g= Rb(u\u2217) (4.16)\nThus, (4.15) (4.16) lead to a PDE foredriven by the residuals of the MLP solution,\nL(e) = R(u\u2217), in \u2126\nB(e) = Rb(u\u2217), on \u2126 (4.17)\nIf the residuals ofu\u2217 were zero, thene= 0. Unfortunately, these residuals are not zero. The\nmost that we can say is that they are small at the collocation points. However, from the theory\nof stability of well-posed PDEs, we have\n\u2225e\u2225L2(\u2126) \u2264C1\n(\n\u2225R(u\u2217)\u2225L2(\u2126) + \u2225Rb(u\u2217)\u2225L2(\u2202\u2126)\n)\n(4.18)\n41", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1793, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "974222bc-6c9d-46b2-b80e-e533482200a2": {"__data__": {"id_": "974222bc-6c9d-46b2-b80e-e533482200a2", "embedding": null, "metadata": {"page_label": "42", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b535cfb4-12b0-4197-9250-377620fae3e4", "node_type": "4", "metadata": {"page_label": "42", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "361a8fc88435d688368758617bad2b369db744ecfb8de2286fda72d8d7eb3baf", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "085d1224-3e86-45e0-a95f-dc4ab6e24987", "node_type": "1", "metadata": {}, "hash": "592777387d2f33d0e1de797e7ae32493f28e4ece2e7bea45616d61af513c06af", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "where C1 is a stability constant that depends on the PDE, the domain\u2126, etc. This is a condition\nthat hols for all well-posed PDEs. It says that if the terms driving the PDE are small, then the\nsolution to the PDE will also be small. This equation tells us that we can control the error if\nwe can control the residuals for the MLP solution. However, in practise we know and control\n\u03a0int,\u03a0b and not\u2225R(u\u2217)\u22252\nL2(\u2126),\u2225Rb(u\u2217)\u22252\nL2(\u2202\u2126). The question then becomes, are these quantities\nrelated. This is answered in the analysis below,\n\u2225R(u\u2217)\u2225L2(\u2126) =\n\u23d0\u23d0\u23d0m\u2126\u03a0int(\u03b8\u2217)1/2 + \u2225R(u\u2217)\u2225L2(\u2126) \u2212m\u2126\u03a0int(\u03b8\u2217)1/2\n\u23d0\u23d0\u23d0\n\u2264m\u2126\u03a0int(\u03b8\u2217)1/2 +\n\u23d0\u23d0\u23d0\u2225R(u\u2217)\u2225L2(\u2126) \u2212m\u2126\u03a0int(\u03b8\u2217)1/2\n\u23d0\u23d0\u23d0\n\u2264m\u2126\u03a0int(\u03b8\u2217)1/2 + C2(Nv)\u2212\u03b1\n(4.19)\nwhere m\u2126 is the measure of the domain\u2126, C2 and \u03b1> 0 will depend on the type of quadrature\npoints chosen. In the equation above the \ufb01rst line is obtained by adding and subtracting the\nterm m\u2126\u03a0int(\u03b8\u2217)1/2. The second line is obtained by using the triangle inequality. The third line\nis a statement of error in approximating an integral with a \ufb01nite sum of values evaluated at\nquadrature points.\nSimilarly for the boundary residual\n\u2225Rb(u\u2217)\u2225L2(\u2202\u2126) \u2264m\u2202\u2126\u03a0b(\u03b8\u2217)1/2 + C3(Nb)\u2212\u03b2 (4.20)\nwhere m\u2202\u2126 is the measure of\u2202\u2126, C3 and \u03b2 >0 will depend on the type of boundary quadrature\npoints.\nUsing (4.18), (4.19) and (4.20), we get\n\u2225e\u2225L2(\u2126) \u2264C1\n\uf8eb\n\uf8ec\uf8edm\u2126\u03a0int(\u03b8\u2217)1/2 + m\u2202\u2126\u03a0b(\u03b8\u2217)1/2\n\ued19 \ued18\ued17 \ued1a\nreduced byN\u03b8\u2191\n+ C2(Nv)\u2212\u03b1 + C3(Nb)\u2212\u03b2\n\ued19 \ued18\ued17 \ued1a\nreduced byNv,Nb\u2191\n\uf8f6\n\uf8f7\uf8f8 (4.21)\nThis equation tells us that it is possible to control the error in the PINNs solution by reducing\nthe loss functions (by increasingN\u03b8) and by increasing the number of interior and boundary\ncollocation points. For further details about this analysis, the reader is referred to [18].\n4.6 Data assimilation using PINNs\nThe problem of data assimilation is often encountered in the science and engineering. In this\nproblem, we are able to make a few sparse measurements of a quantity, and using these we wish\nto evaluate it everywhere on a \ufb01ne grid. We are also given a physical principal (in the form of a\nPDE) that the variable of interest must adhere to.\nLet us assume that we are given a set of sparse measurements of some quantityu on the\ndomain \u2126\nui = u(xi), xi \u2208\u2126, 1 \u2264i\u2264M\nFurthermore, we are given thatusatis\ufb01es some constraintR(u) = 0 on \u2126. Then, data assimilation\ncorresponds to using this information to \ufb01nd the value ofu at anyx\u2208\u2126.\nWe can solve this problem using PINNs. First, we representuusing a neural networkF(x,\u03b8).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2441, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "085d1224-3e86-45e0-a95f-dc4ab6e24987": {"__data__": {"id_": "085d1224-3e86-45e0-a95f-dc4ab6e24987", "embedding": null, "metadata": {"page_label": "42", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b535cfb4-12b0-4197-9250-377620fae3e4", "node_type": "4", "metadata": {"page_label": "42", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "361a8fc88435d688368758617bad2b369db744ecfb8de2286fda72d8d7eb3baf", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "974222bc-6c9d-46b2-b80e-e533482200a2", "node_type": "1", "metadata": {"page_label": "42", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "e88774ba44f43acab84e298075b29d37d3faf4444b7bb2ddd0abc9f68899ebf7", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In this\nproblem, we are able to make a few sparse measurements of a quantity, and using these we wish\nto evaluate it everywhere on a \ufb01ne grid. We are also given a physical principal (in the form of a\nPDE) that the variable of interest must adhere to.\nLet us assume that we are given a set of sparse measurements of some quantityu on the\ndomain \u2126\nui = u(xi), xi \u2208\u2126, 1 \u2264i\u2264M\nFurthermore, we are given thatusatis\ufb01es some constraintR(u) = 0 on \u2126. Then, data assimilation\ncorresponds to using this information to \ufb01nd the value ofu at anyx\u2208\u2126.\nWe can solve this problem using PINNs. First, we representuusing a neural networkF(x,\u03b8).\nNext, we de\ufb01ne a loss function\n\u03a0(\u03b8) = \u03bbI\nM\nM\u2211\ni=1\n(ui \u2212F(xi,\u03b8))2\n\ued19 \ued18\ued17 \ued1a\ndata matching\n+ 1\nNv\nM+Nv\u2211\ni=M+1\n|R(F(xi,\u03b8))|2\n\ued19 \ued18\ued17 \ued1a\nphysical constraint\n+ \u03bb\u2225\u03b8\u22252\n\ued19 \ued18\ued17 \ued1a\nsmoothness\n42", "mimetype": "text/plain", "start_char_idx": 1817, "end_char_idx": 2616, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "23696645-d29f-47e8-b676-cdd0e90e7674": {"__data__": {"id_": "23696645-d29f-47e8-b676-cdd0e90e7674", "embedding": null, "metadata": {"page_label": "43", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9156c4f1-b290-44e0-8d1e-24d2e26909ed", "node_type": "4", "metadata": {"page_label": "43", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "9f6acb453fa97fc055a3053d3eff451559dc5e9ab759f68798a0df5e4c1667fd", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "where xi, M+ 1 \u2264i \u2264M + Nv are some collocation points chosen to evaluate the residual,\nwhile \u03bbI,\u03bb are hyper-parameters. Then we train the network by \ufb01nding\u03b8\u2217= arg min\n\u03b8\n\u03a0(\u03b8), and\nset the PINNs solution asu\u2217= F(x; \u03b8\u2217).\n43", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 220, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b724ab26-df5e-49f1-9267-3a737c5e117c": {"__data__": {"id_": "b724ab26-df5e-49f1-9267-3a737c5e117c", "embedding": null, "metadata": {"page_label": "44", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "18ff8af6-4eb7-4aa0-a944-3ea2d01da36e", "node_type": "4", "metadata": {"page_label": "44", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "f5626f2c6cb107c6e3610cd78fd10102f85f984d4dfc38aa185eb0b5fd85559a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Chapter 5\nConvolutional Neural Networks\nIn the previous chapters, we have seen how to construct neural networks using fully-connected\nlayers. We will now look at a di\ufb00erent class of layers, called convolution layers, which are very\nuseful when handling inputs which are images. These tasks include classifying images into\ncategories, performing semantic segmentation on images, and transforming images from one type\nto another.\n5.1 Functions and images\nConsider a functionu(x) de\ufb01ned onx\u2208[a,b] \u00d7[c,d] \u2282R2. Then we can visualize the discretized\nversion of this function as an imageU \u2208RN1\u00d7N2 , where\nU[i,j] = u(ih,jh), 1 \u2264i\u2264N1, 1 \u2264j \u2264N2, h= pixel size. (5.1)\nNote that the image in(5.1) de\ufb01nes a grayscale image where the value ofu at each pixel is\njust the intensity. If we work with color images, then it would be a three-dimensional tensor,\nwith the third dimension corresponding to the red, blue and green channels. In other words,\nU \u2208RN1\u00d7N2\u00d73.\nIf we want to use a fully-connected neural network (MLP) which takes as input a colored 2D\nimage of size100 \u00d7100, then the input dimension after unravelling the entire image as a single\nvector would be3 \u00d7104, which is very large. This would in turn lead to very large connected\nlayers which is not computationally feasible. Secondly, when unravelling the image, we lose all\nspatial context of the initial image. Finally, one would expect that local operations, such as\nedge detection, would be the same in any region of the image. Consider the weights for a fully\nconnected layer. These would be represented by the matrixWij, where thei index represent the\noutput of the linear transform and thej index represents the input. If the operation was the\nsame for every output index, we would apply the same operation for everyi and therefore not\nneed the matrix. To address all these issues, we can use the convolution operator on functions.\n5.2 Convolutions of functions\nThe convolution operator maps functions to functions. Consider the functionu(x), x \u2208Rd,\nand a su\ufb03ciently smooth kernel functiong(x) which decays as|x|\u2192\u221e . Then the convolution\noperator is given by\nu(x) =\n\u222b\nRd\ng(y\u2212x)u(y)dy (5.2)\n44", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2145, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "4bf2f767-6be2-4db0-85e4-2098f6478a3f": {"__data__": {"id_": "4bf2f767-6be2-4db0-85e4-2098f6478a3f", "embedding": null, "metadata": {"page_label": "45", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4fdb394b-7948-454d-b9a2-8909e7f62ca6", "node_type": "4", "metadata": {"page_label": "45", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "7cf5c2b744b880aab21670909d13c6459b8dd5fef45497335c76a8169445637e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We can interpret the convolution operator as samplingu by varyingx. For example, in 1D, let\nu(x) and g(x) be as shown in Figure 5.1, and\nu(x) =\n\u222b\nR\ng(y\u2212x)u(y)dy.\nConsider a pointx0. Then g(y\u2212x0) shifts the kernel to the locationx0 which will sample the\nfunction uin the orange shaded region. Similarly, for another pointx1, g(y\u2212x1) shifts the kernel\nto the locationx1 which will sample the functionu in the green shaded region. So as the kernel\nmoves, it samplesu in di\ufb00erent windows. Note that the same operation is applied regardless of\nthe value ofx. Lets now consider a few typical kernel functions.\n(a) Kernel g(x)\n (b) Sampling\nFigure 5.1: Sampling with shifted kernel in 1D.\n5.2.1 Example 1\nA popular choice is the Gaussian kernel\ng(\u03be) = \u03c1(|\u03be|),\nwhere for any scalarr,\n\u03c1(r) = exp(\u2212r2/2\u03c32)\u221a\n(2\u03c0\u03c32)d .\nwhich is used as a smoothing/blurring \ufb01lter. Hered is the dimension while\u03c3 denotes the spread.\nThis kernel in 1D is shown in 5.1(a) for\u03c3= 0.2. Some important properties of this kernel are:\n\u2022 It is isotropic, as it only depends on the magnitude of\u03be.\n\u2022 The integral over the whole domain is unity.\n\u2022 It is parameterized by\u03c3, which represents a frequency cut-o\ufb00, as scales \ufb01ner than\u03c3 are\n\ufb01ltered out by the convolution. This smoothing e\ufb00ect is demonstrated in Figure 5.2\n45", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1277, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "72a1c232-1cd6-453a-a576-eb6b956b7a3f": {"__data__": {"id_": "72a1c232-1cd6-453a-a576-eb6b956b7a3f", "embedding": null, "metadata": {"page_label": "46", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d0bf7c98-55e5-4c65-a3e7-758fbd108e46", "node_type": "4", "metadata": {"page_label": "46", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "f7005659cd27cd00497dca801966df2cb44121546c26767048446b702dc961e2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Figure 5.2: 1D convolution with Gaussian kernel.\n5.2.2 Example 2\nLet us consider another example of a kernel that would produce the derivative of a smooth\nversion ofu. In 2D, we want this to look like\nu(x) = \u2202\n\u2202x1\n(\u222b \u221e\n\u2212\u221e\n\u222b \u221e\n\u2212\u221e\n\u03c1(|y\u2212x|)u(y)dy1dy2\n)\n=\n\u222b \u221e\n\u2212\u221e\n\u222b \u221e\n\u2212\u221e\n\u2202\u03c1(|y\u2212x|)\n\u2202x1\nu(y)dy1dy2\n=\n\u222b \u221e\n\u2212\u221e\n\u222b \u221e\n\u2212\u221e\n(\n\u2212\u2202\u03c1(|y\u2212x|)\n\u2202y1\n)\n\ued19 \ued18\ued17 \ued1a\nrequired kernel\nu(y)dy1dy2\n(5.3)\nThis kernel is shown in both 1D and 2D in Figure 5.3. Note that the action of this kernel look\nlike a smoothed \ufb01nite di\ufb00erence operation. That is the region to the left of the center of the\nkernel is weighted by a negative value and the region to the right is weighted by a positive value.\n5.3 Discrete convolutions\nTo evaluate the discrete convolution in 2D, consider(5.2) and discretize it using quadrature.\nThen, we will have\nU[i,j] =\nN1\u2211\nm=1\nN2\u2211\nn=1\ng[m\u2212i,n \u2212j]U[m,n] (5.4)\nwhere we have absorbed the measureh2 into the de\ufb01nition of the kernel. As in the continuous\ncase, we will assume thatg vanishes after a certain distance\ng[p,q] = 0, |p|,|q|> \u00afN (measure of width of the kernel).\n46", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1057, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "6b613688-b1d8-4ea4-b159-dea680b50df9": {"__data__": {"id_": "6b613688-b1d8-4ea4-b159-dea680b50df9", "embedding": null, "metadata": {"page_label": "47", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e2d4d540-8b29-4b34-b448-408686daa67e", "node_type": "4", "metadata": {"page_label": "47", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "b000c4ed80fa33fdf65dff9defc8cfbafc273184b07037a2177c9c0a6ba6c95f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "(a) 1D\n (b) 2D, with a derivative along the 1-direction\nFigure 5.3: Derivative kernel. The blue curve denotes the Gaussian kernel and the orange curve\ndenotes the derivative.\nThus, the limits of the sum are reduced by exclusing all the pixels over which the convolution\nwill be zero,\nU[i,j] =\ni+ \u00afN\u2211\nm=i\u2212\u00afN\nj+ \u00afN\u2211\nn=j\u2212\u00afN\ng[m\u2212i,n \u2212j]U[m,n]. (5.5)\nLet m\u2032= m\u2212i and n\u2032= n\u2212j. Then\nU[i,j] =\n\u00afN\u2211\nm\u2032=\u2212\u00afN\n\u00afN\u2211\nn\u2032=\u2212\u00afN\ng[m\u2032,n\u2032]U[i+ m\u2032,j + n\u2032]. (5.6)\nThis is precisely how a convolution is applied in deep learning. Thus, the convolution is entirely\ndetermined by\ng[m,n], |m|,|n|\u2264 \u00afN,\nwhich become the weights of the convolution layer, with the number of weight being( \u00afN + 1)2.\nLet\u2019s consider some examples:\n\u2022 A smoothing kernel would be\n1\n8\n\uf8ee\n\uf8f0\n1\n4 1 1\n4\n1 3 1\n1\n4 1 1\n4\n\uf8f9\n\uf8fb\u2248Gaussian kernel with some\u03c3\n\u2022 Kernels that lead to the derivative along thex-direction andy-direction are given by\n\uf8ee\n\uf8f0\n0 0 0\n\u22121 0 1\n0 0 0\n\uf8f9\n\uf8fb and\n\uf8ee\n\uf8f0\n0 1 0\n0 0 0\n0 \u22121 0\n\uf8f9\n\uf8fb\n\u2022 Similarly, the second derivatives anlong thex and y-directions are given by kernels of the\nform \uf8ee\n\uf8f0\n0 0 0\n\u22121 2 \u22121\n0 0 0\n\uf8f9\n\uf8fb and\n\uf8ee\n\uf8f0\n0 \u22121 0\n0 2 0\n0 \u22121 0\n\uf8f9\n\uf8fb\n47", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1096, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "6e432994-c9a8-4467-ae64-8cf787f41453": {"__data__": {"id_": "6e432994-c9a8-4467-ae64-8cf787f41453", "embedding": null, "metadata": {"page_label": "48", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "54351abb-e44c-4f09-b0b2-bd52fe977ba1", "node_type": "4", "metadata": {"page_label": "48", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "7d85114c64c2f13c8edc9cd131a169051c283f655867d867b3dd7e18567445f6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "\u2022 While the Laplacian is given by the kernel of the type\n\uf8ee\n\uf8f0\n0 \u22121 0\n\u22121 4 \u22121\n0 \u22121 0\n\uf8f9\n\uf8fb\nRemark 5.3.1.We can have di\ufb00erent\u00afN in di\ufb00erent directions. That is, we can have kernels\nwith di\ufb00erent widths along each direction.\n5.4 Connection to \ufb01nite di\ufb00erences\nThere is a very strong connection between the concept of convolution and the stencil of a \ufb01nite\ndi\ufb00erence scheme. This is made clear in the discussion below.\nSay some functionu(x1,x2) is represented on a \ufb01nite grid, where the grid points are indexed\nby (i,j) with a grid sizeh. Then we use the notationU[i,j] = u(xi\n1,xj\n2). Using Taylor series\nexpansion about(i,j)\nU[i+ 1,j] \u2212U[i\u22121,j]\n2h = u(xi+1\n1 ,xj\n2) \u2212u(xi\u22121\n1 ,xj\n2)\n2h = \u2202\n\u2202x1\nu(xi\n1,xj\n2) + O(h2).\nThe operation above isidentical to the operation of a discrete convolution with weights given by\n\uf8ee\n\uf8f0\n0 0 0\n\u22121 0 1\n0 0 0\n\uf8f9\n\uf8fb 1\n2h \u2248 \u2202u\n\u2202x1\n.\nThus we may say that this convolution operation approximates a derivative along the 1-direction.\nSimilarly, we can show that\nU[i+ 1,j] \u22122U[i,j] + U[i\u22121,j]\nh2 = \u2202\n\u2202x2\n1\nu(xi\n1,xj\n2) + O(h2).\nand thus the convolution with the kernel given by\n\uf8ee\n\uf8f0\n0 0 0\n1 \u22122 1\n0 0 0\n\uf8f9\n\uf8fb 1\nh2 \u2248 \u2202u\n\u2202x2\n1\n,\napproximates the computation of the second derivative along the 1-direction.\n5.5 Convolution layers\nThe key things to remember are:\n\u2022 Each convolution layer consists of several discrete convolutions, each with its own kernel.\n\u2022 The weights of the kernel, which determine its action (smoothing, \ufb01rst derivative, second\nderivative etc.), are learnable parameters and are determined when training the network.\nThus the way to think about the learning process is that the network learns the operations\n(convolutions) that are appropriate for its task. The task can be a classi\ufb01cation problem,\nfor example.\n48", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1740, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5dd5afb7-c496-4dad-8131-86b0d8edf44b": {"__data__": {"id_": "5dd5afb7-c496-4dad-8131-86b0d8edf44b", "embedding": null, "metadata": {"page_label": "49", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "15c15b76-a7b1-43fa-a92d-9cf86445052d", "node_type": "4", "metadata": {"page_label": "49", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "75b87056e7dadc544e29eb96c52cc2e58566ca998088e23bac5406d46ef81cf8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Let us assume we have anN1 \u00d7N2 image as an input. Then, we will have multiple convolutions\nin a convolution layer, each of which will generate a di\ufb00erent image, as shown in Figure 5.4(a).\nThe trainable parameters of this layer are the weights of each convolution kernel. Assuming the\nwidth of the kernels is\u00afN in each direction, and there areP kernels, then the number of trainable\nweights will beP \u00d7(2 \u00afN + 1)2.\nNext let us consider the size of the output image after applying a single kernel operation.\nNote that we will not be able to apply the kernel on the boundary pixels since there are no\npixel-values available beyond the image boundary (see Figure 5.4(b)). Thus, we will have to\nskip \u00afN pixels at each boundary when applying the kernel, leading to an output image of shape\n(N1 \u2212 \u00afN + 1) \u00d7(N2 \u2212 \u00afN + 1).\nOne way to overcome this is bypadding the image with \u00afN pixels with zero value on each\nedge. Now we can apply the kernel on the boundary pixels and the output image will be the\nsame size as the input image, as can be seen in Figure 5.4(c).\nAnother feature of convolutions is known as thestride which determines the number of pixels\nby which the kernel is shifted as we move over the image. In the examples above, the stride was\n1 in both directions. In practice, we can choose a stride>1 which will further shrink the size of\nthe output image. For instance, if stride was taken asS in each direction (with zero-padding\napplied), then the output image size would reduce by a factor ofS in each direction (see Figure\n5.4(d)).\n(a) Action of a convolution layer\n (b) Convolution without padding\n(c) Convolution with zero-padding\n (d) Convolution with zero-padding and stride 2\nFigure 5.4: Action of a convolution layer/kernel.\n49", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1738, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ee936b87-dd41-44c6-b38c-b557bb052e45": {"__data__": {"id_": "ee936b87-dd41-44c6-b38c-b557bb052e45", "embedding": null, "metadata": {"page_label": "50", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e7fd7b48-74f2-4cf9-89ff-f8dd86e55b9e", "node_type": "4", "metadata": {"page_label": "50", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "a0e24726b4eff4593dfa808c3f45dd47347ab5abaf51033b53bade288ea4b56b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "5.5.1 Average and Max Pooling\nPooling operations are generally used to reduce the size of an image, and allowing you to step\nthrough di\ufb00erent scales of the image. If applied on an image of sizeN \u00d7N over patches of size\nS\u00d7S, the new image will have dimensionsN\nS \u00d7N\nS, whereS is the stride of the pooling operation.\nThis is shown in Figure 5.5 forS = 2. Note it is typical to select the patch of pixels over which\nthe max or average is computed to be(S\u00d7S), whereS is the stride. This is true for Figure 5.5\n(b) but not for 5.5 (a).\nAlso, we show in Figure 5.6 how pooling allows us to move through various scales of the\nimage, where the image gets coarser as more pooling operations are applied. Note that pooling\noperations do not have any trainable parameters. The pooling operation has strong analog\nin similar operators that are used when designing multigrid preconditioners for solving linear\nsystems of algebraic equations.\n(a) Stride 1\n (b) Stride 2\nFigure 5.5: Max pooling applied to an image over patches of size(2 \u00d72).\n(a) Original\n (b) After 1 pooling op.\n (c) After 2 pooling op.\n(d) After 3 pooling op.\n (e) After 4 pooling op.\n (f) After 5 pooling op.\nFigure 5.6: Max pooling applied repeatedly to an image over patches of size(2 \u00d72) with stride 2.\n50", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1264, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "eb09f623-cf9a-4f24-a46e-8b093ac0ba52": {"__data__": {"id_": "eb09f623-cf9a-4f24-a46e-8b093ac0ba52", "embedding": null, "metadata": {"page_label": "51", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1cdf99cf-eb30-423e-a995-98433423add4", "node_type": "4", "metadata": {"page_label": "51", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "c4e71834671d100999c1445d6afb419a426feb7de8f453773ef7366bfc8fbf66", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "5.5.2 Convolution for inputs with multiple channels\nAssume that the input to a convolution layer is of sizeN1 \u00d7N2 \u00d7C, whereC is the number of\nchannels in the input image. Then a convolution layer applyP convolutions on this input and\ngive an output of sizeM1 \u00d7M2 \u00d7P. Note that both the spatial resolution as well as the number\nof channels of the output image might be di\ufb00erent from the input image. Furthermore, if a single\nconvolution in the layer uses a kernel of widthk= 2 \u00afN + 1, then the kernel will be of the shape\nk\u00d7k\u00d7C, i.e., the kernel will havek\u00d7k weights for each of theC input channels of the input\nimage. Each convolution will act on the input to give an output image of shapeM1 \u00d7M2 \u00d71.\nThe output of each convolution are stacked together to give the \ufb01nal output of the convolution\nlayer. This can be written as,\n\u00afU[i,j,k ] =\n\u00afN\u2211\nm=\u2212\u00afN\n\u00afN\u2211\nn=\u2212\u00afN\nC\u2211\nc=1\ngk[m,n,c ]U[i+ m,j + n,c], 1 \u2264i\u2264M1, 1 \u2264j \u2264M2, 1 \u2264k\u2264P,\nwhere gk is the kernel of thek-th convolution in the layer. Note that the total number of trainable\nparameters will be(2 \u00afN + 1) \u00d7(2 \u00afN + 1) \u00d7P \u00d7C. This is the type of convolutional layer most\nfrequently encountered in a convolutional neural network, which is described in the following\nsection.\n5.6 Convolution Neural Network (CNN)\nNow let\u2019s put all the elements together to form the full network. Consider an image classi\ufb01cation\nproblem. Then the CNN will be given byy= F(x; \u03b8) where x\u2208RN1\u00d7N2\u00d7N3 is the input image\nwith N3 channels, whiley\u2208RC is the probability vector whosei-th component of denotes the\nprobability that the input image belongs thei-th class among a total ofC classes. The yare\ntypically one-hot encoded.\nThe possible architecture of this network is shown in Figure 5.7. This consists of a number of\nconvolution layers followed by pooling layers, which will reduce the spatial resolution of the input\nimage while increasing the number of channels. The output of the \ufb01nal pooling layer is \ufb02attened\nto form a vector, which is then passed though a number of fully connected layers with some\nactivation function (say ReLU). The \ufb01nal fully connected layer reduced the size of the vector to\nC (which is taken to be 10 in the Figure), which is then passed through a softmax function to\ngenerate the predicted probability vectory. Since we are solving a classi\ufb01cation problem, the\nloss function is taken to be the cross-entropy function\n\u03a0(\u03b8) = \u2212\nNtrain\u2211\ni=1\nC\u2211\nc=1\n[\ny(i)\nc log\n(\nFc(x(i); \u03b8)\n)]\n.\nWe train the CNN by trying to \ufb01nd\u03b8\u2217= arg min\n\u03b8\n\u03a0(\u03b8) with the \ufb01nal network beingy= F(x; \u03b8\u2217).\nWe make some important remarks:\n1. The convolution operation is also a linear operation on the input, as is the case for a fully\nconnected layer. The only di\ufb00erence is that in a fully-connected layer, the weights connect\nevery pixel in the output to every pixel of the input, while in a convolution layer the weights\nconnect one pixel of the output only to a patch of pixels in the input. Furthermore, the\nsame weights are applied on each patch of the input.\n2. In the CNN shown in Figure 5.7, the convolution layers can be interpreted as encoding\ninformation about the input image, while the fully connected layers can be interpreted as\n51", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3160, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "83d1ff0c-c425-46a2-9bcb-abe89cef003d": {"__data__": {"id_": "83d1ff0c-c425-46a2-9bcb-abe89cef003d", "embedding": null, "metadata": {"page_label": "52", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "69f216c0-09b8-4677-ba8c-ab93527c0eea", "node_type": "4", "metadata": {"page_label": "52", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "2e74a1fc48bc59133ec36d28ee54f0af52efd345f4b790b2b8cd5ded46ecccce", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Figure 5.7: Example of a CNN architecture for an image classi\ufb01cation problem, for 10 classes.\nusing this information to solve the classi\ufb01cation problem. This is why in the literature,\nconvolution layers are said to performfeature selection. Further the part of the network\nleading up to the \u201c\ufb02attened\u201d vector is sometimes referred to as theencoder.\n3. Sometime, activation is also applied to the output of the convolution layer along with a\nbias\nx(l+1)[i,j,k ] = \u03c3(\n\u2211\nm,n,c\ngk[m,n,c ]x(l)[i+ m,j + n,c] + bk)\nwhere a single biasbk is used for a given output channelk.\n4. In the example above we considered an image classi\ufb01cation problem. That is, the network\nwas a transform from an image to a class label. We can think of other similar cases. For\nexample, when the network is a transform from an image to a real number. This might\nhave several useful applications in computational physics. Consider the case where you\nwant to create an enstrophy calculator. That is a network that will take as input images of\nthe velocity components of a \ufb02uid de\ufb01ned on a grid, and produce as output the integral of\nthe square of the vorticity (called the enstrophy) over the entire domain. Another example\nwould be a network that takes as input the components of the displacement of an elastic\nsolid and produces as output the total strain energy stored within the solid.\n5. The architecture we have considered allows us to transform images to vectors, which is\nuseful in problems involving image classi\ufb01cation, image analysis, etc. However, there is\nanother architecture that does the opposite, i.e., maps vectors into images. This is useful in\napplications involving image synthesis. Finally, by putting these two architecture together,\nwe can transform an image to a vector and back to another image. Such image-to-image\ntransformations are useful in applications such as image semantic segmentation. These\nideas are described in the following Sections.\n6. It is worth taking a moment to analyze how convolution layers act on images and why\nthey are so useful. When dealing with images input in the context of deep learning, a \ufb01rst\nnaive approach could be to \ufb02atten the image and feed it to a regular fully connected MLP.\n52", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2213, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "670c5bea-a3ab-4db6-843f-3198dd2518c3": {"__data__": {"id_": "670c5bea-a3ab-4db6-843f-3198dd2518c3", "embedding": null, "metadata": {"page_label": "53", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c57292c7-148f-44be-be23-551f81f6a484", "node_type": "4", "metadata": {"page_label": "53", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "3a40b1b4e71186994f89348d723ad2c681edcbc3d8401f3e50594c4185a624d7", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "However, this would lead to di\ufb00erent problems. In fact, for regular images, the size of the\n\ufb02atten input would be extremely large. In that case, we would have 2 possibilities when\nde\ufb01ning the architecture of the network:\n(a) We can size the \ufb01rst layers of the network to have a width comparable to the (large)\ndimension of the input.\n(b) We can have a sharp decrease in the width of the second hidden layer.\nEither of the strategies would lead to issues. In the \ufb01rst case, the size of the network would\nbe too large. Hence, there would be too many trainable parameters which would require\nan unrealistic amount of data to train the network. In the second case, the compression\nof information happening between the \ufb01rst two layers would be too aggressive, making\nit very hard for the network to learn how to extract the right features across di\ufb00erent\nimages. Moreover, important spatial relationship among pixels (like edges, shapes, etc.)\nare lost by \ufb02attening an image. Ideally we would like to leverage these relations as much as\npossible, since they carry important spatial information. Convolution layers can solve both\nissues. They allow the input to be a 2D image, while drastically decreasing the number\nof learnable parameters needed for the feature extraction task. In fact, kernels introduce\na limited number of parameters compared to a classic fully connected layer. Since the\nsame kernel is applied at di\ufb00erent pixel locations in an image, .i.e.parameter sharing, they\nutilize the computational resources in an e\ufb03cient and smart manner.\n5.7 Transpose convolution layers\nWe have seen how convolution and pooling layers can be used to scale down images. We now\nconsider layers that do the opposite, i.e, scale up images. To understand what this operation\nwould look like, let us look at a few examples\n1. Consider a 1D image of size 4\nInput =\n[\nu1, u2, u3, u4\n]\nConsider a kernel of size3 \u00d71\nk=\n[\nx, y, z\n]\nConsider a convolution layer with the kernelk, stride 1 and zero-padding layer of size 1.\nThen, the output of the layer acting on the input is\nOutput =\n[\nyu1 + zu2, xu1 + yu2 + zu3, xu2 + yu3 + zu4, xu3 + yu4\n]\nThe steps involved in convolution operator are: pad, dot-product, stride. Note that using\npadding and stride 1 have ensured the output has the same size as the input.\n2. Consider another convolution with the same kernelk, zero-padding layer 1 but stride 2.\nThen, the output of the layer acting on the same input as earlier is\nOutput =\n[\nyu1 + zu2, xu2 + yu3 + zu4\n]\nNote that the size of the output has reduced by a factor of 2. In other words, increasing\nthe stride has allowed us to downsample the input. The question we want to ask is whether\nwe can perform an upsampling in a similar way? This can indeed be done by transposing\nevery operation in a convolution layer.\n53", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2802, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f3284bd5-4742-49bc-b322-fab3b81f223a": {"__data__": {"id_": "f3284bd5-4742-49bc-b322-fab3b81f223a", "embedding": null, "metadata": {"page_label": "54", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9b8101e8-9e2c-4d3b-838c-4a9e624a3c80", "node_type": "4", "metadata": {"page_label": "54", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "cfe5ec47093e994c9c10c1f0523418eafb941c50c3e2af2544ee7de487e627cb", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "\u2022 Instead of using a dot-product (inner-product), we will use an outer-product.\n\u2022 Instead of skipping pixels in the input (stride) we will skip pixels in the output.\n\u2022 Instead of padding, we will need to crop the output.\n3. Let us now see an example of a transpose convolution layer. Consider a 1D input image of\nsize 2 \u00d71\nInput =\n[\nu1, u2\n]\nand a kernel of size3 \u00d71\nk=\n[\nx, y, z\n]\n.\nif we perform the outer-product of the input withk, we will get\nOuter product=\n[u1x u1y u1z\nu2x u2y u2z\n]\n.\nIf we use a stride of 2, we will need to shift the rows of the outer-product by 2\nStriding =\n[u1x u1y u 1z 0 0\n0 0 u2x u2y u2z\n]\n.\nAfter striding is performed we need to add the entries in each column and crop the vector\nto get the output\nOutput = Crop(\n[\nu1x, u1y, u1z+ u2x, u2y, u2z\n]\n) =\n[\nu1x, u1y, u1z+ u2x, u2y\n]\nwhere we have cropped out the last few elements (by convention) to get an output which\nhas 2 times the size of the input.\n4. We consider transpose convolution in 2D applied on a 2D image of size(2 \u00d72). The kernel\nis taken to be of shape(3 \u00d73) with stride 2 and padding (cropping). The action of this\ntranspose convolution is shown in Figure 5.8(a), where we \ufb01rst obtain an image of size\n(5 \u00d75) which is then cropped to give an output of size(4 \u00d74). Note that the output\npixels get an unequal contribution from the various patches (indicated by numbers in the\nFigure), which leads to checker-boarding, which is undesirable. Checker-boarding refers to\npixel-to-pixel variations in the values of the output image.\n5. One way to avoid checker-boarding, is by ensuring that the \ufb01lter size is an integer multiple\nof the stride. Let us repeat the previous example but with a(2 \u00d72) kernel. The operation\nis illustrated in Figure 5.8(b)In this case, we do not need to pad (crop) and each output\npixel has an equal contribution.\nWe make some remarks:\n1. Transpose convolution layers are also called fractionally-strided layers, because for every\none step in the input, we take greater than one step in the output. This is the opposite of\nwhat happens in a convolution layer. In a convolution layer, we take step greater than one\nin the input image, for step of uint one in the output image.\n2. Transpose convolutions are a tool of choice for upscalingthrough learnable parameters.\n3. Upscaling is typically done with a reduction in the number of channels, which is once again\nthe opposite of what is done in convolution layers.\n54", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2431, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a335367e-9c89-4ca5-965a-2db86a918a83": {"__data__": {"id_": "a335367e-9c89-4ca5-965a-2db86a918a83", "embedding": null, "metadata": {"page_label": "55", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "335b341f-1b61-47d9-ad56-aabd3f0e18ac", "node_type": "4", "metadata": {"page_label": "55", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "c00f8890584c21f55cdef12f8e3de38d9b196354da42007f353fd043723c7f05", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "(a) kernel size(3 \u00d7 3), checker-boarding\n(b) kernel size(2 \u00d7 2), no checker-boarding\nFigure 5.8: Example of a transpose convolution operation. The cells marked with red X\u2019s in the\noutput are cropped out. The numbers denote the number of patches that contributed to the\nvalue in the output pixel.\n55", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 298, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "85086b2c-3cdd-4e68-8a99-5803d67abfc6": {"__data__": {"id_": "85086b2c-3cdd-4e68-8a99-5803d67abfc6", "embedding": null, "metadata": {"page_label": "56", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "69a1b0b1-754a-47da-b8f1-688d44bf219e", "node_type": "4", "metadata": {"page_label": "56", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "5043c3aa481aa17c0c147083e58bf6df410a1b7c9390de68f7617a59236bbe34", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "5.8 Image-to-image transformations\nImage-to-image to transformations can be seen analogous to function-to-function transformations.\nThese types of networks are typically used in computer vision, super-resolution, style transfer,\nand also in computational physics where we (say) map the source (RHS) \ufb01eld to the solution of\nthe PDE.\nWe will discuss a particular type of network for such transformations, which is known as\nU-Nets [26]. In a U-Net (see Figure 5.9), there a down is a downward branch which takes an input\nimage and downscales the images using a number of convolution layers and pooling operations.\nAs we go down this branch, the number of channels typically increase. After we reach the coarsest\nlevel, we have an upward branch that scales up the image and reduced the number of channels\nusing transpose convolution type operations, to \ufb01nally give the output image. In addition to\nthese branches, the U-Net also makes use of skip connections that combines information at a\nparticular scale in the downward branch to the information in the upward branch at the same\nscale. These connections are similar to what are used in ResNets. In the upscaling branch of\nthe U-net, if you consider the activation at one point, you will see they come from two di\ufb00erent\nsources. One of these is the from the same spatial scale in the down-scaling branch of the U-net,\nand the other is from the coarser scales of the upscaling branch of the U-net.\nRemark 5.8.1.The U-net architecture shares many common features with the V-cycle that is\ntypically used in multigrid preconditioners.\nRemark 5.8.2.We can also think of a the U-net as an encode-decoder network with the additional\nfeature of including skip connections.\nFigure 5.9: Example of a U-Net taken from [26].\n56", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1763, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "fa38f147-4f8c-425e-a520-9c1e959c1cf7": {"__data__": {"id_": "fa38f147-4f8c-425e-a520-9c1e959c1cf7", "embedding": null, "metadata": {"page_label": "57", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "becc8b28-a7cd-43fd-9e3e-bed107ead8e3", "node_type": "4", "metadata": {"page_label": "57", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "e3a8da3558729ccb9a664008b7aac2f24f848449c0f702c96201db76533a539a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Chapter 6\nOperator Networks\n6.1 The problem with PINNs\nRecall that a typical MLPy= F(x; \u03b8) is a function that takes as inputx\u2208Rd and gives an\noutput y\u2208Rd with trainable weights\u03b8. Also, as we discussed in Chapter 4, a PINN is a network\nof the formu(x) = F(x; \u03b8) taking as input the independent variablexof the underlying PDE\nand giving the solutionu(x) (of the PDE) as output. The network is trained by minimizing\nthe weighted sum of the PDE and boundary residual. However, this is just one instance of the\nsolution of the PDE for some given boundary condition and source term. For instance, if we\nconsider the PDE\n\u2207\u00b7(\u03ba\u2207u) = f(x), x\u2208\u2126 = [0,1] \u00d7[0,1]\nu(x) = g(x), x\u2208\u2202\u2126 (6.1)\nand train a PINNF(x; \u03b8) to minimize the loss function\n\u03a0(\u03b8) = 1\nNv\nNv\u2211\ni=1\n|\u2207\u00b7(\u03ba\u2207F(xi; \u03b8)) \u2212f(xi)|2 + \u03bbb\nNb\nNb\u2211\ni=1\n|F(xi; \u03b8) \u2212g(xi)|2\nThen, if \u03b8\u2217 = arg min\n\u03b8\n\u03a0(\u03b8), the PINN solving(6.1) will be u(x) = F(x; \u03b8\u2217). However, if we\nchange f or g in (6.1), we have no reason to believe that the same trained network would work.\nIn fact, we would need to retrain the network (with perhaps the same architecture) for the new\nf and g. This can be quite cumbersome to do, and we would ideally like to avoid it. In this\nchapter, we will see ways by which we can overcome this issue.\n6.2 Parametrized PDEs\nAssume the the source termf in (6.1) is given as a parametric functionf(x; \u03b1). For instance,\nwe could have\nf(x1,x2; \u03b1) = 4\u03b1x1(1 \u2212x1)x2(1 \u2212x2)\nThen we could train a PINN that accommodates for the parametrization by considering a network\nthat takes as input bothxand \u03b1, i.e.,F(x,\u03b1; \u03b8). This is shown in Figure 6.1 This network can\nbe trained by minimizing the loss function\n\u03a0(\u03b8) = 1\nNa\nNa\u2211\nj=1\n[\n1\nNv\nNv\u2211\ni=1\n|\u2207\u00b7(\u03ba\u2207F(xi,\u03b1j; \u03b8)) \u2212f(xi,\u03b1j)|2 + \u03bbb\nNb\nNb\u2211\ni=1\n|F(xi,\u03b1j; \u03b8) \u2212g(xi)|2\n]\n57", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1746, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "9486bcc0-8330-4407-a7f8-effa09ab135c": {"__data__": {"id_": "9486bcc0-8330-4407-a7f8-effa09ab135c", "embedding": null, "metadata": {"page_label": "58", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "381593af-638e-45b3-8a3c-bf260d76b812", "node_type": "4", "metadata": {"page_label": "58", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "5976a7ce296a2245d714cb7f60819822934bc78916dedf0af4e9628cc4fcb85f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Note that we have to also consider collocation points for the parameter\u03b1 while constructing\nthe loss function. If \u03b8\u2217 = arg min\n\u03b8\n\u03a0(\u03b8), then the solution to the parameterized PDE would\nbe u(x,\u03b1) = F(x,\u03b1; \u03b8\u2217). Further, for any new value of\u03b1 = \u02c6\u03b1 we could \ufb01nd the solution by\nevaluating F(x,\u02c6\u03b1; \u03b8\u2217). We could use the same approach if there was a way of parameterizing\nthe functions\u03ba(x) and g(x).\n\ud835\udc65!\n\ud835\udc65\"\n\ud835\udefc \u2131 \ud835\udc62\nFigure 6.1: Schematic of a PINN with a parameterized input.\nHowever, what if we wanted the solution for an arbitrary, non-parametricf? In order to do\nthis, we need to \ufb01nd a way to approximateoperators that map functions to functions.\n6.3 Operators\nConsider a class of functionsa(y) \u2208A such thata: \u2126Y \u2192RD. The functions in this class\nmight have certain properties, such asa\u2208C(\u2126Y) or a\u2208L2(\u2126Y). Also consider the operator\nN: A\u21a6\u2192C(\u2126X), withu(x) = N(a)(x) for x\u2208\u2126X. Let us see some examples of operatorsN.\n1. Consider the PDE\n\u2207\u00b7(\u03ba\u2207u) = f, x\u2208\u2126\nu= g, x\u2208\u2202\u2126 (6.2)\nFor this PDE,\u2126X = \u2126 Y = \u2126 and the operator N maps the RHS f to the solution\n(temperature) u of the PDE. That is,u = N(f)(x). The input and the output to the\noperator are related by the equation above where it is assumed that\u03ba and g are given and\n\ufb01xed.\n2. Consider the PDE\n\u2207\u00b7(\u03ba\u2207u) = f, x\u2208\u2126\nu= g, x\u2208\u2202\u2126 (6.3)\nwhich is the same as the previous PDE but we are assuming that the conductivity \ufb01eld\u03ba\nmight change for the model, instead of the RHS. Then,\u2126X = \u2126Y = \u2126 and the operatorN\nmaps the conductivity\u03ba to the solutionu of the PDE. That is,u= N(\u03ba)(x). The input\nand the output to the operator are related by the equation above where it is assumed that\nf and g are given and \ufb01xed.\n58", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1637, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "9e57ffa4-8266-4c97-aaa3-39948246437e": {"__data__": {"id_": "9e57ffa4-8266-4c97-aaa3-39948246437e", "embedding": null, "metadata": {"page_label": "59", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "fff1f62d-f8d2-4ffc-bd7d-bc745aa724e5", "node_type": "4", "metadata": {"page_label": "59", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "35ecf74f71221b15a8e3b29e27505ab481e2c835ffa3ababfed5affa91a2f170", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "3. Once again, consider the same PDE but with conductivity and the boundary condition\nbeing allowed to change\n\u2207\u00b7(\u03ba\u2207u) = f(x), x\u2208\u2126\nu(x) = g(x), x\u2208\u2202\u2126 (6.4)\nThen, the operatorNmaps the boundary conditiongand the conductivity\u03bato the solution\nu of the PDE. That is,u = N(\u03ba,g)(x). In this case the input to the operator are two\nfunctions (g,\u03ba) and the output is a single function. Therefore\u2126X = \u2126, while\u2126Y = \u2126 \u00d7\u2202\u2126.\nThe input and the output are related through the solution to the PDE above where it is\nassumed thatf is given and \ufb01xed.\n4. Now consider the equations of linear isotropic elasticity posed on a three-dimensional\ndomain \u2126 \u2282R3,\n\u2207\u00b7\n(\n\u03bb(\u2207\u00b7u)I+ 2\u00b5\u2207S(u)\n)\n= f(x), x\u2208\u2126\nu(x) = g(x), x\u2208\u2202\u2126. (6.5)\nConsider the operator de\ufb01ned byu(x) = N(f)(x). Here the input function,f : \u2126 \u2192R3,\nand the output functionu: \u2126 \u2192R3. The two are related by the equations above where\n\u03bb,\u00b5, gare given and \ufb01xed.\n5. Now, consider a di\ufb00erent PDE. In particular, the advection-di\ufb00usion-reaction equation,\n\u2202u\n\u2202t + a\u00b7\u2207u\u2212\u03ba\u22072u+ u(1 \u2212u) = f, (x,t) \u2208\u2126 \u00d7(0,T]\nu(x,t) = g(x,t), (x,t) \u2208\u2202\u2126 \u00d7(0,T]\nu(x,0) = u0(x), x\u2208\u2126.\n(6.6)\nWe want to \ufb01nd the operatorNmaps the initial conditionu0 to the solutionuat the \ufb01nal\ntime T, i.e.,u(x,T) = N(u0)(x). In this case\u2126X = \u2126Y = \u2126. Further the input and the\noutput functions are related to each other via the solution of the PDE above witha,\u03ba,f,g\ngiven and \ufb01xed.\nRemark 6.3.1.It is often useful to determine whether an operator is linear or non-linear. This\nis because if it is linear it can be well approximated by another linear operator. In the cases\nconsidered above the operators in examples 1 and 4 were linear whereas those in examples 2,3,\nand 5 were nonlinear.\nWe are interested in networks that approximate the operatorN. We will see how we can do\nthis in the next section. These types of networks are often referred to as Operator Networks.\nThey are two popular versions of these networks. One is referred to as a Deep Operator Network,\nor a DeepONet, and the other is referred to as a Fourier Neural Operator. We describe the\nDeepONet in the next section.\n6.4 Deep Operator Network (DeepONet) Architecture\nOperator networks were \ufb01rst proposed by Chen and Chen [5], where they considered only shallow\nnetworks with a single hidden layer. This idea was rediscovered and extended to deep architectures\nmore recently in [14] and were called DeepONets. A standard DeepONet comprises two neural\nnetworks. We describe below its construction to approximate an operatorN: A\u2192U, where\nA is a set of functions of the forma: \u2126Y \u2282Rd \u2192R while U consists of functions of the form\nu: \u2126X \u2282RD \u2192R. Furthermore, we assume that point-wise evaluations of both class of functions\nis possible. The architecture for the DeepONet for this operator is illustrated in Figure 6.2. It is\nexplained below:\n59", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2781, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f3a0c624-0b0e-41f4-bafd-03bd91b5a3c7": {"__data__": {"id_": "f3a0c624-0b0e-41f4-bafd-03bd91b5a3c7", "embedding": null, "metadata": {"page_label": "60", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b0f7085f-6fab-48ac-b13d-84caeb9733b8", "node_type": "4", "metadata": {"page_label": "60", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "4a5b8604c54e84c75128a2536add1d718061b03af2190e6fd5a303739cc59a03", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "\u2022 Fix M distinct sensor pointsy(1),..., y(M) in \u2126Y.\n\u2022 Sampleafunction a\u2208Aatthesesensorpointstogetthevector a= [a(y(1)),..., a(y(M))]\u22a4\u2208\nRM.\n\u2022 Supply aas the input to a sub-network, called thebranch netB(.; \u03b8B) : RM \u2192Rp, whose\noutput would be the vector\u03b2 = [ \u03b21(a),..., \u03b2p(a)]\u22a4 \u2208Rp. Here \u03b8B are the trainable\nparameters of the branch net. The dimension of the output of the branch is relatively small,\nsay p\u2248100.\n\u2022 Supply xas an input to a second sub-network, called thetrunk netT(.; \u03b8T) : RD \u2192Rp,\nwhose output would be the vector\u03c4 = [\u03c41(x),..., \u03c4p(x)]\u22a4\u2208Rp. Here \u03b8T are the trainable\nparameters of the trunk net.\n\u2022 Take a dot product of the outputs of the branch and trunk nets to get the \ufb01nal output of\nthe DeepONet \u02dcN(.,.; \u03b8) : RD \u00d7RM \u2192R which will approximate the value ofu(x)\nu(x) \u2248 \u02dcN(x,a; \u03b8) =\np\u2211\nk=1\n\u03b2k(a)\u03c4k(x). (6.7)\nwhere the trainable parameters of the DeepONet will be the combined parameters of the\nbranch and trunk nets, i.e.,\u03b8= [\u03b8T,\u03b8M].\nFigure 6.2: Schematic of a DeepONet\nIn the above construction, once the DeepONet is trained (we will discuss the training in the\nfollowing section), it will approximate the underlying operatorN, and allow us to approximate\nthe value of anyN(a)(x) for anya\u2208A and anyx\u2208\u2126X. Note that in the construction of the\nDeepONet, theM sensor points need to be pre-de\ufb01ned and cannot change during the training\nand evaluation phases.\nWe can make the following observations regarding the DeepONet architecture:\n1. The expression in (6.7) has the form of representing the solution as the sum of a series of\ncoe\ufb03cients and functions. The coe\ufb03cients are determined by the branch network, while the\n60", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1631, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d7c75882-374e-4608-9b47-e6b37590f4b3": {"__data__": {"id_": "d7c75882-374e-4608-9b47-e6b37590f4b3", "embedding": null, "metadata": {"page_label": "61", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "edafc58b-3767-4a84-8a55-46c512ba63f6", "node_type": "4", "metadata": {"page_label": "61", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "821e572d2f395b08b681ba0155d96b4e639c563366c00c40936bcc599c9d7cb0", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "functions are determined by the trunk network. In that sense the DeepONet construction is\nsimilar to that of what is used in the spectral method or the \ufb01nite element method. There\nis a critical di\ufb00erence though. In these methods, the basis functions are pre-determined\nand selected by the user. However, in the DeepONet these functions are determined by the\ntrunk network and their \ufb01nal form depends on the data used to train the DeepONet.\n2. Architecture of the branch sub-network: When points for sampling the input function\nare chosen randomly, the appropriate architecture for the branch network comprises fully\nconnected layers. Further recognizing that the dimension of the input to this network can\nbe rather largeN1 \u2248104, while the output is typically smallp\u2248102, this network can be\nthought of as an encoder.\nWhen points for sampling the input function are chosen on a uniform grid, the appropriate\narchitecture for the branch network comprises convolutional layer layers. In that case this\nnetwork maps an image of large dimension (N1 \u2248104) to a latent vector of small dimension,\np\u2248102. Thus it is best represented by a convolutional neural network.\n3. Broadly speaking, there are two ways of improving the experssivity of the DeepONet. These\ninvolve increase the number of network parameters in the branch and trunk sub-networks,\nand increasing the dimensionp of the latent vectors formed by these sub-networks.\n6.5 Training DeepONets\nTraining a DeepONet is typically supervised, and requires pairwise data. The following are the\nmain steps involved:\n1. Select N1 representative functiona(i), 1 \u2264i\u2264N1 from the setA. Evaluate the values of\nthese functions at theM sensor points, i.e.,a(i)\nj = a(i)(y(j)) for 1 \u2264j \u2264M. This gives us\nthe vectorsa(i) = [a(i)(y(1)),...,a (i)(y(M))]\u22a4\u2208RM for each1 \u2264i\u2264N1.\n2. For eacha(i), determine (numerically or analytically) the corresponding functionsu(i) given\nby the operatorN.\n3. Sample the functionu(i) at N2 points in\u2126X, i.e.,u(i)(x(k)) for 1 \u2264k\u2264N2.\n4. Construct the training set\nS=\n{(\na(i),x(k),u(i)(x(k))\n)\n: 1 \u2264i\u2264N1, 1 \u2264k\u2264N2\n}\nwhich will haveN1 \u00d7N2 samples.\n5. De\ufb01ne the loss function\n\u03a0(\u03b8) = 1\nN1N2\nN1\u2211\ni=1\nN2\u2211\nk=1\n|\u02dcN(x(k),a(i); \u03b8) \u2212u(i)(x(k))|2.\n6. Training the DeepONet corresponds to \ufb01nding\u03b8\u2217= arg min\n\u03b8\n\u03a0(\u03b8).\n7. Once trained, then given any newa\u2208A samples at theM sensor points (which gives the\nvector a\u2208RM), and a new pointx\u2208\u2126X, we can evaluate the corresponding prediction\nu\u2217(x) = \u02dcN(x,a; \u03b8\u2217).\n61", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2453, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ff2d0a97-48d4-480f-b8e2-ae19b94f4278": {"__data__": {"id_": "ff2d0a97-48d4-480f-b8e2-ae19b94f4278", "embedding": null, "metadata": {"page_label": "62", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "015de721-0ca6-435e-a2e8-19ac57134a83", "node_type": "4", "metadata": {"page_label": "62", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "875088fc895332c340fc5f9a9e57b8fa68fbe4503a04674f66559440f8f72884", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Remark 6.5.1.We need not choose the sameN2 points across alli in the training set. In fact,\nthese can be chosen randomly leading to a more diverse dataset.\nRemark 6.5.2. The DeepONet can be easily extended to the case where the input comprises\nmultiple functions. In this case, the trunk network remains the same, however the branch network\nnow has multiple vectors as input. The case corresponding to two input functions,a(y) and b(y),\nwhich when sampled yield the vectors,aand b, is shown in Figure 6.3.\n\ud835\udc82\ud835\udc83\n\ud835\udc99\nBranch\ud835\udf37\nTrunk \ud835\udf49\n\ud835\udc96Dot Product\nFigure 6.3: Schematic of a DeepONet with two input functions.\nRemark 6.5.3.The DeepONet can be easily extended to the case where the output comprises\nmultiple functions (sayD such functions). In this case, the output of the branch and trunk\nnetwork leads toD vectors each with dimensionp. The solution is then obtained by taking the\ndot product of each one of these vectors. The case corresponding to two output functionsu1(x)\nand u2(x) is shown in Figure 6.3.\n\ud835\udc82\n\ud835\udc99\nBranch\ud835\udf37\ud835\udfd0\nTrunk \ud835\udf49\ud835\udfcf\n\ud835\udc96\ud835\udfcfDot Product\ud835\udf37\ud835\udfcf\n\ud835\udf49\ud835\udfd0 \ud835\udc96\ud835\udfd0Dot Product\nFigure 6.4: Schematic of a DeepONet with two output functions.\n6.6 Error Analysis for DeepONets\nThere is a universal approximation theorem for a shallow version of DeepONets by Chen and\nChen [5]\n62", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1253, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "85aab95b-e950-4ade-b8a9-810611a92c55": {"__data__": {"id_": "85aab95b-e950-4ade-b8a9-810611a92c55", "embedding": null, "metadata": {"page_label": "63", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d3ae84b9-c424-4056-92d8-ca476d79952c", "node_type": "4", "metadata": {"page_label": "63", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "15e9dd9160f96e3c417f42c61c88e3cce0ce8d06bf3445a8d0820806ab00fd7a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Theorem 6.6.1. Suppose \u2126X and \u2126Y are compact sets inRD (or more generally a Banach\nspace) andRd, respectively. LetNbe a nonlinear, continuous operator mappingV \u2282C(\u2126Y) into\nC(\u2126X). Then given\u03f5> 0, there exists a DeepONet withM sensors and a single hidden layer of\nwidth P in the branch and trunk nets such that\nmax\nx\u2208\u2126X\na\u2208A\n|\u02dcN(x,a; \u03b8) \u2212N(a)(x)|<\u03f5\nfor a large enoughP and M.\nThis result has been extended to a deeper version of the network in [14], and generalized\nfurther by removing the compactness assumptions on the spaces [12].\nRecently in [20], the authors have developed an estimate of the error in a DeepONet that\nclearly pinpoints the di\ufb00erent sources of error in a DeepONet. This estimate states, the error\nmeasured in theL2(\u2126) norm is bounded as\nmax\na\u2208A\n\u2225\u02dcN(x,a; \u03b8) \u2212N(a)(x)\u2225L2(\u2126) \u2264C\n(\n\u03f5h + \u221a\u03f5t + \u03f5s + M\u2212\u03b11 + (N2)\u2212\u03b12\n)\n(6.8)\nwhere \u03f5h is the error made by the numerical solver used to generate the approximate target\nsolutions u(i) in the training set (as compared to the exact solutions),\u03f5t is the \ufb01nal training\nerror/loss attained, while\u03f5s bounds the distance of anya\u2208Afrom the set of functions{a(i)}N1\ni=1\nused to construct the construct the training set, i.e., an estimate of how well the training samples\ncovers the input spaceA. Further, since the input function is evaluated atM \ufb01nite sensor nodes,\nwhile the output is evaluated atN2 output nodes, this will lead to an additional discretization\n(or quadrature) error which is given by the last two terms in(6.8). Note that this is similar to\nthe error estimates obtained for PINNs in (4.21).\n6.7 Physics-Informed DeepONets\nRecall that DeepONets approximatesu(x) = N(a)(x) \u2248 \u02dcN(x,a; \u03b8). Assume that the pairaand\nu satisfy a PDE. For example,\n\u2207\u00b7(\u03bau) = f in \u2126\nu= g on \u2202\u2126 (6.9)\nwhere \u03ba and g are prescribed. To construct the operatorNthat mapsf to u, we need to solve\nthe PDE externally. However, in addition to this, we can also use a PINN-type loss function and\nadd that to the total loss. This is the idea of Physic-Informed DeepONets proposed in [30]. So\nfor the above model PDE, the loss additional physics-based loss would would be,\n\u03a0p(\u03b8) = 1\n\u00afN1\n1\n\u00afN2\n\u00afN1\u2211\ni=1\n\u00afN2\u2211\nk=1\n\u23d0\u23d0\u23d0\u2207x\u00b7\n(\n\u03ba\u2207x \u02dcN(x(k),f(i); \u03b8)\n)\n\u2212f(i)(x(k))\n\u23d0\u23d0\u23d0\n2\n. (6.10)\nThis is in addition to the standard data-driven loss function which, for this example is given by\n\u03a0d(\u03b8) = 1\nN1N2\nN1\u2211\ni=1\nN2\u2211\nk=1\n|\u02dcN(x(k),f(i); \u03b8) \u2212u(i)(x(k))|2. (6.11)\nThe total loss function is a weighted sum of these two terms:\n\u03a0(\u03b8) = \u03a0d(\u03b8) + \u03bb\u03a0p(\u03b8), (6.12)\nwhere \u03bb is a hyper-parameter. A few comments are in order:\n63", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2519, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3b8cac5b-6ffa-4707-9ae6-9f54417057ce": {"__data__": {"id_": "3b8cac5b-6ffa-4707-9ae6-9f54417057ce", "embedding": null, "metadata": {"page_label": "64", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9a2abde5-e80a-4a3e-8de7-8319bf297e4e", "node_type": "4", "metadata": {"page_label": "64", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "369aff3911a805d7ac7f9c605ac9fe8c9506cd8182a2b0a2a9c5d175a75c4995", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "1. The output sensor points used in the physics-based loss function are usually distinct from\nthe output sensor points used in the data-driven loss term. The former represent the\nlocations at which we wish to minimize the residual of the PDE, while the latter represent\nthe points at which the solution is available to us through external means. The total\nnumber of the output sensor points in the physics-based portion of the loss function is\ndenoted by \u00afN2, whereas in the data-driven loss function it is denoted byN2.\n2. The set of input functions used to construct the physics-based loss function is usually\ndistinct from the set of input functions used to construct the data-driven loss function.\nThe former set represents the functions for which we wish to minimize the residual of the\nPDE, while the latter set represents the collection of input functions for which the solution\nis available to us through external means. The total number of functions in the set used to\nconstruct the the physics-based portion of the loss function is denoted by\u00afN1, whereas the\ntotal number of functions in the set used to construct the data-driven portion of the loss\nfunction is denoted byN \u22121.\nAs earlier, we train the network by \ufb01nding\u03b8\u2217= arg min\n\u03b8\n\u03a0(\u03b8) and approximate the solution\nfor a newa by u\u2217(x) = \u02dcN(x,a; \u03b8\u2217). The advantages of adding the extra physics-based loss are:\n1. It reduces the demand on the amount of data in the data-driven loss term. What is means\nis that we don\u2019t have to generate as many solutions of the PDE for training the DeepONet.\n2. It makes the network more robust in that it becomes more likely to produce accurate\nsolutions for the type of input functions not included in the training set for the data-driven\nloss term.\n6.8 Fourier Neural Operators - Architecture\nWe now introduce and discuss Fourier Nerual Operators (FNOs) [13]. We discuss their architecture\nin this section, and then discuss other aspects in the following section.\nOur approach in developing the architecture for a FNO will be to begin with the architecture\nof a typical feed-forward MLP that maps a scalar to another scalar, and systematically extend it\nso that the extended version maps a scalar valued function to another scalar valued function.\nA(1)s A(l+1)ss\ud835\udc99(\ud835\udfce) \ud835\udf43(\ud835\udfcf) \ud835\udc99(\ud835\udfcf) \ud835\udc99(\ud835\udc8d) \ud835\udc99(\ud835\udc8d&\ud835\udfcf) \ud835\udc99(\ud835\udc73&\ud835\udfcf)\ud835\udf43(\ud835\udc8d) \ud835\udf43(\ud835\udc8d&\ud835\udfcf) \ud835\udf43(\ud835\udc73&\ud835\udfcf)\ns\nFigure 6.5: Computational graph for a feed-forward MLP.\nIn Figure 6.5 we have plotted the computational graph of an MLP. We are focused only on\nthe forward part (not the back-propagation) part of this network. For simplcity, we assume that\nthe inputx(0) = x is a scalar and the outputx(L) = y is also a scalar. Further all the other\nhidden variables (with the exception of\u03be(L+1)) are vectors withH components. That is, the\nwidth of each layer isH.\nThe \ufb01rst step in this process will be to replace the input and the output with functions. The\ninput will now be the functiona(x) : \u2126 \u21a6\u2192R1. Similarly the output is the functionu(x) : \u2126 \u21a6\u2192R1.\nThis leads us to the computational graph shown in Figure 6.6.\n64", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3015, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "0bcb1d91-96c7-483c-852b-4fede3a3a011": {"__data__": {"id_": "0bcb1d91-96c7-483c-852b-4fede3a3a011", "embedding": null, "metadata": {"page_label": "65", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ce09470b-27bf-486e-9cc6-50afc19c9344", "node_type": "4", "metadata": {"page_label": "65", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "49985c85be42ad2b1f1ea5ff0f66bacd708295894fe9a921c8de700d72aa850a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "A(1)s A(l+1)ss\ud835\udc4e(\ud835\udc99) \ud835\udc97(\ud835\udfcf) \ud835\udc96(\ud835\udfcf) \ud835\udc96(\ud835\udc8d) \ud835\udc96(\ud835\udc8d%\ud835\udfcf)\ud835\udc97(\ud835\udc8d) \ud835\udc97(\ud835\udc8d%\ud835\udfcf) \ud835\udc97(\ud835\udc73%\ud835\udfcf)\ns\ud835\udc62(\ud835\udc99)\nFigure 6.6: Computational graph for a feed-forward Fourier Neural Operator (FNO) network.\nThe next step is consider the variables in the hidden layers. In the MLP, these were all\nvectors withH components. In the FNO, these will be functions withH components. That is,\nv(1),\u00b7\u00b7\u00b7 ,v(L),u(1),\u00b7\u00b7\u00b7 ,u(L) : \u2126 \u21a6\u2192RH. (6.13)\nAs shown in Figure 6.6,v(n) and u(n) are the counterparts of\u03be(n) and x(n), respectively. Further\nsince \u03be(L+1) was a scalar, we will setv(L+1) to be a scalar valued function.\nWe are now done with extending the input, the output and the variables in the hidden layers\nfrom vectors to functions. Next, we need to extend the operators that transform vectors to\nvectors within an MLP to those that transform functions to functions within an FNO.\nWe begin with the operatorA(1), which in an MLP is an a\ufb03ne map from a vector with one\ncomponent to a vector withH components. Its straightforward extension to functions is,\nv(1)(x) = A(1)(a)(x), (6.14)\nwhere\nv(1)\ni (x) = W(1)\ni a(x) + b(1)\ni , i = 1,\u00b7\u00b7\u00b7 ,H. (6.15)\nHere W(1)\ni and b(1)\ni are the weights and biases associated with this layer.\nSimilarly, in an MLP the operatorA(L+1) is an a\ufb03ne map from a vector withH components\nto a vector with1 component. It\u2019s straightforward extension to functions is,\nv(L+1)(x) = A(L+1)(u(L))(x), (6.16)\nwhere\nv(L+1)(x) = W(L+1)\ni u(L)\ni (x) + b(L+1), i = 1,\u00b7\u00b7\u00b7 ,H. (6.17)\nHere W(L+1)\ni and b(L+1) are the weights and the bias associated with this layer.\nNext we describe the action of the activation on input functions. It is a simple extension of\nthe activation function applied to the point-wise values of the input function. That is,\nu(n)(x) = \u03c3(v(n))(x), (6.18)\nwhere\nu(n)\ni (x) = \u03c3(v(n)\ni (x)), i = 1,\u00b7\u00b7\u00b7 ,H. (6.19)\nFinally it remains to extend the operatorsA(n),n = 2,\u00b7\u00b7\u00b7 ,L to functions. These are de\ufb01ned\nas,\nv(n+1)(x) = A(n+1)(u(n))(x), (6.20)\nwhere\nv(n+1)\ni (x) = W(n+1)\nij u(n)\nj (x) + b(n+1)\ni (6.21)\n+\n\u222b\n\u2126\n\u03ba(n+1)\nij (y\u2212x)u(n)\nj (y)dy, i = 1,\u00b7\u00b7\u00b7 ,H. (6.22)\n65", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2030, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "77737234-7603-4fd3-9f55-efd32080cd45": {"__data__": {"id_": "77737234-7603-4fd3-9f55-efd32080cd45", "embedding": null, "metadata": {"page_label": "66", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1ac9952c-8e56-4f42-bc03-89499d61f274", "node_type": "4", "metadata": {"page_label": "66", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "8f805ac49911adcc4cc78c2fdc1edd04e3e8133e9ac465acb444466be1794e7f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In the equation above the summation over the dummy indexj (from 1 toH) is implied. The\nnew term that appears in this equation is a convolution. It is motivated by the observation that\na large class of linear operators can be represented as convolutions. An example is the so-called\nGreen\u2019s operator which maps the right hand side (also called the forcing function) of a linear\nPDE to its solution. The functions\u03ba(n+1)\nij (z) are called the kernels of the convolution. We note\nthat there areH2 of these functions in each layer.\nIt is instructive to examine a speci\ufb01c case of a convolution. Let us consider\u2126 = [0,L1]\u00d7[0,L2],\nwhere we denote the two coordinates by eitherx1 and x2, ory1 or y2. In this case we may write\nthe convolution as,\nvi(x1,x2) =\n\u222b L1\n0\n\u222b L2\n0\n\u03baij(y1 \u2212x1,y2 \u2212x2)uj(y1,y2) dy2dy1, i = 1,\u00b7\u00b7\u00b7 ,H. (6.23)\nIn the equation above, we have dropped the superscripts since they are not relevant to the\ndiscussion.\nRemark 6.8.1. We may interpret the FNO as a sequence of an a\ufb03ne transform and convo-\nlution followed by a point-wise nonlinear activation. This combination of linear and nonlinear\n(activation) operations allows us to approximate nonlinear operator using this architecture.\nRemark 6.8.2.It is instructive to list all the trainable entities in a FNO. First we list all the\ntrainable parameters:\nW(1)\ni ,W(2)\nij ,\u00b7\u00b7\u00b7 ,W(L)\nij ,W(L+1)\ni ; b(1)\ni ,b(2)\ni ,\u00b7\u00b7\u00b7 ,b(L)\ni ,b(L+1). (6.24)\nThereafter, all the trainable kernel functions\n\u03ba(n)\nij (z), n = 2,\u00b7\u00b7\u00b7 ,L. (6.25)\nThe neural operator introduced in this section acts directly on functions and transforms them\ninto functions. However, when implementing this operator on a computer the functions have to\nbe represented discretely. This is described in the following section.\n6.9 Discretization of the Fourier Neural Operator\nThe functions that appear in the neural operator described in the previous section are:\na,v(1),u(1),\u00b7\u00b7\u00b7 ,v(L),u(L),v(L+1),u. (6.26)\nEach of these functions is de\ufb01ned on the domain\u2126. We discretize this domain withN uniformly\ndistributed points, and represent each function using its values on these points.\nAs an example, in two dimensions, with\u2126 = [0 ,L1] \u00d7[0,L2], we represent the function\na(x1,x2) as,\na[m,n] = a(x1m,x2n), m = 1 \u00b7\u00b7\u00b7 ,N1, n= 1 \u00b7\u00b7\u00b7 ,N2. (6.27)\nwhere\nx1m = ( m\u22121) \u00d7 L1\nN1 \u22121 (6.28)\nx1n = ( n\u22121) \u00d7 L2\nN2 \u22121. (6.29)\nThe same representation will be used for all other functions.\n66", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2385, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "0e3cdeea-6308-41d6-89c8-b07d1766002c": {"__data__": {"id_": "0e3cdeea-6308-41d6-89c8-b07d1766002c", "embedding": null, "metadata": {"page_label": "67", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "dc5d61f6-8449-4666-8803-ec6b05e4624c", "node_type": "4", "metadata": {"page_label": "67", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "fd4d78a935d382f7367646dd2d26f1a6319ab7848eb4d3576240bf14c17c65ad", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We now have to consider the discrete version of all operations on these functions as well.\nThis is described below for the special case of\u2126 = [0,L1] \u00d7[0,L2].\nWe begin with the operatorA(1). The discretized version is\nv(1)[m,n] = A(1)(a)[m,n], (6.30)\nwhere\nv(1)\ni [m,n] = W(1)\ni a[m,n] + b(1)\ni , i = 1,\u00b7\u00b7\u00b7 ,H. (6.31)\nSimilarly, the discretized version of the operatorA(L+1) is,\nv(L+1)[m,n] = A(L+1)(u(L))[m,n], (6.32)\nwhere\nv(L+1)[m,n] = W(L+1)\ni u(L)\ni [m,n] + b(L+1), i = 1,\u00b7\u00b7\u00b7 ,H. (6.33)\nNext we describe the action of the activation function on discretized input functions. It is given\nby\nu(n)[m,n] = \u03c3(v(n))[m,n], (6.34)\nwhere\nu(n)\ni [m,n] = \u03c3(v(n)\ni [m,n]), i = 1,\u00b7\u00b7\u00b7 ,H. (6.35)\nFinally it remains to develop the discrete version of the operatorsA(n),n = 2,\u00b7\u00b7\u00b7 ,L. These are\nde\ufb01ned as,\nv(p+1)[m,n] = A(p+1)(u(p))[m,n], (6.36)\nwhere\nv(p+1)\ni [m,n] = W(p+1)\nij u(p)\nj [m,n] + b(p+1)\ni (6.37)\n+\nN1\u2211\nr=1\nN2\u2211\ns=1\n\u03ba(p+1)\nij [r\u2212m,s \u2212n]u(p)\nj [r,s]h1h2, i = 1,\u00b7\u00b7\u00b7 ,H, (6.38)\nwhere h1 = L1\nN1\u22121 and h2 = L2\nN2\u22121 . Note that the integral in the convolution is now replaced\nby a sum over all the grid points. Computing this integral for each value of i and m,n\ninvolves O(N1N2H) \ufb02ops. And since this needs to be done for H di\ufb00erent values ofi, N1\nvalues of M, and N2 values of j, the total cost of discretizing the convolution operation is\nO(N2\n1 N2\n2 H) = O(N2H2), whereN = N1 \u00d7N2. The factor ofN2 in this cost is not acceptable\nand makes the implementation of this algorithm impractical. In the following section we describe\nhow the use of Fourier Transforms (forward and inverse) overcomes this bottleneck and leads\nto a practical algorithm. This is also the reason that this algorithm is referred to as a \u201cFourier\nNeural Operator.\"\n6.10 The Use of Fourier Transforms\nConsider a periodic functionu(x2,x2) de\ufb01ne on\u2126 \u2261[0,L1]\u00d7[0,L2]. If this function is su\ufb03ciently\nsmooth it may be approximated by a truncated Fourier series,\nu(x1,x2) \u2248\nN1/2\u2211\nm=\u2212N1/2\nN2/2\u2211\nn=\u2212N2/2\n\u02c6u[m,n]e2\u03c0i( mx1\nL1\n+ nx2\nL2\n). (6.39)\nHere N1 and N2 are even integers, the coe\ufb03cients\u02c6u[m,n] are the Fourier coe\ufb03cients andi = \u221a\u22121.\nWe note that while the functionu is real-valued the coe\ufb03cients are complex-valued. However,\n67", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2188, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "630c7b20-7592-4575-966f-bf66e5679b26": {"__data__": {"id_": "630c7b20-7592-4575-966f-bf66e5679b26", "embedding": null, "metadata": {"page_label": "68", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5004794f-8fad-4cfa-873c-afee88c05512", "node_type": "4", "metadata": {"page_label": "68", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "34169702c6217f555052f4f971468d5f515348f35ce9b547d48f56eeb091539e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "717fdeb9-3400-458b-9f3f-c6dd08e4d794", "node_type": "1", "metadata": {}, "hash": "9508662ea3eb7a7c6dfea4a8d73ab6ecd9bdff3bbf513557109dd2f0f6356a6b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "since u is real-valued, they obey the rule\u02c6u[\u2212m,\u2212n] = \u02c6u\u2217[m,n], where(.)\u2217denotes the complex-\nconjugate of a complex number. The approximation can be made more accurate by increasing\nN1 and N2, and as these numbers tend to in\ufb01nity, we recover the equality. The relation above is\noften referred to as the inverse Fourier transform, since it maps the Fourier coe\ufb03cients to the\nfunction in the physical space.\nThe forward Fourier transform (which maps the function in the physical space to the Fourier\ncoe\ufb03cients) can be obtained from the relation above by\n1. Multiplying both sides bye\u22122\u03c0i( rx1\nL1\n+ sx2\nL2\n), wherer and s are integers.\n2. Integrating both sides over\u2126.\n3. Recognizing that the integral\n\u222b\n\u2126 e2\u03c0i( (m\u2212r)x1\nL1\n+ (n\u2212s)x2\nL2\n)dx1dx2 is non-zero only whenm= r\nand n= s, and in that case it evaluates toL1L2.\nThese steps yield the \ufb01nal relation:\n\u02c6u[r,s] = 1\nL1L2\n\u222b L1\n0\n\u222b L2\n0\nu(x1,x2)e\u22122\u03c0i( rx1\nL1\n+ sx2\nL2\n)dx1dx2. (6.40)\nWe now describe how Fourier transforms can be used to evaluate the convolution e\ufb03ciently.\nTo do this we consider the special case of 2D convolution in (6.23). We begin with substituting\nuj(y1,y2) = \u2211N1/2\nm=\u2212N1/2\n\u2211N2/2\nn=\u2212N2/2 \u02c6uj[m,n]e2\u03c0i( my1\nL1\n+ ny2\nL2\n) in this equation to get,\nvi(x1,x2) =\n\u222b L1\n0\n\u222b L2\n0\n\u03baij(y1 \u2212x1,y2 \u2212x2)\n\u2211\nm,n\n\u02c6uj[m,n]e2\u03c0i( my1\nL1\n+ ny2\nL2\n) dy2dy1\n=\n\u2211\nm,n\n\u02c6uj[m,n]\n\u222b L1\n0\n\u222b L2\n0\n\u03baij(y1 \u2212x1,y2 \u2212x2)e2\u03c0i( my1\nL1\n+ ny2\nL2\n) dy2dy1\n=\n\u2211\nm,n\n\u02c6uj[m,n]\n\u222b L1\u2212x1\n\u2212x1\n\u222b L2\u2212x2\n\u2212x2\n\u03baij(z1,z2)e2\u03c0i( m(z1+x1)\nL1\n+ n(z2+x2)\nL2\n) dz2dz1\n=\n\u2211\nm,n\n\u02c6uj[m,n]e2\u03c0i( mx1\nL1\n+ nx2\nL2\n)\n\u222b L1\n0\n\u222b L2\n0\n\u03baij(z1,z2)e2\u03c0i( mz1\nL1\n+ nz2\nL2\n) dz2dz1\n= L1L2\n\u2211\nm,n\n\u02c6uj[m,n]\u02c6\u03baij[\u2212m,\u2212n]e2\u03c0i( mx1\nL1\n+ nx2\nL2\n). (6.41)\nIn the development above, in going from the \ufb01rst to the second line we have taken the summation\noutside the integral and recognized that the coe\ufb03cients\u02c6uj[m,n] do not depend ony1 and y2.\nIn going from the second to the third line we have introduced the variablesz1 = y1 \u2212x1 and\nz2 = y2 \u2212x2. In going from the third to the fourth line we have made use of the fact that\nthe functions\u03baij(z1,z2) are periodic. Finally in going from the fourth to the \ufb01fth line we have\nmade use of the de\ufb01nition of the Fourier Transform (6.40). This \ufb01nal relation tells us that the\nconvolution can be computed by:\n1. Computing the Fourier Transform ofuj.\n2. Computing the Fourier Transform of\u03baij.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2310, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "717fdeb9-3400-458b-9f3f-c6dd08e4d794": {"__data__": {"id_": "717fdeb9-3400-458b-9f3f-c6dd08e4d794", "embedding": null, "metadata": {"page_label": "68", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5004794f-8fad-4cfa-873c-afee88c05512", "node_type": "4", "metadata": {"page_label": "68", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "34169702c6217f555052f4f971468d5f515348f35ce9b547d48f56eeb091539e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "630c7b20-7592-4575-966f-bf66e5679b26", "node_type": "1", "metadata": {"page_label": "68", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "8ad8b2f96a69bade09366419d086218ee4564f7b91280b25869c00b8ffaa9779", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "(6.41)\nIn the development above, in going from the \ufb01rst to the second line we have taken the summation\noutside the integral and recognized that the coe\ufb03cients\u02c6uj[m,n] do not depend ony1 and y2.\nIn going from the second to the third line we have introduced the variablesz1 = y1 \u2212x1 and\nz2 = y2 \u2212x2. In going from the third to the fourth line we have made use of the fact that\nthe functions\u03baij(z1,z2) are periodic. Finally in going from the fourth to the \ufb01fth line we have\nmade use of the de\ufb01nition of the Fourier Transform (6.40). This \ufb01nal relation tells us that the\nconvolution can be computed by:\n1. Computing the Fourier Transform ofuj.\n2. Computing the Fourier Transform of\u03baij.\n3. Computing the product of the coe\ufb03cients of these two transforms.\n4. Computing the inverse Fourier Transform of the product.\n68", "mimetype": "text/plain", "start_char_idx": 1629, "end_char_idx": 2440, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "20fa629d-cbf9-4446-8dc1-6360b64098cb": {"__data__": {"id_": "20fa629d-cbf9-4446-8dc1-6360b64098cb", "embedding": null, "metadata": {"page_label": "69", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "fa380b50-86e2-4706-b4df-51e92f1485e3", "node_type": "4", "metadata": {"page_label": "69", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "88fa7f83a467c3564dcf8a29cd8b14922f1cb8b06da8d9be5e714c11006a1fe8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Next, we account for the fact that we will only work with the discrete forms of the functions\nuj and \u03baij. This means that we evaluate the inverse Fourier transform (6.39) at a \ufb01nite set of\ngrid points. Further, it means that we have to approximate the integral in the Fourier transform\n(6.40). This alternate form is given by\n\u02c6u[r,s] = h1h2\nL1L2\nN1\u2211\nm=1\nN2\u2211\nn=1\nu[m,n]e\u22122\u03c0i( rx1m\nL1\n+ sx2n\nL2\n). (6.42)\nHere h1 = L1\nN1\nand h2 = L2\nN2\n, x1m = (m\u22121)h1 and x2n = (n\u22121)h2.\nThe \ufb01nal observation is that the evaluating the sums in (6.39) and (6.42) requireO(N2)\noperations. This would make the evaluation of the convolution via the Fourier method impractical\nexcept for whenN is very small. However, the use of Fast Fourier Transform (FFT) reduces this\ncost toO(Nlog N). Thus the cost of implementing the convolution reduces toO(Nlog NH2).\nThis makes the implementation of Fourier Neural Operators practical.\n69", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 905, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c1df10ca-9475-48ae-96bc-a843073e20c2": {"__data__": {"id_": "c1df10ca-9475-48ae-96bc-a843073e20c2", "embedding": null, "metadata": {"page_label": "70", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a8e6f383-662c-432d-bee7-b76c229710bb", "node_type": "4", "metadata": {"page_label": "70", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "7908e72df36a69a49d72f4db5f59c5853e076aaefe331b4bc5b9fe787e1a58ea", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Chapter 7\nProbabilistic Deep Learning\nSo far, we have considered regression and classi\ufb01cation problems, where for a given inputxwe\nneed to compute asingle output y. However, this may not be enough for many problems of\ninterest. In fact, there may bemany y\u2019s for a givenx. For example,\n1. yand xmight be measured with somerandom noise.\n2. yand xmight beinherently stochastic. For instance,ycould be the pressure measured in\na turbulent \ufb02ow at some pointxin space.\n3. inverse problemscan have multiple solutions. For instance, the forward/direct problems\nwould be determining the temperature \ufb01eld given the head conductivity, while the inverse\nproblem could be determining the conductivity \ufb01eld given the (possibly noisy) temperature\n\ufb01eld.\nThus, we need to formulate a probabilistic framework to use deep learning algorithms to\nsolve such problems. Recall, that our deterministic model was given byy= F(x; \u03b8). In the\nprobabilistic setup,y, xand \u03b8are treated asrandom variables.\nBefore we can work with random variables we need to understand some key elements of the\ntheory of probability that are necessary in de\ufb01ning random variables.\n7.1 Key elements of Probability Theory\nA random experiment is described by a procedure and a set of one or more observa-\ntions/measurements. For example,\n1. Observe a switch and determine whether it ison or o\ufb00.\n2. Toss a coin 3 times and note the sequence of headsH or tailsT.\n3. Toss a coin 3 times and count the number of timesH appears. Note that this is the same\nexperiment as earlier but the measurement is di\ufb00erent.\n4. Spin a spinner, and measure the \ufb01nal angle in radians.\nThe outcome is the results of the experiment that cannot be broken down into smaller parts.\nThe sample space, denoted byS, is the set of all possible outcomes of an experiment. For each\nof the four random experiments observed above, we have\n1. S = {on, o\ufb00}.\n70", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1874, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "424ddae3-fcfe-47ea-b4fc-e85206c08df8": {"__data__": {"id_": "424ddae3-fcfe-47ea-b4fc-e85206c08df8", "embedding": null, "metadata": {"page_label": "71", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a20ad3c1-4e85-4a6b-8f98-0120e65fcbaa", "node_type": "4", "metadata": {"page_label": "71", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "d536dddc8b87bdfc2a92d97c057713f43e91b3023c6d063e5d9dae4b6c0a5502", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "2. S = {TTT, TTH, THT, HTT, THH, HTH, HHT, HHH }.\n3. S = {0, 1, 2, 3}.\n4. S = {\u03c8 : \u03c8\u2208(0,2\u03c0]}.\nNote that there is a fundamental di\ufb00erence between the \ufb01rst 3 experiments, whereS is discrete\nand countable, and the last experiment where theS is uncountable.\nAn eventis a collection of outcomes, i.e., a subset ofS. Typically the outcomes in an event\nsatisfy a condition. Let\u2019s see some examples for the above experiments\n1. A= {on}or A= {on, o\ufb00}= S .\n2. A are all outcomes with at least 2H, i.e.,A= {THH, HTH, HHT, HHH }.\n3. A are all outcomes with at least 2H, i.e.,A= {2, 3}. If we de\ufb01neB to be all outcomes\nwith 4H, then no outcome would satisfy this condition, i.e.,B = \u2205the null set.\n4. A are all outcomes with\u03c8 >\u03c0/4, i.e,A= {\u03c8 : \u03c8\u2208(\u03c0/4,2\u03c0]}.\nAn event classE is a collection of all event (sets) over which probabilities can be de\ufb01ned.\nWhen S is countable,E is all subsets ofS. When S is not countable,E is theBorel \ufb01eld (or\nBorel algebra), which is the collection of all open and closed sets inS.\nThe probability lawis a rule that assigns a probability to all sets in an event classE . We\nlist theaxioms of probability, which are the requirements of a probability law.\nConsider a sample spaceS for an experiment and the corresponding event classE . Let\nP : E \u21a6\u2192[0,1] satisfy\n1. P[A] \u22650 for allA\u2208E .\n2. P[S] = 1.\n3. If A1,A2,... are events such thatAi \u2229Aj = \u2205for alli \u0338= j, i.e., the events aremutually\nexclusive, then\nP[\n\u221e\u22c3\ni=1\nAi] =\n\u221e\u2211\ni=1\nP[Ai].\nAny assignmentP that satis\ufb01es the above conditions is said to be avalid probability law. Note\nthat probability is like mass. It is non-negative (axiom 1), conserved (total mass is always 1,\naxiom 2), and for distinct points the total mass is obtained by adding individual masses (axiom\n3).\nIf S is countable, then it is su\ufb03cient to de\ufb01ne a probability law for all elements ofS, i.e., for\nall elementary outcomes, while making sure that the probabilities are non-negative and add up\nto 1 (the \ufb01rst two axioms). Let us try to assign probability laws for the \ufb01rst three examples\nwhich have a countableS using these criteria.\n1. For somep\u2208[0,1], de\ufb01neP[on] = p; P[o\ufb00] = 1 \u2212p.\n2. For a fair die with no memory,P[ai] = 1/8, whereai \u2208S,i = 1,\u00b7\u00b7\u00b7 ,8.\n3. For a fair die with no memory,P[0] = 1/8, P[1] = 3/8, P[2] = 3/8, P[3] = 1/8.\nRemark 7.1.1.As an exercise, verify that the axioms are satis\ufb01ed for each of these cases.\n71", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2365, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "2bb98602-e045-4794-ac3b-121321276418": {"__data__": {"id_": "2bb98602-e045-4794-ac3b-121321276418", "embedding": null, "metadata": {"page_label": "72", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "826dc389-ac19-4651-a617-723e560d5274", "node_type": "4", "metadata": {"page_label": "72", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "b17094572a8983ea3616442f56d23f3c5589930ccebdede1a250c90c4b707524", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "For a continuous sample space, it is su\ufb03cient to de\ufb01ne a probability law for all open and\nclosed intervals, while ensuring axioms 1 and 2. Let us consider the fourth example which\nhas an uncountableS. If the spinner is completely unbiased, then the probability is uniformly\ndistributed. Then forb\u2265a, we say thatP[(a,b]] = (b\u2212a)/(2\u03c0). Note that the probability of\nsingleton sets in a continuous sample space is zero (for any distribution).\nRemark 7.1.2. From this point on, whenever we talk about the sample spaceS, we will im-\nplicitly assume that we are referring to the triplet(S,E ,P). This triplet is also known as a\n\"measure space\".\n7.2 Random Variables\nA random variableX is a function de\ufb01ned fromS to the real line with the property that the set\nAb = {\u03be\u2208S : X(\u03be) \u2264b}belongs toE for allb\u2208R. Note that in the measure theoretic language,\nwe are requiringX to be ameasurable function. Also note that according to the de\ufb01nition ofAb,\nwe are enforcing the requirement that we should be able to evaluateP[Ab] for allb\u2208R.\nLet us de\ufb01ne random variables (RVs) for the above examples:\n1. For S = {on,o\ufb00}with P[on] = p, P[o\ufb00] = 1 \u2212p, de\ufb01ne the RV\nX(\u03be) =\n{\n0 if \u03be= o\ufb00\n1 if \u03be= on. (7.1)\nThis is also known as aBernoulli Random Variable.\n2. For S = {TTT, TTH, THT, HTT, THH, HTH, HHT, HHH }with P[ai] = 1/8 for\nall ai \u2208S, de\ufb01ne\nX(\u03be) = Number of heads in\u03be. (7.2)\nNote that this is the random event that was described in Experiment 3.\n3. This random event is already a random variable.\n4. For the spinner experiment withS = {\u03c8 : \u03c8\u2208(0,2\u03c0]}with P[(a,b]] = (b\u2212a)/(2\u03c0), de\ufb01ne\nX(\u03c8) = \u03c8\n2\u03c0. (7.3)\nIf X is de\ufb01ned on a discrete sample space, it is called adiscrete random variable, while if it is\nde\ufb01ned on a continuous sample space, it is called acontinuous random variable.\nAsdescribedabove, arandomvariableinheritsitsprobabilisticinterpretationfromthemeasure\nspace used to de\ufb01ne it. In the following sections we de\ufb01ne the probabilistic interpretation of a\nrandom variable.\n7.2.1 Cumulative distribution function\nThe cumulative distribution function(cdf) of a random variableX is given by\nFX(x) = P[\u03be: X(\u03be) \u2264x]\nwhich de\ufb01nes a probability onR of X taking values in the interval(\u2212\u221e,x]. Let us de\ufb01ne the\ncdf for the above examples:\n1. For the Bernoulli RV de\ufb01ned by (7.1)\n72", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2255, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "43f4798f-f176-4d13-b629-e1c81f922ecf": {"__data__": {"id_": "43f4798f-f176-4d13-b629-e1c81f922ecf", "embedding": null, "metadata": {"page_label": "73", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "13946762-11ea-4dfa-8b54-aa0c1d1182d6", "node_type": "4", "metadata": {"page_label": "73", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "12ba8834b7ce1fb86c8f26661a7b91ae3c49dc655148095b15db3fc1579119df", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "\u2022 if x< 0, thenFX(x) = P[\u2205] = 0\n\u2022 if 0 \u2264x< 1, thenFX(x) = P[{o\ufb00}] = 1 \u2212p\n\u2022 if x\u22651, thenFX(x) = P[{on,o\ufb00}] = 1\nThe full cdf is shown in Figure 7.1(a).\n2. For the RV de\ufb01ned by (7.2)\n\u2022 if x< 0, thenFX(x) = P[\u2205] = 0\n\u2022 if 0 \u2264x< 1, thenFX(x) = P[#ofH = 0] = P[{TTT }] = 1/8\n\u2022 if 1 \u2264x< 2, thenFX(x) = P[#ofH = 0,1] = 1/8 + 3/8 = 4/8\n\u2022 if 3 \u2264x< 3, thenFX(x) = P[#ofH = 0,1,2] = 1/8 + 3/8 + 3/8 = 7/8\n\u2022 if x\u22653, theFX(x) = P[#ofH = 0,1,2,3] = 1\nThe full cdf is shown in Figure 7.1(b).\n3. This random variable is the same as Example 2.\n4. For the spinner experiment with the RV de\ufb01ned by (7.3)\nFX(x) = P[\u03c8: X(\u03c8) \u2264x] = P[{\u03c8: \u03c8\u22642\u03c0x}]\n\u2022 if x< 0, thenFX(x) = P[\u2205] = 0\n\u2022 if 0 \u2264x< 1, thenFX(x) = P[{\u03c8\u2208(0,2\u03c0x]}] = 2\u03c0x\n2\u03c0 = x\n\u2022 if x\u22651, thenFX(x) = P[{\u03c8\u22642\u03c0}] = 1\nThe full cdf is shown in Figure 7.1(c).\n(a) Bernoulli\n (b) 3 coin tosses\n (c) Spinner\nFigure 7.1: Examples of cumulative distribution functions\nLet us discuss some properties ofFX:\n1. 0 \u2264FX(x) \u22641.\n2. limx\u2192\u221eFX(x) = 1.\n3. limx\u2192\u2212\u221eFX(x) = 0.\n73", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 983, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d81d0f14-6eb4-49b0-9d8d-a3e5478ecfff": {"__data__": {"id_": "d81d0f14-6eb4-49b0-9d8d-a3e5478ecfff", "embedding": null, "metadata": {"page_label": "74", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "05d16aed-5cf9-47c5-a726-0078f2338d97", "node_type": "4", "metadata": {"page_label": "74", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "2b64c10caac797b2630772d24f64c2f16830b21f2554e770fda00e40fb3a530d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "4. FX is monotonically increasing.\n5. The cdf is always continuous from the right\nFX(x) = lim\nh\u21920+\nFx(x+ h).\nNote that theFX for discrete RV (see Figure 7.1) are discontinuous at \ufb01nitely manyx. In\nfact, the cdf for discrete RVs can be written as a \ufb01nite sum of the form\nFX(x) =\nK\u2211\nk=1\npkH(x\u2212xk),\nK\u2211\nk=1\npk = 1,\nwhere pk is the probability mass andH is the Heaviside function\nH(x) =\n{\n1 if x> 0\n0 if x\u22640 .\nRemark 7.2.1.Once we have theFX we can calculate the probability thatX will take values in\n\"any\" interval inR, i.e., we can computeP[a<X \u2264b]. Note that\nFX(b) = P[X \u2264b] = P[(X \u2264a) \u222a(a<X \u2264b)]\n= P[X \u2264a] + P[a<X \u2264b] (mutually exclusive events)\n= FX(a) + P[a<X \u2264b].\nThus,\nP[a<X \u2264b] = FX(b) \u2212FX(a).\n7.2.2 Probability density function\nWe de\ufb01ne theprobability density function(pdf). For a continuousFX, it is de\ufb01ned as\nfX(x) = d\ndxFX(x) (7.4)\nwhich enjoys the following properties inherited from the cdf:\n1. fX(x) \u22650, \u2200x\u2208R, sinceFX is monotonically increasing.\n2. limx\u2192\u2212\u221efX(x) = limx\u2192\u221efX(x) = 0.\n3. Integrating (7.4) from(\u2212\u221e,x] gives us\n\u222b x\n\u2212\u221e\nfX(y)dy= FX(x) \u2212 lim\nx\u2192\u2212\u221e\nFX(x) = FX(x).\n4. Also\nP[a<X \u2264b] = FX(b) \u2212FX(a) =\n\u222b b\n\u2212\u221e\nfX(y)dy\u2212\n\u222b a\n\u2212\u221e\nfX(y)dy=\n\u222b b\na\nfX(y)dy.\nThus, the integral of a pdf in an interval gives the \"probability mass\" which is the probability\nthat the RV lies in that interval. This is the reason why the pdf is called a \"density\".\n74", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1352, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "2371ed6e-074a-444b-97ba-bc07ced68a98": {"__data__": {"id_": "2371ed6e-074a-444b-97ba-bc07ced68a98", "embedding": null, "metadata": {"page_label": "75", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "64a04ac2-ba78-4ded-af50-1cc8ac85cae4", "node_type": "4", "metadata": {"page_label": "75", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "7078a1913610fff3aff735d36c9d691f1dc57929b454c324a655f741e4bf85af", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "5. Furthermore, \u222b \u221e\n\u2212\u221e\nfX(y)dy= lim\nx\u2192\u221e\nFX(x) \u2212 lim\nx\u2192\u2212\u221e\nFX(x) = 1.\n6. For a very smallh> 0, we have the interpretation\nP[a<X \u2264a+ h] =\n\u222b a+h\na\nfX(y)dy\u2248hfX(a).\nNote that ash\u21920+, P[a<X \u2264a+ h] \u21920. That is, for a continuous RV the probability\nof attaining a single value is zero.\n7.2.3 Examples of Important RVs\nLet us look at some important random variables and the associated cdf, pdf (also see Figure 7.2):\n1. Uniform RV:for some interval(a,b], the pdf is given by\nfX(x) =\n{\n1\nb\u2212a if x\u2208(a,b]\n0 other wise ,\nwhile the cdf is given by\nFX(x) =\n\uf8f1\n\uf8f4\uf8f2\n\uf8f4\uf8f3\n0 if x<a\nx\u2212a\nb\u2212a if x\u2208(a,b]\n1 if x>b\n.\n2. Exponential RV:used to model lifetime of devices/humans after a critical event. In\nthis case,X represents the time to failure andP[X >x] = e\u2212\u03bbx where \u03bb> 0 is a model\nparameter which denotes the rate of failure. Thus,\nFX(x) = P[X \u2264x] = 1 \u2212P[X >x] = 1 \u2212e\u2212\u03bbx,\nand\nfX(x) = d\ndxFX(x) = \u03bbe\u2212\u03bbx.\n3. Gaussian RV:used to model natural things like height, weight, etc. In fact, through the\nCentral Limit Theorem, this is also the distribution given by an aggregate of many RVs.\nThe pdf is given by\nfX(x) = 1\u221a\n2\u03c0\u03c3e\u22121\n2 (x\u2212\u00b5\n\u03c3 )\n2\nwhich is parameterized by themean \u00b5 which denotes the center of this distributions, and\nthe variance \u03c32 which denotes its spread. The corresponding cdf is given by\nFX(x) = 1\n2\n[\n1 + erf\n(x\u2212\u00b5\n\u03c3\n\u221a\n2\n)]\n, erf(x) = 2\u221a\u03c0\n\u222b x\n0\ne\u2212t2\ndt.\nIn probabilistic Machine Learning one makes extensive use of uniform and Gaussian random\nvariables.\n75", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1440, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "003c077d-1796-4738-a3c3-55f88ba0db54": {"__data__": {"id_": "003c077d-1796-4738-a3c3-55f88ba0db54", "embedding": null, "metadata": {"page_label": "76", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "00447671-771d-4dd4-a593-08d6c6461507", "node_type": "4", "metadata": {"page_label": "76", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "69274e21eef45c39ce5b558bd10bcf40bab13ec5aa2ff5340395c5ac35035dfe", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "(a) Uniform RV (a= \u22121,b = 1)\n (b) Exponential RV (\u03bb= 0.8)\n (c) Gaussian RV\nFigure 7.2: Continuous random variables\n7.2.4 Expectation and variance of RVs\nGiven a RVX with pdffX, we can calculate itsexpected valueor expectation or mean as\n\u00b5X := E[X] =\n\u222b \u221e\n\u2212\u221e\nxfX(x)dx.\nThe expectation has the following properties:\n\u2022 Note that if a pdf is symmetric aboutx = m, then E[X] = m. To see this, note that\n(m\u2212x)fX(x) will be anti-symmetric aboutm. Thus\n0 =\n\u222b \u221e\n\u2212\u221e\n(m\u2212x)fX(x)dx= m\n\u222b \u221e\n\u2212\u221e\nfX(x)dx\u2212\n\u222b \u221e\n\u2212\u221e\nxfX(x)dx =\u21d2\n\u222b \u221e\n\u2212\u221e\nxfX(x)dx= m.\nUsing this property, we can easily say the mean for a uniform RV is(a+ b)/2, while for a\nGaussian RV it is\u00b5.\n\u2022 E[c] = c for a constantc.\n\u2022 We can calculate the expected value of functions of RVs as\nE[g(X)] =\n\u222b \u221e\n\u2212\u221e\ng(x)fX(x)dx.\n\u2022 The expectation is linear, i.e.,\nE[g(X) + ch(X)] = E[g(X)] + cE[h(X)].\nThe varianceof a RV measures its variation about the mean. It is evaluated as\nVAR[X] =\n\u222b \u221e\n\u2212\u221e\n(x\u2212\u00b5X)2fX(x)dx.\nFurthermore, we denote thestandard deviationas\n\u03c3X := STD[X] =\n\u221a\nVAR[X].\n76", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1011, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c6c6f28c-af97-49a7-bb44-22a87a15232c": {"__data__": {"id_": "c6c6f28c-af97-49a7-bb44-22a87a15232c", "embedding": null, "metadata": {"page_label": "77", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "69e12c8d-b042-4093-b98a-68474f1fa878", "node_type": "4", "metadata": {"page_label": "77", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "ab6ebcd4d0539753aab29963616e05a6f4ddf9ee38ef3cc92d159067faaed350", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "For a uniform RV\nVAR[X] =\n\u222b b\na\n(\nx\u2212b+ a\n2\n)2 1\nb\u2212adx= (b\u2212a)2\n12 .\nFor a Gaussian RV, we \ufb01rst use the property of the pdf to write\n\u222b \u221e\n\u2212\u221e\ne\u22121\n2 (x\u2212\u00b5\n\u03c3 )\n2\ndx=\n\u221a\n2\u03c0\u03c3.\nTaking a derivative with respect to\u03c3 on both sides lead to\n\u222b \u221e\n\u2212\u221e\ne\u22121\n2 (x\u2212\u00b5\n\u03c3 )\n2\n(x\u2212\u00b5)2\u03c3\u22123dx=\n\u221a\n2\u03c0\nwhich after a bit of algebra gives us\nVAR[X] =\n\u222b \u221e\n\u2212\u221e\n(x\u2212\u00b5)2 1\u221a\n2\u03c0\u03c3e\u22121\n2 (x\u2212\u00b5\n\u03c3 )\n2\ndx= \u03c32.\n7.2.5 Pair of RVs\nIn probabilistic ML we deal with multiple random variables. For example, the input, the output\nand the weights might all be RVs. Thus we need to extend concepts from a single RV to a vector\nof RVs. We do this in this section by \ufb01rst considering a pair of RVs. Most of the concepts de\ufb01ned\nfor a pair of RVs carry forward to a vector of RVs.\nA pair of RVs is a mapping from the measure space, with event classE , of the form\nX: E \u2192R2,\nwhere the mapping can be discrete or continuous. We will sometimes use the notationX= (X,Y ).\nFor example, we can spin the spinner twice and measure\u03c81 \u2208(0,2\u03c0], \u03c82 \u2208(0,2\u03c0]. In this case,\nwe can de\ufb01ne the two RVs\nX(\u03c81) = \u03c81\n2\u03c0, Y (\u03c82) = \u03c82\n2\u03c0.\nEvents forX are sets inR2. To compute probability of events, we need to de\ufb01ne thejoint\ncdf FXY : R2 \u2192R, where\nFXY (x,y) = P[X \u2264x,Y \u2264y] = P[\u03be\u2208S : X(\u03be) \u2264x,Y (\u03be) \u2264y].\nAnalogous to single RVs\n\u2022 Joint cdfs are non-increasing functions ofx, y. In other words, forx\u2265x\u2032and y\u2265y\u2032\nFXY (x,y) \u2265FXY (x\u2032,y\u2032).\n\u2022 limx\u2192\u2212\u221eF(x,y) = 0, limy\u2192\u2212\u221eF(x,y) = 0, limx,y\u2192\u221eF(x,y) = 1.\n\u2022 We can calculate\nP[x1 <X \u2264x2,y1 <Y \u2264y2] = FXY (x2,y2) + FXY (x1,y1) \u2212FXY (x1,y2) \u2212FXY (x2,y1).\nFor X,Y jointly continuous, we can de\ufb01ne thejoint pdfas\nfXY (x,y) = \u22022FXY (x,y)\n\u2202x\u2202y\nwhich enjoys the following properties\n77", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1628, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "94a79259-faa5-4eba-a8f8-954f86b7f338": {"__data__": {"id_": "94a79259-faa5-4eba-a8f8-954f86b7f338", "embedding": null, "metadata": {"page_label": "78", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "da936525-a75c-4d6f-8a38-dda6c858d471", "node_type": "4", "metadata": {"page_label": "78", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "63020c2edbefe95635251fef3fb1f5fd32fce9cda11ab3a7d642db605d998014", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "\u2022 fXY (x,y) = 0 as x\u2192\u00b1\u221e or y\u2192\u00b1\u221e.\n\u2022 FXY (x,y) =\n\u222bx\n\u2212\u221e\n\u222by\n\u2212\u221efXY (r,s)drds.\n\u2022\n\u222b\u221e\n\u2212\u221e\n\u222b\u221e\n\u2212\u221efXY (r,s)drds= 1.\n\u2022 P[x1 <X \u2264x2,y1 <Y \u2264y2] =\n\u222bx2\nx1\n\u222by2\ny1\nfXY (x,y)dxdy.\n\u2022 P[X\u2208B] =\n\u222b\u222b\nBfXY (x,y)dxdy.\nLet us look at some important joint random variables :\n1. Joint uniform RV:for some region(a,b] \u00d7(c,d], the joint pdf is given by\nfXY (x,y) =\n{ 1\n(b\u2212a)(d\u2212c) if (x,y) \u2208(a,b] \u00d7(c,d]\n0 otherwise\n. (7.5)\n2. Joint Gaussian RV:the joint pdf is given by\nfXY (x,y) = 1\u221a\n(2\u03c0)2det(\u03a3)\nexp\n[\n\u22121\n2(x\u2212\u00b5)\u22a4\u03a3\u22121(x\u2212\u00b5)\n]\nwhere x= (x,y), \u00b5= (\u00b5x,\u00b5y) is the mean, and\u03a3 is called the covariance matrix\n\u03a3 =\n[ \u03c32\nx \u03c1\u03c3x\u03c3y\n\u03c1\u03c3x\u03c3y \u03c32\ny\n]\n.\nThe covariance matrix is symmetric and positive de\ufb01nite.\nWe can de\ufb01ne themarginal PDFof the RVX, which is the pdf ofX assuming Y attains\nall possible values\nfX(x) =\n\u222b \u221e\n\u2212\u221e\nfXY (x,y)dy.\nSimilarly, the marginal ofY is\nfY(y) =\n\u222b \u221e\n\u2212\u221e\nfXY (x,y)dx.\nThe RVsX and Y are said to beindependent if fXY (x,y) = fX(x)fY(y).\nQuestion 7.2.1.Show that the joint uniform RVs with joint pdf(7.5) are independent.\nConsider the functiong(X), which can be scalar-, vector-, or tensor-valued, then its expected\nvalue is given by\nE[g(X)] =\n\u222b \u221e\n\u2212\u221e\n\u222b \u221e\n\u2212\u221e\ng(x)fXY (x,y)dxdy\nas long as the integral is de\ufb01ned. For instance:\n\u2022 For g(X) = X, we haveE[g(X)] =\n\u222b\u221e\n\u2212\u221e\n\u222b\u221e\n\u2212\u221exfXY (x,y)dxdy.\n\u2022 For g(X) = X, we have a vector valued expectationE[g(X)] = [E[X],E[Y]].\n\u2022 For g(X) = X+ Y, we haveE[g(X)] = E[X] + E[Y].\n78", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1384, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "fa5c73c6-1a08-485c-8677-e20dc1e13cc3": {"__data__": {"id_": "fa5c73c6-1a08-485c-8677-e20dc1e13cc3", "embedding": null, "metadata": {"page_label": "79", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "88fd3487-c9a5-4186-8451-f12151bc2d5d", "node_type": "4", "metadata": {"page_label": "79", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "5bffd276bf0f473a2e1527705f9ff64f14850b23b41ea6a6d6b185ad16c0b4e5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The covarianceof X is given by\nCOV[X] = E[(X\u2212E[X]) \u2297(X\u2212E[X])]\nwhere\nCOV[X]11 = E[(X\u2212E[X])2] = VAR[X]\nCOV[X]22 = E[(Y \u2212E[Y])2] = VAR[Y]\nCOV[X]12 = COV[X]21 = E[(X\u2212E[X])(Y \u2212E[Y])].\nX and Y are said to beuncorrelated if COV[X]12 = 0. Furthermore, COV[X]12 = 0 for\nindependent RVs.Caution: COV[X]12 = 0 does not imply the RVs are independent!\nFinally, we are interested in looking at the pdf ofY when we knowX = \u02c6x. A good guess\nwould befXY (\u02c6x,y). However, this need not be a pdf as it need not integrate to unity overy.\nThis leads us to theconditional pdfof Y when we knowX = \u02c6x,\nfY|X(y|\u02c6x) = fXY (\u02c6x,y)\u222b\u221e\n\u2212\u221efXY (\u02c6x,y)dy = fXY (\u02c6x,y)\nfX(\u02c6x) .\nSimilarly, we can write the conditional pdf ofX given Y = \u02c6y as\nfX|Y(x|\u02c6y) = fXY (x,\u02c6y)\nfY(\u02c6y) .\nWe note that the extension of a regression problem to probabilistic framework leads us to\ndetermining the conditional distribution of the output given an instance of the input. This will\nbe discussed in Section 7.4.\n7.3 Unsupervised probabilistic deep learning algorithms\nWe begin with a vector of RVsX with NX components with a pdf given byfX. Let\u2019s assume\nthat we are given a dataset of samples{xi}sampled from the densityfX, which we denote by\nxi \u223cfX. For instance, these samples could correspond to RGB images of cars, with a resolution\nof 512 \u00d7512. Note that this would mean that the samples would lie in a space with dimension\nNX = 512 \u00d7512 \u00d73, which is quite large! We can treat each pixel of the images as a RV,\ntaking values given by the pixel intensities (across all 3 channels). Thus, these images can be\nseen as samples of aNX-dimensional RV with some unknown densityfX. Also, because of the\ninherent structure of the objects (i.e. the cars) in these images, the various components of the\nmultidimensional RV can be expected to be highly correlated, leading to a non-trivial form of\nfX. This correlation also implies that it might be possible to reduce the dimension ofX from\nNX to a smaller number and thus make the representation simpler.\nWe are interested in using the given \ufb01nite set of samples{xi}to learning the densityfX of\nthe data, and generating new samples from the learned distribution. Such methods are known as\ngenerative algorithms. Although a number of generative algorithms are available, we focus on a\nspeci\ufb01c type of deep learning algorithm known as Generative Adversarial Networks, or GANs for\nshort.\n7.3.1 GANs\nGANs were \ufb01rst proposed by Goodfellow et al. [6] in 2014. Since then, many variants of GANs\nhave been proposed which di\ufb00er based on the network architecture and the objective function\n79", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2568, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "16143dfc-e6f4-48ba-b8fc-07b9f23b4ae1": {"__data__": {"id_": "16143dfc-e6f4-48ba-b8fc-07b9f23b4ae1", "embedding": null, "metadata": {"page_label": "80", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "cc7f1185-7492-4cfe-9125-de2d18d7c034", "node_type": "4", "metadata": {"page_label": "80", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "fdecc283399fbf43d668f329045da3209dcf92606005acc33cb8c79dacef3142", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "used to train the GAN. We begin by describing the abstract problem setup followed by the\narchitecture and training procedure of a GAN.\nConsider the datasetS= {xi \u2208\u2126X \u2282RNX : 1 \u2264i\u2264Ntrain}. We assume the samples are\nrealizations of some RVX with densityfX, i.e.,xi \u223cfX. We want to train a GAN to learnfX\nand generate new samples from it.\nA GAN typically comprises two sub-networks, a generator and a discriminator (or critic).\nThe generator is a network of the form\ng(.; \u03b8g) : \u2126Z \u2192\u2126X, g: z\u21a6\u2192x (7.6)\nwhere \u03b8g are the trainable parameters andz\u2208\u2126Z \u2282RNZ is the realization of a RVZ following\na simple distribution, such as an uncorrelated multivariate Gaussian with density\nfZ(z) = 1\u221a\n(2\u03c0)2det(\u03a3)\nexp\n[\n\u22121\n2(z\u2212\u00b5)\u22a4\u03a3\u22121(z\u2212\u00b5)\n]\nwith \u00b5= 0, \u03a3 = I.\nTypically,NZ \u226aNX with Z known as thelatent variableof the GAN. The architecture of the\ngenerator will depend on the size/shape ofx. If x is a vector, theng can be an MLP with\ninput dimensionNZ and output dimensionNX. If xis an image, say of shapeH\u00d7W \u00d73, then\nthe generator architecture will have a few fully connected layers, followed by a reshape into a\ncoarse image with many channels, which is pushed through a number of transpose convolution\nchannels that gradually increase the spatial resolution and compress the number of channels to\n\ufb01nally scale up to the shapeH\u00d7W \u00d73. This is also known as adecoder architecture, similar\nto the upward branch of a U-Net (see Figure 5.9.) In either case, for a \ufb01xed\u03b8g, the generator\ngtransforms the RVZ to another RV,Xg = g(Z; \u03b8g) with densityfg\nX, which corresponds to\nthe latent densityfZ pushed-forward byg. We want to choose\u03b8g such thatfg\nX is close to the\nunknown target distributionfX.\nThe critic network is of the form\nd(.; \u03b8d) : \u2126X \u2192R (7.7)\nwith the trainable parameters\u03b8d. Once again, the critic architecture will depend on the shape of\nx. If xis a vector thend can be an MLP with input dimensionNX and output dimension1.\nIf xis an image, then the critic architecture will have a few convolution layers, followed be a\n\ufb02attening layer and a number of fully connected layers. This is similar to the CNN architecture\nshown in Figure 5.7 but with a scalar output and without an output function.\nFigure 7.3: Schematic of a GAN\nThe schematic of the GAN along with the inputs and outputs of the sub-networks is shown\nin Figure 7.3. The generator and critic play adversarial roles. The critic is trained to distinguish\n80", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2398, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "af72a553-f066-4d2a-bf94-60ca58d8be9d": {"__data__": {"id_": "af72a553-f066-4d2a-bf94-60ca58d8be9d", "embedding": null, "metadata": {"page_label": "81", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b9d45763-1d10-42ab-bfa2-6de099eeb82b", "node_type": "4", "metadata": {"page_label": "81", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "d50bc17b33bb39687a3cdda129eb595fee563ee14d57f750380e5cd56645c3fc", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "between true samples coming fromfX and fake samples generated bygwith the densityfg\nX. The\ngenerator on the other hand is trained to fool the critic by trying to generate realistic samples,\ni.e., samples similar to those sampled fromfX.\nWe de\ufb01ne the objective function describing aWasserstein GAN (WGAN)[2], which has\nbetter robustness and convergence properties compared to the original GAN. The objective\nfunction is given by\n\u03a0(\u03b8g,\u03b8d) = 1\nNtrain\nNtrain\u2211\ni=1\nd(xi; \u03b8d)\n\ued19 \ued18\ued17 \ued1a\ncritic value on real samples\n\u2212 1\nNtrain\nNtrain\u2211\ni=1\nd(g(zi; \u03b8g); \u03b8d)\n\ued19 \ued18\ued17 \ued1a\ncritic value on fake samples\n(7.8)\nwhere xi \u2208S are samples from the true target distributionfX, whilezi \u223cfZ are passed through\ngto generate the fake samples. To distinguish between true and fake samples, the critic attains\nlarge positive values when evaluated on real samples and large negative values on fake generated\nsamples. Thus, critic is trained to maximize objective function. In other words, we want to solve\nthe problem\n\u03b8\u2217\nd(\u03b8g) = arg max\n\u03b8d\n\u03a0(\u03b8g,\u03b8d) for any\u03b8g. (7.9)\nNote that the optimal parameters of the critic will depend on\u03b8g. Now to fool the critic, the\ngenerator gtries to minimize the objective function,\n\u03b8\u2217\ng = arg min\n\u03b8g\n\u03a0(\u03b8g,\u03b8\u2217\nd). (7.10)\nThus, training the WGAN corresponds to solving a minmax optimization problem. We note that\nthe critic and the generator are working in an adversarial manner. That is, while the former is\ntrying to maximize the objective function, the latter is trying to minimize it. Hence the name\ngenerative adversarial network.\nIn practice, we need to add a stabilizing term to the critic loss. So the critic is trained to\nmaximize\n\u03a0c(\u03b8g,\u03b8d) = \u03a0(\u03b8g,\u03b8d) \u2212\u03bb\n\u00afN\n\u00afN\u2211\ni=1\n(\ued79\ued79\ued79\ued79\n\u2202d\n\u2202\u02c6x(\u02c6xi; \u03b8d)\n\ued79\ued79\ued79\ued79\u22121\n)2\n(7.11)\nwhere \u02c6xi = \u03b1xi + (1 \u2212\u03b1)g(zi; \u03b8g) and \u03b1 is sampled from a uniform RV in(0,1). The additional\nterm in(7.11) is known as agradient penaltyterm and is used to constraining the (norm of)\ngradient of the criticdwith respect to its input to be close to 1, and thus be 1-Lipschitz function.\nFor further details on this term, we direct the interested readers to [7].\nThe iterative Algorithm 1 is used traingand dsimultaneously, which is also calledalternating\nsteepest descent, where\u03b7d and \u03b7g are the learning rates for the critic and the generator, respectively.\nNote that we takeK >1 optimization steps for the critic followed by a single optimization step\nfor the generator. This is because we want to solve the inner maximization problem \ufb01rst so that\nthe critic is able to distinguish between real and fake samples. Although taking a very largeK\nwould lead to a more accurate solve of the minmax problem, it would also make the training\nalgorithm computationally intractable for moderately sized networks. Thus,K is typically chosen\nbetween 4 to 6 in practice.\nThe minmax problem is a hard optimization problem to solve, and convergence is usually\nreached after training for many epochs. Alternatively, the critic optimization steps can be done\nover mini-batches of the training data, with many mini-batches taken per epoch, leading to a\nsimilar number of optimization steps for a relatively small number of epochs. As the iterations\ngo on,dbecomes better at detecting fake samples andgbecomes better at creating samples that\ncan fool the critic.\n81", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3251, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "36913ad0-643d-438e-8656-f4882cb3e7cb": {"__data__": {"id_": "36913ad0-643d-438e-8656-f4882cb3e7cb", "embedding": null, "metadata": {"page_label": "82", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b4fd648e-8a8d-485d-9c52-882d80a446f6", "node_type": "4", "metadata": {"page_label": "82", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "866031743aabdfe2d4396f5ccd49ab7be368193049b8b0fcae0e0e03f5320164", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Algorithm 1:Algorithm to train a GAN\nInput: \u03b80\nd,\u03b80\ng,K,N _epochs,\u03b7d,\u03b7g\nfor n= 1,...,N _epochsdo\n\u02c6\u03b8d \u2190\u03b8(n\u22121)\nd\nfor k= 1,...,K do\nMaximization update:\n\u02c6\u03b8d \u2190\u02c6\u03b8d + \u03b7d\n\u2202\u03a0c\n\u2202\u03b8d\n(\u03b8(n\u22121)\ng ,\u02c6\u03b8d)\nend\n\u03b8(n)\nd \u2190\u02c6\u03b8d\nMinimization update:\n\u03b8n\ng \u2190\u03b8(n\u22121)\ng \u2212\u03b7g\n\u2202\u03a0\n\u2202\u03b8g\n(\u03b8(n\u22121)\ng ,\u03b8(n)\nd )\nend\nUnder the assumption of in\ufb01nite capacity (N\u03b8g,N\u03b8d \u2192\u221e), in\ufb01nite data (Ntrain \u2192\u221e) and a\nperfect optimizer, we can prove that the generated distributionfg\nX converges weaklyto the\ntarget distributionfX [2]. This is equivalent to saying\nEZ[\u2113(g(Z; \u03b8\u2217\ng))] \u2212\u2192EX[\u2113(X)], (7.12)\nfor every continuous, bounded function\u2113 on \u2126X, i.e.,\u2113\u2208Cb(\u2126X). Once the GAN is trained, we\ncan use the optimizedgto generate new samples fromfg\nX \u2248fX by \ufb01rst samplingz\u223cfZ, and\nthen passing it through the generator to get the samplex= g(z; \u03b8\u2217\ng). Furthermore, due to the\nweak convergence described above, the statistics (mean, variance, etc) of the generated samples\nwill convergence to the true statistics associated withfX.\nRemark 7.3.1.We make a few important remarks here:\n1. Once the GAN is trained, we typically only retain the generator and don\u2019t need the critic.\nThe primary role of training the critic is to obtain a suitableg that can generate realistic\nsamples.\n2. The reason the term \"Wasserstein\" appears in the name WGAN is because one can show\nthat solving the minmax problem is equivalent to minimizing the Wasserstein-1 distance\nbetweenfg\nX and fX [2, 28]. The Wasserstein-1 distance is a popular metric used to measure\ndiscrepancies between two probability measures.\n3. Since the dimensionNZ of the latent variable is typically much smaller than the dimension\nNX of samples in\u2126X, the trained generator also provides a low dimensional representation\nof high-dimensional data, which can be very useful in several downstream tasks [21, 22].\n7.4 Supervised probabilistic deep learning algorithms\nRecall the deterministic problem where given the labelled/pairwise dataset\nS= {(xi,yi) : xi \u2208\u2126X \u2282RNX, y\u2208\u2126Y \u2282RNY }Ntrain\ni=1 (7.13)\n82", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1986, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "bc1c2bc5-8655-4af5-9e9d-e9019dfb7b60": {"__data__": {"id_": "bc1c2bc5-8655-4af5-9e9d-e9019dfb7b60", "embedding": null, "metadata": {"page_label": "83", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "00ad8487-36d0-4826-bcbd-6b03fb8dfebc", "node_type": "4", "metadata": {"page_label": "83", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "a14883ba0ae94856b73b641b7086ebdeeb9ed20a25d103a10811bf47983f17ac", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "we want to \ufb01ndyfor a newxnot appearing inS. We have seen in the previous chapters how\nneural networks can be used to solve such a regression (or classi\ufb01cation) problem.\nNow let us consider the probabilistic version of this problem. We assume thatxand yare\nmodelled using RVsX and Y, respectively. Further, let the paired samples in(7.13) be drawn\nfrom the unknown joint distributionfXY . Then given a realizationX= \u02c6x, we wish to useSto\ndetermine the conditional distributionfY|X(y|\u02c6x) and generate samples from it.\nThere are several popular approaches to solve this probabilistic problem, such as Bayesian\nneural networks, variational inference, dropouts or deep Boltzman machines. But we will focus\non an extension of GANs which also addresses these type of problems.\n7.4.1 Conditional GANs\nConditional GANs were \ufb01rst proposed in [17] to learn conditional distributions. We will discuss\na special variant of these models known as conditional Wasserstein GANS (cWGANs) which\nwere developed in [1], and used to solve a number of physics-based (inverse) problems in [25].\nFigure 7.4: Schematic of a conditional GAN\nThe schematic of a conditional GAN is depicted in Figure 7.4. The generator is a network of\nthe form\ng(.; \u03b8g) : \u2126Z \u00d7\u2126X \u2192\u2126Y, g: (z,x) \u21a6\u2192y (7.14)\nwhere z \u223cfZ is the latent variable. Note that unlike a GAN, the generator in a conditional\nGAN also takes as inputx. For a given value ofX= \u02c6x, samplingz\u223cfZ will generate many\nsamples ofyfrom some induced conditional distributionfg\nY|X(y|\u02c6x). The goal is to prescribe the\nparameters \u03b8g such thatfg\nY|X(y|\u02c6x) approximates the true conditionalfY|X(y|\u02c6x) for (almost)\nevery value of\u02c6x.\nThe critic is a network of the form\nd(.; \u03b8d) : \u2126X \u00d7\u2126Y \u2192R (7.15)\nwhich is trained to distinguish between paired samples(x,y) generated from the true joint\ndistribution fXY and the fake pairs(x,\u02c6y) where \u02c6yis generated byggiven (real)x.\n83", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1878, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "aa0c9a56-eff2-49dd-85e0-07a8a93069aa": {"__data__": {"id_": "aa0c9a56-eff2-49dd-85e0-07a8a93069aa", "embedding": null, "metadata": {"page_label": "84", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3d8f54e0-fa0b-4c5f-8438-a58023c3b962", "node_type": "4", "metadata": {"page_label": "84", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "c8a289a6123cfb79cfd3efe70a276c2dd737c405d97929700a9d9fc0f0745977", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The objective function for a cWGAN is given by\n\u03a0(\u03b8g,\u03b8d) = 1\nNtrain\nNtrain\u2211\ni=1\nd(xi,yi; \u03b8d)\n\ued19 \ued18\ued17 \ued1a\ncritic value on real pairs\n\u2212 1\nNtrain\nNtrain\u2211\ni=1\nd(xi,g(zi,xi; \u03b8g); \u03b8d)\n\ued19 \ued18\ued17 \ued1a\ncritic value on fake pairs\n. (7.16)\nAs earlier, the critic is trained to maximize the objective function (given by(7.9)) while the\ngenerator is trained to minimize it (given by(7.10)). Further, a stabilizating gradient penalty\nterm needs to be included when optimizing the critic (see [25]). The generator and critic are\ntrained using the alternating steepest descent algorithm described for GANs.\nUnder the assumption of in\ufb01nite capacity (N\u03b8g,N\u03b8d \u2192\u221e), in\ufb01nite data (Ntrain \u2192\u221e) and\na perfect optimizer, we can prove [1] that the generated conditional distributionfg\nY|X(y|\u02c6x)\nconverges in a weak sense to the target condition distributionfY|X(y|\u02c6x) (on average) for a given\nX= \u02c6x.\n84", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 862, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "144253ef-9821-4751-9f5f-0dde630deaba": {"__data__": {"id_": "144253ef-9821-4751-9f5f-0dde630deaba", "embedding": null, "metadata": {"page_label": "85", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7a756b8f-0693-4e36-973f-219388407f63", "node_type": "4", "metadata": {"page_label": "85", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "ae0a1ca0bbe54d0f766cd067df330a77f543a260caea4b570ce13d632a537c48", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Bibliography\n[1] J. Adler and O. \u00d6ktem, Deep bayesian inversion. https://arxiv.org/abs/1811.05910,\n2018.\n[2] M. Arjovsky, S. Chintala, and L. Bottou, Wasserstein generative adversarial net-\nworks, in Proceedings of the 34th International Conference on Machine Learning, D. Precup\nand Y. W. Teh, eds., vol. 70 of Proceedings of Machine Learning Research, International\nConvention Centre, Sydney, Australia, 06\u201311 Aug 2017, PMLR, pp. 214\u2013223.\n[3] R. Bischof and M. Kraus, Multi-objective loss balancing for physics-informed deep\nlearning. http://rgdoi.net/10.13140/RG.2.2.20057.24169, 2021.\n[4] R. T. Q. Chen, Y. Rubanova, J. Bettencourt, and D. Duvenaud, Neural ordinary\ndi\ufb00erential equations. https://arxiv.org/abs/1806.07366, 2018.\n[5] T. Chen and H. Chen, Universal approximation to nonlinear operators by neural net-\nworks with arbitrary activation functions and its application to dynamical systems, IEEE\nTransactions on Neural Networks, 6 (1995), pp. 911\u2013917.\n[6] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair,\nA. Courville, and Y. Bengio, Generative adversarial nets, in Advances in neural\ninformation processing systems, 2014, pp. 2672\u20132680.\n[7] I. Gulrajani, F. Ahmed, M. Arjovsky, V. Dumoulin, and A. C. Courville,\nImproved training of wasserstein gans, in Advances in neural information processing systems,\n2017, pp. 5767\u20135777.\n[8] K. He, X. Zhang, S. Ren, and J. Sun, Deep residual learning for image recognition,\nin 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016,\npp. 770\u2013778.\n[9] P. Kidger and T. Lyons, Universal Approximation with Deep Narrow Networks, in\nProceedings of Thirty Third Conference on Learning Theory, J. Abernethy and S. Agarwal,\neds., vol. 125 of Proceedings of Machine Learning Research, PMLR, 09\u201312 Jul 2020, pp. 2306\u2013\n2327.\n[10] D. P. Kingma and J. Ba, Adam: A method for stochastic optimization. https://arxiv.\norg/abs/1412.6980v9, 2017.\n[11] I. Lagaris, A. Likas, and D. Papageorgiou, Neural-network methods for boundary\nvalue problems with irregular boundaries, IEEE Transactions on Neural Networks, 11 (2000),\npp. 1041\u20131049.\n85", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2124, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5d55c68c-e41a-4adb-85d2-80b3cfecc8ac": {"__data__": {"id_": "5d55c68c-e41a-4adb-85d2-80b3cfecc8ac", "embedding": null, "metadata": {"page_label": "86", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "67691e55-6477-4441-9f2d-3624e6feaeda", "node_type": "4", "metadata": {"page_label": "86", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "3d060d74a37691f1e7585f48e328d132b021fabd8a0ad8981b0372cf012d09c3", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[12] S. Lanthaler, S. Mishra, and G. E. Karniadakis, Error estimates for DeepONets:\na deep learning framework in in\ufb01nite dimensions, Transactions of Mathematics and Its\nApplications, 6 (2022).\n[13] Z. Li, N. Kovachki, K. Azizzadenesheli, B. Liu, K. Bhattacharya, A. Stuart,\nand A. Anandkumar, Fourier neural operator for parametric partial di\ufb00erential equations.\nhttps://arxiv.org/abs/2010.08895, 2020.\n[14] L. Lu, P. Jin, G. Pang, Z. Zhang, and G. E. Karniadakis, Learning nonlinear\noperators via deeponet based on the universal approximation theorem of operators, Nature\nMachine Intelligence, 3 (2021), pp. 218\u2013229.\n[15] A. L. Maas, A. Y. Hannun, A. Y. Ng, et al., Recti\ufb01er nonlinearities improve neural\nnetwork acoustic models, in Proc. ICML, vol. 30, 2013.\n[16] L. McClenny and U. Braga-Neto, Self-adaptive physics-informed neural networks using\na soft attention mechanism. https://arxiv.org/abs/2009.04544, 2020.\n[17] M. Mirza and S. Osindero, Conditional generative adversarial nets. https://arxiv.\norg/abs/1411.1784, 2014.\n[18] S. Mishra and R. Molinaro, Estimates on the generalization error of physics-informed\nneural networks for approximating PDEs, IMA Journal of Numerical Analysis, (2022).\n[19] A. Nemirovski, A. Juditsky, G. Lan, and A. Shapiro, Robust stochastic approximation\napproach to stochastic programming, SIAM Journal on Optimization, 19 (2009), pp. 1574\u2013\n1609.\n[20] D. Patel, D. Ray, M. R. A. Abdelmalik, T. J. R. Hughes, and A. A. Oberai,\nVariationally mimetic operator networks. https://arxiv.org/abs/2209.12871, 2022.\n[21] D. V. Patel and A. A. Oberai, Gan-based priors for quantifying uncertainty in supervised\nlearning, SIAM/ASA Journal on Uncertainty Quanti\ufb01cation, 9 (2021), pp. 1314\u20131343.\n[22] D. V. Patel, D. Ray, and A. A. Oberai, Solution of physics-based bayesian inverse prob-\nlems with deep generative priors, Computer Methods in Applied Mechanics and Engineering,\n400 (2022), p. 115428.\n[23] A. Pinkus, Approximation theory of the mlp model in neural networks, Acta Numerica, 8\n(1999), pp. 143\u2013195.\n[24] M. Raissi, P. Perdikaris, and G. Karniadakis, Physics-informed neural networks: A\ndeep learning framework for solving forward and inverse problems involving nonlinear partial\ndi\ufb00erential equations, Journal of Computational Physics, 378 (2019), pp. 686\u2013707.\n[25] D. Ray, H. Ramaswamy, D. V. Patel, and A. A. Oberai, The e\ufb03cacy and gen-\neralizability of conditional gans for posterior inference in physics-based inverse problems.\nhttps://arxiv.org/abs/2202.07773, 2022.\n[26] O. Ronneberger, P. Fischer, and T. Brox, U-net: Convolutional networks for biomed-\nical image segmentation, in Medical Image Computing and Computer-Assisted Intervention \u2013\nMICCAI 2015, N. Navab, J. Hornegger, W. M. Wells, and A. F. Frangi, eds., Cham, 2015,\nSpringer International Publishing, pp. 234\u2013241.\n86", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2824, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c76a81d4-d386-4d3a-a588-9d076ea25bf7": {"__data__": {"id_": "c76a81d4-d386-4d3a-a588-9d076ea25bf7", "embedding": null, "metadata": {"page_label": "87", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9187776a-cf65-4167-813c-c2e4f2f04fcb", "node_type": "4", "metadata": {"page_label": "87", "file_name": "Deep Learning and Computational Physics Lecture Notes.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Deep Learning and Computational Physics Lecture Notes.pdf", "file_type": "application/pdf", "file_size": 7620995, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "09ae7f30b7161f27d665df7efa25233383eeee7e5c45ba35e5d7806988325fc9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[27] V. Sitzmann, J. N. P. Martel, A. W. Bergman, D. B. Lindell, and G. Wetzstein,\nImplicit neural representations with periodic activation functions. https://arxiv.org/abs/\n2006.09661, 2020.\n[28] C. Villani, Optimal Transport: Old and New, Grundlehren der mathematischen Wis-\nsenschaften, Springer Berlin Heidelberg, 2008.\n[29] S. Wang, Y. Teng, and P. Perdikaris, Understanding and mitigating gradient \ufb02ow\npathologies in physics-informed neural networks, SIAM Journal on Scienti\ufb01c Computing, 43\n(2021), pp. A3055\u2013A3081.\n[30] S. Wang, H. Wang, and P. Perdikaris, Learning the solution operator of parametric\npartial di\ufb00erential equations with physics-informed deeponets, Science Advances, 7 (2021).\n[31] L. Wu, C. Ma, and W. E,How sgd selects the global minima in over-parameterized learning:\nA dynamical stability perspective, in Advances in Neural Information Processing Systems,\nS. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, eds.,\nvol. 31, Curran Associates, Inc., 2018.\n[32] M. Yang, K. Yu, C. Zhang, Z. Li, and K. Yang, Denseaspp for semantic segmentation\nin street scenes, in 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition,\n2018, pp. 3684\u20133692.\n[33] D. Yarotsky and A. Zhevnerchuk, The phase diagram of approximation rates for deep\nneural networks. https://arxiv.org/abs/1906.09477, 2019.\n87", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1356, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "6ed1605a-a802-4fc5-9e1c-15e594141d40": {"__data__": {"id_": "6ed1605a-a802-4fc5-9e1c-15e594141d40", "embedding": null, "metadata": {"page_label": "1", "file_name": "Exploring Hierarchy-Aware Inverse Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Exploring Hierarchy-Aware Inverse Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 850491, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f7101bac-4f92-4921-9f37-b74d5f74b7b8", "node_type": "4", "metadata": {"page_label": "1", "file_name": "Exploring Hierarchy-Aware Inverse Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Exploring Hierarchy-Aware Inverse Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 850491, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "87edd03e708af1b582cc35bdde2e96e69e4fbbbd902d26e9c072526d93d084c0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b533a28a-3ab8-421e-ba9d-2f1998af6b59", "node_type": "1", "metadata": {}, "hash": "388ee628a37105214c47daaafa935bd5ea8c398630a478d9fdf0a3ab26e12a38", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Exploring Hierarchy-Aware Inverse Reinforcement Learning\nChris Cundy1 Daniel Filan1\nAbstract\nWe introduce a new generative model for human\nplanning under the Bayesian Inverse Reinforce-\nment Learning (BIRL) framework which takes\ninto account the fact that humans often plan using\nhierarchical strategies. We describe the Bayesian\nInverse Hierarchical RL (BIHRL) algorithm for\ninferring the values of hierarchical planners, and\nuse an illustrative toy model to show that BIHRL\nretains accuracy where standard BIRL fails. Fur-\nthermore, BIHRL is able to accurately predict\nthe goals of \u2018Wikispeedia\u2019 game players, with\ninclusion of hierarchical structure in the model\nresulting in a large boost in accuracy. We show\nthat BIHRL is able to signi\ufb01cantly outperform\nBIRL even when we only have a weak prior on\nthe hierarchical structure of the plans available to\nthe agent, and discuss the signi\ufb01cant challenges\nthat remain for scaling up this framework to more\nrealistic settings.2\n1. Introduction\nAs Reinforcement Learning (RL) algorithms have become\nmore and more capable, we are increasingly aware of the\nlimitations of how we specify their goals. While these goals\ncan be hand-crafted for simple environments, this approach\nrequires expert knowledge in the domain. If we are to\neventually use AI to perform tasks that are beyond human\nabilities (e.g. \u2018plan a city\u2019), we have to develop a more\nrobust method of goal speci\ufb01cation. Our algorithms would\nideally be able to learn what goals they should pursue by\ninferring human preferences: this is often known as value\nlearning, or preference elicitation.\nA leading approach to value learning from observed human\n1Department of Electrical Engineering and Computer Science,\nUniversity of California Berkeley, Berkeley, CA, 94720, USA.\nCorrespondence to: Chris J. Cundy <chris.j.cundy@gmail.com>.\nAccepted at the 1st Workshop on Goal Speci\ufb01cations for Reinforce-\nment Learning, FAIM 2018, Stockholm, Sweden, 2018. Copyright\n2018 by the author(s).\n2Our implementation of the algorithm can be found at\nhttps://github.com/C-J-Cundy\nactions is inverse optimal control (K\u00b4alm\u00b4an, 1960) or inverse\nreinforcement learning (IRL), formalised by Ng & Russell\n(2000) and Abbeel & Ng (2004). In IRL we treat human\nbehaviour as planning in a Markov decision process (MDP)\nand aim to \ufb01nd a reward function that explains observed\ntrajectories of human agents.\nWhile we may naively assume that human beings always act\nperfectly to achieve their goals (the \u2018principle of revealed\npreference\u2019 in economics (Samuelson, 1938)), human be-\nhaviour often violates this assumption. In general, people\nmake choices that they admit are suboptimal, due to a va-\nriety of biases including lack of willpower, inconsistent\ntime preferences, and lack of perfect foresight. Therefore,\na more accurate inference of \u2018true\u2019 preferences must take\ntypical human irrationality into account. Although initial\napproaches to IRL followed this implicit assumption of ratio-\nnality of the demonstrating expert, the more recent Bayesian\nIRL framework (Ramachandran & Amir, 2007) makes it\nstraightforward to include more realistic models of human\nbehaviour. Previous work in this area has modelled human\nactions as attempting to maximise their utility subject to con-\nstraints such as limited knowledge (Baker & Tenenbaum,\n2014) or inconsistent time preferences (Evans et al., 2016).\nHowever, to our knowledge no previous work has consid-\nered what we believe to be a key feature of human planning:\na tendency to structure our decision-making in a hierarchi-\ncal fashion. Instead of evaluating each individual action in\nterms of the rewards which we expect to obtain from all\nsubsequent actions, humans tend to simplify their planning\nby considering sub-problems and choosing between known\nmethods to solve these problems. For example, when navi-\ngating across a city we might choose between existing skills\nof walking, taking a taxi or public transport. We do not\nchoose between all the trajectories that we could physically\nperform.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4033, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b533a28a-3ab8-421e-ba9d-2f1998af6b59": {"__data__": {"id_": "b533a28a-3ab8-421e-ba9d-2f1998af6b59", "embedding": null, "metadata": {"page_label": "1", "file_name": "Exploring Hierarchy-Aware Inverse Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Exploring Hierarchy-Aware Inverse Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 850491, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f7101bac-4f92-4921-9f37-b74d5f74b7b8", "node_type": "4", "metadata": {"page_label": "1", "file_name": "Exploring Hierarchy-Aware Inverse Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Exploring Hierarchy-Aware Inverse Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 850491, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "87edd03e708af1b582cc35bdde2e96e69e4fbbbd902d26e9c072526d93d084c0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6ed1605a-a802-4fc5-9e1c-15e594141d40", "node_type": "1", "metadata": {"page_label": "1", "file_name": "Exploring Hierarchy-Aware Inverse Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Exploring Hierarchy-Aware Inverse Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 850491, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "d28a580624f92b630bb7b7555945ecb9aa238e18f928605df24ad25eb7e6126a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Previous work in this area has modelled human\nactions as attempting to maximise their utility subject to con-\nstraints such as limited knowledge (Baker & Tenenbaum,\n2014) or inconsistent time preferences (Evans et al., 2016).\nHowever, to our knowledge no previous work has consid-\nered what we believe to be a key feature of human planning:\na tendency to structure our decision-making in a hierarchi-\ncal fashion. Instead of evaluating each individual action in\nterms of the rewards which we expect to obtain from all\nsubsequent actions, humans tend to simplify their planning\nby considering sub-problems and choosing between known\nmethods to solve these problems. For example, when navi-\ngating across a city we might choose between existing skills\nof walking, taking a taxi or public transport. We do not\nchoose between all the trajectories that we could physically\nperform.\nIf we simply apply existing algorithms to observations of\nhumans who plan in this way, we will fail to infer correct\npreferences, running the risk of accidentally inferring patho-\nlogically wrong values in order to explain the hierarchically-\ngenerated plans.\nOur key contributions are as follows:\n\u2022 We introduce a generative model of human decisions\narXiv:1807.05037v1  [cs.AI]  13 Jul 2018", "mimetype": "text/plain", "start_char_idx": 3157, "end_char_idx": 4425, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d85c658f-63b2-455d-b5d7-cf55ab85312f": {"__data__": {"id_": "d85c658f-63b2-455d-b5d7-cf55ab85312f", "embedding": null, "metadata": {"page_label": "2", "file_name": "Exploring Hierarchy-Aware Inverse Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Exploring Hierarchy-Aware Inverse Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 850491, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2f03bb22-cfe0-4c92-8542-b26b0ff63df7", "node_type": "4", "metadata": {"page_label": "2", "file_name": "Exploring Hierarchy-Aware Inverse Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Exploring Hierarchy-Aware Inverse Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 850491, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "61f0738735c51f1a2ee6732a966b32da3f4a133436fd189f2bfc7e4b942a4398", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3563d0fa-8486-45d4-ab88-a5dec22571d9", "node_type": "1", "metadata": {}, "hash": "5763e1cc69f9edb1c2e70274503ffc8c3dcb9eadbe80abe2d84485ef0a11e716", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Exploring Hierarchy-Aware Inverse Reinforcement Learning\nas resulting from hierarchical planning, which uses\nboth primitive actions and extended options comprised\nof sequences of actions.\n\u2022 We discuss the theoretical justi\ufb01cation for considering\nsuch a model and introduce a simple algorithm for\ninference with hierarchically-generated trajectories.\n\u2022 Evaluating our model on trajectories of players of the\n\u2018Wikispeedia\u2019 game shows us that incorporating hi-\nerarchical structure gives us a sizeable boost in goal\nprediction accuracy compared to standard Bayesian\nIRL.\n\u2022 Finally, we discuss how our inference procedure can\nbe extended to jointly infer options and preferences,\nand show that our performance advantage over BIRL\nis retained even when we don\u2019t know what the precise\nhierarchical structure of the agent is.\n2. Our Model\nAn MDP is a tuple (S,A,T,R,\u03b3 ) consisting of a set of\nstates Sand actions A, a transition function T, reward func-\ntion R, and discount rate \u03b3, following the usual de\ufb01nition\nin e.g. Sutton & Barto (1998). In IRL we are given an\nMDP without R and aim to recover the reward from an\nobserved trajectory of an agent\u2019s actions and entered states\nat each timestep Ta = (s0,a0),(s1,a1),.... (We need to\ninclude the states as actions do not uniquely map to states\nin a stochastic MDP). We can typically extend the inference\nover multiple observed trajectories.\nWe describe the behaviour of an agent in an MDP by a\nstochastic policy \u03c0. We write the optimal policy as \u03c0\u2217,\nwith corresponding Q-function Q\u2217. Human planning is\ncommonly modelled as being Boltzmann-rational: that is,\nsatisfying \u03c0(s,a) \u221d exp(\u03b2Q\u2217(s,a)) for a \ufb01xed param-\neter \u03b2. Boltzmann-policies can also be self-consistent,\nso that the value-function is computed taking into ac-\ncount the Boltzmann-rational policy. This gives a policy\n\u03c0(s,a) \u221dexp(\u03b2Q\u2299(s,a)), where Q\u2299is the1 Q-value un-\nder this same Boltzmann-rational policy. The parameter \u03b2\ncan be increased or decreased to model more or less rational\nhumans, respectively.\nOne method for describing the behaviour of agents that plan\nhierarchically is the options framework, comprehensively\ndescribed by Sutton et al. (1999). An option o consists\nof a policy \u03c0o, an initiation set \u03c4 \u2286S, and a termination\nfunction \u03b1 : S\u2192 [0,1]. The initiation set \u03c4\u2286S gives the\nstates where the agent may activate the policy, thereafter\nfollowing the policy \u03c0o. At each state sthe policy enters,\n1In general there is no unique consistent Boltzmann-policy\n(Asadi & Littman, 2016). In practice we have not noticed any\nproblems arising from this non-uniqueness.\nthe termination function \u03b1(s) gives the probability that the\noption terminates, after which the agent no longer follows\n\u03c0o. These parameters de\ufb01ne an exit distribution Po(s,s\u2032)\ngiving the probability that the option o, if initiated in state s,\nwill terminate in state s\u2032, and a reward function ro(s) giving\nthe expected reward for activating option oin state s. For\na given state-action sequence Ta, we can further consider\nthe consistent-exit distribution Poc(s,s\u2032,Ta). This gives the\nprobability that taking the option oin state sresults in the\noption\u2019s policy giving the exact state-action trajectory inTa,\nterminating in state s\u2032. An action ain a state sin an MDP\ncan be described as a degenerate option where \u03c0o(a,s) = 1,\n\u03c4= {s}, and \u03b1(s1) = 1 if T(s1,s,a ) \u0338= 0. Our use of the\nterm \u2018option\u2019 includes these \u2018atomic\u2019 actions as a special\ncase.\nThus the key features of our model are as follows:\n\u2022 The human has an available set of options \u03c9, which\ninclude options with a policy that terminates after one\naction, i.e. the standard actions in the MDP.\n\u2022 The human chooses between options o \u2208\u03c9 with a\nstochastic policy, \u03c0(s,o) \u221d exp(\u03b2Q\u2299(s,o)) for a\n\ufb01xed parameter \u03b2.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3749, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3563d0fa-8486-45d4-ab88-a5dec22571d9": {"__data__": {"id_": "3563d0fa-8486-45d4-ab88-a5dec22571d9", "embedding": null, "metadata": {"page_label": "2", "file_name": "Exploring Hierarchy-Aware Inverse Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Exploring Hierarchy-Aware Inverse Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 850491, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2f03bb22-cfe0-4c92-8542-b26b0ff63df7", "node_type": "4", "metadata": {"page_label": "2", "file_name": "Exploring Hierarchy-Aware Inverse Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Exploring Hierarchy-Aware Inverse Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 850491, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "61f0738735c51f1a2ee6732a966b32da3f4a133436fd189f2bfc7e4b942a4398", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d85c658f-63b2-455d-b5d7-cf55ab85312f", "node_type": "1", "metadata": {"page_label": "2", "file_name": "Exploring Hierarchy-Aware Inverse Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Exploring Hierarchy-Aware Inverse Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 850491, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "a6d0e24a4761297680a68ac139fc5620833867679e54bd23bc3a6af5a0d8d3d0", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "An action ain a state sin an MDP\ncan be described as a degenerate option where \u03c0o(a,s) = 1,\n\u03c4= {s}, and \u03b1(s1) = 1 if T(s1,s,a ) \u0338= 0. Our use of the\nterm \u2018option\u2019 includes these \u2018atomic\u2019 actions as a special\ncase.\nThus the key features of our model are as follows:\n\u2022 The human has an available set of options \u03c9, which\ninclude options with a policy that terminates after one\naction, i.e. the standard actions in the MDP.\n\u2022 The human chooses between options o \u2208\u03c9 with a\nstochastic policy, \u03c0(s,o) \u221d exp(\u03b2Q\u2299(s,o)) for a\n\ufb01xed parameter \u03b2.\n\u2022 We do not observe the sequence of options that the\nagent executes: we only observe the sequence of states\nand actions Ta, that the agent executes, some of which\nmay have been executed as part of a compound option.\nWe denote the unobserved state-option trajectory by\nTo.\nA key feature of our model is the inclusion of Boltzmann-\nrational decisions over extended options as well as single\nactions. We believe that this feature is important for accu-\nrate modelling of human preferences, after considering the\ncommon everyday situations where the human has options\nthat are well-suited to solving problems, but are not optimal.\nThe human might take those options instead of explicitly\ncomputing the optimal policy because they have a limited\nability to optimally plan. For instance, if they wish to get\nacross the city, they might choose between a taxi and walk-\ning, as those skills have served them well in the past. They\nmight not even consider asking to borrow a friend\u2019s bicycle,\neven if this might be the fastest method, and certainly within\ntheir abilities. We wouldn\u2019t want our preference inference\nalgorithm to conclude that the human prefers sitting in taxis\nbecause they chose to do that over taking the optimal policy.\nFor an overview of the psychology and neuroscience litera-\nture on the importance of hierarchy in human planning and\nthe neural basis thereof, see Botvinick et al. (2009).", "mimetype": "text/plain", "start_char_idx": 3216, "end_char_idx": 5151, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "88d1cc86-e414-4d69-a36c-97779c8c1936": {"__data__": {"id_": "88d1cc86-e414-4d69-a36c-97779c8c1936", "embedding": null, "metadata": {"page_label": "3", "file_name": "Exploring Hierarchy-Aware Inverse Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Exploring Hierarchy-Aware Inverse Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 850491, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e483e7bb-9482-43cf-8a6e-9fb7ecf842b1", "node_type": "4", "metadata": {"page_label": "3", "file_name": "Exploring Hierarchy-Aware Inverse Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Exploring Hierarchy-Aware Inverse Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 850491, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "6eede1919fd13fedfddb75a3331191161a2de77aaaeffd23e62e2dde56f06230", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "40ef311c-1225-444e-af32-41caf868b7e5", "node_type": "1", "metadata": {}, "hash": "b334b5609f9e65f70ed9a2a82fd34aeaf312237e36a9583f3aa597570f2f64df", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Exploring Hierarchy-Aware Inverse Reinforcement Learning\n3. Related Work\n3.1. Boltzmann-rationality\nThe Boltzmann-rationality model of human behaviour is\none of the simplest variations on the naive assumption that\nhumans are completely rational, and has a long history in\nthe literature. While it violates certain assumptions of how\nagents should act, such as the principle of independence\nof irrelevant alternatives introduced by Debreu (1960), in\npractice the model has found widespread use in explaining\nhow people make bets (Rieskamp, 2008); in modelling the\nattention of people looking at adverts (Yang et al., 2015);\nand understanding the decisions taken in the brain itself\n(Glascher et al., 2010).\nPrevious work (Ortega & Braun, 2013) has shown how\na modi\ufb01ed Boltzmann-policy can arise from modelling\nbounded agents as they trade off gains in utility against\nexpending energy to transform their prior probability distri-\nbutions into posterior distributions (quanti\ufb01ed as a regulari-\nsation on the relative entropy between the two distributions).\nUnder this framework, a Boltzmann-policy is the optimal\npolicy for an agent which starts out indifferent to its actions,\nand can spend an amount of energy characterised by \u03b2on\ninvestigating which actions are likely to give it high reward.\nSeen through this lens, the Boltzmann-rational human agent\nhas a certain theoretical justi\ufb01cation, in addition to being\ncommonly used in practice.\n3.2. Incorporating human decision-making in IRL\nInitial work on inverse reinforcement learning (Ng & Rus-\nsell, 2000) did not discuss the procedure the human used\nto generate the policy and so implicitly assumed optimal-\nity of the human policy. Contemporary work in IRL tends\nto build on one of two frameworks: Maximum Entropy\nIRL, introduced by Ziebart et al. (2008); or Bayesian IRL\n(BIRL), introduced by Ramachandran & Amir (2007). For\nthe present work, we work within the Bayesian IRL frame-\nwork due to its conceptual simplicity and straightforward\ninversion of planning to inference. Recent work has also\nbuilt on BIRL to incorporate non-optimal human behaviour,\nsuch as inconsistent time preferences (Evans et al., 2016) or\nlimited knowledge (Baker & Tenenbaum, 2014).\nThe most closely related work is by Nakahashi et al. (2016),\nwho assume that humans attempt to ful\ufb01ll a set of goals,\nwhich may consist of subgoals. A Bayesian method is\nthen used to \ufb01nd which parts of the observed trajectory\ncorrespond to ful\ufb01lling each goal/subgoal. While this\ngoal/subgoal setting seems a reasonable assumption for\nmany of the trajectories, an arbitrarily parameterised reward\nfunction can more \ufb02exibly model a wider variety of tasks,\nrequiring less domain-speci\ufb01c knowledge. Secondly, their\nwork assumes an inherent hierarchical structure of tasks,\nRR1 G\nB B1Y\nFigure 1.The modi\ufb01ed taxi-driver situation considered here. Two\ntrajectories shown are drawn from an agent that has hierarchical\noptions go to R1 and go to B1. In both trajectories the pas-\nsenger starts at R, while the destination is B in the \ufb01rst and G in the\nsecond. Greyed-out cells represent destinations of the options in\nthe uniform prior over option-sets used in section 8.\nwhilst our approach assumes that human planners impose\nthis structure as a shortcut for ef\ufb01cient planning, possibly\nleading to hierarchically optimal but globally suboptimal\ntrajectories.\n4. Taxi-Driver Environment\nThe taxi driver environment was \ufb01rst introduced by Diet-\nterich (2000) as an example of a task that is particularly\namenable to hierarchical reinforcement learning (HRL)\nmethods. It is a useful running example to describe the\nmechanics of hierarchical planning.\nThe problem consists of a 5\u00d75 gridworld, depicted in \ufb01gure\n1, with four special landmarks, labelled R, G, B and Y. An\nagent (the \u2018taxi driver\u2019) moves in this world, starting at a\nrandom cell. Additionally, there is a passenger who ini-\ntially starts from one of the landmark cells, with a randomly\nchosen landmark as their destination.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3994, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "40ef311c-1225-444e-af32-41caf868b7e5": {"__data__": {"id_": "40ef311c-1225-444e-af32-41caf868b7e5", "embedding": null, "metadata": {"page_label": "3", "file_name": "Exploring Hierarchy-Aware Inverse Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Exploring Hierarchy-Aware Inverse Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 850491, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e483e7bb-9482-43cf-8a6e-9fb7ecf842b1", "node_type": "4", "metadata": {"page_label": "3", "file_name": "Exploring Hierarchy-Aware Inverse Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Exploring Hierarchy-Aware Inverse Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 850491, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "6eede1919fd13fedfddb75a3331191161a2de77aaaeffd23e62e2dde56f06230", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "88d1cc86-e414-4d69-a36c-97779c8c1936", "node_type": "1", "metadata": {"page_label": "3", "file_name": "Exploring Hierarchy-Aware Inverse Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Exploring Hierarchy-Aware Inverse Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 850491, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "ada85bdbb22f3fc4543f5a3292283015df5156104ddef34a8c5da3e8929ffad8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "4. Taxi-Driver Environment\nThe taxi driver environment was \ufb01rst introduced by Diet-\nterich (2000) as an example of a task that is particularly\namenable to hierarchical reinforcement learning (HRL)\nmethods. It is a useful running example to describe the\nmechanics of hierarchical planning.\nThe problem consists of a 5\u00d75 gridworld, depicted in \ufb01gure\n1, with four special landmarks, labelled R, G, B and Y. An\nagent (the \u2018taxi driver\u2019) moves in this world, starting at a\nrandom cell. Additionally, there is a passenger who ini-\ntially starts from one of the landmark cells, with a randomly\nchosen landmark as their destination. The driver has six dif-\nferent actions: as well as moving in the cardinal directions\nwith actions N, E, S, W, they can also attempt to Pickup or\nPutdown the passenger. The environment gives rewards of\n\u22121 on any movement action (attempts to move into walls or\noutside the grid fail with no additional penalty), \u221210 on un-\nsuccessful attempts to Pickup or Putdown, and +20 on\nsuccessfully putting the passenger down at their destination,\nat which point the episode terminates. The state consists of\nthe grid coordinate, the location of the passenger (either at\none of the four landmarks or in the taxi), and the desired\ndestination, giving 5 \u00d75 \u00d75 \u00d74 = 500 possible states.\nWhen presented in previous work, the taxi driver is usu-\nally equipped with hierarchical options, such as Go to\nx, where x is any of R, G, B, or Y and the environment is\nused to show how these allow the problem to be solved", "mimetype": "text/plain", "start_char_idx": 3370, "end_char_idx": 4890, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "978b2435-7c45-4a6d-881a-b1ccc8752393": {"__data__": {"id_": "978b2435-7c45-4a6d-881a-b1ccc8752393", "embedding": null, "metadata": {"page_label": "4", "file_name": "Exploring Hierarchy-Aware Inverse Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Exploring Hierarchy-Aware Inverse Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 850491, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a701a624-44d3-4760-b045-f85ac38bcdfb", "node_type": "4", "metadata": {"page_label": "4", "file_name": "Exploring Hierarchy-Aware Inverse Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Exploring Hierarchy-Aware Inverse Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 850491, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "b92d81b56c8ee70df89367e41a5f9a94270257711bd76cfe71bc3488ab9dbb5a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "53620188-954c-4559-a6bb-716c35af91ce", "node_type": "1", "metadata": {}, "hash": "12b54d56cd4332bd6b268fc1d573ff16ad5bf603ccaf75a7878db11d8f0868bf", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Exploring Hierarchy-Aware Inverse Reinforcement Learning\nfaster than without imposing this structure. Of course, it is\nsomewhat to be expected that an agent will do well if it is\nprovided with options that are exact sub-components of the\noptimal policy \u03c0\u2217. We wish to consider the more realistic\nsetting where the taxi driver has skills that are well-suited\nto the task at hand, but not optimal, i.e. they are not exact\nsub-components of the optimal policy \u03c0\u2217, although they are\ngenerally much more useful than random policies. Perhaps\nthe human knows how to get to their place of work which is\nlocated in a cell to the right of B, so \ufb01nds it easier to drive to\nB by \ufb01rst going to their place of work, then going west to B.\nSince our aim is to perform IRL in the environment, we\nconsider the variant of the taxi-driver case with a partially\nobserved reward function. We know that the reward is as\ndescribed above, except that up to \ufb01ve cells have reward 0\nto enter (instead of \u22121 in the standard formulation). We\ncan imagine this reward as modelling some areas with little\ntraf\ufb01c, or areas that the driver enjoys driving along to get to\nthe destination. This means that we are considering \u03b8which\nare drawn from a \ufb01nite set with approximately 6.7 million\npossible reward functions, parameterised by \ufb01ve coordinates\ngiving the locations of the free-to-enter cells.\n5. Bayesian Description\nGiven a human state-action trajectory Ta and a set of possi-\nble options \u03c9, we wish to compute the posterior distribution\nover a particular parameterisation of the reward function \u03b8.\nIn the taxi driver example, Ta corresponds to the sequence\nof observed actions N, E, W, etc; \u03c9is a set consisting of\nconcrete actions N, S . . . , along with some extended options\nsuch as Go to B1.\nIn principle, there is no reason why we cannot consider op-\ntions consisting of any stochastic policy, but in order to sim-\nplify the experiments we choose to consider either options\nwith deterministic policies, or options which are themselves\nBoltzmann-rational with parameter \u03b2o >\u03b2, where \u03b2is the\nrationality parameter for the agent\u2019s planning over top-level\noptions. This mirrors the everyday experience of having\na set of well-honed skills that we can count on to give us\nthe outcome we expect. We choose this model as we feel\nit combines being able to plan at different levels of abstrac-\ntion (modelled with the availability of multi-action options)\nwith the limited resources available to plan modelled by the\nBoltzmann-rationality (Ortega & Braun, 2013).\nOur inference problem is given by\nP(\u03b8|Ta,\u03b2,\u03c9 ) = P(Ta|\u03b2,\u03c9,\u03b8 )P(\u03b8)\nP(Ta|\u03b2,\u03c9) .\nEach observed state-action trajectory Ta could have been\nproduced by several state-option trajectories To,i, indexed\nby i. For example, in the taxi-driver case, we don\u2019t know if\nthe driver navigating to B1 is due to the driver executing a\nseries of atomic options (North, West, ...), or by execut-\ning the single compound option Go to B1. So we express\nP(Ta|\u03b2,\u03c9) in terms of the unobserved option-trajectories\nToi with P(Ta|\u03b2,\u03c9) = \u2211\ni P(Ta|To,i)P(To,i|\u03b2,\u03c9).2\nThen:\nP(\u03b8|Ta,\u03b2,\u03c9 ) =\n\u2211\ni P(Ta|To,i)P(To,i|\u03b2,\u03c9,\u03b8 )P(\u03b8)\u2211\ni P(Ta|To,i)P(To,i|\u03b2,\u03c9) .\nOnce we have a trajectory in terms of options, the likelihood\nof taking that trajectory is straightforward to compute given\nour model of the stochastic human policy:\nP(To,i|\u03b2,\u03c9,\u03b8 ) =\n\u220f\nk\nexp(\u03b2Q\u2299(sik,oik))\u2211\no\u2032\u2208\u03c9 exp(\u03b2Q\u2299(sik,o\u2032)),\nwhere oik denotes the option chosen in thekth step of theith\nstate-option trajectory, and sik denotes the corresponding\nstate.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3504, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "53620188-954c-4559-a6bb-716c35af91ce": {"__data__": {"id_": "53620188-954c-4559-a6bb-716c35af91ce", "embedding": null, "metadata": {"page_label": "4", "file_name": "Exploring Hierarchy-Aware Inverse Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Exploring Hierarchy-Aware Inverse Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 850491, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a701a624-44d3-4760-b045-f85ac38bcdfb", "node_type": "4", "metadata": {"page_label": "4", "file_name": "Exploring Hierarchy-Aware Inverse Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Exploring Hierarchy-Aware Inverse Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 850491, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "b92d81b56c8ee70df89367e41a5f9a94270257711bd76cfe71bc3488ab9dbb5a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "978b2435-7c45-4a6d-881a-b1ccc8752393", "node_type": "1", "metadata": {"page_label": "4", "file_name": "Exploring Hierarchy-Aware Inverse Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Exploring Hierarchy-Aware Inverse Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 850491, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "8a711302721f2bf61cd2cae93bdceb6d51737398a66849e88f3e5a84b54b46dd", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Once we have a trajectory in terms of options, the likelihood\nof taking that trajectory is straightforward to compute given\nour model of the stochastic human policy:\nP(To,i|\u03b2,\u03c9,\u03b8 ) =\n\u220f\nk\nexp(\u03b2Q\u2299(sik,oik))\u2211\no\u2032\u2208\u03c9 exp(\u03b2Q\u2299(sik,o\u2032)),\nwhere oik denotes the option chosen in thekth step of theith\nstate-option trajectory, and sik denotes the corresponding\nstate. To get the probability of the trajectory we multiply\nthe probability of taking the individual option (given by\nour Boltzmann-rational model) across all options in the\ntrajectory. The likelihood for multiple observed trajectories\nfollows straightforwardly.\nProcedure 1 gives a method to compute all of the option-\ntrajectories which are consistent with a given action-\ntrajectory. This requires knowing the consistent-exit dis-\ntribution Poc(si,si+k,Ta), as we need to know how likely\nactivating an option is to give us the observed trajectory.\nSince we have to enumerate each state-option trajectory\nTo which can produce the observed state-action trajectory\nTa, we should consider how many of these state-option\ntrajectories we may have. The Taxi-Driver case has a few\n\u2018landmark\u2019 states which can be reached directly (via options)\nby many other states, while most states can only be reached\nby atomic actions from neighbouring states. If there are m\nof these landmark states which can each be reached by n\nother states, there are nm possible option-trajectories con-\nsistent with the observed trajectory of actions. If we start\nintroducing many states which can be destinations of land-\nmarks, then the number of trajectories we have to consider\nincreases exponentially. Of course, in principle humans\ncan choose an arbitrary destination state for options, so in\ngeneral the complexity of evaluating the BIHRL algorithm\ngrows exponentially with the number of states in the prob-\nlem.\nWe could consider pruning the trees of option-trajectories by\nremoving any trajectories that have a very low probability as\nwe create the sets of possible option-trajectories. However,\nthis requires that we are very con\ufb01dent in our model of\nhuman behaviour in order to avoid removing trajectories\nthat we erroneously think are unlikely.\n2P(Ta|To,i) might be less than 1 if the option follows a stochas-\ntic policy, e.g. an option which itself has a Boltzmann-policy.", "mimetype": "text/plain", "start_char_idx": 3149, "end_char_idx": 5455, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e59a67ec-9086-4f9e-be0a-17939446ca48": {"__data__": {"id_": "e59a67ec-9086-4f9e-be0a-17939446ca48", "embedding": null, "metadata": {"page_label": "5", "file_name": "Exploring Hierarchy-Aware Inverse Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Exploring Hierarchy-Aware Inverse Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 850491, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6cfb97aa-3b95-434f-933e-3f6d7b96ef4d", "node_type": "4", "metadata": {"page_label": "5", "file_name": "Exploring Hierarchy-Aware Inverse Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Exploring Hierarchy-Aware Inverse Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 850491, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "bf95306b6d3d53baa14c58c0835e6fedc7a8e6f44f1bc963f2547489b3636806", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2ee96738-d045-4805-99e1-a6f65be8a03f", "node_type": "1", "metadata": {}, "hash": "c0325a3a48c9b501f697b9c4fd9d7b2585f2548f149602182a5582d6fea1a60a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Exploring Hierarchy-Aware Inverse Reinforcement Learning\nProcedure 1Computing the full set of option-trajectories\nthat are consistent with the observed state-action trajectory,\nand their corresponding probability.\nWe successively step through the states in the observed\ntrajectory. At each state we search to \ufb01nd all states that we\ncan reach by triggering options in the current state. We form\nthe list of all option-trajectories that can reach those states\nby concatenating the options that reach them with the list of\noptions-trajectories that reach the current state.\nWe sucessively update two sets: Toi is the set of possible\noption sequences that account for the \ufb01rst iactions, and Poi\nare the corresponding probabilities that each sequence of\noptions would produce the observed sequence of actions.\nIn: \u2022 A computed optimal value function V\u2299\nB under a\nset of options \u03c9with rationality parameter \u03b2\n\u2022 A function Poc(si,si+k,Ta,i:i+k) as de\ufb01ned in sec-\ntion 2\n\u2022 An observed state-action trajectory Ta of length n,\nwith sub-trajectories between the iand i+ kstates\nTa,i:i+k\nOut: A set of all trajectories of options that are consis-\ntent with the observed action-trajectory, along with the\ncorresponding probabilities that taking that trajectory of\noptions would result in the observed action-trajectory.\nfor i\u2208{1,...,n}do\nToi \u2190\u2205, Poi \u2190\u2205\nend for\nfor i\u2208{1,...,n}do\nfor k\u2208{1,...,n \u2212i}do\nfor Each o \u2208\u03c9 with Poc(si, si+k,Ta,i:i+k) \u0338= 0\ndo\nGenerate a set of option-paths by appending all\npaths in Toi with oand append these new option-\npaths to To(i+k).\nGenerate the corresponding probability by\nmultiplying the probabilities in Poi by\nPoc(si,si+k,Ta,i:i+k) and append these to\nPo(i+k).\nend for\nend for\nend for\nreturn Ton,Pon\n6. Taxi-Driver Experimental Results\nTo illustrate how we carry out inference in this framework,\nwe start by analysing our running example of the the taxi-\ndriver environment. We use a simple MCMC method based\non the Policy-Walk algorithm from Ramachandran &\nAmir (2007), which we describe in Appendix A. We use\nthe family of reward functions described in section 4, and\nplace a uniform prior over the number of cells that are free\nto enter, running our method over \ufb01ve trajectories drawn\nFigure 2.Bar chart showing the performance of the Bayesian IRL\nalgorithm, with and without knowledge of hierarchical plans, at\ndetermining the true \u03b8from ntrajectories. Error bars show one\nstandard error in the mean over different MCMC seeds.\nfrom a hierarchically-planning agent with a given true \u03b8.\nAs we can see from the results in \ufb01gure 2, our knowledge\nof the hierarchical structure of the agent\u2019s planning allows\nus to discern the true \u03b8much better than assuming that\nthe agent is merely a self-consistent Boltzmann planner.\nWe retain con\ufb01dence in the true \u03b8when seeing more and\nmore trajectories, whilst the IRL algorithm without options\nbecomes increasingly convinced that the true \u03b8is not the\ncorrect reward.\nWe can extend this simple example by analysing agents mov-\ning in much more complicated environments, or by attempt-\ning to infer the option-sets that the agents have available to\nthem. We perform both in the following two sections.\n7. Large-Scale Analysis: Wikispeedia\nWikispeedia is an online game where players are given two\nrandom articles from a subset of Wikipedia pages, and navi-\ngate from one page to the other by clicking on hyperlinks,\nattempting to \ufb01nd the shortest path from the \ufb01rst to the\nsecond. We apply our algorithm to a public dataset of thou-\nsands of Wikispeedia games, predicting the player\u2019s target\nWikipedia page from the links traversed so far. This bench-\nmark task has previously been studied by West & Leskovec\n(2012). They hand-crafted a set of features, leaning heavily\non the textual information in the pages to explain human\nplanning in the space.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3809, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "2ee96738-d045-4805-99e1-a6f65be8a03f": {"__data__": {"id_": "2ee96738-d045-4805-99e1-a6f65be8a03f", "embedding": null, "metadata": {"page_label": "5", "file_name": "Exploring Hierarchy-Aware Inverse Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Exploring Hierarchy-Aware Inverse Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 850491, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6cfb97aa-3b95-434f-933e-3f6d7b96ef4d", "node_type": "4", "metadata": {"page_label": "5", "file_name": "Exploring Hierarchy-Aware Inverse Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Exploring Hierarchy-Aware Inverse Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 850491, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "bf95306b6d3d53baa14c58c0835e6fedc7a8e6f44f1bc963f2547489b3636806", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e59a67ec-9086-4f9e-be0a-17939446ca48", "node_type": "1", "metadata": {"page_label": "5", "file_name": "Exploring Hierarchy-Aware Inverse Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Exploring Hierarchy-Aware Inverse Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 850491, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "35d364c51ee4dbc901308ec92edcfc481f860fcad0599b0335c56ee309083141", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We perform both in the following two sections.\n7. Large-Scale Analysis: Wikispeedia\nWikispeedia is an online game where players are given two\nrandom articles from a subset of Wikipedia pages, and navi-\ngate from one page to the other by clicking on hyperlinks,\nattempting to \ufb01nd the shortest path from the \ufb01rst to the\nsecond. We apply our algorithm to a public dataset of thou-\nsands of Wikispeedia games, predicting the player\u2019s target\nWikipedia page from the links traversed so far. This bench-\nmark task has previously been studied by West & Leskovec\n(2012). They hand-crafted a set of features, leaning heavily\non the textual information in the pages to explain human\nplanning in the space. We apply our self-consistent hierar-\nchical Boltzmann planner to this task, to evaluate whether\nit can achieve comparable performance without having to\nfeaturise the graph by hand.\nThis problem is conceptually similar to the taxi-driver prob-", "mimetype": "text/plain", "start_char_idx": 3115, "end_char_idx": 4052, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "42c7a770-8911-44c9-ba64-ff61dbffc0f5": {"__data__": {"id_": "42c7a770-8911-44c9-ba64-ff61dbffc0f5", "embedding": null, "metadata": {"page_label": "6", "file_name": "Exploring Hierarchy-Aware Inverse Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Exploring Hierarchy-Aware Inverse Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 850491, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "fc9ca98c-5219-4d03-84aa-68573f75be36", "node_type": "4", "metadata": {"page_label": "6", "file_name": "Exploring Hierarchy-Aware Inverse Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Exploring Hierarchy-Aware Inverse Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 850491, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "fbc7aaf14ca6e251f9f3a6e0c3c3eb7cc0c48bed4ee26b20742b760d05b9b049", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "578b9fb6-e009-4823-a130-8b367d3fb4f4", "node_type": "1", "metadata": {}, "hash": "4c389a9a627d4e0a75258aff176fb96d01583c52c0ecb1968a9a3d46fd57ca0d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Exploring Hierarchy-Aware Inverse Reinforcement Learning\nFigure 3.Showing the negative log marginal likelihood on the train\nset (lower is better) for various combinations of the rationality\nconstant \u03b2, and the number of hierarchical options m, with darker\nbars corresponding to more available options.The rationality of the\noptions, \u03b2o, was \ufb01xed at 3.0.\nlem, except that the available actions are state-dependent,\nconsisting of the hyperlinks that may be clicked on each\npage. In the actual game, the players are able to click the\n\u2018back\u2019 button on the browser, which injects an additional\naction to consider. If we were to include this action we\nwould violate the Markov property of an MDP (or compli-\ncate the analysis by squaring the size of the state space),\nso we only consider those trajectories which don\u2019t use the\nback button. In order to simplify our algorithm, we also\nignore \u2018dead-end\u2019 pages which don\u2019t link anywhere. Finally,\nwe removed paths longer than 20 steps long as they led to\ncomputation dif\ufb01culties and comprised less than 0.3% of\nthe dataset. We evenly split the paths in the dataset into a\ntraining and testing set.\nWe model the player as an agent with uniform rewards\nof \u22121 on all state transitions except to the winning page,\nwhich delivers reward +20. We postulate that humans may\nchoose long-time-scale strategies that attempt to navigate to\nspeci\ufb01c pages in particular. Hence, we equip our agent with\noptions that go to the mpages that appear most frequently\nin the training set, with a common Boltzmann-rationality\nparameter \u03b2o >\u03b2. As an example, the top \ufb01ve pages in the\ntraining set were United States, Europe, United\nKingdom, England, and Earth. With the choices made\nabove, our agents are parameterised by the numbers m, \u03b2,\nand \u03b2o. We kept \u03b2o \ufb01xed at 3.0 as initial exploration showed\nlittle variation for different values as long as they were\nsubstantially greater than \u03b2. The discount rate \u03b3was \ufb01xed\nat 0.9. In order to \ufb01nd the the collection of hyperparameters\n\u03b7= (\u03b2,m), that best characterises the data, we compute\nthe negative log marginal likelihood (NLML), given by\nNLML = \u2212log(P({(Ta,\u03b8)}|\u03b7))\n\u221d\u2212 log\n(\u220f\ni\nP(Ta,i|\u03b8i,\u03b7)\n)\nover all trajectories in the training set, and choose \u03b7such\nthat the NLML is minimised.\nTo compare our hierarchical planning model with West &\nLeskovec (2012), we consider trajectories u1,u2,...,un =\nu1:n consisting of nvisited articles u, and observe the \ufb01rst\nknodes. We then look at the likelihood of predicting the\ncorrect target node compared to predicting another node\nchosen uniformly at random from the nodes with the same\nshortest path length from uk. This is given by\nP(\u03b8|u1:k,\u03b7)\nP(\u03b8\u2032|u1:k,\u03b7) = P(u1:k|\u03b8,\u03b7)\nP(u1:k|\u03b8\u2032,\u03b7). (1)\nWe want to evaluate the ratio above for all of the data in\nthe test set. Since the overwhelmingly most costly part\nof computing P(u1:k|\u03b8,\u03b7) is running the value iteration\nuntil convergence for each possible goal \u03b8, we are able to\nspeed up evaluation by precomputing the value functions\nbeforehand.\n7.1. Results\nFigure 3 shows that including a set of hierarchical options\ndecreases the NLML by a factor of two. When our agents\nhave no hierarchical actions, changing \u03b2 has a negligible\neffect on the NLML. We also observe that the minimal\nNLML is obtained with a large set of around 150 available\nhierarchical options. It seems reasonable to us that a typical\nplayer may know one or two hundred topics well enough\nto navigate expertly to them (with \u03b20 = 3 .0), whilst the\nother randomly drawn topics are not known well at all (with\n\u03b2= 0.4).\nFigure 4 shows the predictive performance of our hierar-\nchical model. We note that including hierarchical policies\nprovides a substantial bene\ufb01t over the BIRL baseline, taking\nthe accuracy from an average of 62% to 66%.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3753, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "578b9fb6-e009-4823-a130-8b367d3fb4f4": {"__data__": {"id_": "578b9fb6-e009-4823-a130-8b367d3fb4f4", "embedding": null, "metadata": {"page_label": "6", "file_name": "Exploring Hierarchy-Aware Inverse Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Exploring Hierarchy-Aware Inverse Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 850491, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "fc9ca98c-5219-4d03-84aa-68573f75be36", "node_type": "4", "metadata": {"page_label": "6", "file_name": "Exploring Hierarchy-Aware Inverse Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Exploring Hierarchy-Aware Inverse Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 850491, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "fbc7aaf14ca6e251f9f3a6e0c3c3eb7cc0c48bed4ee26b20742b760d05b9b049", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "42c7a770-8911-44c9-ba64-ff61dbffc0f5", "node_type": "1", "metadata": {"page_label": "6", "file_name": "Exploring Hierarchy-Aware Inverse Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Exploring Hierarchy-Aware Inverse Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 850491, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "adfa65c47be86493592b0c2d04c716470de8732cd3b665386ed42561b0544912", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "7.1. Results\nFigure 3 shows that including a set of hierarchical options\ndecreases the NLML by a factor of two. When our agents\nhave no hierarchical actions, changing \u03b2 has a negligible\neffect on the NLML. We also observe that the minimal\nNLML is obtained with a large set of around 150 available\nhierarchical options. It seems reasonable to us that a typical\nplayer may know one or two hundred topics well enough\nto navigate expertly to them (with \u03b20 = 3 .0), whilst the\nother randomly drawn topics are not known well at all (with\n\u03b2= 0.4).\nFigure 4 shows the predictive performance of our hierar-\nchical model. We note that including hierarchical policies\nprovides a substantial bene\ufb01t over the BIRL baseline, taking\nthe accuracy from an average of 62% to 66%. The model\nwith hierarchical policies performs comparably to West &\nLeskovec (2012)\u2019s TF-IDF algorithm based on semantic\nsimilarity of topics, although we remain below the state-of-\nthe-art results obtained by their hand-crafted featurisation.\n8. Inferring Option-Sets\nIf we don\u2019t know the options available to the human, we\nmight want to infer what those are, and marginalise over\nthese, i.e. compute\nP(\u03b8|\u03b2,Ta) =\n\u222b\n\u2126\nP(\u03b8|Ta,\u03b2,\u03c9 )P(\u03c9)d\u03c9,\nintegrating over all sets of options \u03c9in the space of possible\nsets of options \u2126 . In general, there are a very large number\nof possible options. Even simply considering deterministic\noptions, there are |S||A|possible options, and the set of\nall possible sets of options is exponentially larger again:\n|\u2126 |= 2|S||A|\n.", "mimetype": "text/plain", "start_char_idx": 2992, "end_char_idx": 4508, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "76e50d05-b188-4184-af94-23eca4412709": {"__data__": {"id_": "76e50d05-b188-4184-af94-23eca4412709", "embedding": null, "metadata": {"page_label": "7", "file_name": "Exploring Hierarchy-Aware Inverse Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Exploring Hierarchy-Aware Inverse Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 850491, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8d680df-8f79-42be-a9e0-b6d1ac8d377e", "node_type": "4", "metadata": {"page_label": "7", "file_name": "Exploring Hierarchy-Aware Inverse Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Exploring Hierarchy-Aware Inverse Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 850491, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "d48b4cd438505595319935144bf02f1dfc0584586c4f7efdda12fb5b72720461", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Exploring Hierarchy-Aware Inverse Reinforcement Learning\nFigure 4.The accuracy on predicting \u03b8for a path of length ngiven the \ufb01rst knodes.\nGiven the large size of the latent space, marginalising over\nall option-sets to infer the posterior distribution over \u03b8\nquickly becomes computationally intractable. Future work\ncould try to tame this intractability by utilising recent ad-\nvances in Hamiltonian Monte-Carlo approaches and vari-\national inference. Here, we tackle the simpler case of the\ntaxi-driver with the naive MCMC approach to show that this\napproach can learn interesting results.\nWe equip the MCMC method with a prior over\u2126 which is\nuniform over all sets of up to three options, with each option\nconsisting of a deterministic policy that executes direction\nsteps in order to optimally navigate to a given destination\nwhich is chosen from a set of 16 cells which are close to the\nlandmarks and shown in \ufb01gure 1. Note that this excludes\nthe 9 cells in the middle of the grid which aren\u2019t close to\nany destinations. This captures the skills we would expect\na driver to use in the environment, with skills that go to\nthe areas of the grid that are near the landmarks where the\npassengers are picked up and put down. We keep our prior\nover \u03b8as before.\n8.1. Results\nThe results in \ufb01gure 5 show that even if we do not know the\noptions used to plan, but merely have a prior distribution\nover them, BIHRL predicts the ground truth reward \u03b80 with\nhigher probability than BIRL. BIRL predicts a probability of\nless than 0.03 and BIHRL a probability 0.55 at the ground-\ntruth \u03b2.\nThis experiment demonstrates that the BIHRL model is able\nto infer the preferences from the actions of hierarchical\nplanners, without necessarily knowing the options a priori.\nHowever, our naive MCMC method will not scale to sub-\nstantially larger latent state spaces, such as the space of 150\nlatent options that would be required to extend this to the\nWikispeedia dataset.\nFigure 5.Probabilities assigned to \u03b80, the ground truth reward,\nwhen conditioned on \ufb01ve trajectories from a hierarchical planner\nwith \u03b2= 0.8, marginalising over the option-sets described in the\ntext.\n9. Conclusion\nWe have extended inverse reinforcement learning to infer\npreferences from hierarchical planners which choose among\noptions with a self-consistent Boltzmann-policy. We show\nthat these agents capture many of the tradeoffs between the\nreward and the cost of gathering information that humans\nintuitively make.\nWe introduce an inference algorithm based on the\nPolicy-Walk algorithm developed by Ramachandran\n& Amir (2007) and show that it infers preferences of hi-\nerarchical planners much more accurately than standard\nBayesian IRL on an illustrative toy example based on the\ntaxi-driver environment from Dietterich (2000). Further,\nincluding a straightforward set of hierarchical plans signi\ufb01-\ncantly increases the accuracy of modelled human planning\nin the \u2018Wikispeedia\u2019 dataset introduced by West & Leskovec\n(2012), taking the accuracy from an average of 62% to 66%.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3032, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3979d82e-e9db-4cca-9437-9d23c8ef8ecb": {"__data__": {"id_": "3979d82e-e9db-4cca-9437-9d23c8ef8ecb", "embedding": null, "metadata": {"page_label": "8", "file_name": "Exploring Hierarchy-Aware Inverse Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Exploring Hierarchy-Aware Inverse Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 850491, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6d377f55-0a40-4441-bea7-d3d008dc73de", "node_type": "4", "metadata": {"page_label": "8", "file_name": "Exploring Hierarchy-Aware Inverse Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Exploring Hierarchy-Aware Inverse Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 850491, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "4c911c53f7fb06052a90e74dd0fd7fe567ba1195350735fe088d0ceb279348b7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3b7475b6-c6eb-49fd-ae2f-cdc66965437e", "node_type": "1", "metadata": {}, "hash": "eb296ed16933df63e285edf5fec2cbd8e4d6532a6eb066b54838ca1f789efd73", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Exploring Hierarchy-Aware Inverse Reinforcement Learning\nOur method obtains comparable accuracy to the baseline\nof West & Leskovec (2012), despite not relying on any\nhand-engineered features.\nWe discussed how we would deal with the case where we\ndo not know our planners\u2019 hierarchical options a priori, and\nare forced to infer agents\u2019 available options jointly along\nwith the reward. We introduce a toy MCMC approach that\nis able to infer the correct option-sets and reward for small\nenvironments. Given the correct \u03b2, BIHRL assigns 20 times\nmore probability mass to the ground-truth \u03b8than standard\nBIRL.\nHowever, at present signi\ufb01cant challenges remain for us-\ning BIHRL in practical environments, consisting of long\ntrajectories of agents with complex options. The large num-\nber of possible options that realistic planners could use\nmeans that any inference procedure must deal with very\nhigh-dimensional probability distributions, while the rel-\native complexity of actual human options means that it\nis computationally intractable to generate the exponential\nnumbers of plausible option-trajectories that are consistent\nwith the observed action-trajectory. It is possible that very\ngood models of human behaviour may be able to cut down\nthe exponential numbers of human choices, by assigning\nstrong priors over which human behvaiors and actions are\nlikely. Furthermore, modern Hamiltonian MC and varia-\ntional inference may be able to assist with the inference in\nhigh-dimensional spaces. If we can solve these daunting\nproblems, we may be able to use BIHRL to more accu-\nrately infer human preferences in a variety of complicated\nsituations.\nReferences\nAbbeel, Pieter and Ng, Andrew Y . Apprenticeship\nlearning via inverse reinforcement learning. Twenty-\n\ufb01rst international conference on Machine learning\n- ICML \u201904 , pp. 1\u20138, 2004. doi: 10.1145/\n1015330.1015430. URL http://portal.acm.\norg/citation.cfm?doid=1015330.1015430.\nAsadi, Kavosh and Littman, Michael L. A New Softmax\nOperator for Reinforcement Learning. arXiv, 2016. URL\nhttp://arxiv.org/abs/1612.05628.\nBaker, Chris L. and Tenenbaum, Joshua B.\nModeling Human Plan Recognition using\nBayesian Theory of Mind. Plan, Activity,\nand Intent Recognition , pp. 1\u201324, 2014. doi:\n10.1016/B978-0-12-398532-3.00007-5. URL https:\n//pdfs.semanticscholar.org/4cbb/\n1ea46c09d11b0b986a7baaac7215006504f8.\npdf.\nBotvinick, Matthew M, Niv, Yael, and Barto, Andrew C.\nHierarchically organized behavior and its neural founda-\ntions: a reinforcement learning perspective. Cognition,\n113(3):262\u2013280, 2009. URL https://www.ncbi.\nnlm.nih.gov/pmc/articles/PMC2783353/.\nDebreu, G \u00b4erard. Topological Methods in Cardinal Util-\nity Theory. Mathematical Methods in the Social Sci-\nences, Stanford University Press, 1960 , pp. 16\u201326,\n1960. URL https://econpapers.repec.org/\nRePEc:cwl:cwldpp:76.\nDietterich, Thomas G. Hierarchical Reinforcement\nLearning with the MAXQ Value Function Decompo-\nsition. Journal of Arti\ufb01cial Intelligence Research , 13:\n227\u2013303, 2000. ISSN 10769757. doi: 10.1613/\njair.639. URL https://www.jair.org/index.\nphp/jair/article/view/10266.\nEvans, Owain, Stuhlm \u00a8uller, Andreas, and Goodman,\nNoah D. Learning the Preferences of Ignorant, Incon-\nsistent Agents. Proceedings of the 30th Conference on\nArti\ufb01cial Intelligence (AAAI 2016), pp. 323\u2013329, 2016.\nURL http://arxiv.org/abs/1512.05832.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3347, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3b7475b6-c6eb-49fd-ae2f-cdc66965437e": {"__data__": {"id_": "3b7475b6-c6eb-49fd-ae2f-cdc66965437e", "embedding": null, "metadata": {"page_label": "8", "file_name": "Exploring Hierarchy-Aware Inverse Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Exploring Hierarchy-Aware Inverse Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 850491, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6d377f55-0a40-4441-bea7-d3d008dc73de", "node_type": "4", "metadata": {"page_label": "8", "file_name": "Exploring Hierarchy-Aware Inverse Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Exploring Hierarchy-Aware Inverse Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 850491, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "4c911c53f7fb06052a90e74dd0fd7fe567ba1195350735fe088d0ceb279348b7", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3979d82e-e9db-4cca-9437-9d23c8ef8ecb", "node_type": "1", "metadata": {"page_label": "8", "file_name": "Exploring Hierarchy-Aware Inverse Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Exploring Hierarchy-Aware Inverse Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 850491, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "ced46726258d652f6528cd676ea6be9ae5be2da24f2038e10c6d9951e38600f2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "URL https://econpapers.repec.org/\nRePEc:cwl:cwldpp:76.\nDietterich, Thomas G. Hierarchical Reinforcement\nLearning with the MAXQ Value Function Decompo-\nsition. Journal of Arti\ufb01cial Intelligence Research , 13:\n227\u2013303, 2000. ISSN 10769757. doi: 10.1613/\njair.639. URL https://www.jair.org/index.\nphp/jair/article/view/10266.\nEvans, Owain, Stuhlm \u00a8uller, Andreas, and Goodman,\nNoah D. Learning the Preferences of Ignorant, Incon-\nsistent Agents. Proceedings of the 30th Conference on\nArti\ufb01cial Intelligence (AAAI 2016), pp. 323\u2013329, 2016.\nURL http://arxiv.org/abs/1512.05832.\nGlascher, Jan, Daw, Nathaniel, Dayan, Peter, and O\u2019Doherty,\nJohn P. States versus Rewards: Dissociable neural\nprediction error signals underlying model-based and\nmodel-free reinforcement learning. Neuron, 66(4):585\u2013\n595, 2010. doi: 10.1016/j.neuron.2010.04.016.States.\nURL https://www.sciencedirect.com/\nscience/article/pii/S0896627310002874.\nK\u00b4alm\u00b4an, Rudolf E. Contributions to the theory of\noptimal control. Boletin de la Sociedad Matem-\natica Mexicana , 5:102\u2013119, 1960. URL https:\n//pdfs.semanticscholar.org/4602/\na97c4965a9f6c41c9a7eeaef5be8333dbaef.\npdf.\nNakahashi, Ryo, Baker, Chris L., and Tenenbaum, Joshua B.\nModeling Human Understanding of Complex Intentional\nAction with a Bayesian Nonparametric Subgoal Model.\nProceedings of the 30th Conference on Arti\ufb01cial In-\ntelligence (AAAI 2016) , pp. 3754\u20133760, 2016. URL\nhttp://arxiv.org/abs/1512.00964.\nNg, Andrew and Russell, Stuart. Algorithms for inverse\nreinforcement learning. Proceedings of the Seventeenth\nInternational Conference on Machine Learning, 0:663\u2013\n670, 2000. ISSN 00029645. doi: 10.2460/ajvr.67.2.323.\nURL http://www-cs.stanford.edu/people/\nang/papers/icml00-irl.pdf.\nOrtega, Pedro A. and Braun, Daniel A. Thermodynamics as\na theory of decision-making with information-processing\ncosts. Proceedings of the Royal Society A: Mathematical,\nPhysical and Engineering Sciences , 469, 2013. ISSN", "mimetype": "text/plain", "start_char_idx": 2775, "end_char_idx": 4710, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "99ef0f17-ec63-4e3f-b57b-2092c34a6cee": {"__data__": {"id_": "99ef0f17-ec63-4e3f-b57b-2092c34a6cee", "embedding": null, "metadata": {"page_label": "9", "file_name": "Exploring Hierarchy-Aware Inverse Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Exploring Hierarchy-Aware Inverse Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 850491, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4ca49f59-d30e-44d1-88ba-56e61c5fec91", "node_type": "4", "metadata": {"page_label": "9", "file_name": "Exploring Hierarchy-Aware Inverse Reinforcement Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Exploring Hierarchy-Aware Inverse Reinforcement Learning.pdf", "file_type": "application/pdf", "file_size": 850491, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "1cfdd2a956badcb013231738dd72596e1bbef23dbddad86b4973116f2c88e6d7", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Exploring Hierarchy-Aware Inverse Reinforcement Learning\n1364-5021. doi: 10.1098/rspa.2012.0683. URL\nhttp://rspa.royalsocietypublishing.\norg/content/469/2153/20120683.short.\nRamachandran, Deepak and Amir, Eyal. Bayesian inverse\nreinforcement learning. IJCAI International Joint Con-\nference on Arti\ufb01cial Intelligence, pp. 2586\u20132591, 2007.\nISSN 10450823. URL http://www.aaai.org/\nPapers/IJCAI/2007/IJCAI07-416.pdf.\nRieskamp, J \u00a8org. The probabilistic nature of pref-\nerential choice. Journal of Experimental Psychol-\nogy: Learning, Memory, and Cognition , 34(6):1446\u2013\n1465, 2008. ISSN 1939-1285. doi: 10.1037/\na0013646. URL http://doi.apa.org/getdoi.\ncfm?doi=10.1037/a0013646.\nSamuelson, Paul A. A Note on the Pure Theory of Con-\nsumer\u2019s Behaviour.Economica, 5(17):61\u201371, 1938. ISSN\n00130427, 14680335. doi: 10.2307/2548836. URL\nhttp://www.jstor.org/stable/2548836.\nSutton, Richard S and Barto, Andrew G. Reinforce-\nment learning: An introduction . MIT press Cam-\nbridge, 1998. URL http://incompleteideas.\nnet/book/the-book.html.\nSutton, Richard S, Precup, Doina, and Singh, Satinder.\nBetween MDPs and semi-MDPs: A framework for\ntemporal abstraction in reinforcement learning. Ar-\nti\ufb01cial intelligence , 112(1-2):181\u2013211, 1999. URL\nhttps://scholarworks.umass.edu/cgi/\nviewcontent.cgi?article=1212&context=\ncs_faculty_pubs.\nWest, Robert and Leskovec, Jure. Human way\ufb01nding in in-\nformation networks. Proceedings of the 21st international\nconference on World Wide Web, pp. 619\u2013628, 2012. doi:\n10.1145/2187836.2187920. URL http://learning.\nmpi-sws.org/networks-seminar/papers/\nwayfinding-www12.pdf.\nYang, Liu (Cathy), Toubia, Olivier, and De Jong, Martijn G.\nA Bounded Rationality Model of Information Search and\nChoice in Preference Measurement. Journal of Marketing\nResearch, 52(2):166\u2013183, 2015. ISSN 0022-2437. doi:\n10.1509/jmr.13.0288. URL http://journals.ama.\norg/doi/10.1509/jmr.13.0288.\nZiebart, Brian D., Maas, Andrew, Bagnell, J. An-\ndrew, and Dey, Anind K. Maximum Entropy\nInverse Reinforcement Learning. AAAI Confer-\nence on Arti\ufb01cial Intelligence , pp. 1433\u20131438, 2008.\nISSN 10450823. URL http://www.aaai.org/\nPapers/AAAI/2008/AAAI08-227.pdf.\nA. MCMC Sampling Procedure\nProcedure 2MCMC sampling in the latent space of \u0398 ,\u2126 .\nIn: \u2022 A set of possible reward functions \u0398\n\u2022 A set of possible options \u2126\n\u2022 A set of trajectories Ta,i\nOut: Samples from the posterior distribution P(\u03b8,\u03c9|Ta,\u03b2)\np\u2190 0.5\nV \u2190 0\n\u03b8\u2190 Random Draw(\u0398)\n\u03c9 \u2190 Random Draw(\u2126)\nSamples \u2190 Empty list\nrepeat\nPick \u03b81 and \u03c91 randomly amongst the neighbours of \u03b8,\n\u03c9\nV1 \u2190 Value Iteration(\u03b2,\u03c91,\u03b81), where the value it-\neration is initialised with V\nCompute p1 = P(Ta|\u03b2,\u03c91,\u03b81) \u00d7P(\u03c91|\u2126)\nWith probability min(1,p1/p):\np\u2190 p1\nV \u2190 V1\n\u03b8\u2190 \u03b81\n\u03c9 \u2190 \u03c91\nAppend (\u03b8,\u03c9) to Samples\nuntil Desired number of samples obtained\nreturn Samples", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2778, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1f6b1282-3467-4b3d-ab05-b18b90cd6d94": {"__data__": {"id_": "1f6b1282-3467-4b3d-ab05-b18b90cd6d94", "embedding": null, "metadata": {"page_label": "1", "file_name": "Foundations of GenIR.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Foundations of GenIR.pdf", "file_type": "application/pdf", "file_size": 699017, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b12c88d4-4ca3-409c-ab0f-a2311e307575", "node_type": "4", "metadata": {"page_label": "1", "file_name": "Foundations of GenIR.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Foundations of GenIR.pdf", "file_type": "application/pdf", "file_size": 699017, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "1d4bcc8bebab336365cb9800eda157ea9126333f7d39e1701182159b55271a4d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Foundations of GenIR\nQingyao Ai1\u2020, Jingtao Zhan1\u2020, Yiqun Liu1\n1Dept. of Computer Science and Technology, Tsinghua University,\nBeijing, China.\nContributing authors: aiqy@tsinghua.edu.cn;\nzhanjt20@mails.tsinghua.edu.cn; yiqunliu@tsinghua.edu.cn;\n\u2020These authors contributed equally to this work.\nAbstract\nThe chapter discusses the foundational impact of modern generative AI models\non information access (IA) systems. In contrast to traditional AI, the large-scale\ntraining and superior data modeling of generative AI models enable them to pro-\nduce high-quality, human-like responses, which brings brand new opportunities\nfor the development of IA paradigms. In this chapter, we identify and introduce\ntwo of them in details, i.e., information generation and information synthesis.\nInformation generation allows AI to create tailored content addressing user needs\ndirectly, enhancing user experience with immediate, relevant outputs. Information\nsynthesis leverages the ability of generative AI to integrate and reorganize exist-\ning information, providing grounded responses and mitigating issues like model\nhallucination, which is particularly valuable in scenarios requiring precision and\nexternal knowledge. This chapter delves into the foundational aspects of gener-\native models, including architecture, scaling, and training, and discusses their\napplications in multi-modal scenarios. Additionally, it examines the retrieval-\naugmented generation paradigm and other methods for corpus modeling and\nunderstanding, demonstrating how generative AI can enhance information access\nsystems. It also summarizes potential challenges and fruitful directions for future\nstudies.\nThe primary distinction between modern generative models and traditional AI tech-\nniques lies in their capability to generate complicated and high-quality output based\non human instructions. As shown by many studies [1\u20133], modern generative AI models\npossess remarkable abilities to generate responses that closely mimic human inter-\naction. General speaking, such impressive performance comes from their large-scale\n1\narXiv:2501.02842v1  [cs.IR]  6 Jan 2025", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2132, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "0b544a7c-a71e-4705-887f-420c244dc31c": {"__data__": {"id_": "0b544a7c-a71e-4705-887f-420c244dc31c", "embedding": null, "metadata": {"page_label": "2", "file_name": "Foundations of GenIR.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Foundations of GenIR.pdf", "file_type": "application/pdf", "file_size": 699017, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6449787e-09fe-4a04-b4d7-928c95cc64fd", "node_type": "4", "metadata": {"page_label": "2", "file_name": "Foundations of GenIR.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Foundations of GenIR.pdf", "file_type": "application/pdf", "file_size": 699017, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "f45085afffc6b1046fedab884d347340cc562e7f838692f2dc14571b900d8906", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "training collections and their advanced data modeling algorithms. Their superior data\nunderstanding ability can benefit almost every components of existing information\naccess systems, from document encoding and index construction, to query process-\ning and relevance analysis, etc. However, when talking about new opportunities or\nparadigms that are uniquely brought by the generative AI to information access, they\ncan be broadly categorized in two directions. The first one is to create content that\ndirectly addresses user\u2019s information needs. By understanding and taking user queries\nas input instructions, generative AI models are able to generate specific answers or\nproducts tailored to the individual\u2019s request. This direct approach to information gen-\neration can significantly enhance user experience by providing immediate and relevant\nresponses. The second direction is to leverage the advanced instruction-following capa-\nbilities of generative AI models to synthesize and recombine existing information in\ninnovative ways. Generative AI such as large language models (LLMs) can take exist-\ning data and transform it into new, coherent pieces of information that may not have\nbeen explicitly outlined before. This ability to reinterpret and organize information\nopens up new possibilities for retrieval system design and applications. Therefore, in\nthis chapter, we discuss how generative AI models could help information access from\ntwo perspectives, namely information generation and information synthesis.\n1 Information Generation\nInformation need is diverse and typically long-tail. Traditional information retrieval\nsystems, such as search engines and recommendation platforms, are designed to present\ninformation that already exists. However, these systems often fall short when it comes\nto fulfilling the less common information needs. This is particularly evident in scenarios\nrequiring creative creation, where users seek not just information but inspiration and\nnovel ideas. The limitations of traditional information systems in addressing these\nunique demands have paved the way for the emergence of generative models, which\nhold the promise of creating new information that aligns closely with the long-tail\ninformation needs.\nIn recent years, generative models have made significant developments. For\ninstance, ChatGPT can respond to user questions, Bing enhances its responses\nwith retrieval-augmented generation, and Midjourney generate images based on user\nprompts, and recommendation systems generate personal contents for different users.\nThe development is mainly driven by the capable model architectures, computational\nresources, and the large-scale internet data. These elements have facilitated the per-\nformance of generative models to new heights. With the continuous efforts on scaling\nup these elements, the model performance is still rapidly improving. Nowadays, gener-\native models have gradually been integrated into various workflows and everyday life\nactivities.\nIn this section, we present the foundation of generative models. This section is\norganized as follows. Section 1.1 shows the efforts on designing the model architectures\nfor large language models. Section 1.2 discusses how scaling facilitates the development\nof generative models and its potential future. Section 1.3 presents the different training\nstages of large language models. Finally, Section 1.4 introduces how large language\nmodels are used in multi-modal scenarios.\n2", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3485, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "590058da-4478-4de4-968b-718fd3e0e07f": {"__data__": {"id_": "590058da-4478-4de4-968b-718fd3e0e07f", "embedding": null, "metadata": {"page_label": "3", "file_name": "Foundations of GenIR.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Foundations of GenIR.pdf", "file_type": "application/pdf", "file_size": 699017, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ca4cf3c9-fdcb-4553-96d9-17729835c1d8", "node_type": "4", "metadata": {"page_label": "3", "file_name": "Foundations of GenIR.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Foundations of GenIR.pdf", "file_type": "application/pdf", "file_size": 699017, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "dba8616aaaa68deb1bd9d73ec087af0253defa4700bf309490e9d4c1ec78df39", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "1.1 Model Architecture\nIn different generation scenarios like ChatGPT or SoRA, Transformer [4] has emerged\nas the predominant model structure. It starts with an embedding layer, followed by\nmultiple neural layers. Within each layer, an attention mechanism models the interac-\ntions between words, creating contextualized embeddings. The final decision on word\ngeneration probabilities is derived by comparing the output embedding with the vocab-\nulary embeddings. We illustrate the model architecture in Figure 1. Unlike traditional\nRecurrent Neural Networks [5], Transformers are capable of modeling long-distance\ninteractions between words directly, which provides a more powerful representational\ncapability. Numerous enhancements to the Transformer architecture have been pro-\nposed. In the following, we will explore various modifications to each component of the\nTransformer, highlighting the advancements that have further improved its efficacy\nand efficiency.\nTransformer\n1234\nOutput\nTokensPositions Hidden States\nAttention\nFeed Forward\nTransformer Layer\nFig. 1 Transformer architecture: the overview on the left and the illustration of one layer on the\nright [4].\n1.1.1 Word Embedding\nWord embedding module is at the bottom of the Transformer architecture. Initially, a\ntokenizer breaks down a sentence into tokens, which the Word embedding module then\nmaps into embeddings. These are combined with position embeddings and fed into\nsubsequent neural layers. Recent research on large-scale language models has identified\nword embeddings as one of the main sources to training instability [6]. Particularly\nin the early stages of training, the gradients of word embeddings are often orders of\nmagnitude larger than those of other parameters. To address this issue, Le Scao et al.\n[7] introduced a layer normalization immediately after the word embedding layer,\nstabilizing the distribution effectively. Besides, Zeng et al. [6] opted to scale down the\ngradients of the word embeddings by an order of magnitude to prevent substantial\nupdates. Both approaches have been proven effective in stabilizing the training of\nlanguage models at the 100 billion parameter scale. Yet, whether they are still effective\nfor larger models remains to be investigated.\n3", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2260, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c5a905af-93ca-43db-98aa-d2babbc59a5f": {"__data__": {"id_": "c5a905af-93ca-43db-98aa-d2babbc59a5f", "embedding": null, "metadata": {"page_label": "4", "file_name": "Foundations of GenIR.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Foundations of GenIR.pdf", "file_type": "application/pdf", "file_size": 699017, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "525bdc5d-41de-4064-85e1-f65b2263582e", "node_type": "4", "metadata": {"page_label": "4", "file_name": "Foundations of GenIR.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Foundations of GenIR.pdf", "file_type": "application/pdf", "file_size": 699017, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "7e4553664fd728c8353ab09278d132126a2a97d7ce0626bcd0157ea84d9d5cde", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "1.1.2 Position Embedding\nPosition embedding is essential for Transformer. Unlike RNNs that inherently process\nsequences in order, vanilla attention mechanism disregards the positional distances\nbetween words and Transformer has to rely on position embeddings for position mod-\neling. Initially, Transformer [4] utilized Sinusoidal embeddings, a non-trainable form\nof position embedding that is added directly to word embeddings. Later, Devlin et al.\n[8] introduced trainable position embeddings, which is initialized randomly and are\nupdated through gradient descent during training. Subsequently, Raffel et al. [9] and\nPress et al. [10] proposed relative positioning, where the attention mechanism incorpo-\nrates biases based on the relative positions of words to better model varying distances.\nRecently, Su et al. [11] introduced the concept of rope position embedding, based on\nthe principle that the dot product of vectors correlates with their magnitudes and the\nangles between them. By rotating vectors in space proportionally to their positions,\nthis method naturally integrates positional information into attention scores. Black\net al. [12] has found that this approach outperforms trainable position embeddings.\nYet, these approaches may not work well when extrapolated to long sequences and\nmore effective methods need to be explored.\n1.1.3 Attention\nThe attention mechanism models interactions between words and is a significant com-\nponent of the Transformer architecture. Enhancements to the attention module have\npredominantly focused on two aspects: modeling long texts and optimizing the Key-\nValue (KV) cache. (1) Modeling Long Texts: The vanilla attention mechanism has\na complexity of O(n2), which significantly increases computational costs for long\ntexts. To address this, Sparse Transformer [13] employs sparse attention, utilizing pre-\ndesigned attention patterns to avoid the computation of attention over long sequences.\nAnother approach, Reformer [14], uses Locality-Sensitive Hashing (LSH) to reduce\ncomputational complexity. Additionally, Munkhdalai et al. [15] compressed context\ninformation to shorten sequences, thereby reducing overhead. Others have explored\nretrieval-based methods [16, 17]. This area of research continues to hold considerable\npotential for future advancements. (2) Optimizing KV Cache: classic Transformers\nuse multi-head attention (MHA), which requires storing extensive key-value caches\nduring inference, slowing down model generation. To mitigate this, Shazeer [18] pro-\nposed multi-query attention, which employs multiple key heads but only a single\nvalue head, substantially reducing the key-value cache and enhancing computational\nspeed. However, Ainslie et al. [19] found that this could degrade model performance,\nleading to the development of grouped query attention. This method allows multiple\nkey heads to share a single value head, effectively serving as a hybrid between MQA\nand MHA, balancing computational complexity and performance more effectively.\nRecently, DeepSeek-AI [20] introduced multi-head latent attention, which compresses\nkeys and values into a single latent space, thereby reducing the key-value cache while\nmaintaining robust representational capacity.\n4", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3239, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "18047a63-286a-4b95-87da-318c0594ff66": {"__data__": {"id_": "18047a63-286a-4b95-87da-318c0594ff66", "embedding": null, "metadata": {"page_label": "5", "file_name": "Foundations of GenIR.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Foundations of GenIR.pdf", "file_type": "application/pdf", "file_size": 699017, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a2f2ca14-c42c-4e98-8b29-c24c33642c77", "node_type": "4", "metadata": {"page_label": "5", "file_name": "Foundations of GenIR.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Foundations of GenIR.pdf", "file_type": "application/pdf", "file_size": 699017, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "77d9202cb7a335a3599942cf5b37dd04a0a28b6e78bfdb623e92f23a91bb1978", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "1.1.4 Layer normalization\nLayer normalization (LayerNorm) is important for stabilizing the distribution of\nhidden states, a key to train large language models. In the classical Transformer archi-\ntecture, LayerNorm is positioned between residual blocks, hence termed Post-LN.\nResearchers [21] observed that this configuration could lead to high gradients near\nthe output layers and very small gradients near the input layers, resulting in unstable\ngradients and challenging training dynamics. To address this issue, the Pre-LN configu-\nration was proposed [21], placing LayerNorm on the residual pathways before attention\nor feed-forward network (FFN) module. Experiments have shown that this adjustment\nleads to more uniform gradient distribution. Building upon Pre-LN, other researchers\nintroduced Sandwich-LN [22], which adds an additional LayerNorm at the output of\nthe residual pathways, further enhancing the training stability. Beyond merely adjust-\ning the position of LayerNorm, researchers have developed DeepNorm [23], which\ncombines a tailored parameter initialization strategy with modified residual connec-\ntions to stabilize training. This approach enables the training of Transformers with\ndepths reaching up to 1000 layers. Nevertheless, there still lacks a theoretical under-\nstanding about how layer normalization affects the training stability and more work\nneeds to be done for scaling the model even further.\n1.2 Scaling\nAcross different information generation scenarios, scaling has been a siginificant factor\nto the performance improvement. It is largely attributed to the discovery of scaling\nlaws [24]. Scaling laws describe how loss decreases in a log-linear manner as model size\nor training data volume increases. It can be formulated as follows:\nL(x) = L\u221e + k \u00b7 x\u2212\u03b1, (1)\nwhere L is the loss,x is model size or data size, andk and \u03b1 are coefficients. This scaling\nformula has become a crucial theoretical guide in the era of large models, suggesting\nthat performance can be enhanced at a log-linear rate simply by scaling up the model\nsize or training data. Based on these scaling laws, researchers also derived optimal\nmodel sizes given fixed computational resources [25]. Their findings indicate that as\ncomputational capacity expands, it is beneficial not only to increase the training step\nbut also the model size. This insight has further facilitated the pursuit of large models.\nThe correctness of scaling laws was first proposed in language modeling field and then\nvalidated in many other areas, including data mixture scaling laws [26], multimodal\nscaling laws [27], and scaling laws specific to information retrieval [28].\nDespite wide recognition of scaling laws, there remains disagreement among\nresearchers about whether scaling is the correct path to the future. This stems from\ntwo main concerns: the uncertain relationship between loss and practical metrics, and\nthe inference costs associated with large models.\n\u2022 Loss vs. Metric Improvement: The first arguing point is whether a linear reduction\nin loss can translate into super-linear improvements in actual metrics. If metrics\ncould improve super-linearly with linear increases in computational effort, scaling\n5", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3209, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c7d15724-c13a-4d04-b039-8fd07a79fb2a": {"__data__": {"id_": "c7d15724-c13a-4d04-b039-8fd07a79fb2a", "embedding": null, "metadata": {"page_label": "6", "file_name": "Foundations of GenIR.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Foundations of GenIR.pdf", "file_type": "application/pdf", "file_size": 699017, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "83b52b8f-4a88-4463-8c40-c6fe192e7996", "node_type": "4", "metadata": {"page_label": "6", "file_name": "Foundations of GenIR.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Foundations of GenIR.pdf", "file_type": "application/pdf", "file_size": 699017, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "96ed720f8e87036da95b5726646653dd74621fd3d0f128dfdeda2fc01f166355", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "up models would be highly advantageous. However, if the decrease in loss only\nresults in linear or sublinear metric improvements, the diminishing improvements\nmake scaling an inefficient option. The relationship between loss and metric per-\nformance remains an open question. Some researchers [29] believe that metrics can\nimprove super-linearly, which is termed emergent abilities. This is further supported\nby Du et al. [30], who observed a jump in metrics when loss reaches a certain thresh-\nold. Additionally, Power et al. [31] introduced the concept of \u201cgrokking\u201d to explain\nemergence, showing that models might suddenly exhibit strong generalization capa-\nbilities when provided with sufficient computational resources. Nevertheless, some\nresearchers [25] argued that such phenomena do not exist, showing that a well-\ntrained smaller model can outperform a larger, undertrained one. Schaeffer et al.\n[32] demonstrated that emergent abilities are artifacts of discrete metric functions\nand found that continuous metric functions do not exhibit such behaviors. McKen-\nzie et al. [33] even found that scaling results in worse metric scores. The existence of\nspecific emergent abilities remains unresolved and needs to be investigated in future\nwork.\n\u2022 Inference Cost Considerations: Early studies on scaling laws did not account for the\nhigher inference costs associated with larger models. Thus, the arguments that larger\nmodels are better [25] do not apply when the inference costs are considered. Instead,\nsmall models demonstrate potential to lower the inference costs. As shown by Fang\net al. [28], the optimal model sizes become significantly smaller when accounting\nfor inference costs. Besides, Mei et al. [34] show that smaller models can utilize\nmore sampling steps during inference and thus perform better. Consequently, many\nrecent studies focus on extensively training small models. For example, Llama [3]\nand MiniCPM [35] are trained with data and steps that far exceed the guidance\nsuggested by scaling laws. In the future, the models may be used on a phone to\nbuild up intelligent interaction with users. Thus, it is important to develop high-\nperforming small models.\n1.3 Training\nGenerative models in different scenarios are similar in training. For example, they\nusually use autoregressive training objectives, pretraining-sft-rlhf training stages, and\nprompt tuning procedure. In this section, we focus on the text generation scenario. We\nfirst discuss the training objectives and then show the three training stages. Finally,\nwe discuss how to design the prompts after the model is trained.\n1.3.1 Training Objectives\nFor generative language models, the training objective is usually next token prediction.\nHowever, this was not widely used when Transformers first appeared. Initially, masked\nlanguage modeling was the prevalent training objective during the BERT era [8]. It\nmasks 15% of the words in a text randomly, and the model is tasked with predicting\nthese masked words. This approach allows the model to utilize bidirectional attention,\nenhancing its representational capabilities. Even today, BERT models perform bet-\nter than autoregressive models on tasks requiring bidirectional attention. However, a\n6", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3238, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "51996d81-b287-48e2-8b83-3ba979fb3a17": {"__data__": {"id_": "51996d81-b287-48e2-8b83-3ba979fb3a17", "embedding": null, "metadata": {"page_label": "7", "file_name": "Foundations of GenIR.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Foundations of GenIR.pdf", "file_type": "application/pdf", "file_size": 699017, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b2f66f50-9bf8-461f-859d-e07db1006098", "node_type": "4", "metadata": {"page_label": "7", "file_name": "Foundations of GenIR.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Foundations of GenIR.pdf", "file_type": "application/pdf", "file_size": 699017, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "f4359b034fc9557a38f33e81d15de499418cc00c3a066d334e3402928c8f3e40", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "significant drawback of this method is the gap between its training setup and down-\nstream tasks, necessitating a fine-tuning phase for adaptation to various applications.\nThus, its zero-shot generalization capabilities are very limited.\nNext token prediction was developed to address the inability of masked language\nmodeling to generalize zero-shot to downstream tasks. The authors of GPT-2 [36]\nproposed that all natural language processing tasks could be reformulated as next\ntoken prediction tasks. By training models on this task, models could be directly\napplied to any downstream task without the need for specific fine-tuning. In fact,\nresearch nowadays demonstrates the effectiveness of this idea. Mathematically, next\ntoken prediction can be represented with the following formula:\nP (xt+1 | x1, . . . , xt) , (2)\nwhich is to predict the probability of the next tokenxt+1 given the sequence of previous\ntokens.\n1.3.2 Training Stages\nThe training process of language models typically unfolds in three stages: pre-training,\nsupervised fine-tuning (SFT), and reinforcement learning from human feedback\n(RLHF). Each phase presents unique challenges and methodologies.\nPre-training is the most resource-intensive stage. It is training a randomly ini-\ntialized model on a large dataset to develop a robust linguistic capability. Several\nchallenges arise during this stage: (1) Large models are especially difficult to train\nfrom random initialization. During training, there are often spikes in training loss or\ndifficulty in converging [6, 23, 37]. We discussed various architectural improvements\nin Section 1.1 to address these instabilities, yet a definitive solution remains an open\nissue. (2) The computational demand is substantial. Pre-training requires stable and\nefficient use of computational resources [1]. It often involves parallel processing across\nmultiple machines, which can lead to low utilization rates of computing resources [38].\nZeng et al. [6] reported numerous hardware failures during pre-training. (3) The qual-\nity of pre-training data is crucial [39]. Given the vast amount of data needed, efficiently\nfiltering out low-quality data is essential. The filtering methods usually employ neural\nscoring models and based on the credibility of the site [40, 41].\nSupervised Fine-Tuning (SFT) is to train the model on instruction-response\npairs [42]. The model can thus learns to follow instructions or engage in dialogue [3]. To\nenhance dataset diversity, researchers often leverage different types of NLP tasks. The\nquality of the dataset is significant and requires a skilled annotation team. Besides, it\nis also important to label safety-related data, which helps instruct the models to learn\nto reject inappropriate requests [3].\nReinforcement Learning from Human Feedback (RLHF) focuses on aligning the\nmodel with human preferences based on human feedback [43, 44]. The process starts\nby sampling real human prompts to which the model generates multiple responses.\nThese responses are then compared by users or third-party annotators. A reward model\nis trained based on these human preferences. Subsequently, reinforcement learning\n7", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3165, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "2d2ce17f-39f4-4596-ae29-7085bbd3b4af": {"__data__": {"id_": "2d2ce17f-39f4-4596-ae29-7085bbd3b4af", "embedding": null, "metadata": {"page_label": "8", "file_name": "Foundations of GenIR.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Foundations of GenIR.pdf", "file_type": "application/pdf", "file_size": 699017, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "72723e0a-6f18-4d27-99b7-de3cc6ae58af", "node_type": "4", "metadata": {"page_label": "8", "file_name": "Foundations of GenIR.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Foundations of GenIR.pdf", "file_type": "application/pdf", "file_size": 699017, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "6bba17ab5bec9cfe45680b1e236f066811f443f85239f83a2d8c00d7d60c8222", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "techniques utilize the reward model to guide the model updates. This approach sig-\nnificantly enhances the quality of model outputs, especially in creative writing tasks.\nHowever, a major challenge is the generalizability of the reward model; as the model\nevolves, the reward model may no longer accurately assess the quality of outputs.\nContinuous iterations of this process are necessary to mitigate this issue [3]. Recently,\nthere are also some offline reinforcement learning algorithms that do not necessaite\ntraining a reward model, such as DPO [45]. Yet studies [46] show that such offline\nlearning methods still underperform the online learning methods.\n1.3.3 Prompt Optimization\nGenerative models are highly sensitive to the input prompts; an effective prompt can\nsignificantly enhance the quality of the model\u2019s output [47]. Therefore, optimizing\nprompts for a generative model is a crucial area of research. Here are three main\ndirections:\n\u2022 Designing Prompt Templates: Researchers often design prompts that mimic\nhuman thought processes to guide the model effectively. This includes using\nstructured thought patterns like chain-of-thought [48], tree-of-thought [49], and self-\nconsistency [50], which help the model organize and process information in a logical\nmanner.\n\u2022 Iterative Optimization of Prompt Templates: like reinforcement learning, this\nmethod continuously iterate and refine the prompt templates based on the gen-\neration feedback. Given that prompt templates are typically discrete, researchers\nusually employ large language models to conduct prompt updates [51, 52].\n\u2022 Training Prompt Rewriting Models Using User Interaction Logs: This approach\nharnesses the rich feedback contained within user interaction logs to tap into user\ninsights. By analyzing how users interact with the model, researchers can train an\nautomated model to rewrite prompts more effectively. This method leverages real-\nworld data to better align the prompts with user intentions and improve the model\u2019s\nresponses [53, 54].\n1.4 Multi-modal Applications\nThe rapid advancement of language models has significantly helped progress in the\nmultimodal domain. Language models facilitate the understanding of multimodal\ndata and developments in multimodal generation. We will discuss these two aspects\nseparately.\n1.4.1 Multi-modal Understanding\nMultimodal Understanding involves models processing inputs from multiple modali-\nties to produce relevant textual responses. For example, GPT-4o can process textual,\nvisual, and auditory input. The challenges in this area include designing model struc-\ntures that can handle multimodal inputs and crafting appropriate training objectives.\nHere, we focus on how visual signals are integrated into large language models:\nIn terms of aligning multimodal inputs, there are mainly three approaches:\n8", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2834, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "44b285b6-89c7-41b3-9ecf-ffb64dad2419": {"__data__": {"id_": "44b285b6-89c7-41b3-9ecf-ffb64dad2419", "embedding": null, "metadata": {"page_label": "9", "file_name": "Foundations of GenIR.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Foundations of GenIR.pdf", "file_type": "application/pdf", "file_size": 699017, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "df290570-5fc0-4389-a1eb-7897e876af3d", "node_type": "4", "metadata": {"page_label": "9", "file_name": "Foundations of GenIR.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Foundations of GenIR.pdf", "file_type": "application/pdf", "file_size": 699017, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "3c5f68c4fa35b64127e12cb19496cbf175468dfb9193b9472cf2da609440e986", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "\u2022 Object Detection-Based Input: This method involves detecting objects within an\nimage, extracting their features and associated spatial information, and then feeding\nthis data into the language model [55, 56]. While this approach is effective, it tends\nto be slow due to the processing time required for object detection.\n\u2022 Visual Encoding: Another method encodes images directly using a visual encoder,\nwhich converts images into a latent vector representation before integration with\nthe model [57\u201361]. This method can sometimes result in the loss of detail.\n\u2022 Patch-Based Input: The most efficient approach involves dividing images into several\npatches, transforming them with a simple linear layer, and directly inputting them\ninto the model without the need for a complex visual encoder [62].\nIn terms of training methods, there are mainly four types of training objectives:\n\u2022 Contrastive Learning or Image-Text matching: These tasks require the model to\ncorrectly categorize images and their corresponding textual descriptions, aligning\nthe representations of text and images [61, 63, 64].\n\u2022 Image Captioning: The model generates captions based on images, which helps it\nlearn to understand the visual content [58\u201361].\n\u2022 Fine-Grained Image Understanding: The model is tasked to describe specific areas\nof an image or locate particular objects within an image. This helps enhance the\nmodel\u2019s detailed comprehension of visual elements [58, 65].\n\u2022 Image Generation: This task is reconstructing the original pixels of an image that\nhas been blurred or corrupted [58, 66].\nThese methodologies and training objectives are crucial for advancing models\u2019\ncapabilities to process and interpret complex multimodal information effectively. This\nfacilitate a more natural interaction with users.\n1.4.2 Multi-modal Generation\nMulti-modal generation models, such as text-to-image generation, have substantially\nrevolutionized the field of art creation. Traditionally, GAN [67] and autoregressive\nmethods [68] are mainstream methods. However, they are computationally expensive\nand can not produce high-quality results. Recently, diffusion [69, 70] emerges as a new\nstate-of-the-art method in multimodal generation. It perturbs the data with noise and\nlearns to reconstruct the original data.\nLanguage models are increasingly applied in the multimodal generation domain,\nsuch as in image [71, 72] and video generation [73, 74]. Language models are primarily\nutilized for processing training data and reformulating prompts.\nIn terms of training data, the titles associated with real-world images or videos\noften contain significant noise. If generative models are trained directly on these noisy\ntitles, it could lead to inaccurate semantic understanding. To address this, language\nmodels can be used to filter and regenerate text descriptions within the training\ndata [75, 76]. For instance, a multimodal understanding model could first be trained,\nthen used to relabel videos or images to obtain more precise and detailed text descrip-\ntions. Experimental results have shown that this method significantly improves the\nfidelity of model generations to prompts.\n9", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3154, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "41996106-4dd8-49b7-ac40-2a3375ed63d0": {"__data__": {"id_": "41996106-4dd8-49b7-ac40-2a3375ed63d0", "embedding": null, "metadata": {"page_label": "10", "file_name": "Foundations of GenIR.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Foundations of GenIR.pdf", "file_type": "application/pdf", "file_size": 699017, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "380d8a71-ad8a-46db-9653-7526828b4e56", "node_type": "4", "metadata": {"page_label": "10", "file_name": "Foundations of GenIR.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Foundations of GenIR.pdf", "file_type": "application/pdf", "file_size": 699017, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "c6d794c1492bf8ed85e07df10aabaea6f4305edefba350213a0e332b6810961f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "During inference, multimodal generation models are highly sensitive to the input\nprompts. Many users do not know how to craft effective prompts and thus get unsat-\nisfying responses [77]. As a result, it is common to train a language model to rewrite\nuser-provided prompts to enhance the quality of the generated images [75]. One of\nthe challenges here is the difficulty in annotating such rewriting training data, as even\nsystem developers may not always know the optimal prompts, let alone crowd-sourced\nworkers [78]. To overcome this, some researchers collect a large number of user-shared\neffective prompts as training data [79]. Others build prompt-rewriting models based\non user log data, capturing preferences and feedback for training [53].\n2 Information Synthesis\nOther than generating information directly, another important research and appli-\ncation direction is to use the power of generative AI models, particularly LLMs, to\nintegrate existing information and generate grounded responses accordingly. For sim-\nplicity, we refer to this paradigm as information synthesis. The key difference between\ninformation generation and information synthesis is the source of information. Infor-\nmation generation relies on the internal knowledge and information gathered through\nthe training of generative AI models to create the model outputs, while informa-\ntion synthesis requires external sources to provide information to the models, and the\nmodels serve more as a integrator than a creator. There are multiple reasons why\ninformation synthesis is considered more reliable than generation in several IA sce-\nnarios. Here we discuss two of the most significant ones, i.e., model hallucination and\nexternal knowledge.\nHallucinating, which refers to the behavior of generative AI models that create\nresponses and outputs that are not grounded by facts or existing supporting materials,\nis rooted in the foundation of most existing generative AI systems. For instance,\nLLMs create responses based on the next token prediction task, which formulates the\ngeneration of language as a probabilistic process and generates the next token in the\noutput based on a probabilistic distribution (over the vocabulary) predicted by neural\nnetworks [1, 3]. The probabilistic model of LLMs allows them to capture knowledge in\nlarge scale data efficiently and effectively, but it also introduces inevitable variance in\ntheir generation process. In other words, it is well acknowledged that it\u2019s theoretically\nimpossible to prevent LLMs from generate data that are not seen in their training\nprocess [80]. While the ability of hallucinating is the source of creativity for LLMs (and\nfor human as well), it\u2019s not always desirable in practice, particularly for tasks with\nhigh requirements on result precision, reliability, and explanability. Therefore, asking\nthe generative AI models to integrate human created or factually grounded materials\ninstead of generating information on their own is often considered more effective and\nrobust to hallucination-sensitive applications.\nThe need of external knowledge is another key reason why we may prefer informa-\ntion synthesis over information generation. Despite the fact that modern generative\nAI models are trained with incredibly large amount of data gathered from the Web,\nthere are many cases where we still need to retrieve and find supports from external\n10", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3397, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "fe6ec668-fdc8-4808-9ea4-7956f6096185": {"__data__": {"id_": "fe6ec668-fdc8-4808-9ea4-7956f6096185", "embedding": null, "metadata": {"page_label": "11", "file_name": "Foundations of GenIR.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Foundations of GenIR.pdf", "file_type": "application/pdf", "file_size": 699017, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6df5af8d-7975-48ad-af80-1e1836876ef9", "node_type": "4", "metadata": {"page_label": "11", "file_name": "Foundations of GenIR.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Foundations of GenIR.pdf", "file_type": "application/pdf", "file_size": 699017, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "ca92b8d5f281da1cb4a228a88306dc6ea5c96db5a3729578496174c6257aa89c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "knowledge collections to finish certain tasks. Examples including the use of pri-\nvate datasets, vertical domain applications that require special knowledge, tasks that\ninvolve time-sensitive data, etc. It is usually inefficient or prohibitive to update large-\nscale generative AI models such as LLMs with task-oriented external data through\nmodel pre-training or supervised fine-tuning (SFT) [81\u201383]. Even if possible, such\nparadigm is not preferred because the internal knowledge structures of most genera-\ntive AI models are still mystery (at least of today), and there is no guarantee that\nthe models could behavior and use the external information as we expect. In con-\ntrast, using generative AI models as information synthesizer gives us not only more\nflexibility, but also more transparency and control over system outputs.\nIn this section, we discuss how generative AI models, particularly LLMs, can serve\nas effective information synthesizers for IA. We start with introducing one of the most\npopular information synthesis paradigm, i.e., retrieval augmented generation (RAG),\nand then discuss several other directions that utilize LLMs for corpus modeling and\nunderstanding.\n2.1 Retrieval Augmented Generation\nRetrieval Augmented Generation, or RAG, refers to the process of augmenting LLMs\nwith data retrieved from external collections or synthesizing multiple retrieval results\nwith LLMs for downstream applications [84, 85]. While the popularity of RAG rose\nafter the release of large-scale pre-trained language models such as GPT [1] and\nBART [86], relevant topics and techniques have already been studied for at least more\nthan two decades in both IR and NLP communities, e.g., extractive and abstractive\nsummarization that generates summary based on retrieved sentences [87, 88] or answer\nextraction from top retrieved document [89]. A major reason why RAG-like techniques\nwere not as attractive as they are today is the limited performance of generative mod-\nels before the era of LLMs. After ChatGPT [1] demonstrated superior ability text\ngeneration at the end of 2022, there have been many studies and surveys on RAG and\nits applications in LLMs [84, 90, 91]. As the intent of this chapter is not to provide yet\nanother survey on existing RAG papers, we focus the following discussions on several\npresent and future directions for RAG and their relations underneath.\n2.1.1 Naive RAG\nNaive RAG refers to the paradigm that directly feeds documents or other types of\ninformation retrieved by a retrieval system to the input (e.g., prompts) of a generative\nAI model and hope that the model can generate better output with or without a\nspecific target task [92]. It is also referred to as the \u201cRetrieve-then-Read\u201d framework\nthat has been used in reading comprehension and text summarization before LLMs\nhit the world [93]. Given an input (could be a query or a specific task instruction),\nwe first retrieve relevant information (usually entities, passages, or documents) from\nan external corpus or previous inputs (e.g., the memory of an agent [94, 95]) with a\nretrieval system. Then, we craft a input prompt with the retrieval results and feed it\nto the LLM. The LLM will generate the final response based on the input request and\n11", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3249, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "0bf60834-6a4e-40b8-8f7d-d55185dc9c71": {"__data__": {"id_": "0bf60834-6a4e-40b8-8f7d-d55185dc9c71", "embedding": null, "metadata": {"page_label": "12", "file_name": "Foundations of GenIR.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Foundations of GenIR.pdf", "file_type": "application/pdf", "file_size": 699017, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "329477dc-805e-4977-af4d-a763da688782", "node_type": "4", "metadata": {"page_label": "12", "file_name": "Foundations of GenIR.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Foundations of GenIR.pdf", "file_type": "application/pdf", "file_size": 699017, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "05d692a65263ccddb1cf90c7292289adcc14acead672773ff86e848e29471a8c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "the retrieved information. This paradigm has already been proven to be effective in\nmultiple IA tasks such as question answering [85].\nBecause LLMs are purely used as black-box tools to process the retrieved doc-\numents and input request in naive RAG, existing studies on this direction mainly\nfocus on the development of better retrieval systems and prompt design for RAG. The\nstudies on retrieval systems, unsurprisingly, are highly similar to those in IR, which\ninvolve indexing, query processing, first-stage retrieval, re-ranking, etc. These topics\nand system components have already been studied in the IR community for more than\nfive decades. Perhaps the most notable difference is that recent studies on naive RAG\noften prefer the use of neural retrieval models (e.g., dense retrieval models [96]) over\ntraditional term-matching models (e.g., BM25 [97]). An important reason behind this\nis that neural retrieval models share similar theoretical background and model struc-\ntures with LLMs. This makes joint optimization possible in modern RAG systems,\nwhich we discuss in Section 2.1.3.\nThe design of input prompts with retrieval results, on the other hand, is relatively\nmore under-explored before the rise of LLMs. It has been well recognized that prompt\nformats, even when the contents are same, could significantly affect the performance of\nLLMs. How to feed retrieval results effectively into the prompts of LLMs for RAG has\nthus attracted a lot of attention recently [93, 98, 99]. Studies have found that LLMs\nexhibit significant position bias over the input result sequences [100, 101], and has\ndifferent perspectives on relevance with human experts [102]. Since prompts are the\nmain interaction interface between retrieval and generation, their design principles and\ndownstream effects on naive RAG are of great value both in research and real-world\napplications. Particularly, how to craft effective RAG prompts automatically could be\na fruitful direction to explore. Existing studies have shown that high-quality prompt\nwriters can be automatically learned based on downstream task performance and user\nlogs in image generation [53], and it is widely believed that similar techniques have\nalso been used in popular LLM chatbots [103]. Yet, how to do this for RAG remains\nto be a question to be answered.\n2.1.2 Modular RAG\nIn contrast to naive RAG methods, modular RAG treats retrieval systems as func-\ntional modules to support LLMs [104]. While some works view this retrieval module\nas one type of many tools that can be learned and used by LLMs [105], it is widely\nacknowledged that retrieval systems possesses a irreplaceable position in modern\nLLM applications due to its diverse nature and significant importance [84]. Broadly\nspeaking, existing studies on using retrieval systems as functional modules for LLM\ngeneration mainly focus on the three \u201cW\u201d questions, namely when to retrieve,what to\nretrieve, and where to retrieve.\nThe question of when to retrieverefers to the timing of functional call for retrieval\nsystems. In contrast to LLMs that directly create responses based on their internal\nparameter space without explicit evidence grounding, retrieval systems produces reli-\nable and explainable information directly by searching external corpus. From this\nperspective, the best timing to call the retrieval system is when LLMs start to hallu-\ncinate or produce wrong results. Yet, identifying such timing is difficult because we\n12", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3464, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "151c90ef-8368-4361-bf03-1a8da75f2f5d": {"__data__": {"id_": "151c90ef-8368-4361-bf03-1a8da75f2f5d", "embedding": null, "metadata": {"page_label": "13", "file_name": "Foundations of GenIR.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Foundations of GenIR.pdf", "file_type": "application/pdf", "file_size": 699017, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "42718cef-5d44-42a7-82b5-7f4fe3085b17", "node_type": "4", "metadata": {"page_label": "13", "file_name": "Foundations of GenIR.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Foundations of GenIR.pdf", "file_type": "application/pdf", "file_size": 699017, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "38d016a56e53a1de47404e649076631af478f935872bae756ad181f4d1656e95", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "neither know the correct answers in advance or understand the internal mechanism of\nLLMs (at least of today) [106]. One naive yet effective method is to retrieve supporting\nevidence for LLM inference with a fixed time interval, such as every fixed number of\ngenerated tokens[107, 108] or every sentence [109]. More advanced paradigms involve\nthe analyze of knowledge boundary [110] and the estimation of prediction uncertainty\nin LLMs [106, 111]. Theoretically speaking, since the study of when to retrieveshares\nsimilar motivations and foundations with the study of hallucination detection, exist-\ning studies on LLM hallucination [112, 113] could provide important inspiration for\nresearch on this topic. Promising directions including better fact checking systems\nfor LLMs [114] and more investigations on how to characterize the confidence and\nuncertainty of LLM predictions based on both external behavior and internal state\nanalysis [111].\nThe question of what to retrieve focuses on analysis the intents and information\nneeds of LLMs in inference. LLMs often need the help of different tools and systems to\nfinish different tasks [105]. However, in contrast to other tools widely studied in tool\nlearning, retrieval itself is a complicated systems with dynamic and free-form inputs,\ndata collections, and outputs. Therefore, understanding what exactly is needed by\nLLMs and how to formulate it in the language of retrieval systems is an important\nproblem. Most existing studies on RAG naively use the whole or local context of LLM\ninference as the queries to retrieval systems and assume that these context contain\nenough information to guide retrieval [90]. A slightly better solution is to use the terms\nthat LLMs have low confidence to formulate queries since uncertain tokens represent\ncases where LLMs have limited knowledge to generate responses and thus need more\ninformation [106]. As long studied in the IR community, the formulation of an effective\nquery requires deep understanding of the user\u2019s intent, and many of the important\ncontext information behind a user intent is not explicitly expressed in the words they\nwrote [115]. Therefore, a more theoretically principled method to answer what to\nretrieve in RAG is to analyze the internal state of LLMs and infer their information\nneeds directly. For example, Su et al. [111] directly formulate queries based on the\ninternal attention distribution of LLMs (Figure 2) and improve the performance of\nRAG for nearly 20% on several benchmark datasets without changing the retrieval\nsystem. This demonstrates the potential of future studies on this direction.\nWhere to retrieverefers to the question of how to identify the correct information\nsources for RAG. Studies on this direction is particularly related to the research on\nmulti-source retrieval [116] and tool learning [105]. To answer different requests related\nto the use of information collected from different databases or data collections, LLMs\nneed to learn how to interact with each information sources effectively and efficiently.\nThe studies of tool learning focus on teaching LLMs to use tools according to the\ncontext, and retrieval systems are usually considered as one type of tools to use.\nHowever, retrieval itself could be a complicated problem when we possess multiple data\ncollections with different characteristics. In search engines, information sources are\nbroadly categorized based on their modality, and we usually build separate systems for\neach of them (e.g., the \u201cImages\u201d, \u201cNews\u201d, \u201cVideos\u201d tabs on Google). While commercial\nsearch engines may aggregate results from different sources into a single page, the\nultimate search engine result page (SERP) shown to users are just a list of results and\n13", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3746, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "7818ea45-b4ad-4e9b-89d6-26084a8d09fe": {"__data__": {"id_": "7818ea45-b4ad-4e9b-89d6-26084a8d09fe", "embedding": null, "metadata": {"page_label": "14", "file_name": "Foundations of GenIR.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Foundations of GenIR.pdf", "file_type": "application/pdf", "file_size": 699017, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "813bf178-50a6-4a4a-9fd5-86b059566ab4", "node_type": "4", "metadata": {"page_label": "14", "file_name": "Foundations of GenIR.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Foundations of GenIR.pdf", "file_type": "application/pdf", "file_size": 699017, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "a458b37c459bd9bf074d08daa9bc08a5775540537ae4d146e57b7f8d00a7bb3d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Fig. 2 Su et al. [111] generate queries for RAG based on the internal attention distribution of LLMs.\nit\u2019s up to the users to decide which they want to see and how to use these results for\ndownstream applications. In contrast, when using LLMs, users often request LLMs\nto directly answer their question instead of listing a couple of candidates [117, 118],\nso it\u2019s the job of LLMs to decide where to retrieve the information given the current\ncontext. While the studies of how to navigate user queries to search indexes built from\ndifferent information sources have been widely studied in the IR community [119\u2013\n122], how to do it for RAG with modern generative AI models is, to the best of our\nknowledge, still underexplored. Existing literature on RAG mostly works on a single\nretrieval collection (usually a text corpus), but it\u2019s obvious that no single collection\ncan satisfy the needs of LLMs in different tasks. For instance, when writing a legal\n14", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 955, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "0661582b-9d37-4292-959c-b4ef5d494e34": {"__data__": {"id_": "0661582b-9d37-4292-959c-b4ef5d494e34", "embedding": null, "metadata": {"page_label": "15", "file_name": "Foundations of GenIR.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Foundations of GenIR.pdf", "file_type": "application/pdf", "file_size": 699017, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "669fa008-2820-464d-a69a-ae0af5251564", "node_type": "4", "metadata": {"page_label": "15", "file_name": "Foundations of GenIR.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Foundations of GenIR.pdf", "file_type": "application/pdf", "file_size": 699017, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "86d74b765a2136f3c2245df11f9cdf58998685b3947a21a55ea029f4aed873d1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "case document, the judge needs to collect and organize information from evidences,\ncomplaints, counterclaims, court records, as well as legal articles and previous cases.\nHow to navigate the generation model to retrieve and integrate information from\ndifferent sources jointly for downstream applications is a practical and potentially\nfruitful research question for RAG.\n2.1.3 Optimization of Retrieval and Generation\nAs discussed in several RAG surveys [84, 90], the optimization of RAG systems usually\ninvolves the optimization of three components, i.e., the retriever, the generator, and the\naugmentation method. If we further step back and look at the high-level goals of RAG\noptimization, we could also categorize it based on how we evaluate the RAG system,\nnamely the evaluation from the perspectives of retrievers, generators, or the joint\nsystems. The evaluation from the retriever perspectives is not particularly different\nfrom existing studies on ranking evaluation. The underlining assumption of this is\nthat, once the LLMs are fed with the passages or documents that contain the correct\ninformation, they should be able to produce the correct answers directly. Therefore,\nthe evaluation and optimization of a RAG system could downgrade to the evaluation\nand optimization of a classic retrieval/ranking systems, to where most existing works\non dense retrieval and LTR could be applied [123, 124]. Yet, there are still differences\nbetween RAG and traditional retrieval tasks as the queries are no long issued by users.\nHow to formulate queries efficiently and effectively from LLMs for the retriever is\nworthy research question, and studies on this direction has already shown potentials\nin improving the overall quality of RAG systems [111].\nFrom the perspective of generators, RAG evaluation and optimization focus more\non improving the robustness and effectiveness of LLM generation based on a fixed set\nof retrieval results [108]. This often means extra training or fine-tuning on LLMs to\nimprove their fundamental ability in information processing. For example, retrieved\ndocuments could be lengthy, and LLMs are usually not good at processing long input\ncontext [101]. Therefore, how to design efficient LLMs that can take long context\ninputs efficiently and effectively has been a popular research problem that have been\nwidely studied by researchers from both academia and industry [100]. We have seen\nmany companies show off their models based on how many input tokens they can\nprocess in one request. In addition, since retrieval results are fed as a part of the\nLLM inputs, whether the LLMs can generate the response based on the retrieved\ndocuments instead of their internal knowledge could be seen as a special type of\ninstruction-following ability. Studies have been conducted to teach LLMs to utilize\nretrieval results faithfully and constantly in RAG systems [125] On the other hand,\nfactors such as irrelevant results and ranking perturbations are well acknowledged to\nbe harmful for the performance of generators in RAG, so there are also studies that\ntry to improve the robustness of LLMs from the perspective of RAG. For example,\nZhang et al. [126] proposes to fine tune LLMs with the presence of retrieval results\n(i.e., retrieval augmented fine tuning) so that LLMs can learn the domain-specific\nknowledge introduced by the retriever and improve their robustness against potential\ndistracting information from retrieval.\n15", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3457, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f611d476-7408-4084-8294-504b5344f102": {"__data__": {"id_": "f611d476-7408-4084-8294-504b5344f102", "embedding": null, "metadata": {"page_label": "16", "file_name": "Foundations of GenIR.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Foundations of GenIR.pdf", "file_type": "application/pdf", "file_size": 699017, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b5d9ed9f-db03-4238-955e-7dd3e87b2f6a", "node_type": "4", "metadata": {"page_label": "16", "file_name": "Foundations of GenIR.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Foundations of GenIR.pdf", "file_type": "application/pdf", "file_size": 699017, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "ddc99db33ca45c66fc20c89d33f7cb7c0422f03caef5861d77afec4260a63c52", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "From the perspective of augmentation methods, existing research mostly focuses\non the joint optimization of RAG system as a whole. In other words, the loss functions\nof RAG optimization should be built from the performance metrics of downstream\ntasks directly. While this paradigm is appealing, it often has strict requirements on\nthe design of RAG systems. Particularly, it\u2019s difficult to apply such joint optimiza-\ntion algorithms on a RAG system in which retrievers and generators are loosely\nconnected through prompts constructed from discrete retrieval results. While rein-\nforcement learning could solve the problem in theory, its empirical performance when\nbeing used as the solo optimization algorithms for ranking systems is still not satisfying\nat this point [127]. If you already have a good retriever and only conduct fine-tuning\nwith a fixed LLM, then it may work [128], but this still doesn\u2019t look like a perfect\nsolution because reinforcement learning usually subject to large variance in practice.\nTo the best of our knowledge, how to directly connect the training of retrievers with\nthe auto-regressive loss of the generators in RAG is still an open question. Answering\nthis question requires us to go deep into the structure of generative AI models and\nretrieval models, and develop new model structures that can take advantages from\nstudies on both sides.\n2.1.4 Retrieval Planning and Composite Information Needs\nAs discussed above, the initial motivation behind the studies of RAG mostly focuses\non using the power of retrieval systems to improve the quality of responses generated\nby LLMs in terms of reliability and informativeness. While it is widely acknowledged\nthat problems such as hallucination and high computation cost in supervised fine-\ntuning will continue to be significant for generative AI models in a short period of\ntime, there are also concerns, especially from the IR community, that retrieval could\nbecome less important with the rapid evolution of LLMs [129]. In fact, ChatGPT\nhas already shown similar accuracy and better user satisfaction on factoid question\nanswering than traditional web search engines [1]. However, the rise of generative AI\nmodels also brings brand new opportunities for IR. One of them is the possibility of\nmoving from SERPs that simply list result candidates to a real information agent that\nsolve complicated tasks with composite information needs.\nToday, most people treat IR systems as unit information solvers. Despite of their\nactual task characteristics, users first decompose their goals into a couple of unit\ninformation need (usually expressed with separate queries), and then issue them one by\none to search engines or recommendation systems to find the corresponding answers.\nAn important reason behind the popularity of this paradigm is that, at least of today,\nIR systems are not capable of doing complicated information tasks with composite\nneeds and multi-step planning. For example, we can use a search engine to find a survey\non RAG by searching \u201dsurvey of RAG\u201d, but cannot write such a survey directly by\nretrieving and analyzing papers from publication collections. The job of information\nneed decomposition and retrieval planning has always been human\u2019s.\nFortunately, with the help of generative AI models like LLMs, it is now possi-\nble to push the boundary of IR systems and tackle such advanced information tasks\nfor users. Composite retrieval is not a new concept in IR [130], but previous studies\nrefer to the phrase as retrieval paradigms that cluster results from multiple sources\n16", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3578, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c5b7aa51-5701-4f22-b03e-81657e369dcb": {"__data__": {"id_": "c5b7aa51-5701-4f22-b03e-81657e369dcb", "embedding": null, "metadata": {"page_label": "17", "file_name": "Foundations of GenIR.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Foundations of GenIR.pdf", "file_type": "application/pdf", "file_size": 699017, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5f66b373-401d-44cf-b190-2d29d945b673", "node_type": "4", "metadata": {"page_label": "17", "file_name": "Foundations of GenIR.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Foundations of GenIR.pdf", "file_type": "application/pdf", "file_size": 699017, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "aa7130c1b1c367b69b8c62801b38154b6bbeac397fbc4a9b4286724e9527d61e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "and show them in groups for specific user queries [131]. While this represents one\ntype of composite needs, it is relatively simple as the target user queries usually are\nmostly topic-specific and keyword-based. Complicated information tasks such as sur-\nvey generation and professional document writing often involve multi-step planning\nand multi-round interactions between the retrieval results and response generation. To\nbuild powerful IR systems or agents that can solve such composite information tasks,\nwe need to construct collaborative systems that deeply connect the retrieval, plan-\nning, and generation. For instance, we need to conduct generation-oriented retrieval\noptimization to build retrieval framework and model interfaces for downstream task\nplanner and response generators; we also need to design retrieval-oriented generation\nmodels that can decompose information needs, navigate the retrieval process, gather\ninformation from multiple sources to generate the final results. Research on these direc-\ntions could be fruitful and significantly extend the scope of IR in the era of generative\nAI.\n2.2 Corpus Modeling and Understanding\nIn contrast to using RAG, another line of studies try to use generative AI models\nto replace traditional retrieval systems. Directly answering user\u2019s information need\ninstead of showing ten blue links has long been an important goal for the development\nof intelligent IR systems [132]. With the rise of LLMs, such vision is now achievable\nin a significant extent. For example, LLM-based chatbots like ChatGPT can answer\nmultiple types of user queries with direct answers [118]. Metzler et al. [133] has dis-\ncussed several paradigms in which pre-trained language models can help IR systems\nanswer user\u2019s information needs directly without listing references. The intuition is to\nuse neural network based language models to store the corpus knowledge in parameter\nspace and pull relevant answers or information directly from it based on user\u2019s queries.\nDepending on how the problem is formulated, several research directions have emerged.\nSpecifically, in this section, we discuss two of them, namely generative retrieval and\ndomain-specific modeling.\n2.2.1 Generative Retrieval\nThe idea of Generative Retrievalcomes from the idea of differentiable index proposed\nby Metzler et al. [133]. The original name used in the paper was Model-based IR, but\nafter the rise of generative AI models, some researchers start to refer to studies on\nthis direction as generative retrieval (GR). The core idea of GR is two-fold, i.e., the\ndifferentiable index and the generation of doc IDs.\nInspired by the superior performance of pre-trained language models, particularly\nBERT [8] and GPT [1], generative retrieval wants to explore the possibility of replacing\ntraditional term-based index (e.g., inverted index) in retrieval systems with large-\nscale neural networks. In contrast to dense retrieval models that build neural encoders\nto project documents to latent semantic spaces and build explicit indexes based on\ndocument vectors, GR tries to build implicit indexes in the parameter space of neural\nnetworks. For instance, DSI and its variations [134\u2013137] have tried to train pretrained\nlanguage models on the target corpus directly and then treat the model\u2019s parameter\n17", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3314, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ffdb55d8-66c7-4305-8217-160b4ecf2093": {"__data__": {"id_": "ffdb55d8-66c7-4305-8217-160b4ecf2093", "embedding": null, "metadata": {"page_label": "18", "file_name": "Foundations of GenIR.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Foundations of GenIR.pdf", "file_type": "application/pdf", "file_size": 699017, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a87cca8c-519c-4df0-92d9-65e171968f32", "node_type": "4", "metadata": {"page_label": "18", "file_name": "Foundations of GenIR.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Foundations of GenIR.pdf", "file_type": "application/pdf", "file_size": 699017, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "b9d2d1214a2f020c6387c76b7c7c22f9e73b618338d144e2f5667dcdaa090452", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "as a \u201cindex\u201d of the corpus. Studies on this direction argue that, by training the neural\nmodels to encode the whole corpus, documents and information would be implicitly\nstored in the parameters of the models, and this parameter-based indexes have better\nstorage efficiency than traditional term-based or vector-based indexes [135]. They also\nargue that such paradigm can unify the multi-stage retrieval pipeline so that indexes\ncan be trained directly for the final retrieval objectives. However, storing raw document\ncontent directly in limited parameter spaces often lead to significant information loss\n(which is reflected in the suboptimal retrieval performance of GR models [138]), and\nusing model parameters as indexes make the whole system uncontrollable by both\nsystem developers and users. While the former could be alleviated by using large-scale\nmodels, the later is still an unresolved problem for GR. For example, it\u2019s difficult, if\nnot impossible, to remove or update a document indexed in the parameter space when\nwe don\u2019t know what exactly each parameter do in the neural models. Considering\nthat dense retrieval models built with product quantization and inverted file systems\ncan achieve state-of-the-art retrieval performance with similar latency and less storage\nthan term-based models with inverted indexes [139], whether the idea of differentiable\nindexes in GR worth its price is still a controversial question.\nAnother important characteristic of GR models is to retrieve documents by gen-\nerating sequences of doc IDs through auto-regression. Because documents are stored\nimplicitly in model parameters, to actually retrieve a real document, GR models use\nuser\u2019s queries as prompts to generate document IDs, which usually consist of a couple\nof special tokens, that exclusively identify each relevant document. Since the birth of\nGR, a variety types of document IDs have been proposed, which can be broadly catego-\nrized as IDs with explicit tokens [134\u2013136] and IDs with implicit tokens [137, 140, 141].\nGR models with explicit ID tokens try to label each document with sequences of real\nterms that have semantic or numerical meanings. Examples including keyword-based\ndoc IDs and tree-based doc IDs [135]. Compared to vectors in dense retrieval, these\nmethods have less flexibility and capability in document modeling as they discretize\ndocument semantic meanings with limited number of tokens, and their retrieval per-\nformance is usually poor [140]. However, they have better explanability than other\nneural retrieval models because their doc ID tokens are constructed from real words\nor document clusters. To avoid the theoretical limitation of explicit token IDs and\ngrant GR models with the same modeling capacity of dense retrieval models, several\nstudies have proposed to build implicit token IDs with latent vectors [137, 140, 141].\nThe idea is to represent each document with a sequence of latent vectors so that fine-\ngrained semantic information would not be lost. These types of GR models are highly\nsimilar to existing dense retrieval models since both of them represent each document\nwith latent vectors. The major difference is that the former uses a sequence of vectors\nfrom a learned codebook constructed in training, while the later builds separate vec-\ntors for each document directly from their raw content. Wu et al. [142] have proved\nthat the GR models with implicit tokens are equal to a multi-vector dense retrieval\nmodels in theory. Also, the use of a learned codebook for implicit token vectors is\ntheoretically the same with a dense retrieval system that uses cluster-based product\nquantization [139, 143]. Therefore, the performance upper bound of GR (with implicit\ntokens) and dense retrieval is the same in theory. While some believe that GR models\n18", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3809, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "45485170-99f4-4d02-a6a9-94ba57f4e6af": {"__data__": {"id_": "45485170-99f4-4d02-a6a9-94ba57f4e6af", "embedding": null, "metadata": {"page_label": "19", "file_name": "Foundations of GenIR.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Foundations of GenIR.pdf", "file_type": "application/pdf", "file_size": 699017, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3e106a48-5437-4ade-a8cd-5d91545a38be", "node_type": "4", "metadata": {"page_label": "19", "file_name": "Foundations of GenIR.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Foundations of GenIR.pdf", "file_type": "application/pdf", "file_size": 699017, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "5c8a61f31bf1c2d52734a4313ede3081e4837b0481ead2165edeb3cefcdd0368", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "could have lower latency as they don\u2019t need to search among millions of documents\non the fly, this is a questionable argument because the inference of a large-scale neu-\nral model is usually much slower than a vector-based search on distributed systems.\nAlso, the maintenance of information in a neural model is much more difficult than\na vector-based database. Perhaps the future potential of GR does not lay in retrieval\neffectiveness or efficiency, but some other perspectives such as explainability.\n2.2.2 Domain-specific Modeling\nLLMs, particularly those with instruction tuning, can response to user\u2019s queries\ndirectly. This exactly matches the initiative of a long-standing vision of IR systems\nto directly answer user\u2019s need without listing a couple of documents [133]. Therefore,\never since the rise of ChatGPT, there has been serious discussion on whether LLMs\nare future seach engines in practice [129]. Yet, apart from the hallucination problem\ndiscussed in previous sections, there are other challenges that prevent generative AI\nmodels like LLMs to serve as a major information accessing tool for modern users.\nOne of them is how to teach LLMs to understand and use knowledge from external\ncorpus not included in their initial training process. If we treat each external corpus\nas a domain-specific dataset, then the studies on this direction is essentially the same\nwith the construction of domain-specific LLMs. While RAG can help LLMs adapt to\nnew domains quickly, their performance is limited when the understanding of input\ndocuments from the external corpus requires domain knowledge that the LLMs do not\npossess in advance [83].\nTo solve the above problem and build usable IA systems with LLMs on domain-\nspecific data, one of the most popular method is to conduct continue pre-training or\nsupervised fine-tuning of LLMs on the target domain corpus. The idea is to apply\nsimilar training strategies used in model pre-training on the new corpus so that LLMs\ncan better capture knowledge in the new domain. Example studies on this direction\ninclude techniques on data selection [82] and tokenizers adaptation [144] that directly\nuse the target corpus to train LLMs. Many domain-specific LLMs have been developed,\ninclude legal LLMs, financial LLMs, etc. [145\u2013147] The continue pre-training of LLMs\non external corpus has been shown to be effective on many domain-specific tasks such\nas domain QA and text generation. However, modeling external corpus through this\nmethod may not be preferred in practice when we don\u2019t have enough computation\nresources to train LLMs or can\u2019t access the parameters of them. Also, till the end\nof the today, the internal knowledge structure and learning mechanism of LLMs are\nstill unknown, and applying naive continue pre-training algorithms on external corpus\ncould hurt the performance of LLMs in unexpected way. Therefore, researchers have\ndesigned several knowledge editing techniques on LLMs to explore the possibility of\ninjecting knowledge with no or low cost on the general effectiveness of LLMs [148, 149].\nStudies on this direction is still in an early stage as most existing methods only work\non fixed and limited updating rules and knowledge entity triples [150], but it could be\nfruitful in future since domain adaption and external corpus modeling is a wide need\nof LLM applications in practice.\nBesides continue pre-training, another paradigm to model external corpus and\ndomain knowledge is to build separate language models for each corpus and combine\n19", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3522, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8dca77ab-e30c-4d4b-b1ee-a4cf14f9c978": {"__data__": {"id_": "8dca77ab-e30c-4d4b-b1ee-a4cf14f9c978", "embedding": null, "metadata": {"page_label": "20", "file_name": "Foundations of GenIR.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Foundations of GenIR.pdf", "file_type": "application/pdf", "file_size": 699017, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b03bb92d-f069-4964-976d-f515fe1a2f0a", "node_type": "4", "metadata": {"page_label": "20", "file_name": "Foundations of GenIR.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Foundations of GenIR.pdf", "file_type": "application/pdf", "file_size": 699017, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "3d0125aefc5d9cea1cb40bb5965b16b4cf5faa0d29dab90510000aad3da0b3c5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "them with the large general LLMs to form a collaborative system. The intuition behind\nthis is relevant to the idea of LLM agents where each LLM could serve as different roles\nin the system to accomplish tasks together. It is widely acknowledged that the phe-\nnomenon of emergent abilities only present in large-scale models [29], but the training\ncost of such models (e.g., GPT-4 [1]) is usually prohibitive to research institutes and\nsmall companies, even with parameter efficient algorithms [151] Inspired by the supe-\nrior instruction following ability of LLMs, researchers have explored the possibility\nof building small models for external corpus modeling and use them to communicate\ndomain-specific knowledge to large general LMs [83]. In other words, the small mod-\nels can serve as domain knowledge \u201c consultants\u201d and the large general models can\nserve as the decision makers that finish domain-specific tasks based on the guidance\nof the small models. Experiments have shown that such paradigm can improve black-\nbox LLMs\u2019 performance on domain-specific tasks with low cost and high flexibility.\nWhile the overall idea of prompt general LLMs with domain-specific prompts is sim-\nilar to the framework of RAG, building an actual LM for corpus modeling enable us\nto capture implicit domain knowledge (e.g., the fine-grained differences between law\narticles [152]) and potentially save tokens in prompts. There are concerns on whether\nthis paradigm is still worthy when we have more powerful LLMs that include more\ndomain-specific data in training. However, since many users prefer to keep their data\nprivate to themselves due to multiple safety and privacy concerns, this paradigm and\nRAG could continue to be appealing in practice.\n3 Summary and Future Directions\nIn this chapter, we introduce the foundations and applications of generative AI models\nin information accessing. Instead of analyzing how generative AI models like LLMs\ncould improve the existing modules of search engines and recommendation systems, we\nfocus on how the they could revolutionize information access with new methodologies\nand system design. Particularly, we discuss two new paradigms brought by generaive\nAI models, namely information generation and information synthesis.\nInformation generation refers to the scenarios where users can use generative AI\nmodels to create information that directly satisfies their information needs. Here, we\ndelved into the core components of generative models, including model architectures\n(with a focus on Transformers and their improvements), scaling laws, and training\nmethodologies. We examined the debates surrounding continual model scaling, the\nimportance of prompt optimization, and the extension of these models to multi-modal\napplications for information access.\nInformation synthesis refers to the paradigm that utilizes the superior instruction-\nfollowing and logic-reasoning ability of LLMs to aggregate and synthesize existing\ninformation. We extensively discuss one of the most representative techniques, i.e.,\nRetrieval Augmented Generation (RAG), on this direction, and introduce various\napproaches from naive implementations to more sophisticated modular systems. We\ndescribe the challenges and opportunities in optimizing RAG systems, highlighting\nthe need for joint retrieval-generation optimization and the potential of several rele-\nvant research directions such as composite retrieval with planning. Besides RAG, we\n20", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3463, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "19652b9b-5aa7-4627-ba76-cc4c071ce05c": {"__data__": {"id_": "19652b9b-5aa7-4627-ba76-cc4c071ce05c", "embedding": null, "metadata": {"page_label": "21", "file_name": "Foundations of GenIR.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Foundations of GenIR.pdf", "file_type": "application/pdf", "file_size": 699017, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9fa45690-f2cc-450e-9f1a-d4de1a061488", "node_type": "4", "metadata": {"page_label": "21", "file_name": "Foundations of GenIR.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Foundations of GenIR.pdf", "file_type": "application/pdf", "file_size": 699017, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "215eceed10712d0f186cb6baa0fd90af71feedb8b03bb9249e29fda8eb39b3b2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "also discuss some alternative paradigms that use generative AI models to model cor-\npus knowledge directly, such as generative retrieval, which aims to replace traditional\nindexing methods with neural network-based approaches, and domain-specific model\ntraining, which conducts continue pre-training or fine-tuning on LLMs with the tar-\nget corpus. We discussed the potential and limitations of these approaches, including\nissues of system controllability and cost efficiency.\nOverall, research on how generative AI models could reshape modern information\naccess systems is still at an early stage today. As discussed above, existing studies on\ninformation generation and information synthesis either focus on simple information\ntasks (such as writing a poem, answering a factoid question, etc.) or reply on simple\nsystem design (e.g., feeding all documents to LLMs as prompts) that obviously can-\nnot fully exploit the power of modern retrieval and generation models. Therefore, we\nbelieve that there are two major directions worth exploring in the next couple of years\n(at least). The first one is to move from simple and unit information retrieval tasks\n(e.g., factoid QA) to more complicated information tasks that used to be \u201cimpossible\u201d\nfor modern IR systems. Examples include retrieval with composite needs (e.g., \u201dhelp\nme plan a wedding in Amherst, MA\u201d) or tasks that requires planning and multiple\nrounds of retrieval and generations (e.g., \u201dwrite a survey on RAG\u201d). These tasks used\nto require human experts to decompose the needs and conduct retrieval, analysis, and\nresult aggregations. With the help of generative AI, accomplishing them automatically\nwith machines is now possible. The second direction is to explore better techniques\nto communicate, collaborate, or even unify retrieval and generation systems for infor-\nmation accessing. While the studies of RAG have attracted considerable attention,\nexisting works mostly use retrieval systems as plug-in tools for LLMs without digging\ninto their internal connections and differences. Examples such as how to understand\nthe information needs of LLMs, how to communicate the retrieved results to LLMs,\nand how to optimize generators for retrieval and retriever for generation are all impor-\ntant yet underexplored research topics. There are many questions related to each of\nthese topics that worth detailed investigation, including the design of new training\nparadigms, the development of agent-like system frameworks, potential problems and\nbias introduced by off-policy and on-policy training for the joint system, etc.\nWhen ChatGPT first arrives, there are people from the IR community worried\nthat such generative AI models could overthrow all existing IR systems and crush\neverything in the field [129], as it has almost happened in NLP. Interestingly, in sim-\nulated social experiments on human-AI competitions, Yao et al. [153] find that, if\nhuman producers don\u2019t extend their capacities with the help of generative AI, they\nwill eventually be \u201creplaced\u201d by AI. From this perspective, the future of IR research\nin the era of generative AI lies in how to extend the scope of IR with generative AI\nmodels to finish more complicated information tasks and develop more general system\narchitectures that not just retrieve a list of document, but perform more sophisticated\ninformation processing and planning.\nReferences\n[1] OpenAI: GPT-4 technical report. CoRR abs/2303.08774 (2023) https://doi.\n21", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3468, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "76b8dba6-f235-4056-8d3b-9f078718c2ad": {"__data__": {"id_": "76b8dba6-f235-4056-8d3b-9f078718c2ad", "embedding": null, "metadata": {"page_label": "22", "file_name": "Foundations of GenIR.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Foundations of GenIR.pdf", "file_type": "application/pdf", "file_size": 699017, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9f4973cb-4178-4156-b9aa-ecade5f50bd0", "node_type": "4", "metadata": {"page_label": "22", "file_name": "Foundations of GenIR.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Foundations of GenIR.pdf", "file_type": "application/pdf", "file_size": 699017, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "27010afb16786ecc5df0a7b6ad4a74a2f02849949b27f0feb97353b764fe69fa", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "org/10.48550/ARXIV.2303.08774 2303.08774\n[2] Zhao, W.X., Zhou, K., Li, J., Tang, T., Wang, X., Hou, Y., Min, Y., Zhang, B.,\nZhang, J., Dong, Z., Du, Y., Yang, C., Chen, Y., Chen, Z., Jiang, J., Ren, R.,\nLi, Y., Tang, X., Liu, Z., Liu, P., Nie, J., Wen, J.: A survey of large language\nmodels. CoRR abs/2303.18223 (2023) https://doi.org/10.48550/ARXIV.2303.\n18223 2303.18223\n[3] Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T.,\nRozi` ere, B., Goyal, N., Hambro, E., Azhar, F., et al.: Llama: Open and efficient\nfoundation language models. arXiv preprint arXiv:2302.13971 (2023)\n[4] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N.,\nKaiser, L.u., Polosukhin, I.: Attention is all you need. In: Guyon, I., Luxburg,\nU.V., Bengio, S., Wallach, H., Fergus, R., Vishwanathan, S., Garnett, R. (eds.)\nAdvances in Neural Information Processing Systems, vol. 30. Curran Associates,\nInc., ??? (2017)\n[5] Schuster, M., Paliwal, K.K.: Bidirectional recurrent neural networks. IEEE\ntransactions on Signal Processing 45(11), 2673\u20132681 (1997)\n[6] Zeng, A., Liu, X., Du, Z., Wang, Z., Lai, H., Ding, M., Yang, Z., Xu, Y.,\nZheng, W., Xia, X., et al.: Glm-130b: An open bilingual pre-trained model. arXiv\npreprint arXiv:2210.02414 (2022)\n[7] Le Scao, T., Fan, A., Akiki, C., Pavlick, E., Ili\u00b4 c, S., Hesslow, D., Castagn\u00b4 e, R.,\nLuccioni, A.S., Yvon, F., Gall\u00b4 e, M., et al.: Bloom: A 176b-parameter open-access\nmultilingual language model (2023)\n[8] Devlin, J., Chang, M., Lee, K., Toutanova, K.: BERT: pre-training of deep bidi-\nrectional transformers for language understanding. In: NAACL-HLT (1), pp.\n4171\u20134186. Association for Computational Linguistics, ??? (2019)\n[9] Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou,\nY., Li, W., Liu, P.J.: Exploring the limits of transfer learning with a unified\ntext-to-text transformer. J. Mach. Learn. Res. 21, 140\u2013114067 (2020)\n[10] Press, O., Smith, N.A., Lewis, M.: Train short, test long: Attention with lin-\near biases enables input length extrapolation. arXiv preprint arXiv:2108.12409\n(2021)\n[11] Su, J., Lu, Y., Pan, S., Wen, B., RoFormer, Y.L.: Enhanced transformer with\nrotary position embedding., 2021. DOI: https://doi. org/10.1016/j. neucom\n(2023)\n[12] Black, S., Biderman, S., Hallahan, E., Anthony, Q., Gao, L., Golding, L., He,\nH., Leahy, C., McDonell, K., Phang, J., et al.: Gpt-neox-20b: An open-source\n22", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2432, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "068b3422-3149-46d2-8d67-a40a1246fdb9": {"__data__": {"id_": "068b3422-3149-46d2-8d67-a40a1246fdb9", "embedding": null, "metadata": {"page_label": "23", "file_name": "Foundations of GenIR.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Foundations of GenIR.pdf", "file_type": "application/pdf", "file_size": 699017, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "dd634a05-fdfc-411f-ab90-5ec2396cb6b8", "node_type": "4", "metadata": {"page_label": "23", "file_name": "Foundations of GenIR.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Foundations of GenIR.pdf", "file_type": "application/pdf", "file_size": 699017, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "deecae2776de9fdbc78c63a01ae191e6b70d0d5eda01d23105d03c656f114813", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "autoregressive language model. arXiv preprint arXiv:2204.06745 (2022)\n[13] Child, R., Gray, S., Radford, A., Sutskever, I.: Generating long sequences with\nsparse transformers. arXiv preprint arXiv:1904.10509 (2019)\n[14] Kitaev, N., Kaiser,  L., Levskaya, A.: Reformer: The efficient transformer. arXiv\npreprint arXiv:2001.04451 (2020)\n[15] Munkhdalai, T., Faruqui, M., Gopal, S.: Leave no context behind: Efficient infi-\nnite context transformers with infini-attention. arXiv preprint arXiv:2404.07143\n(2024)\n[16] Grave, E., Joulin, A., Usunier, N.: Improving neural language models with a\ncontinuous cache. arXiv preprint arXiv:1612.04426 (2016)\n[17] Izacard, G., Grave, E.: Leveraging Passage Retrieval with Generative Models for\nOpen Domain Question Answering. arXiv (2020). https://arxiv.org/abs/2007.\n0128\n[18] Shazeer, N.: Fast transformer decoding: One write-head is all you need. arXiv\npreprint arXiv:1911.02150 (2019)\n[19] Ainslie, J., Lee-Thorp, J., Jong, M., Zemlyanskiy, Y., Lebr\u00b4 on, F., Sanghai,\nS.: Gqa: Training generalized multi-query transformer models from multi-head\ncheckpoints. arXiv preprint arXiv:2305.13245 (2023)\n[20] DeepSeek-AI: DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-\nExperts Language Model (2024)\n[21] Xiong, R., Yang, Y., He, D., Zheng, K., Zheng, S., Xing, C., Zhang, H., Lan, Y.,\nWang, L., Liu, T.: On layer normalization in the transformer architecture. In:\nInternational Conference on Machine Learning, pp. 10524\u201310533 (2020). PMLR\n[22] Ding, M., Yang, Z., Hong, W., Zheng, W., Zhou, C., Yin, D., Lin, J., Zou,\nX., Shao, Z., Yang, H., et al.: Cogview: Mastering text-to-image generation via\ntransformers. Advances in Neural Information Processing Systems 34, 19822\u2013\n19835 (2021)\n[23] Wang, H., Ma, S., Dong, L., Huang, S., Zhang, D., Wei, F.: Deepnet: Scal-\ning transformers to 1,000 layers. IEEE Transactions on Pattern Analysis and\nMachine Intelligence (2024)\n[24] Kaplan, J., McCandlish, S., Henighan, T., Brown, T.B., Chess, B., Child, R.,\nGray, S., Radford, A., Wu, J., Amodei, D.: Scaling laws for neural language\nmodels. arXiv preprint arXiv:2001.08361 (2020)\n[25] Hoffmann, J., Borgeaud, S., Mensch, A., Buchatskaya, E., Cai, T., Rutherford,\n23", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2208, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b1130f0c-ba1d-43da-84ea-50a98b198609": {"__data__": {"id_": "b1130f0c-ba1d-43da-84ea-50a98b198609", "embedding": null, "metadata": {"page_label": "24", "file_name": "Foundations of GenIR.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Foundations of GenIR.pdf", "file_type": "application/pdf", "file_size": 699017, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "dffb8c57-0d5c-4fa8-8cb7-7640c078dc1c", "node_type": "4", "metadata": {"page_label": "24", "file_name": "Foundations of GenIR.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Foundations of GenIR.pdf", "file_type": "application/pdf", "file_size": 699017, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "e91c6aef4137e90240a11301eca40a9701073fefe09840b1491a926f5b23598c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "E., Casas, D.d.L., Hendricks, L.A., Welbl, J., Clark, A., et al.: Training compute-\noptimal large language models. arXiv preprint arXiv:2203.15556 (2022)\n[26] Ye, J., Liu, P., Sun, T., Zhou, Y., Zhan, J., Qiu, X.: Data mixing laws: Optimizing\ndata mixtures by predicting language modeling performance. arXiv preprint\narXiv:2403.16952 (2024)\n[27] Henighan, T., Kaplan, J., Katz, M., Chen, M., Hesse, C., Jackson, J., Jun,\nH., Brown, T.B., Dhariwal, P., Gray, S., et al.: Scaling laws for autoregressive\ngenerative modeling. arXiv preprint arXiv:2010.14701 (2020)\n[28] Fang, Y., Zhan, J., Ai, Q., Mao, J., Su, W., Chen, J., Liu, Y.: Scal-\ning laws for dense retrieval. In: Proceedings of the 47th International\nACM SIGIR Conference on Research and Development in Information\nRetrieval. SIGIR \u201924, pp. 1339\u20131349. Association for Computing Machin-\nery, New York, NY, USA (2024). https://doi.org/10.1145/3626772.3657743 .\nhttps://doi.org/10.1145/3626772.3657743\n[29] Wei, J., Tay, Y., Bommasani, R., Raffel, C., Zoph, B., Borgeaud, S., Yogatama,\nD., Bosma, M., Zhou, D., Metzler, D., et al.: Emergent abilities of large language\nmodels. arXiv preprint arXiv:2206.07682 (2022)\n[30] Du, Z., Zeng, A., Dong, Y., Tang, J.: Understanding emergent abilities of\nlanguage models from the loss perspective. arXiv preprint arXiv:2403.15796\n(2024)\n[31] Power, A., Burda, Y., Edwards, H., Babuschkin, I., Misra, V.: Grokking: Gen-\neralization beyond overfitting on small algorithmic datasets. arXiv preprint\narXiv:2201.02177 (2022)\n[32] Schaeffer, R., Miranda, B., Koyejo, S.: Are emergent abilities of large language\nmodels a mirage? In: Proceedings of the 37th International Conference on Neural\nInformation Processing Systems. NIPS \u201923, pp. 1\u201313. Curran Associates Inc.,\nRed Hook, NY, USA (2024)\n[33] McKenzie, I.R., Lyzhov, A., Pieler, M., Parrish, A., Mueller, A., Prabhu, A.,\nMcLean, E., Kirtland, A., Ross, A., Liu, A., et al.: Inverse scaling: When bigger\nisn\u2019t better. arXiv preprint arXiv:2306.09479 (2023)\n[34] Mei, K., Tu, Z., Delbracio, M., Talebi, H., Patel, V.M., Milanfar, P.: Bigger is\nnot always better: Scaling properties of latent diffusion models. arXiv preprint\narXiv:2404.01367 (2024)\n[35] Hu, S., Tu, Y., Han, X., He, C., Cui, G., Long, X., Zheng, Z., Fang, Y., Huang,\nY., Zhao, W., et al.: Minicpm: Unveiling the potential of small language models\nwith scalable training strategies. arXiv preprint arXiv:2404.06395 (2024)\n24", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2432, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "52e5970d-a7bd-412c-9aa5-e3f468121357": {"__data__": {"id_": "52e5970d-a7bd-412c-9aa5-e3f468121357", "embedding": null, "metadata": {"page_label": "25", "file_name": "Foundations of GenIR.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Foundations of GenIR.pdf", "file_type": "application/pdf", "file_size": 699017, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "46104237-96f5-4c1b-aa16-7bc7349188b8", "node_type": "4", "metadata": {"page_label": "25", "file_name": "Foundations of GenIR.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Foundations of GenIR.pdf", "file_type": "application/pdf", "file_size": 699017, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "56611869454e66fb258d074d371453577831ff9b0d4bb53152b8bd63ae55120c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[36] Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I., et al.:\nLanguage models are unsupervised multitask learners. OpenAI blog 1(8), 9\n(2019)\n[37] Zhang, S., Roller, S., Goyal, N., Artetxe, M., Chen, M., Chen, S., Dewan, C.,\nDiab, M., Li, X., Lin, X.V., et al.: Opt: Open pre-trained transformer language\nmodels. arXiv preprint arXiv:2205.01068 (2022)\n[38] Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A.,\nBarham, P., Chung, H.W., Sutton, C., Gehrmann, S., et al.: Palm: Scaling lan-\nguage modeling with pathways. Journal of Machine Learning Research 24(240),\n1\u2013113 (2023)\n[39] Gunasekar, S., Zhang, Y., Aneja, J., Mendes, C.C.T., Del Giorno, A., Gopi, S.,\nJavaheripi, M., Kauffmann, P., Rosa, G., Saarikivi, O., et al.: Textbooks are all\nyou need. arXiv preprint arXiv:2306.11644 (2023)\n[40] Yang, A., Xiao, B., Wang, B., Zhang, B., Bian, C., Yin, C., Lv, C., Pan, D.,\nWang, D., Yan, D., et al.: Baichuan 2: Open large-scale language models. arXiv\npreprint arXiv:2309.10305 (2023)\n[41] Bi, X., Chen, D., Chen, G., Chen, S., Dai, D., Deng, C., Ding, H., Dong, K.,\nDu, Q., Fu, Z., et al.: Deepseek llm: Scaling open-source language models with\nlongtermism. arXiv preprint arXiv:2401.02954 (2024)\n[42] Chung, H.W., Hou, L., Longpre, S., Zoph, B., Tay, Y., Fedus, W., Li, E., Wang,\nX., Dehghani, M., Brahma, S., Webson, A., Gu, S.S., Dai, Z., Suzgun, M., Chen,\nX., Chowdhery, A., Valter, D., Narang, S., Mishra, G., Yu, A.W., Zhao, V.,\nHuang, Y., Dai, A.M., Yu, H., Petrov, S., Chi, E.H.-h., Dean, J., Devlin, J.,\nRoberts, A., Zhou, D., Le, Q.V., Wei, J.: Scaling instruction-finetuned language\nmodels. ArXiv abs/2210.11416 (2022)\n[43] Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang,\nC., Agarwal, S., Slama, K., Ray, A., et al.: Training language models to follow\ninstructions with human feedback. Advances in neural information processing\nsystems 35, 27730\u201327744 (2022)\n[44] Schulman, J., Wolski, F., Dhariwal, P., Radford, A., Klimov, O.: Proximal Policy\nOptimization Algorithms (2017)\n[45] Rafailov, R., Sharma, A., Mitchell, E., Manning, C.D., Ermon, S., Finn, C.:\nDirect preference optimization: Your language model is secretly a reward model.\nIn: Oh, A., Neumann, T., Globerson, A., Saenko, K., Hardt, M., Levine, S. (eds.)\nAdvances in Neural Information Processing Systems, vol. 36, pp. 53728\u201353741.\nCurran Associates, Inc., ??? (2023)\n[46] Xu, S., Fu, W., Gao, J., Ye, W., Liu, W., Mei, Z., Wang, G., Yu, C., Wu, Y.: Is\n25", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2505, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c95842e3-fde7-41ce-8713-890b29df36b0": {"__data__": {"id_": "c95842e3-fde7-41ce-8713-890b29df36b0", "embedding": null, "metadata": {"page_label": "26", "file_name": "Foundations of GenIR.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Foundations of GenIR.pdf", "file_type": "application/pdf", "file_size": 699017, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2fbeedce-cd28-4e24-991d-9223580ff5b3", "node_type": "4", "metadata": {"page_label": "26", "file_name": "Foundations of GenIR.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Foundations of GenIR.pdf", "file_type": "application/pdf", "file_size": 699017, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "ec13e3768c413b42636672886cb10c4d55f31d956fc0ed1ec5839b70465bfb61", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "dpo superior to ppo for llm alignment? a comprehensive study. arXiv preprint\narXiv:2404.10719 (2024)\n[47] Liu, P., Yuan, W., Fu, J., Jiang, Z., Hayashi, H., Neubig, G.: Pre-train, prompt,\nand predict: A systematic survey of prompting methods in natural language\nprocessing. ACM Computing Surveys 55(9), 1\u201335 (2023)\n[48] Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., Le, Q.V., Zhou,\nD., et al.: Chain-of-thought prompting elicits reasoning in large language models.\nAdvances in neural information processing systems 35, 24824\u201324837 (2022)\n[49] Yao, S., Yu, D., Zhao, J., Shafran, I., Griffiths, T.L., Cao, Y., Narasimhan,\nK.: Tree of thoughts: deliberate problem solving with large language models.\nIn: Proceedings of the 37th International Conference on Neural Information\nProcessing Systems. NIPS \u201923, pp. 1\u201314. Curran Associates Inc., Red Hook, NY,\nUSA (2024)\n[50] Wang, X., Wei, J., Schuurmans, D., Le, Q., Chi, E., Zhou, D.: Self-consistency\nimproves chain of thought reasoning in language models. In: 11th International\nConference on Learning Representations, ICLR 2023, pp. 1\u201315 (2023)\n[51] Zhou, Y., Muresanu, A.I., Han, Z., Paster, K., Pitis, S., Chan, H., Ba,\nJ.: Large language models are human-level prompt engineers. arXiv preprint\narXiv:2211.01910 (2022)\n[52] Yang, C., Wang, X., Lu, Y., Liu, H., Le, Q.V., Zhou, D., Chen, X.: Large\nlanguage models as optimizers. arXiv preprint arXiv:2309.03409 (2023)\n[53] Zhan, J., Ai, Q., Liu, Y., Chen, J., Ma, S.: Capability-aware prompt refor-\nmulation learning for text-to-image generation. In: Proceedings of the 47th\nInternational ACM SIGIR Conference on Research and Development in Infor-\nmation Retrieval. SIGIR \u201924, pp. 2145\u20132155. Association for Computing Machin-\nery, New York, NY, USA (2024). https://doi.org/10.1145/3626772.3657787 .\nhttps://doi.org/10.1145/3626772.3657787\n[54] Zhan, J., Ai, Q., Liu, Y., Pan, Y., Yao, T., Mao, J., Ma, S., Mei,\nT.: Prompt refinement with image pivot for text-to-image generation. In:\nKu, L.-W., Martins, A., Srikumar, V. (eds.) Proceedings of the 62nd\nAnnual Meeting of the Association for Computational Linguistics (Vol-\nume 1: Long Papers), pp. 941\u2013954. Association for Computational Linguis-\ntics, Bangkok, Thailand (2024). https://doi.org/10.18653/v1/2024.acl-long.53 .\nhttps://aclanthology.org/2024.acl-long.53/\n[55] Lu, J., Batra, D., Parikh, D., Lee, S.: Vilbert: Pretraining task-agnostic visi-\nolinguistic representations for vision-and-language tasks. Advances in neural\ninformation processing systems 32 (2019)\n26", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2540, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "39e23b5f-9fbd-463d-b1dc-f1f2fcf12038": {"__data__": {"id_": "39e23b5f-9fbd-463d-b1dc-f1f2fcf12038", "embedding": null, "metadata": {"page_label": "27", "file_name": "Foundations of GenIR.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Foundations of GenIR.pdf", "file_type": "application/pdf", "file_size": 699017, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a0b6ff7f-2024-45e0-98f1-1e9e02537530", "node_type": "4", "metadata": {"page_label": "27", "file_name": "Foundations of GenIR.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Foundations of GenIR.pdf", "file_type": "application/pdf", "file_size": 699017, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "758bba1995e55a5d202ffda5494dafdbede9c8196b08838ba85b705e04b1da01", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[56] Chen, Y.-C., Li, L., Yu, L., El Kholy, A., Ahmed, F., Gan, Z., Cheng, Y., Liu, J.:\nUniter: Universal image-text representation learning. In: European Conference\non Computer Vision, pp. 104\u2013120 (2020). Springer\n[57] Huang, Z., Zeng, Z., Liu, B., Fu, D., Fu, J.: Pixel-bert: Aligning image pixels\nwith text by deep multi-modal transformers. arXiv preprint arXiv:2004.00849\n(2020)\n[58] Wang, P., Yang, A., Men, R., Lin, J., Bai, S., Li, Z., Ma, J., Zhou, C., Zhou,\nJ., Yang, H.: Ofa: Unifying architectures, tasks, and modalities through a sim-\nple sequence-to-sequence learning framework. In: International Conference on\nMachine Learning, pp. 23318\u201323340 (2022). PMLR\n[59] Alayrac, J.-B., Donahue, J., Luc, P., Miech, A., Barr, I., Hasson, Y., Lenc, K.,\nMensch, A., Millican, K., Reynolds, M.,et al.: Flamingo: a visual language model\nfor few-shot learning. Advances in neural information processing systems 35,\n23716\u201323736 (2022)\n[60] Wang, W., Lv, Q., Yu, W., Hong, W., Qi, J., Wang, Y., Ji, J., Yang, Z., Zhao,\nL., Song, X., et al.: Cogvlm: Visual expert for pretrained language models. arXiv\npreprint arXiv:2311.03079 (2023)\n[61] Li, J., Li, D., Savarese, S., Hoi, S.: Blip-2: Bootstrapping language-image pre-\ntraining with frozen image encoders and large language models. In: International\nConference on Machine Learning, pp. 19730\u201319742 (2023). PMLR\n[62] Kim, W., Son, B., Kim, I.: Vilt: Vision-and-language transformer without convo-\nlution or region supervision. In: International Conference on Machine Learning,\npp. 5583\u20135594 (2021). PMLR\n[63] Li, J., Selvaraju, R., Gotmare, A., Joty, S., Xiong, C., Hoi, S.C.H.: Align before\nfuse: Vision and language representation learning with momentum distillation.\nAdvances in neural information processing systems 34, 9694\u20139705 (2021)\n[64] Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry,\nG., Askell, A., Mishkin, P., Clark, J., et al.: Learning transferable visual mod-\nels from natural language supervision. In: International Conference on Machine\nLearning, pp. 8748\u20138763 (2021). PMLR\n[65] Yu, T., Yao, Y., Zhang, H., He, T., Han, Y., Cui, G., Hu, J., Liu, Z., Zheng, H.-\nT., Sun, M., et al.: Rlhf-v: Towards trustworthy mllms via behavior alignment\nfrom fine-grained correctional human feedback. arXiv preprint arXiv:2312.00849\n(2023)\n[66] Bao, H., Dong, L., Piao, S., Wei, F.: Beit: Bert pre-training of image transform-\ners. arXiv preprint arXiv:2106.08254 (2021)\n27", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2455, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "0a7e44d4-c747-4d1a-b3fc-a6573bf9c2a6": {"__data__": {"id_": "0a7e44d4-c747-4d1a-b3fc-a6573bf9c2a6", "embedding": null, "metadata": {"page_label": "28", "file_name": "Foundations of GenIR.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Foundations of GenIR.pdf", "file_type": "application/pdf", "file_size": 699017, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c4f5e536-9a83-4c5f-8176-3303f5eaf92a", "node_type": "4", "metadata": {"page_label": "28", "file_name": "Foundations of GenIR.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Foundations of GenIR.pdf", "file_type": "application/pdf", "file_size": 699017, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "b3735f88a63cf35c7b2352c7d37c50ab1f702366bb1dba1df569688c9de9f8dc", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[67] Reed, S., Akata, Z., Yan, X., Logeswaran, L., Schiele, B., Lee, H.: Generative\nadversarial text to image synthesis. In: International Conference on Machine\nLearning, pp. 1060\u20131069 (2016). PMLR\n[68] Ramesh, A., Pavlov, M., Goh, G., Gray, S., Voss, C., Radford, A., Chen, M.,\nSutskever, I.: Zero-shot text-to-image generation. In: International Conference\non Machine Learning, pp. 8821\u20138831 (2021). Pmlr\n[69] Nichol, A., Dhariwal, P., Ramesh, A., Shyam, P., Mishkin, P., McGrew, B.,\nSutskever, I., Chen, M.: Glide: Towards photorealistic image generation and\nediting with text-guided diffusion models. arXiv preprint arXiv:2112.10741\n(2021)\n[70] Rombach, R., Blattmann, A., Lorenz, D., Esser, P., Ommer, B.: High-resolution\nimage synthesis with latent diffusion models. In: Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition, pp. 10684\u201310695\n(2022)\n[71] Ho, J., Jain, A., Abbeel, P.: Denoising diffusion probabilistic models. Advances\nin neural information processing systems 33, 6840\u20136851 (2020)\n[72] Zhang, C., Zhang, C., Zhang, M., Kweon, I.S.: Text-to-image diffusion model in\ngenerative ai: A survey. arXiv preprint arXiv:2303.07909 (2023)\n[73] Peebles, W., Xie, S.: Scalable diffusion models with transformers. In: Proceedings\nof the IEEE/CVF International Conference on Computer Vision, pp. 4195\u20134205\n(2023)\n[74] Singh, A.: A survey of ai text-to-image and ai text-to-video generators. In: 2023\n4th International Conference on Artificial Intelligence, Robotics and Control\n(AIRC), pp. 32\u201336 (2023). IEEE\n[75] Betker, J., Goh, G., Jing, L., Brooks, T., Wang, J., Li, L., Ouyang, L., Zhuang,\nJ., Lee, J., Guo, Y., et al.: Improving image generation with better captions.\nComputer Science. https://cdn. openai. com/papers/dall-e-3. pdf 2(3), 8 (2023)\n[76] Brooks, T., Peebles, B., Holmes, C., DePue, W., Guo, Y., Jing, L., Schnurr,\nD., Taylor, J., Luhman, T., Luhman, E., Ng, C., Wang, R., Ramesh, A.: Video\ngeneration models as world simulators (2024)\n[77] Oppenlaender, J.: A taxonomy of prompt modifiers for text-to-image generation.\nBehaviour & Information Technology, 1\u201314 (2023)\n[78] Liu, V., Chilton, L.B.: Design guidelines for prompt engineering text-to-image\ngenerative models. In: Proceedings of the 2022 CHI Conference on Human\nFactors in Computing Systems, pp. 1\u201323 (2022)\n28", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2326, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ce1a5097-643e-4633-8e68-e8ec7c8eece6": {"__data__": {"id_": "ce1a5097-643e-4633-8e68-e8ec7c8eece6", "embedding": null, "metadata": {"page_label": "29", "file_name": "Foundations of GenIR.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Foundations of GenIR.pdf", "file_type": "application/pdf", "file_size": 699017, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "af6125f2-2c55-40d5-aeb1-bf13c2fce76e", "node_type": "4", "metadata": {"page_label": "29", "file_name": "Foundations of GenIR.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Foundations of GenIR.pdf", "file_type": "application/pdf", "file_size": 699017, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "e1e019a8d26e4f49d155090897a0013d97f11edcfd0be902cbebf2069370f97e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[79] Hao, Y., Chi, Z., Dong, L., Wei, F.: Optimizing prompts for text-to-image\ngeneration, pp. 1\u201317. Curran Associates Inc., Red Hook, NY, USA (2024)\n[80] Ji, Z., Lee, N., Frieske, R., Yu, T., Su, D., Xu, Y., Ishii, E., Bang, Y.J., Madotto,\nA., Fung, P.: Survey of hallucination in natural language generation. ACM\nComputing Surveys 55(12), 1\u201338 (2023)\n[81] Arefeen, M.A., Debnath, B., Chakradhar, S.: Leancontext: Cost-efficient\ndomain-specific question answering using llms. Natural Language Processing\nJournal 7, 100065 (2024)\n[82] Aharoni, R., Goldberg, Y.: Unsupervised domain clusters in pretrained language\nmodels. arXiv preprint arXiv:2004.02105 (2020)\n[83] Li, H., Ai, Q., Chen, J., Dong, Q., Wu, Z., Liu, Y., Chen, C., Tian, Q.: Blade:\nEnhancing black-box large language models with small domain-specific models.\narXiv preprint arXiv:2403.18365 (2024)\n[84] Gao, Y., Xiong, Y., Gao, X., Jia, K., Pan, J., Bi, Y., Dai, Y., Sun, J., Wang,\nH.: Retrieval-augmented generation for large language models: A survey. arXiv\npreprint arXiv:2312.10997 (2023)\n[85] Lewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin, V., Goyal, N., K\u00a8 uttler,\nH., Lewis, M., Yih, W.-t., Rockt\u00a8 aschel, T., et al.: Retrieval-augmented gen-\neration for knowledge-intensive nlp tasks. Advances in Neural Information\nProcessing Systems 33, 9459\u20139474 (2020)\n[86] Lewis, M., Liu, Y., Goyal, N., Ghazvininejad, M., Mohamed, A., Levy, O., Stoy-\nanov, V., Zettlemoyer, L.: Bart: Denoising sequence-to-sequence pre-training for\nnatural language generation, translation, and comprehension. In: Proceedings of\nthe 58th Annual Meeting of the Association for Computational Linguistics, pp.\n7871\u20137880 (2020)\n[87] Moratanch, N., Chitrakala, S.: A survey on extractive text summarization.\nIn: 2017 International Conference on Computer, Communication and Signal\nProcessing (ICCCSP), pp. 1\u20136 (2017). IEEE\n[88] Lin, H., Ng, V.: Abstractive summarization: A survey of the state of the art.\nIn: Proceedings of the AAAI Conference on Artificial Intelligence, vol. 33, pp.\n9815\u20139822 (2019)\n[89] Bajaj, P., Campos, D., Craswell, N., Deng, L., Gao, J., Liu, X., Majumder, R.,\nMcNamara, A., Mitra, B., Nguyen, T., Rosenberg, M., Song, X., Stoica, A.,\nTiwary, S., Wang, T.: MS MARCO: A Human Generated MAchine Reading\nCOmprehension Dataset (2018)\n[90] Zhao, P., Zhang, H., Yu, Q., Wang, Z., Geng, Y., Fu, F., Yang, L., Zhang,\n29", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2386, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "56aab57c-0ba3-40ee-8bb8-225965c8e425": {"__data__": {"id_": "56aab57c-0ba3-40ee-8bb8-225965c8e425", "embedding": null, "metadata": {"page_label": "30", "file_name": "Foundations of GenIR.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Foundations of GenIR.pdf", "file_type": "application/pdf", "file_size": 699017, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5381dcf4-c1b0-499b-a167-54df327ca060", "node_type": "4", "metadata": {"page_label": "30", "file_name": "Foundations of GenIR.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Foundations of GenIR.pdf", "file_type": "application/pdf", "file_size": 699017, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "f5cbf5206c8574cdbc926ba99533e7f243efac565895dcf8d0768d13c358ae67", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "W., Cui, B.: Retrieval-augmented generation for ai-generated content: A survey.\narXiv preprint arXiv:2402.19473 (2024)\n[91] Asai, A., Min, S., Zhong, Z., Chen, D.: Retrieval-based language models and\napplications. In: Proceedings of the 61st Annual Meeting of the Association for\nComputational Linguistics (Volume 6: Tutorial Abstracts), pp. 41\u201346 (2023)\n[92] Guu, K., Lee, K., Tung, Z., Pasupat, P., Chang, M.: Retrieval augmented lan-\nguage model pre-training. In: International Conference on Machine Learning,\npp. 3929\u20133938 (2020). PMLR\n[93] Ma, X., Gong, Y., He, P., Zhao, H., Duan, N.: Query rewriting for retrieval-\naugmented large language models. arXiv preprint arXiv:2305.14283 (2023)\n[94] Wang, L., Ma, C., Feng, X., Zhang, Z., Yang, H., Zhang, J., Chen, Z., Tang, J.,\nChen, X., Lin, Y., et al.: A survey on large language model based autonomous\nagents. Frontiers of Computer Science 18(6), 186345 (2024)\n[95] Zhang, Z., Bo, X., Ma, C., Li, R., Chen, X., Dai, Q., Zhu, J., Dong, Z., Wen,\nJ.-R.: A Survey on the Memory Mechanism of Large Language Model based\nAgents (2024)\n[96] Zhan, J., Mao, J., Liu, Y., Guo, J., Zhang, M., Ma, S.: Optimizing dense retrieval\nmodel training with hard negatives. In: Proceedings of the 44th International\nACM SIGIR Conference on Research and Development in Information Retrieval,\npp. 1503\u20131512 (2021)\n[97] Robertson, S., Zaragoza, H., et al.: The probabilistic relevance framework: Bm25\nand beyond. Foundations and Trends\u00ae in Information Retrieval 3(4), 333\u2013389\n(2009)\n[98] Mao, S., Jiang, Y., Chen, B., Li, X., Wang, P., Wang, X., Xie, P., Huang, F.,\nChen, H., Zhang, N.: Rafe: Ranking feedback improves query rewriting for rag.\narXiv preprint arXiv:2405.14431 (2024)\n[99] Chan, C.-M., Xu, C., Yuan, R., Luo, H., Xue, W., Guo, Y., Fu, J.: Rq-rag:\nLearning to refine queries for retrieval augmented generation. arXiv preprint\narXiv:2404.00610 (2024)\n[100] Li, T., Zhang, G., Do, Q.D., Yue, X., Chen, W.: Long-context llms struggle with\nlong in-context learning. arXiv preprint arXiv:2404.02060 (2024)\n[101] Liu, N.F., Lin, K., Hewitt, J., Paranjape, A., Bevilacqua, M., Petroni, F., Liang,\nP.: Lost in the middle: How language models use long contexts. Transactions of\nthe Association for Computational Linguistics 12, 157\u2013173 (2024)\n[102] Faggioli, G., Dietz, L., Clarke, C.L., Demartini, G., Hagen, M., Hauff, C., Kando,\n30", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2368, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "31fc9742-1f40-4124-b401-9b8aa91dd598": {"__data__": {"id_": "31fc9742-1f40-4124-b401-9b8aa91dd598", "embedding": null, "metadata": {"page_label": "31", "file_name": "Foundations of GenIR.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Foundations of GenIR.pdf", "file_type": "application/pdf", "file_size": 699017, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5821c050-5ce0-47f9-adc9-b45e73016fe8", "node_type": "4", "metadata": {"page_label": "31", "file_name": "Foundations of GenIR.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Foundations of GenIR.pdf", "file_type": "application/pdf", "file_size": 699017, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "0429ba003a6b63164ddd550597d5de44978030e159095e3167837ddf619debf5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "N., Kanoulas, E., Potthast, M., Stein, B., et al.: Perspectives on large lan-\nguage models for relevance judgment. In: Proceedings of the 2023 ACM SIGIR\nInternational Conference on Theory of Information Retrieval, pp. 39\u201350 (2023)\n[103] Liu, P., Yuan, W., Fu, J., Jiang, Z., Hayashi, H., Neubig, G.: Pre-train, prompt,\nand predict: A systematic survey of prompting methods in natural language\nprocessing. ACM Comput. Surv. 55(9) (2023) https://doi.org/10.1145/3560815\n[104] Wang, X., Yang, Q., Qiu, Y., Liang, J., He, Q., Gu, Z., Xiao, Y., Wang, W.:\nKnowledgpt: Enhancing large language models with retrieval and storage access\non knowledge bases. arXiv preprint arXiv:2308.11761 (2023)\n[105] Qin, Y., Hu, S., Lin, Y., Chen, W., Ding, N., Cui, G., Zeng, Z., Huang, Y., Xiao,\nC., Han, C., Fung, Y.R., Su, Y., Wang, H., Qian, C., Tian, R., Zhu, K., Liang,\nS., Shen, X., Xu, B., Zhang, Z., Ye, Y., Li, B., Tang, Z., Yi, J., Zhu, Y., Dai,\nZ., Yan, L., Cong, X., Lu, Y., Zhao, W., Huang, Y., Yan, J., Han, X., Sun, X.,\nLi, D., Phang, J., Yang, C., Wu, T., Ji, H., Liu, Z., Sun, M.: Tool Learning with\nFoundation Models (2023)\n[106] Jiang, Z., Xu, F., Gao, L., Sun, Z., Liu, Q., Dwivedi-Yu, J., Yang, Y., Callan,\nJ., Neubig, G.: Active retrieval augmented generation. In: Bouamor, H., Pino,\nJ., Bali, K. (eds.) Proceedings of the 2023 Conference on Empirical Methods\nin Natural Language Processing, pp. 7969\u20137992. Association for Computational\nLinguistics, Singapore (2023). https://doi.org/10.18653/v1/2023.emnlp-main.\n495 . https://aclanthology.org/2023.emnlp-main.495\n[107] Ram, O., Levine, Y., Dalmedigos, I., Muhlgay, D., Shashua, A., Leyton-Brown,\nK., Shoham, Y.: In-context retrieval-augmented language models. Transactions\nof the Association for Computational Linguistics 11, 1316\u20131331 (2023)\n[108] Borgeaud, S., Mensch, A., Hoffmann, J., Cai, T., Rutherford, E., Millican, K.,\nVan Den Driessche, G.B., Lespiau, J.-B., Damoc, B., Clark, A., et al.: Improv-\ning language models by retrieving from trillions of tokens. In: International\nConference on Machine Learning, pp. 2206\u20132240 (2022). PMLR\n[109] Trivedi, H., Balasubramanian, N., Khot, T., Sabharwal, A.: Interleaving retrieval\nwith chain-of-thought reasoning for knowledge-intensive multi-step questions.\nIn: Rogers, A., Boyd-Graber, J., Okazaki, N. (eds.) Proceedings of the 61st\nAnnual Meeting of the Association for Computational Linguistics (Volume\n1: Long Papers), pp. 10014\u201310037. Association for Computational Linguis-\ntics, Toronto, Canada (2023). https://doi.org/10.18653/v1/2023.acl-long.557 .\nhttps://aclanthology.org/2023.acl-long.557\n[110] Ni, S., Bi, K., Guo, J., Cheng, X.: When do llms need retrieval augmentation?\nmitigating llms\u2019 overconfidence helps retrieval augmentation. arXiv preprint\narXiv:2402.11457 (2024)\n31", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2791, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5abca84e-f967-4f7c-b65e-a3d7987552a9": {"__data__": {"id_": "5abca84e-f967-4f7c-b65e-a3d7987552a9", "embedding": null, "metadata": {"page_label": "32", "file_name": "Foundations of GenIR.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Foundations of GenIR.pdf", "file_type": "application/pdf", "file_size": 699017, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6edf2408-9b59-4f48-9b3a-ca94afa1c417", "node_type": "4", "metadata": {"page_label": "32", "file_name": "Foundations of GenIR.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Foundations of GenIR.pdf", "file_type": "application/pdf", "file_size": 699017, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "3b3a19adde44f22742e7d98173413c3cb8ce466d7d4528ad39fc445ac9e55390", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[111] Su, W., Tang, Y., Ai, Q., Wu, Z., Liu, Y.: DRAGIN: Dynamic retrieval aug-\nmented generation based on the real-time information needs of large language\nmodels. In: Ku, L.-W., Martins, A., Srikumar, V. (eds.) Proceedings of the\n62nd Annual Meeting of the Association for Computational Linguistics (Volume\n1: Long Papers), pp. 12991\u201313013. Association for Computational Linguis-\ntics, Bangkok, Thailand (2024). https://doi.org/10.18653/v1/2024.acl-long.702\n. https://aclanthology.org/2024.acl-long.702/\n[112] Su, W., Wang, C., Ai, Q., Hu, Y., Wu, Z., Zhou, Y., Liu, Y.: Unsupervised\nreal-time hallucination detection based on the internal states of large language\nmodels. In: Ku, L.-W., Martins, A., Srikumar, V. (eds.) Findings of the Asso-\nciation for Computational Linguistics: ACL 2024, pp. 14379\u201314391. Association\nfor Computational Linguistics, Bangkok, Thailand (2024). https://doi.org/10.\n18653/v1/2024.findings-acl.854 . https://aclanthology.org/2024.findings-acl.854/\n[113] Liu, T., Zhang, Y., Brockett, C., Mao, Y., Sui, Z., Chen, W., Dolan, B.: A\ntoken-level reference-free hallucination detection benchmark for free-form text\ngeneration. arXiv preprint arXiv:2104.08704 (2021)\n[114] Fadeeva, E., Rubashevskii, A., Shelmanov, A., Petrakov, S., Li, H., Mubarak, H.,\nTsymbalov, E., Kuzmin, G., Panchenko, A., Baldwin, T., et al.: Fact-checking\nthe output of large language models via token-level uncertainty quantification.\narXiv preprint arXiv:2403.04696 (2024)\n[115] Cronen-Townsend, S., Croft, W.B., et al.: Quantifying query ambiguity. In:\nProceedings of HLT, vol. 2, pp. 94\u201398 (2002)\n[116] ARENS, Y., CHEE, C.Y., HSU, C.-N., KNOBLOCK, C.A.: Retrieving and\nintegrating data from multiple information sources. International Journal of\nCooperative Information Systems 02(02), 127\u2013158 (1993) https://doi.org/10.\n1142/S0218215793000071 https://doi.org/10.1142/S0218215793000071\n[117] Wang, J., Mo, F., Ma, W., Sun, P., Zhang, M., Nie, J.-Y.: A User-Centric\nBenchmark for Evaluating Large Language Models (2024)\n[118] Wang, J., Ma, W., Sun, P., Zhang, M., Nie, J.-Y.: Understanding User\nExperience in Large Language Model Interactions (2024)\n[119] Beitzel, S.M., Jensen, E.C., Chowdhury, A., Grossman, D., Frieder, O., Gohar-\nian, N.: Fusion of effective retrieval strategies in the same information retrieval\nsystem. Journal of the American Society for Information Science and Technology\n55(10), 859\u2013868 (2004)\n[120] Wu, S., McClean, S.: Performance prediction of data fusion for information\nretrieval. Information processing & management 42(4), 899\u2013915 (2006)\n32", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2576, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "7abe67c2-2431-495d-8fb4-cd8b18963119": {"__data__": {"id_": "7abe67c2-2431-495d-8fb4-cd8b18963119", "embedding": null, "metadata": {"page_label": "33", "file_name": "Foundations of GenIR.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Foundations of GenIR.pdf", "file_type": "application/pdf", "file_size": 699017, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5b0dbe2e-512b-4045-940d-f1960e5272d4", "node_type": "4", "metadata": {"page_label": "33", "file_name": "Foundations of GenIR.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Foundations of GenIR.pdf", "file_type": "application/pdf", "file_size": 699017, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "dd82492eb2cb1cf8148b456ebe69518dc623e34e48d8788e4311186824e1a2eb", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[121] Cormack, G.V., Clarke, C.L., Buettcher, S.: Reciprocal rank fusion outper-\nforms condorcet and individual rank learning methods. In: Proceedings of the\n32nd International ACM SIGIR Conference on Research and Development in\nInformation Retrieval, pp. 758\u2013759 (2009)\n[122] Lee, C.-J., Ai, Q., Croft, W.B., Sheldon, D.: An optimization framework for\nmerging multiple result lists. In: Proceedings of the 24th ACM International on\nConference on Information and Knowledge Management, pp. 303\u2013312 (2015)\n[123] Liu, T.-Y., et al.: Learning to rank for information retrieval. Foundations and\nTrends\u00ae in Information Retrieval 3(3), 225\u2013331 (2009)\n[124] Zhan, J., Mao, J., Liu, Y., Zhang, M., Ma, S.: Learning to retrieve: How to train a\ndense retrieval model effectively and efficiently. arXiv preprint arXiv:2010.10469\n(2020)\n[125] Arora, D., Kini, A., Chowdhury, S.R., Natarajan, N., Sinha, G., Sharma,\nA.: Gar-meets-rag paradigm for zero-shot information retrieval. arXiv preprint\narXiv:2310.20158 (2023)\n[126] Zhang, T., Patil, S.G., Jain, N., Shen, S., Zaharia, M., Stoica, I., Gonzalez, J.E.:\nRAFT: Adapting Language Model to Domain Specific RAG (2024)\n[127] Xu, Z., Tran, A., Yang, T., Ai, Q.: Reinforcement learning to rank with coarse-\ngrained labels. arXiv preprint arXiv:2208.07563 (2022)\n[128] Shi, W., Min, S., Yasunaga, M., Seo, M., James, R., Lewis, M., Zettlemoyer,\nL., Yih, W.-t.: Replug: Retrieval-augmented black-box language models. arXiv\npreprint arXiv:2301.12652 (2023)\n[129] Ai, Q., Bai, T., Cao, Z., Chang, Y., Chen, J., Chen, Z., Cheng, Z., Dong, S.,\nDou, Z., Feng, F., et al.: Information retrieval meets large language models: a\nstrategic report from chinese ir community. AI Open 4, 80\u201390 (2023)\n[130] Bota, H., Zhou, K., Jose, J.M., Lalmas, M.: Composite retrieval of heterogeneous\nweb search. In: Proceedings of the 23rd International Conference on World Wide\nWeb, pp. 119\u2013130 (2014)\n[131] Amer-Yahia, S., Bonchi, F., Castillo, C., Feuerstein, E., Mendez-Diaz, I.,\nZabala, P.: Composite retrieval of diverse and complementary bundles. IEEE\nTransactions on Knowledge and Data Engineering 26(11), 2662\u20132675 (2014)\n[132] Kolomiyets, O., Moens, M.-F.: A survey on question answering technology from\nan information retrieval perspective. Information Sciences 181(24), 5412\u20135434\n(2011)\n[133] Metzler, D., Tay, Y., Bahri, D., Najork, M.: Rethinking search: making domain\n33", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2393, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5a1da780-6669-4049-8874-ed80e232a40c": {"__data__": {"id_": "5a1da780-6669-4049-8874-ed80e232a40c", "embedding": null, "metadata": {"page_label": "34", "file_name": "Foundations of GenIR.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Foundations of GenIR.pdf", "file_type": "application/pdf", "file_size": 699017, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8692766b-4ae5-4305-9005-2f1ab011afd7", "node_type": "4", "metadata": {"page_label": "34", "file_name": "Foundations of GenIR.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Foundations of GenIR.pdf", "file_type": "application/pdf", "file_size": 699017, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "e80d057f9181ea88f4e3f92980c8dd638422731b1f3a6dd025f373a6a047fba6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "experts out of dilettantes. SIGIR Forum 55(1) (2021) https://doi.org/10.1145/\n3476415.3476428\n[134] Zhuang, S., Ren, H., Shou, L., Pei, J., Gong, M., Zuccon, G., Jiang, D.: Bridging\nthe Gap Between Indexing and Retrieval for Differentiable Search Index with\nQuery Generation (2023). https://arxiv.org/abs/2206.10128\n[135] Tay, Y., Tran, V., Dehghani, M., Ni, J., Bahri, D., Mehta, H., Qin, Z., Hui, K.,\nZhao, Z., Gupta, J., et al.: Transformer memory as a differentiable search index.\nAdvances in Neural Information Processing Systems 35, 21831\u201321843 (2022)\n[136] Tang, Y., Zhang, R., Guo, J., Chen, J., Zhu, Z., Wang, S., Yin, D., Cheng, X.:\nSemantic-enhanced differentiable search index inspired by learning strategies.\nIn: Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery\nand Data Mining, pp. 4904\u20134913 (2023)\n[137] Sun, W., Yan, L., Chen, Z., Wang, S., Zhu, H., Ren, P., Chen, Z., Yin, D., Rijke,\nM., Ren, Z.: Learning to tokenize for generative retrieval. In: Proceedings of the\n37th International Conference on Neural Information Processing Systems. NIPS\n\u201923, pp. 1\u201317. Curran Associates Inc., Red Hook, NY, USA (2024)\n[138] Nguyen, T., Yates, A.: Generative retrieval as dense retrieval. arXiv preprint\narXiv:2306.11397 (2023)\n[139] Zhan, J., Mao, J., Liu, Y., Guo, J., Zhang, M., Ma, S.: Learning discrete repre-\nsentations via constrained clustering for effective and efficient dense retrieval. In:\nProceedings of the Fifteenth ACM International Conference on Web Search and\nData Mining. WSDM \u201922, pp. 1328\u20131336. Association for Computing Machin-\nery, New York, NY, USA (2022). https://doi.org/10.1145/3488560.3498443 .\nhttps://doi.org/10.1145/3488560.3498443\n[140] Zeng, H., Luo, C., Jin, B., Sarwar, S.M., Wei, T., Zamani, H.: Scalable and\neffective generative information retrieval. In: Proceedings of the ACM on Web\nConference 2024. WWW \u201924, pp. 1441\u20131452. Association for Computing Machin-\nery, New York, NY, USA (2024). https://doi.org/10.1145/3589334.3645477 .\nhttps://doi.org/10.1145/3589334.3645477\n[141] Zeng, H., Luo, C., Zamani, H.: Planning Ahead in Generative Retrieval: Guiding\nAutoregressive Generation through Simultaneous Decoding (2024)\n[142] Wu, S., Wei, W., Zhang, M., Chen, Z., Ma, J., Ren, Z., Rijke, M., Ren,\nP.: Generative retrieval as multi-vector dense retrieval. In: Proceedings of the\n47th International ACM SIGIR Conference on Research and Development in\nInformation Retrieval, pp. 1828\u20131838 (2024)\n[143] Zhan, J., Mao, J., Liu, Y., Guo, J., Zhang, M., Ma, S.: Jointly optimizing query\n34", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2547, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "320ee3c1-0dc6-43d2-84d8-f6ca288b1f0a": {"__data__": {"id_": "320ee3c1-0dc6-43d2-84d8-f6ca288b1f0a", "embedding": null, "metadata": {"page_label": "35", "file_name": "Foundations of GenIR.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Foundations of GenIR.pdf", "file_type": "application/pdf", "file_size": 699017, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8757888-f049-404c-8c99-07bc1e9bfb24", "node_type": "4", "metadata": {"page_label": "35", "file_name": "Foundations of GenIR.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Foundations of GenIR.pdf", "file_type": "application/pdf", "file_size": 699017, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "44e360069f89df9ae88fd204d3c9e7c72684dab5df4f6ebb1e79d118ec174b1d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "encoder and product quantization to improve retrieval performance. In: Proceed-\nings of the 30th ACM International Conference on Information & Knowledge\nManagement. CIKM \u201921, pp. 2487\u20132496. Association for Computing Machin-\nery, New York, NY, USA (2021). https://doi.org/10.1145/3459637.3482358 .\nhttps://doi.org/10.1145/3459637.3482358\n[144] Sachidananda, V., Kessler, J.S., Lai, Y.-A.: Efficient domain adaptation of\nlanguage models via adaptive tokenization. arXiv preprint arXiv:2109.07460\n(2021)\n[145] Huang, Q., Tao, M., Zhang, C., An, Z., Jiang, C., Chen, Z., Wu, Z., Feng, Y.:\nLawyer llama technical report. arXiv preprint arXiv:2305.15062 (2023)\n[146] Cui, J., Li, Z., Yan, Y., Chen, B., Yuan, L.: Chatlaw: Open-source legal\nlarge language model with integrated external knowledge bases. arXiv preprint\narXiv:2306.16092 (2023)\n[147] Wu, S., Irsoy, O., Lu, S., Dabravolski, V., Dredze, M., Gehrmann, S., Kambadur,\nP., Rosenberg, D., Mann, G.: Bloomberggpt: A large language model for finance.\narXiv preprint arXiv:2303.17564 (2023)\n[148] Dai, D., Dong, L., Hao, Y., Sui, Z., Chang, B., Wei, F.: Knowledge neurons in\npretrained transformers. arXiv preprint arXiv:2104.08696 (2021)\n[149] Meng, K., Bau, D., Andonian, A., Belinkov, Y.: Locating and editing factual\nassociations in gpt. Advances in Neural Information Processing Systems 35,\n17359\u201317372 (2022)\n[150] Liu, J., Yu, P., Zhang, Y., Li, S., Zhang, Z., Ji, H.: Evedit: Event-based knowl-\nedge editing with deductive editing boundaries. arXiv preprint arXiv:2402.11324\n(2024)\n[151] Hu, E.J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L.,\nChen, W.: Lora: Low-rank adaptation of large language models. arXiv preprint\narXiv:2106.09685 (2021)\n[152] Li, H., Ai, Q., Chen, J., Dong, Q., Wu, Y., Liu, Y., Chen, C., Tian, Q.: Sailer:\nstructure-aware pre-trained language model for legal case retrieval. In: Pro-\nceedings of the 46th International ACM SIGIR Conference on Research and\nDevelopment in Information Retrieval, pp. 1035\u20131044 (2023)\n[153] Yao, F., Li, C., Nekipelov, D., Wang, H., Xu, H.: Human vs. Generative AI in\nContent Creation Competition: Symbiosis or Conflict? (2024)\n35", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2162, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "27415eb5-3f0a-4bba-96f3-d8faa6592550": {"__data__": {"id_": "27415eb5-3f0a-4bba-96f3-d8faa6592550", "embedding": null, "metadata": {"page_label": "1", "file_name": "GR-NLP-TOOLKIT An Open-Source NLP Toolkit for Modern Greek.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\GR-NLP-TOOLKIT An Open-Source NLP Toolkit for Modern Greek.pdf", "file_type": "application/pdf", "file_size": 556151, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "91966957-7747-45bd-a03f-d0c43453dedb", "node_type": "4", "metadata": {"page_label": "1", "file_name": "GR-NLP-TOOLKIT An Open-Source NLP Toolkit for Modern Greek.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\GR-NLP-TOOLKIT An Open-Source NLP Toolkit for Modern Greek.pdf", "file_type": "application/pdf", "file_size": 556151, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "608c108d59507acac8c3f72afdf987cb7815571634e3f6daef1a50675693a004", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0a59d981-25f3-4ae7-aece-52b512e55688", "node_type": "1", "metadata": {}, "hash": "c47aa989ce3c5185a54a985fd074c08f80262e010ac1420794f90f7d6fabf704", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "GR-NLP -TOOLKIT : An Open-Source NLP Toolkit for Modern Greek\nLefteris Loukas1,3, Nikolaos Smyrnioudis1, Chrysa Dikonomaki1, Spyros Barbakos1,\nAnastasios Toumazatos1, John Koutsikakis1, Manolis Kyriakakis1, Mary Georgiou1,\nStavros Vassos3, John Pavlopoulos1,2, Ion Androutsopoulos1,2\n1Department of Informatics, Athens University of Economics and Business, Greece\n2Archimedes/Athena RC, Greece\n3helvia.ai\nAbstract\nWe present GR-NLP -TOOLKIT , an open-source\nnatural language processing (NLP ) toolkit de-\nveloped specifically for modern Greek. The\ntoolkit provides state-of-the-art performance in\nfive core NLP tasks, namely part-of-speech tag-\nging, morphological tagging, dependency pars-\ning, named entity recognition, and Greeklish-\nto-Greek transliteration. The toolkit is based\non pre-trained Transformers, it is freely avail-\nable, and can be easily installed in Python\n(pip install gr-nlp-toolkit). It is also\naccessible through a demonstration platform on\nHuggingFace, along with a publicly available\nAPI for non-commercial use. We discuss the\nfunctionality provided for each task, the under-\nlying methods, experiments against comparable\nopen-source toolkits, and future possible en-\nhancements. The toolkit is available at: https:\n//github.com/nlpaueb/gr-nlp-toolkit\n1 Introduction\nModern Greek is the official language of Greece,\none of the two official languages of Cyprus, and\nthe native language of approximately 13 million\npeople.1 Despite continuous efforts (Papantoniou\nand Tzitzikas, 2020; Bakagianni et al., 2024), there\nare still very few natural language processing (NLP )\ntoolkits that support modern Greek (\u00a72).\nWe present GR-NLP -TOOLKIT , an open-source\nNLP toolkit developed specifically for modern\nGreek. The toolkit supports five core NLP tasks,\nnamely part-of-speech ( POS ) tagging, morpho-\nlogical tagging (tagging for tense, voice, person,\ngender, case, number etc.), dependency parsing,\nnamed entity recognition ( NER ), and Greeklish-\nto-Greek transliteration (converting Greek written\nusing Latin-keyboard characters to the Greek al-\nphabet). We demonstrate the functionality that the\ntoolkit provides per task (\u00a73). We also discuss\n1https://en.wikipedia.org/wiki/Greek_language\nthe underlying methods and experimentally com-\npare GR-NLP -TOOLKIT to STANZA (Qi et al., 2020)\nand SPACY (Honnibal et al., 2020), two multilin-\ngual toolkits that support modern Greek, demon-\nstrating that GR-NLP -TOOLKIT achieves state-of-\nthe-art performance in POS tagging, morphological\ntagging, dependency parsing, and NER (\u00a74). Previ-\nous work (Toumazatos et al., 2024) shows that the\nGreeklish-to-Greek converter included in GR-NLP -\nTOOLKIT is also state-of-the-art.\nThe toolkit can be easily installed in Python\nvia PYPI (pip install gr-nlp-toolkit) and its\ncode is publicly available on Github.2 We showcase\nits functionality in an open-access demonstration\nspace, hosted on HuggingFace.3 We also release\nGREEK -NLP -API , a fully-documented and publicly\navailable HTTP API , which allows using the toolkit\nin (non-commercial) applications developed in any\nprogramming language.4\n2 Background and related work\nGreek has evolved over three millennia. 5 Apart\nfrom its historical interest, Greek is also challeng-\ning from an NLP point of view. For example, it\nhas its own alphabet ( \u03b1,\u03b2,\u03b3,..), and nowadays a\nmuch smaller number of speakers, compared to\nother widely used languages of the modern world.\nAlthough words of Greek origin can be found in\nmany other languages (e.g., medical terms), they\nare written in different alphabets in other languages.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3578, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "0a59d981-25f3-4ae7-aece-52b512e55688": {"__data__": {"id_": "0a59d981-25f3-4ae7-aece-52b512e55688", "embedding": null, "metadata": {"page_label": "1", "file_name": "GR-NLP-TOOLKIT An Open-Source NLP Toolkit for Modern Greek.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\GR-NLP-TOOLKIT An Open-Source NLP Toolkit for Modern Greek.pdf", "file_type": "application/pdf", "file_size": 556151, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "91966957-7747-45bd-a03f-d0c43453dedb", "node_type": "4", "metadata": {"page_label": "1", "file_name": "GR-NLP-TOOLKIT An Open-Source NLP Toolkit for Modern Greek.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\GR-NLP-TOOLKIT An Open-Source NLP Toolkit for Modern Greek.pdf", "file_type": "application/pdf", "file_size": 556151, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "608c108d59507acac8c3f72afdf987cb7815571634e3f6daef1a50675693a004", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "27415eb5-3f0a-4bba-96f3-d8faa6592550", "node_type": "1", "metadata": {"page_label": "1", "file_name": "GR-NLP-TOOLKIT An Open-Source NLP Toolkit for Modern Greek.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\GR-NLP-TOOLKIT An Open-Source NLP Toolkit for Modern Greek.pdf", "file_type": "application/pdf", "file_size": 556151, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "a682da09e1c419872714499d34d36b941871ea5c4b42ebea4151893f0dd1d53c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "5 Apart\nfrom its historical interest, Greek is also challeng-\ning from an NLP point of view. For example, it\nhas its own alphabet ( \u03b1,\u03b2,\u03b3,..), and nowadays a\nmuch smaller number of speakers, compared to\nother widely used languages of the modern world.\nAlthough words of Greek origin can be found in\nmany other languages (e.g., medical terms), they\nare written in different alphabets in other languages.\nHence, Greek words written in the Greek alphabet\nare severely under-represented in modern multilin-\ngual corpora and, consequently, in the word and\nsub-word vocabularies of most multilingual Trans-\nformer models, e.g., XLM -R (Conneau et al., 2020).\n2https://github.com/nlpaueb/gr-nlp-toolkit/\n3https://huggingface.co/spaces/AUEB-NLP/\ngreek-nlp-toolkit-demo\n4https://huggingface.co/spaces/AUEB-NLP/\nThe-Greek-NLP-API/\n5www.britannica.com/topic/Greek-language\n1\narXiv:2412.08520v1  [cs.CL]  11 Dec 2024", "mimetype": "text/plain", "start_char_idx": 3176, "end_char_idx": 4080, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b8ff471f-e757-4b86-b535-6c994edcfcdb": {"__data__": {"id_": "b8ff471f-e757-4b86-b535-6c994edcfcdb", "embedding": null, "metadata": {"page_label": "2", "file_name": "GR-NLP-TOOLKIT An Open-Source NLP Toolkit for Modern Greek.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\GR-NLP-TOOLKIT An Open-Source NLP Toolkit for Modern Greek.pdf", "file_type": "application/pdf", "file_size": 556151, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "74eb1b8c-9d9e-4397-be86-3f3612bedf83", "node_type": "4", "metadata": {"page_label": "2", "file_name": "GR-NLP-TOOLKIT An Open-Source NLP Toolkit for Modern Greek.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\GR-NLP-TOOLKIT An Open-Source NLP Toolkit for Modern Greek.pdf", "file_type": "application/pdf", "file_size": 556151, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "c0516a0e99ee7e456e125a18cd8e64138002effdf84840feba7dfa31dcc6e080", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3c5315f1-3832-44df-924b-85f89aa9ea0d", "node_type": "1", "metadata": {}, "hash": "d6d288123a24c535e895fbaa6ba4642c0a62bbf5caddd772e4aec5558162743b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This causes the tokenizers of these models to over-\nfragment Greek words, very often to characters\n(Koutsikakis et al., 2020), which increases process-\ning time and cost, and makes it more difficult for\nmodels to reassemble tokens to more meaningful\nunits. Greek is also highly inflected (e.g., differ-\nent verb forms for different tenses, voices, moods,\npersons, numbers; similarly for nouns, adjectives,\npronouns etc.), which makes POS tagging more dif-\nficult and morphological tagging (tagging also for\ntense, voice, gender, case etc.) desirable. Greek\nis also flexible in word order (e.g., subject-verb-\nobject, object-verb-subject, verb-subject-object etc.\nare all possible with different emphasis), which\nmakes parsing more challenging.\nModern Greek is normally written in the Greek\nalphabet. In online messages, however, especially\ninformal email and chat, it is often written using\ncharacters available on Latin-character keyboards,\na form known as Greeklish (Koutsogiannis and\nMitsikopoulou, 2017). For example, \u2018\u03c9\u2019 (omega)\nmay be written as \u2018w\u2019 based on visual similarity,\nas \u2018o\u2019 based on phonetic similarity, or as \u2018v\u2019 based\non the fact that \u2018 \u03c9\u2019 and \u2018v\u2019 use the same key on\nGreek-Latin keyboards, to mention just some pos-\nsibilities. Greeklish was originally used in older\ncomputers that did not support the Greek alpha-\nbet, but continues to be used to avoid switching\nlanguages on multilingual keyboards, hide spelling\nmistakes (esp. when used by non-native speakers),\nor as a form of slang (mostly by younger people).\nThere is no consensus mapping between Greek\nand Latin-keyboard characters.6 Consequently, the\nsame Greek word can be written in numerous differ-\nent ways in Greeklish (Fig. 1). Even native Greek\nspeakers may struggle to understand, and are of-\nten annoyed by Greeklish, which requires paying\ncareful attention to context to decipher. Moreover,\nmost Greek NLP datasets contain text written in\nthe Greek alphabet, hence models trained on those\ndatasets may be unable to handle Greeklish.\nPhenomena of this kind motivated the develop-\nment of GREEK -BERT (Koutsikakis et al., 2020),\nand more recently the MELTEMI large language\nmodel (LLM ) for modern Greek (V oukoutis et al.,\n2024); the latter is based on MISTRAL -7b (Jiang\net al., 2023). In this work, we leverage GREEK -\nBERT for most tasks, and BYT 5 (Xue et al., 2022)\nfor Greeklish-to-Greek, which can both be used\n6The ISO 843:1997 standard ( https://www.iso.org/\nstandard/5215.html) is almost never used.\nFigure 1: An example of a Greek sentence written in\nGreeklish. There is no consensus mapping. Greek char-\nacters may be replaced by Latin-keyboard characters\nbased on visual similarity, phonetic similarity, shared\nkeys etc. Figure from Toumazatos et al. (2024).\neven with CPU only, unlike larger LLM s (Luccioni\net al., 2024). Nevertheless, in future versions of the\ntoolkit, we plan to investigate how we can integrate\n\u2018small\u2019 Greek LLM s for on-device use.7\nIn previous modern Greek experiments, GREEK -\nBERT , when fine-tuned, was reported to outperform\nthe multilingual XLM -R, again fine-tuned, in NER\nand natural language inference, while it performed\non par with XLM -R in POS tagging (Koutsikakis\net al., 2020). In subsequent work of two under-\ngraduate theses (Dikonimaki, 2021; Smyrnioudis,\n2021), we showed, again using modern Greek data,\nthat GREEK -BERT largely outperformed XLM -R in\ndependency parsing, but found no substantial dif-\nference between the two models in morphological\ntagging and (another dataset of) NER . Greeklish\nwas not considered in any of these previous stud-\nies.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3596, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3c5315f1-3832-44df-924b-85f89aa9ea0d": {"__data__": {"id_": "3c5315f1-3832-44df-924b-85f89aa9ea0d", "embedding": null, "metadata": {"page_label": "2", "file_name": "GR-NLP-TOOLKIT An Open-Source NLP Toolkit for Modern Greek.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\GR-NLP-TOOLKIT An Open-Source NLP Toolkit for Modern Greek.pdf", "file_type": "application/pdf", "file_size": 556151, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "74eb1b8c-9d9e-4397-be86-3f3612bedf83", "node_type": "4", "metadata": {"page_label": "2", "file_name": "GR-NLP-TOOLKIT An Open-Source NLP Toolkit for Modern Greek.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\GR-NLP-TOOLKIT An Open-Source NLP Toolkit for Modern Greek.pdf", "file_type": "application/pdf", "file_size": 556151, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "c0516a0e99ee7e456e125a18cd8e64138002effdf84840feba7dfa31dcc6e080", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b8ff471f-e757-4b86-b535-6c994edcfcdb", "node_type": "1", "metadata": {"page_label": "2", "file_name": "GR-NLP-TOOLKIT An Open-Source NLP Toolkit for Modern Greek.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\GR-NLP-TOOLKIT An Open-Source NLP Toolkit for Modern Greek.pdf", "file_type": "application/pdf", "file_size": 556151, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "d0d0d5077bab42a50ab1ca50b0e4984cd473eb3151faf0b7d34f690a4831e4d4", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In subsequent work of two under-\ngraduate theses (Dikonimaki, 2021; Smyrnioudis,\n2021), we showed, again using modern Greek data,\nthat GREEK -BERT largely outperformed XLM -R in\ndependency parsing, but found no substantial dif-\nference between the two models in morphological\ntagging and (another dataset of) NER . Greeklish\nwas not considered in any of these previous stud-\nies. The two theses also created a first version of\nGR-NLP -TOOLKIT , which was largely experimen-\ntal, did not include Greeklish-to-Greek, and was\nnot published (apart from the two theses). The\nversion of the toolkit that we introduce here has\nbeen completely refactored, it uses more recent li-\nbraries, has been tested more thoroughly, includes\nGreeklish-to-Greek, can be used via both PYPI and\nGREEK -NLP -API , and can also be explored via a\nHuggingFace demo (\u00a71).\nSPACY (Honnibal et al., 2020) and STANZA (Qi\net al., 2020) are widely used multilingualNLP toolk-\nits that support modern Greek. They both have\nlimitations, however, discussed below (Table 1).\n7For example, Meta recently released 1B and 3B\nLLM s for on-device use: https://ai.meta.com/blog/\nllama-3-2-connect-2024-vision-edge-mobile-devices/\n2", "mimetype": "text/plain", "start_char_idx": 3217, "end_char_idx": 4405, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c51b41cf-f6e8-4090-b421-6968114f6eb5": {"__data__": {"id_": "c51b41cf-f6e8-4090-b421-6968114f6eb5", "embedding": null, "metadata": {"page_label": "3", "file_name": "GR-NLP-TOOLKIT An Open-Source NLP Toolkit for Modern Greek.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\GR-NLP-TOOLKIT An Open-Source NLP Toolkit for Modern Greek.pdf", "file_type": "application/pdf", "file_size": 556151, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "80855d06-ef81-4500-9c04-40b832e4221d", "node_type": "4", "metadata": {"page_label": "3", "file_name": "GR-NLP-TOOLKIT An Open-Source NLP Toolkit for Modern Greek.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\GR-NLP-TOOLKIT An Open-Source NLP Toolkit for Modern Greek.pdf", "file_type": "application/pdf", "file_size": 556151, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "823e3db69d2c9a77235f99789810d2343256d3ae4de01a4f3aeb0ebc8b8e26ea", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d57973d9-67ab-4e57-8c6a-e75a8fbe5858", "node_type": "1", "metadata": {}, "hash": "d070d2cca683c5dfee1f998eba523a39682408e018f287ab1ead5ba21eb5d10f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Toolkit POS\nTagging\nMorphological\nTagging\nLemma-\ntization\nNamed Entity\nRecognition\nDependency\nParsing\nGreeklish-to-Greek\nTransliteration\nSPACY \u2713 \u2713 \u2713 \u2713 \u2713 \u2717\nSTANZA \u2713 \u2713 \u2713 \u2717 \u2713 \u2717\nNLTK \u2717 \u2717 \u2717 \u2717 \u2717 \u2717\nGR-NLP -TOOLKIT \u2713 \u2713 \u2713 \u2713 \u2717 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713\nTable 1: Comparison of NLP toolkits that support modern Greek. NLTK provides only a tokenizer and stop-word\nremoval (not shown) for modern Greek. SPACY and STANZA both include a Greek lemmatizer. GR-NLP -TOOLKIT\nis the only one that includes Greeklish-to-Greek transliteration. Its other functions (POS tagging, morphological\ntagging, NER , dependency parsing) are based on GREEK -BERT , whereas SPACY and STANZA are based on Greek\nFASTTEXT embeddings and do not use Transformers. \u2713\u2713 denotes using pretrained Transformers.\nSPACY (Honnibal et al., 2020) is an open-source\nNLP library for efficient processing of text in many\nlanguages. In modern Greek, it supports POS tag-\nging, morphological tagging, lemmatization (map-\nping all inflected forms of verbs, nouns etc. to their\nbase forms), NER , and dependency parsing. How-\never, it relies on static Greek FASTTEXT word em-\nbeddings (Prokopidis and Papageorgiou, 2017; Bo-\njanowski et al., 2017), without utilizing pretrained\nTransformers for modern Greek, which restricts\nits performance Also, SPACY does not support\nGreeklish-to-Greek transliteration.\nStanford\u2019s STANZA (Qi et al., 2020) is a Python-\nbased NLP library with multilingual support. For\nmodern Greek, it providesPOS tagging, morpholog-\nical tagging, dependency parsing, lemmatization,\nbut not NER or Greeklish-to-Greek. Its modern\nGreek components are trained on two Greek Uni-\nversal Dependencies treebanks, the default \u2018GDT \u2019\n(Prokopidis and Papageorgiou, 2017, 2014; Proko-\npidis et al., 2005; Papageorgiou et al., 2006; Ghot-\nsoulia et al., 2007) and \u2018 GUD \u2019 (Markantonatou\net al.). Under the hood, STANZA and SPACY use the\nsame Greek FASTTEXT embeddings (Bojanowski\net al., 2017) and no pretrained Transformers.\nAnother widely used NLP toolkit, NLTK (Bird\net al., 2009), does not provide any functionality\nfor modern Greek, other than a tokenizer and stop-\nword removal. In other related work, Prokopidis\nand Piperidis (2020) introduced models for Greek\nPOS tagging, lemmatization, dependency parsing,\nand text classification, requiring manual integration\nwith FASTTEXT and an outdated STANZA version.\nThey also developed a closed-source API based\non them. By contrast, we focus on ready-to-use\nopen-source NLP toolkits.\n3 Using GR-NLP -TOOLKIT\nUsing our toolkit in Python is straightforward.\nTo install it, use pip install gr-nlp-toolkit.\nSubsequently, you can initialize, e.g., a pipeline\nfor POS tagging (incl. morphological tagging),\nNER , dependency parsing ( DP) by executing\nnlp = Pipeline(\"pos, ner, dp\"). Applying\nthe pipeline to a sentence, e.g., doc = nlp(\u201c \u0397\n\u0399\u03c4\u03b1\u03bb\u03af\u03b1 \u03ba\u03ad\u03c1\u03b4\u03b9\u03c3\u03b5 \u03c4\u03b7\u03bd \u0391\u03b3\u03b3\u03bb\u03af\u03b1 \u03c3\u03c4\u03bf\u03bd \u03c4\u03b5\u03bb\u03b9\u03ba\u03cc \u03c4\u03bf\n2020.\u201d), tokenizes the text and provides linguis-\ntic annotations, including POS and morphological\ntags, NER labels, and dependency relations. In our\nexample, the token \u2018\u0399\u03c4\u03b1\u03bb\u03af\u03b1\u2019 (English: \u2018Italy\u2019) gets\nthe annotations NER = S-ORG (start token of or-\nganization name), UPOS = PROPN (proper name),\nand a dependency relation nsubj(nominal subject)\nlinking it to the verb (see also Fig. 2).\nTransliterating Greeklish to Greek ( G2G) is\nequally simple.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3309, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d57973d9-67ab-4e57-8c6a-e75a8fbe5858": {"__data__": {"id_": "d57973d9-67ab-4e57-8c6a-e75a8fbe5858", "embedding": null, "metadata": {"page_label": "3", "file_name": "GR-NLP-TOOLKIT An Open-Source NLP Toolkit for Modern Greek.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\GR-NLP-TOOLKIT An Open-Source NLP Toolkit for Modern Greek.pdf", "file_type": "application/pdf", "file_size": 556151, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "80855d06-ef81-4500-9c04-40b832e4221d", "node_type": "4", "metadata": {"page_label": "3", "file_name": "GR-NLP-TOOLKIT An Open-Source NLP Toolkit for Modern Greek.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\GR-NLP-TOOLKIT An Open-Source NLP Toolkit for Modern Greek.pdf", "file_type": "application/pdf", "file_size": 556151, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "823e3db69d2c9a77235f99789810d2343256d3ae4de01a4f3aeb0ebc8b8e26ea", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c51b41cf-f6e8-4090-b421-6968114f6eb5", "node_type": "1", "metadata": {"page_label": "3", "file_name": "GR-NLP-TOOLKIT An Open-Source NLP Toolkit for Modern Greek.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\GR-NLP-TOOLKIT An Open-Source NLP Toolkit for Modern Greek.pdf", "file_type": "application/pdf", "file_size": 556151, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "9a9d6ec13e760662f33972ff3c5523c3b1d07d201d04ccb149f115ee45caac18", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Applying\nthe pipeline to a sentence, e.g., doc = nlp(\u201c \u0397\n\u0399\u03c4\u03b1\u03bb\u03af\u03b1 \u03ba\u03ad\u03c1\u03b4\u03b9\u03c3\u03b5 \u03c4\u03b7\u03bd \u0391\u03b3\u03b3\u03bb\u03af\u03b1 \u03c3\u03c4\u03bf\u03bd \u03c4\u03b5\u03bb\u03b9\u03ba\u03cc \u03c4\u03bf\n2020.\u201d), tokenizes the text and provides linguis-\ntic annotations, including POS and morphological\ntags, NER labels, and dependency relations. In our\nexample, the token \u2018\u0399\u03c4\u03b1\u03bb\u03af\u03b1\u2019 (English: \u2018Italy\u2019) gets\nthe annotations NER = S-ORG (start token of or-\nganization name), UPOS = PROPN (proper name),\nand a dependency relation nsubj(nominal subject)\nlinking it to the verb (see also Fig. 2).\nTransliterating Greeklish to Greek ( G2G) is\nequally simple. The G2G converter can be loaded\nby typing nlp = Pipeline(\"g2g\") . Running\ndoc = nlp(\"h athina kai h thessaloniki\neinai poleis\") will convert the text to \u201c \u03b7\n\u03b1\u03b8\u03b7\u03bd\u03b1 \u03ba\u03b1\u03b9 \u03b7 \u03b8\u03b5\u03c3\u03c3\u03b1\u03bb\u03bf\u03bd\u03b9\u03ba\u03b7 \u03b5\u03b9\u03bd\u03b1\u03b9 \u03c0\u03bf\u03bb\u03b5\u03b9\u03c2\u201d (English:\n\u201cathens and thessaloniki are cities\u201d). This makes\nit easy to process Greeklish text before perform-\ning further Greek language processing. For ex-\nample, you can also combine the G2G converter\nwith POS , NER , DP in the same pipeline, using\nnlp = Pipeline(\"g2g, pos, ner, dp\").\n4 Under the hood and experiments\nThe POS tagging, morphological tagging, NER , and\ndependency parsing tools of GR-NLP -TOOLKIT\nare powered by GREEK -BERT (Koutsikakis et al.,\n2020), with task-specific heads.8 For Greeklish-to-\nGreek, we reproduced the BYT 5-based converter\nof citettoumazatos-etal-2024-still-all-greeklish-to-\nme, which was the best among several methods\nconsidered, apart from GPT-4, which we excluded\n8GREEK -BERT works as our backbone model in most\ntasks. While it is powerful, one limitation is that it automati-\ncally converts all text to lowercase and removes Greek accents.\n3", "mimetype": "text/plain", "start_char_idx": 2764, "end_char_idx": 4392, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "0b17b6fc-5626-41cf-a0b7-318620e50530": {"__data__": {"id_": "0b17b6fc-5626-41cf-a0b7-318620e50530", "embedding": null, "metadata": {"page_label": "4", "file_name": "GR-NLP-TOOLKIT An Open-Source NLP Toolkit for Modern Greek.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\GR-NLP-TOOLKIT An Open-Source NLP Toolkit for Modern Greek.pdf", "file_type": "application/pdf", "file_size": 556151, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "38693a26-8c55-4b11-b7eb-b602c5d8efdf", "node_type": "4", "metadata": {"page_label": "4", "file_name": "GR-NLP-TOOLKIT An Open-Source NLP Toolkit for Modern Greek.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\GR-NLP-TOOLKIT An Open-Source NLP Toolkit for Modern Greek.pdf", "file_type": "application/pdf", "file_size": 556151, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "81ee9166a4e3c4fc2a4ea7abd4737a213020d01ccd5555801c86e03010d4fdae", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a4ad48a1-d891-4906-9039-e1a37dce7374", "node_type": "1", "metadata": {}, "hash": "6c8c0f4f9e96b480deae90712c4a186a111f4eccc0fda14711ce1e37eeaba57d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "for efficiency reasons.9\n4.1 Named entity recognition\nFor the NER tool of GR-NLP -TOOLKIT , we fine-\ntuned GREEK -BERT (Koutsikakis et al., 2020) with\na task-specific token classification head. We used\nthe training subset of a modern Greek NER dataset\npublished by Bartziokas et al. (2020). The dataset\ncontains approx. 38,000 tagged entities and 18\nentity types. 10 We tuned hyper-parameters to\nmaximize the macro-F1 score on the develop-\nment subset. We used cross-entropy loss, AdamW\n(Loshchilov et al., 2017), and grid search for hyper-\nparameter tuning (Table 6).\nIn Table 2, we compare SPACY against GR-NLP -\nTOOLKIT on the test subset of the NER dataset of\nBartziokas et al. (2020), for the six entity types\nthat SPACY supports.11 We do not compare against\nSTANZA here, since it does not support NER (Ta-\nble 1). As seen in Table 2, GR-NLP -TOOLKIT out-\nperforms SPACY in all entity types. 12 SPACY \u2019s\nscore in the LOC (location) entity type is partic-\nularly low, because it classified most (truly) LOC\nentities as GPE (geo-political entity).\nEntity type SPACY GR-NLP -TOOLKIT\nEVENT 0.31 0.64\nGPE 0.77 0.93\nPERSON 0.82 0.96\nLOC 0.01 0.80\nORG 0.65 0.88\nPRODUCT 0.27 0.75\nTable 2: F1 test scores of SPACY and GR-NLP -TOOLKIT\nin modern Greek NER , showed for the six entity types\nthat SPACY supports.\n4.2 POS tagging and morphological tagging\nFor POS tagging and morphological tagging, we\nused the modern Greek part of the Universal Depen-\ndencies (UD) treebank (Prokopidis and Papageor-\ngiou, 2017). Every word occurrence is annotated\nwith its gold universal POS tag (UPOS ), morpholog-\nical features (FEATS ), as well as its syntactic head\nand the type of syntactic dependency. We refer\n9LLM s like GPT-4 or the Greek MELTEMI require a signif-\nicant resources (cost, time, lots of VRAM ), which typical end\nusers do not have.\n10The 18 entity types of GR-NLP -TOOLKIT are: ORG,\nPERSON, CARDINAL, GPE, DATE, PERCENT, ORDINAL, LOC,\nNORP, TIME, MONEY, EVENT, PRODUCT, WORK_OF_ART,\nFAC, QUANTITY, LAW, LANGUAGE.\n11We provide the results only about the six shared NER\nentity types between SPACY and GR-NLP -TOOLKIT .\n12Table 2 shows results ofSPACY \u2019s large model (spaCy-lg).\nThe smaller models (spacy-sm, spacy-md) performed worse.\nthe reader to the UD website, where complete lists\nof UPOS tags, morphological features, and depen-\ndency types are available.13\nWe fine-tuned a single GREEK -BERT instance\nfor both POS tagging and morphological tagging,\nadding 17 token classification heads (linear lay-\ners), 16 for the morphological categories, and 1\nadditional token classification head for UPOS pre-\ndiction. Each classification head takes as input the\ncorresponding output (top-level) token embedding\nof GREEK -BERT . For every head, the class with\nthe highest logit is chosen, as in multi-task learn-\ning. The model hyperparameters were tuned on\nthe validation subset of the dataset optimizing the\nmacro-F1 score, using grid search and AdamW\n(Loshchilov et al., 2017) (Table 6).\nIn Table 3, we compare SPACY and STANZA to\nthe GR-NLP -TOOLKIT on the UPOS and morpho-\nlogical tagging test data of the modern Greek UD\ntreebank. STANZA and GR-NLP -TOOLKIT perform\non par, with SPACY ranking third.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3200, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a4ad48a1-d891-4906-9039-e1a37dce7374": {"__data__": {"id_": "a4ad48a1-d891-4906-9039-e1a37dce7374", "embedding": null, "metadata": {"page_label": "4", "file_name": "GR-NLP-TOOLKIT An Open-Source NLP Toolkit for Modern Greek.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\GR-NLP-TOOLKIT An Open-Source NLP Toolkit for Modern Greek.pdf", "file_type": "application/pdf", "file_size": 556151, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "38693a26-8c55-4b11-b7eb-b602c5d8efdf", "node_type": "4", "metadata": {"page_label": "4", "file_name": "GR-NLP-TOOLKIT An Open-Source NLP Toolkit for Modern Greek.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\GR-NLP-TOOLKIT An Open-Source NLP Toolkit for Modern Greek.pdf", "file_type": "application/pdf", "file_size": 556151, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "81ee9166a4e3c4fc2a4ea7abd4737a213020d01ccd5555801c86e03010d4fdae", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0b17b6fc-5626-41cf-a0b7-318620e50530", "node_type": "1", "metadata": {"page_label": "4", "file_name": "GR-NLP-TOOLKIT An Open-Source NLP Toolkit for Modern Greek.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\GR-NLP-TOOLKIT An Open-Source NLP Toolkit for Modern Greek.pdf", "file_type": "application/pdf", "file_size": 556151, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "f3f2527b79b98ea89fa4691acad42c00f40c291ea6051e91fe5e2fd5b51cdb25", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Each classification head takes as input the\ncorresponding output (top-level) token embedding\nof GREEK -BERT . For every head, the class with\nthe highest logit is chosen, as in multi-task learn-\ning. The model hyperparameters were tuned on\nthe validation subset of the dataset optimizing the\nmacro-F1 score, using grid search and AdamW\n(Loshchilov et al., 2017) (Table 6).\nIn Table 3, we compare SPACY and STANZA to\nthe GR-NLP -TOOLKIT on the UPOS and morpho-\nlogical tagging test data of the modern Greek UD\ntreebank. STANZA and GR-NLP -TOOLKIT perform\non par, with SPACY ranking third.\nMetric SPACY STANZA GR -NLP -TOOLKIT\nMicro-F1 0.95 0.98 0.98\nMacro-F1 0.87 0.96 0.97\nTable 3: Micro-F1 and macro-F1 test scores for UPOS\ntagging. The complete list of UPOS tags can be found in\nhttps://universaldependencies.org/u/pos/.\nIn the more complex morphological tagging task\n(Table 4), the differences between the systems are\nmove visible, with GR-NLP -TOOLKIT performing\nslightly better in most categories than STANZA ,\nwhile SPACY , again, ranks third. The largest differ-\nences are observed in \u2018Mood\u2019 and \u2018Foreign\u2019 (for-\neign word), where GR-NLP -TOOLKIT performs sub-\nstantially better, and \u2018Degree\u2019 (degrees of adjec-\ntives), where STANZA is clearly better. Dikonimaki\n(2021) attributes some of these differences to very\nfew training occurrences of the corresponding tags.\n4.3 Dependency parsing\nFor dependency parsing, we use the model of Dozat\net al. (2017), with the exception that we obtain\ncontextualized word embeddings using GREEK -\nBERT instead of the BILSTM encoder of the original\nmodel.14 Specifically, for each word of the sen-\ntence being parsed, we obtain its output (top-level)\ncontextualized embedding ei from GREEK -BERT .\n13https://universaldependencies.org/\n14When a word is broken into multiple sub-word tokens\nby GREEK -BERT \u2019s tokenizer, we take the embedding of the\nfirst token to represent the entire word.\n4", "mimetype": "text/plain", "start_char_idx": 2614, "end_char_idx": 4545, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "dda88c9e-c0b5-4658-a64c-faa42052def6": {"__data__": {"id_": "dda88c9e-c0b5-4658-a64c-faa42052def6", "embedding": null, "metadata": {"page_label": "5", "file_name": "GR-NLP-TOOLKIT An Open-Source NLP Toolkit for Modern Greek.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\GR-NLP-TOOLKIT An Open-Source NLP Toolkit for Modern Greek.pdf", "file_type": "application/pdf", "file_size": 556151, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "20c73727-d29e-4c9d-8765-e5290761bd01", "node_type": "4", "metadata": {"page_label": "5", "file_name": "GR-NLP-TOOLKIT An Open-Source NLP Toolkit for Modern Greek.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\GR-NLP-TOOLKIT An Open-Source NLP Toolkit for Modern Greek.pdf", "file_type": "application/pdf", "file_size": 556151, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "8861b12dedb6a8768153f3fbfc97258b02506e391e7db186f788472b6e243a9f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "34d749ef-6f2d-41a6-8c9f-6a9f6451e240", "node_type": "1", "metadata": {}, "hash": "8a1de3567e380829383a5ff5420de335dc8cad7f2033d94225da95cc4c285114", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Morphological tag SPACY STANZA GR-NLP -TOOLKIT\nCase 0.68 0.97 0.97\nDefinite 0.89 1.00 1.00\nGender 0.68 0.97 0.98\nNumber 0.69 0.99 0.99\nPronType 0.71 0.94 0.97\nForeign 0.65 0.79 0.88\nAspect 0.65 0.98 0.99\nMood 0.74 0.59 0.83\nPerson 0.68 0.98 1.00\nTense 0.76 0.98 1.00\nVerbForm 0.65 0.97 0.93\nV oice 0.65 0.99 0.96\nNumType 0.67 0.93 0.96\nPoss 0.59 0.96 0.98\nDegree 0.48 0.89 0.50\nAbbr 0.89 0.96 0.94\nTable 4: F1 test scores for all of the morphological tags.\nWe then compute the following four variants ofei.\nThe W(... ) matrices are learnt during fine-tuning.\nh(arc-head)\ni = W(arc-head)ei, h(arc-dep)\ni = W(arc-dep)ei\nh(rel-head)\ni = W(rel-head)ei, h(rel-dep)\ni = W(rel-dep)ei\nh(arc-head)\ni , h(arc-dep)\ni represent the i-th word of the\nsentence when considered as the head or depen-\ndent (child) of a dependency relation, respectively.\nh(rel-head)\ni , h(rel-dep)\ni are similar, but they are used\nwhen predicting the type of a relation (see below).\nEach candidate arc from head word j to depen-\ndent word i is scored using the following formula,\nwhere W(arc) is a learnt biaffine attention layer, and\nb(arc) is a learnt bias capturing the fact that some\nwords tend to be used more (or less) often as heads.\ns(arc)\nij = (h(arc-head)\nj )T W(arc)h(arc-dep)\ni + (h(arc-head)\nj )T b(arc)\nAt inference time, for each word i, we greedily\nselect its (unique) most probable head y(arc)\ni .15\ny(arc)\ni = arg max\nj\ns(arc)\nij\nDuring training, we minimize the categorical cross\nentropy loss of y(arc)\ni , where the possible values of\ny(arc)\ni correspond to the other words of the sentence.\nFor a given arc from head word j to dependent\nword i, its candidate labels k are scored as follows,\nwhere \u2295 denotes vector concatenation.\ns(rel)\nijk = (h(rel-head)\nj )T U(rel)\nk h(rel-dep)\ni + wT\nk (h(rel-head)\ni \u2295 h(rel-dep)\ni ) +b(rel)\nk\nHere U(rel)\nk is a learnt biaffine layer, different per\nlabel k, whereas wT\nk is a learnt vector that in effect\nscores separately the head and the dependent word,\n15We leave for future work the possibility of adding a non-\ngreedy decoder, e.g., based on the work of Chu and Liu (1965)\nand Edmonds (1967), which would also guarantee that the\noutput is always a tree.\nand b(rel)\nk is the bias of label k. At inference time,\nhaving first greedily selected the heady(arc)\ni of each\ndependent word i, we then greedily select the label\nof the arc as follows.\ny(rel)\ni = arg max\nk\ns(rel)\niy(arc)\ni k\nDuring training, we minimize the categorical cross-\nentropy loss of y(rel)\ni . The arc prediction and label\nprediction components are trained jointly, adding\nthe two cross entropy losses.\nThe parser was trained and evaluated on the same\nmodern Greek part of the Universal Dependencies\ndataset of Section 4.2, now using the dependency\nrelation annotations. Consult Dikonimaki (2021)\nand Kyriakakis (2018) for more details.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2831, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "34d749ef-6f2d-41a6-8c9f-6a9f6451e240": {"__data__": {"id_": "34d749ef-6f2d-41a6-8c9f-6a9f6451e240", "embedding": null, "metadata": {"page_label": "5", "file_name": "GR-NLP-TOOLKIT An Open-Source NLP Toolkit for Modern Greek.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\GR-NLP-TOOLKIT An Open-Source NLP Toolkit for Modern Greek.pdf", "file_type": "application/pdf", "file_size": 556151, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "20c73727-d29e-4c9d-8765-e5290761bd01", "node_type": "4", "metadata": {"page_label": "5", "file_name": "GR-NLP-TOOLKIT An Open-Source NLP Toolkit for Modern Greek.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\GR-NLP-TOOLKIT An Open-Source NLP Toolkit for Modern Greek.pdf", "file_type": "application/pdf", "file_size": 556151, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "8861b12dedb6a8768153f3fbfc97258b02506e391e7db186f788472b6e243a9f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "dda88c9e-c0b5-4658-a64c-faa42052def6", "node_type": "1", "metadata": {"page_label": "5", "file_name": "GR-NLP-TOOLKIT An Open-Source NLP Toolkit for Modern Greek.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\GR-NLP-TOOLKIT An Open-Source NLP Toolkit for Modern Greek.pdf", "file_type": "application/pdf", "file_size": 556151, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "c85891606840b515053a98c2a1344cbe57a005142f64fbf64068982fccab9671", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "and b(rel)\nk is the bias of label k. At inference time,\nhaving first greedily selected the heady(arc)\ni of each\ndependent word i, we then greedily select the label\nof the arc as follows.\ny(rel)\ni = arg max\nk\ns(rel)\niy(arc)\ni k\nDuring training, we minimize the categorical cross-\nentropy loss of y(rel)\ni . The arc prediction and label\nprediction components are trained jointly, adding\nthe two cross entropy losses.\nThe parser was trained and evaluated on the same\nmodern Greek part of the Universal Dependencies\ndataset of Section 4.2, now using the dependency\nrelation annotations. Consult Dikonimaki (2021)\nand Kyriakakis (2018) for more details.\nFigure 2: A dependency tree generated by GR-NLP -\nTOOLKIT for a Greek sentence whose English transla-\ntion is \u201cManchester United was defeated by Atletico\nBilbao with a 2:3 score.\u201d Figure from Smyrnioudis\n(2021). Tree drawn using SPACY \u2019s visualizer.\nTable 5 evaluates the dependency parser of\nGR-NLP -TOOLKIT against those of SPACY and\nSTANZA , using Unlabeled Attachment Score (UAS)\nand Labeled Attachment Score ( LAS ) on the test\nsubset. UAS is the percentage of the sentence\u2019s\nwords that get the correct head, while LAS is the\npercentage of words that get both the correct head\nand label. GR-NLP -TOOLKIT clearly provides state-\nof-the-art performance for this task too.\nScore SPACY STANZA GR -NLP -TOOLKIT\nUAS 0.66 0.91 0.94\nLAS 0.64 0.88 0.92\nTable 5: TestUAS and LAS scores (dependency parsing).\n4.4 Greeklish-to-Greek transliteration\nFor Greeklish-to-Greek, we reproduced the BYT 5\nmodel of Toumazatos et al. (2024), which was\nthe best one, excluding GPT-4. BYT 5 (Xue et al.,\n2022) operates directly on bytes, making it partic-\nularly well-suited for tasks involving text written\nin multiple alphabets (Greek and Latin in our case).\nToumazatos et al. (2024) fine-tuned BYT 5 espe-\ncially for Greeklish-to-Greek, using synthetic data.\nThe model was then evaluated on both synthetic\nand real-life Greeklish. Consult Toumazatos et al.\n5", "mimetype": "text/plain", "start_char_idx": 2183, "end_char_idx": 4174, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8965e4d8-d7d7-4017-b5a0-cc7483909ef0": {"__data__": {"id_": "8965e4d8-d7d7-4017-b5a0-cc7483909ef0", "embedding": null, "metadata": {"page_label": "6", "file_name": "GR-NLP-TOOLKIT An Open-Source NLP Toolkit for Modern Greek.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\GR-NLP-TOOLKIT An Open-Source NLP Toolkit for Modern Greek.pdf", "file_type": "application/pdf", "file_size": 556151, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ff9a8e44-a3e4-40c6-917b-f9dd535848f3", "node_type": "4", "metadata": {"page_label": "6", "file_name": "GR-NLP-TOOLKIT An Open-Source NLP Toolkit for Modern Greek.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\GR-NLP-TOOLKIT An Open-Source NLP Toolkit for Modern Greek.pdf", "file_type": "application/pdf", "file_size": 556151, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "e8417f3eb396aac84e5c9bc44761295a455aa6fc1b5bd496ca42a40ac2e81b56", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "(2024) for more details and evaluation results. Re-\ncall that no other modern Greek toolkit currently\nsupports Greeklish-to-Greek (Table 1).\nA limitation of the Greeklish-to-Greek model\nincluded in GR-NLP -TOOLKIT is that it has not\nbeen trained on Greeklish that also includes English\n(code switching), which is a common phenomenon\nin online modern Greek. This is a limitation inher-\nited from the work of Toumazatos et al. (2024). We\nare currently working on an improved Greeklish-\nto-Greek model that will also handle code switch-\ning. We are also considering including in GR-NLP -\nTOOLKIT an older statistical Greeklish-to-Greek\nmodel (Chalamandaris et al., 2006), which still per-\nformed well in the experiments of Toumazatos et al.\n(2024) and can already handle code-switching.\n5 The GR-NLP -TOOLKIT demo space\nFor users wishing to explore GR-NLP -TOOLKIT in-\nstantly, in a no-code fashion, we also developed\na demonstration space, which is open access and\nhosted at https://huggingface.co/spaces/\nAUEB-NLP/greek-nlp-toolkit-demo. Users can\nselect tasks (POS and morphological tagging, NER ,\ndependency parsing, Greeklish-to-Greek), submit\ntheir input and see the results in the user interface.\nFigure 3 shows an example of Greeklish-to-Greek.\nFigure 3: Example of GR-NLP -TOOLKIT \u2019s demon-\nstration space at https://huggingface.co/spaces/\nAUEB-NLP/greek-nlp-toolkit-demo. The example\nshows Greeklish-to-Greek transliteration, but the demo\nprovides access to the other functionalities too (POS and\nmorphological tagging, dependency parsing, NER ).\n6 The GREEK -NLP -API\nBased on GR-NLP -TOOLKIT , we also devel-\noped a publicly available API (with the same\nnon-commercial license). The API is hosted at\nhttps://huggingface.co/spaces/AUEB-NLP/\nThe-Greek-NLP-API. It is intended to be used in\nresearch and educational applications, even appli-\ncations not developed in Python, viaHTTP API calls\nand exchange of JSON objects. GREEK -NLP -API\nconforms to the OPENAPI standards.16\n7 Conclusions\nWe introduced GR-NLP -TOOLKIT , an open-source\nNLP toolkit with state-of-the-art performance for\nmodern Greek. It can be easily installed in\nPython (pip install gr-nlp-toolkit), and its\ncode is available on Github ( https://github.\ncom/nlpaueb/gr-nlp-toolkit/).\nThe toolkit currently supports POS and\nmorphological tagging, dependency parsing,\nnamed entity recognition, and Greeklish-to-\nGreek transliteration. We also presented an\ninteractive no-code demonstration space that\nprovides the full functionality of the toolkit\n(https://huggingface.co/spaces/AUEB-NLP/\ngreek-nlp-toolkit-demo), as well as a publicly\navailable API at https://huggingface.co/\nspaces/AUEB-NLP/The-Greek-NLP-API, which\nallows using the toolkit even in applications not\ndeveloped in Python. We discussed the methods\nthat power the toolkit under the hood, and reported\nexperimental results against SPACY and STANZA .\nIn future work, we plan to add more tools, e.g.,\nfor toxicity detection and sentiment analysis. We\nwelcome open-source collaboration.\nAcknowledgments\nThis work has been partially supported by project\nMIS 5154714 of the National Recovery and Re-\nsilience Plan Greece 2.0 funded by the Euro-\npean Union under the NextGenerationEU Pro-\ngram. Also, a significant portion of this work\nwas also conducted as part of the Google Summer\nof Code (2024) program with the Open Technolo-\ngies Alliance (GFOSS) (https://eellak.ellak.\ngr/). Lastly, we would like to sincerely thank\nthe Hellenic Artificial Intelligence Society (EETN)\n(https://www.eetn.gr/en/) for their sponsor-\nship.\n16https://www.openapis.org/\n6", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3585, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "76c2ca8b-cf8d-47ed-b983-7341d75cf193": {"__data__": {"id_": "76c2ca8b-cf8d-47ed-b983-7341d75cf193", "embedding": null, "metadata": {"page_label": "7", "file_name": "GR-NLP-TOOLKIT An Open-Source NLP Toolkit for Modern Greek.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\GR-NLP-TOOLKIT An Open-Source NLP Toolkit for Modern Greek.pdf", "file_type": "application/pdf", "file_size": 556151, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c2e60a7a-b400-4a65-81eb-ee0a30798d78", "node_type": "4", "metadata": {"page_label": "7", "file_name": "GR-NLP-TOOLKIT An Open-Source NLP Toolkit for Modern Greek.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\GR-NLP-TOOLKIT An Open-Source NLP Toolkit for Modern Greek.pdf", "file_type": "application/pdf", "file_size": 556151, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "7e9652efc63bfb1a1912728dbcce812b3bf31f8ff77d177efcdde6361c6473ff", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0fe8908c-bd22-4eda-90c0-2c04921a5f48", "node_type": "1", "metadata": {}, "hash": "0138b17de0a66d96fcda78d53e529a0c7a3dcea85523dfa80cfbcba5be70c255", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "References\nJuli Bakagianni, Kanella Pouli, Maria Gavriilidou, and\nJohn Pavlopoulos. 2024. Towards Systematic Mono-\nlingual NLP surveys: GenA of Greek NLP. arXiv\npreprint arXiv:2407.09861.\nNikos Bartziokas, Thanassis Mavropoulos, and Con-\nstantine Kotropoulos. 2020. Datasets and Perfor-\nmance Metrics for Greek Named Entity Recogni-\ntion. In 11th Hellenic Conference on Artificial Intel-\nligence (SETN 2020), SETN 2020, pages 160\u2013167,\nNew York, NY , USA. Association for Computing\nMachinery.\nSteven Bird, Ewan Klein, and Edward Loper. 2009.Nat-\nural Language Processing with Python: Analyzing\nText with the Natural Language Toolkit. \" O\u2019Reilly\nMedia, Inc.\".\nPiotr Bojanowski, Edouard Grave, Armand Joulin, and\nTomas Mikolov. 2017. Enriching Word Vectors with\nSubword Information. Transactions of the Associa-\ntion for Computational Linguistics, 5:135\u2013146.\nAimilios Chalamandaris, Athanassios Protopapas, Pir-\nros Tsiakoulis, and Spyros Raptis. 2006. All Greek\nto me! an automatic Greeklish to Greek translitera-\ntion system. In Proceedings of the Fifth International\nConference on Language Resources and Evaluation\n(LREC\u201906), Genoa, Italy. European Language Re-\nsources Association (ELRA).\nY .-J. Chu and T.-H Liu. 1965. On the shortest arbores-\ncence of a directed graph. Science Sinica, 14:1396\u2013\n1400.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzm\u00e1n, Edouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2020. Unsupervised\nCross-lingual Representation Learning at Scale. In\nProceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 8440\u2013\n8451, Online. Association for Computational Lin-\nguistics.\nC. Dikonimaki. 2021. A Transformer-based natural lan-\nguage processing toolkit for Greek \u2013 Part of speech\ntagging and dependency parsing. Technical report,\nBSc thesis, Department of Informatics, Athens Uni-\nversity of Economics and Business. http://nlp.cs.\naueb.gr/theses/dikonimaki_bsc_thesis.pdf.\nTimothy Dozat, Peng Qi, and Christopher D. Manning.\n2017. Stanford\u2019s Graph-based Neural Dependency\nParser at the CoNLL 2017 shared task. In Proceed-\nings of the CoNLL 2017 Shared Task: Multilingual\nParsing from Raw Text to Universal Dependencies,\npages 20\u201330, Vancouver, Canada. Association for\nComputational Linguistics.\nJ. Edmonds. 1967. Optimum branchings. Journal of\nResearch of the National Bureau of Standards B ,\n71(4):233\u2013240.\nV oula Ghotsoulia, Elina Desypri, Maria Koutsombogera,\nProkopis Prokopidis, and Haris Papageorgiou. 2007.\nTowards a Frame Semantics Resource for Greek. In\nProceedings of The Sixth Workshop on Treebanks\nand Linguistic Theories (TLT 2007), Bergen, Norway.\nUniversity of Bergen.\nMatthew Honnibal, Ines Montani, Sofie Van Lan-\ndeghem, and Adriane Boyd. 2020. spaCy: Industrial-\nstrength Natural Language Processing in Python.\nAlbert Q. Jiang, Alexandre Sablayrolles, Arthur Men-\nsch, Chris Bamford, Devendra Singh Chaplot, Diego\nde las Casas, Florian Bressand, Gianna Lengyel, Guil-\nlaume Lample, Lucile Saulnier, L\u00e9lio Renard Lavaud,\nMarie-Anne Lachaux, Pierre Stock, Teven Le Scao,\nThibaut Lavril, Thomas Wang, Timoth\u00e9e Lacroix,\nand William El Sayed. 2023.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3196, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "0fe8908c-bd22-4eda-90c0-2c04921a5f48": {"__data__": {"id_": "0fe8908c-bd22-4eda-90c0-2c04921a5f48", "embedding": null, "metadata": {"page_label": "7", "file_name": "GR-NLP-TOOLKIT An Open-Source NLP Toolkit for Modern Greek.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\GR-NLP-TOOLKIT An Open-Source NLP Toolkit for Modern Greek.pdf", "file_type": "application/pdf", "file_size": 556151, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c2e60a7a-b400-4a65-81eb-ee0a30798d78", "node_type": "4", "metadata": {"page_label": "7", "file_name": "GR-NLP-TOOLKIT An Open-Source NLP Toolkit for Modern Greek.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\GR-NLP-TOOLKIT An Open-Source NLP Toolkit for Modern Greek.pdf", "file_type": "application/pdf", "file_size": 556151, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "7e9652efc63bfb1a1912728dbcce812b3bf31f8ff77d177efcdde6361c6473ff", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "76c2ca8b-cf8d-47ed-b983-7341d75cf193", "node_type": "1", "metadata": {"page_label": "7", "file_name": "GR-NLP-TOOLKIT An Open-Source NLP Toolkit for Modern Greek.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\GR-NLP-TOOLKIT An Open-Source NLP Toolkit for Modern Greek.pdf", "file_type": "application/pdf", "file_size": 556151, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "2f6318a6adfe31343a96cd61d491fe820b2cbec5802eb71fbba0abef1b0f3e96", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "2007.\nTowards a Frame Semantics Resource for Greek. In\nProceedings of The Sixth Workshop on Treebanks\nand Linguistic Theories (TLT 2007), Bergen, Norway.\nUniversity of Bergen.\nMatthew Honnibal, Ines Montani, Sofie Van Lan-\ndeghem, and Adriane Boyd. 2020. spaCy: Industrial-\nstrength Natural Language Processing in Python.\nAlbert Q. Jiang, Alexandre Sablayrolles, Arthur Men-\nsch, Chris Bamford, Devendra Singh Chaplot, Diego\nde las Casas, Florian Bressand, Gianna Lengyel, Guil-\nlaume Lample, Lucile Saulnier, L\u00e9lio Renard Lavaud,\nMarie-Anne Lachaux, Pierre Stock, Teven Le Scao,\nThibaut Lavril, Thomas Wang, Timoth\u00e9e Lacroix,\nand William El Sayed. 2023. Mistral 7B. Preprint,\narXiv:2310.06825.\nJohn Koutsikakis, Ilias Chalkidis, Prodromos Malaka-\nsiotis, and Ion Androutsopoulos. 2020. GREEK-\nBERT: The Greeks Visiting Sesame Street. In 11th\nHellenic Conference on Artificial Intelligence, SETN\n2020, pages 110\u2013117, New York, NY , USA. Associa-\ntion for Computing Machinery.\nDimitris Koutsogiannis and Bessie Mitsikopoulou. 2017.\nGreeklish and Greekness: Trends and Discourses of\n\u201cGlocalness\u201d. Journal of Computer-Mediated Com-\nmunication, 9(1):JCMC918.\nM. Kyriakakis. 2018. Exploring deep neural net-\nwork models of syntax with a focus on Greek.\nTechnical report, MSc thesis, Department of\nInformatics, Athens University of Economics\nand Business. http://nlp.cs.aueb.gr/theses/\nkiriakakis_msc_thesis.pdf.\nIlya Loshchilov, Frank Hutter, et al. 2017. Fixing\nWeight Decay Regularization in Adam. arXiv\npreprint arXiv:1711.05101, 5.\nSasha Luccioni, Yacine Jernite, and Emma Strubell.\n2024. Power Hungry Processing: Watts Driving the\nCost of AI Deployment? In Proceedings of the 2024\nACM Conference on Fairness, Accountability, and\nTransparency, FAccT \u201924, page 85\u201399, New York,\nNY , USA. Association for Computing Machinery.\nStella Markantonatou, Vivian Stamou, and Socrates\nVak. Gud Greek-GUD: Greek Universal De-\npendencies Treebank. https://github.com/\nUniversalDependencies/UD_Greek-GUD.\nHarris Papageorgiou, Elina Desipri, Maria Koutsom-\nbogera, Kanella Pouli, and Prokopis Prokopidis.\n2006. Adding Multi-layer Semantics to the Greek\nDependency Treebank. In Proceedings of The Fifth\nInternational Conference on Language and Evalua-\ntion (LREC-2006), Genoa, Italy. ELRA.\nKaterina Papantoniou and Yannis Tzitzikas. 2020. NLP\nfor the Greek language: A brief survey. In 11th\nHellenic Conference on Artificial Intelligence, SETN\n2020, page 101\u2013109, Athens, Greece.\n7", "mimetype": "text/plain", "start_char_idx": 2542, "end_char_idx": 5006, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f1e1df9f-1c98-4e8b-ab3d-e0d5ae88a74e": {"__data__": {"id_": "f1e1df9f-1c98-4e8b-ab3d-e0d5ae88a74e", "embedding": null, "metadata": {"page_label": "8", "file_name": "GR-NLP-TOOLKIT An Open-Source NLP Toolkit for Modern Greek.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\GR-NLP-TOOLKIT An Open-Source NLP Toolkit for Modern Greek.pdf", "file_type": "application/pdf", "file_size": 556151, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7f5aa553-dfdd-4bab-ba88-6025e3760d92", "node_type": "4", "metadata": {"page_label": "8", "file_name": "GR-NLP-TOOLKIT An Open-Source NLP Toolkit for Modern Greek.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\GR-NLP-TOOLKIT An Open-Source NLP Toolkit for Modern Greek.pdf", "file_type": "application/pdf", "file_size": 556151, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "2154857215658be4a2d9a7a2659b77f0b4f25c1ff7715d37692156500205a949", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f04ecaa4-ac88-4509-80d2-2661c1ed02c8", "node_type": "1", "metadata": {}, "hash": "4b32e807fccb77936eb6b707f1f097fd5ad55d2937e066ad085e2dd3c32954f8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Prokopis Prokopidis, Elina Desypri, Maria Koutsom-\nbogera, Haris Papageorgiou, and Stelios Piperidis.\n2005. Theoretical and Practical Issues in the Con-\nstruction of a Greek Dependency Treebank. In\nProceedings of The Fourth Workshop on Treebanks\nand Linguistic Theories (TLT 2005), pages 149\u2013160,\nBarcelona, Spain. Universitat de Barcelona.\nProkopis Prokopidis and Haris Papageorgiou. 2017.\nUniversal Dependencies for Greek. InProceedings of\nthe NoDaLiDa 2017 Workshop on Universal Depen-\ndencies (UDW 2017), pages 102\u2013106, Gothenburg,\nSweden. Association for Computational Linguistics.\nProkopis Prokopidis and Harris Papageorgiou. 2014.\nExperiments for Dependency Parsing of Greek. In\nProceedings of the First Joint Workshop on Statistical\nParsing of Morphologically Rich Languages and Syn-\ntactic Analysis of Non-Canonical Languages, pages\n89\u201396, Dublin, Ireland.\nProkopis Prokopidis and Stelios Piperidis. 2020. A neu-\nral nlp toolkit for greek. In 11th Hellenic Conference\non Artificial Intelligence, SETN 2020, page 125\u2013128,\nNew York, NY , USA. Association for Computing\nMachinery.\nPeng Qi, Yuhao Zhang, Yuhui Zhang, Jason Bolton, and\nChristopher D. Manning. 2020. Stanza: A Python\nNatural Language Processing Toolkit for Many Hu-\nman Languages. In Proceedings of the 58th Annual\nMeeting of the Association for Computational Lin-\nguistics: System Demonstrations , pages 101\u2013108,\nOnline. Association for Computational Linguistics.\nN. Smyrnioudis. 2021. A Transformer-based natural\nlanguage processing toolkit for Greek \u2013 Named entity\nrecognition and multi-task learning. Technical report,\nBSc thesis, Department of Informatics, Athens Uni-\nversity of Economics and Business. http://nlp.cs.\naueb.gr/theses/smyrnioudis_bsc_thesis.pdf.\nAnastasios Toumazatos, John Pavlopoulos, Ion Androut-\nsopoulos, and Stavros Vassos. 2024. Still All Greek-\nlish to Me: Greeklish to Greek transliteration. In Pro-\nceedings of the 2024 Joint International Conference\non Computational Linguistics, Language Resources\nand Evaluation (LREC-COLING 2024), pages 15309\u2013\n15319, Torino, Italia. ELRA and ICCL.\nLeon V oukoutis, Dimitris Roussis, Georgios\nParaskevopoulos, Sokratis Sofianopoulos, Prokopis\nProkopidis, Vassilis Papavasileiou, Athanasios\nKatsamanis, Stelios Piperidis, and Vassilis Katsouros.\n2024. Meltemi: The first open Large Language\nModel for Greek. Preprint, arXiv:2407.20743.\nLinting Xue, Aditya Barua, Noah Constant, Rami Al-\nRfou, Sharan Narang, Mihir Kale, Adam Roberts,\nand Colin Raffel. 2022. ByT5: Towards a Token-\nFree Future with Pre-trained Byte-to-Byte Models.\nTransactions of the Association for Computational\nLinguistics, 10:291\u2013306.\nA Appendix\nA.1 Hyperparameter tuning\nTable 6 provides information on the hyperparam-\neters of the models we use for NER , POS tagging,\nmorphological tagging, and dependency parsing.\nHyperparameter Range\nLearning rate [5e-5, 3e-5, 2e-5]\nDropout [0, 0.1, 0.2]\nGrad accumulation steps [4, 8]\nWeight decay (\u03bb) [0.2, 0.5, 0.8]\nTable 6: Hyperparameter space of the NER , POS tagging,\nmorphological tagging, and dependency parsing models.\nA.2 List of Contributions17\nLefteris Loukas:Conceptualization, Software,\nProject administration, Funding acquisition, Writ-\ning. Lefteris led the software\u2019s refactoring to the\ncurrent version, after identifying limitations in the\nfirst early one.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3321, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f04ecaa4-ac88-4509-80d2-2661c1ed02c8": {"__data__": {"id_": "f04ecaa4-ac88-4509-80d2-2661c1ed02c8", "embedding": null, "metadata": {"page_label": "8", "file_name": "GR-NLP-TOOLKIT An Open-Source NLP Toolkit for Modern Greek.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\GR-NLP-TOOLKIT An Open-Source NLP Toolkit for Modern Greek.pdf", "file_type": "application/pdf", "file_size": 556151, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7f5aa553-dfdd-4bab-ba88-6025e3760d92", "node_type": "4", "metadata": {"page_label": "8", "file_name": "GR-NLP-TOOLKIT An Open-Source NLP Toolkit for Modern Greek.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\GR-NLP-TOOLKIT An Open-Source NLP Toolkit for Modern Greek.pdf", "file_type": "application/pdf", "file_size": 556151, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "2154857215658be4a2d9a7a2659b77f0b4f25c1ff7715d37692156500205a949", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f1e1df9f-1c98-4e8b-ab3d-e0d5ae88a74e", "node_type": "1", "metadata": {"page_label": "8", "file_name": "GR-NLP-TOOLKIT An Open-Source NLP Toolkit for Modern Greek.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\GR-NLP-TOOLKIT An Open-Source NLP Toolkit for Modern Greek.pdf", "file_type": "application/pdf", "file_size": 556151, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "fce4dce329e531278aac1152c235b7ec8e61824d650d224cb9ba835f46517ca4", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "A Appendix\nA.1 Hyperparameter tuning\nTable 6 provides information on the hyperparam-\neters of the models we use for NER , POS tagging,\nmorphological tagging, and dependency parsing.\nHyperparameter Range\nLearning rate [5e-5, 3e-5, 2e-5]\nDropout [0, 0.1, 0.2]\nGrad accumulation steps [4, 8]\nWeight decay (\u03bb) [0.2, 0.5, 0.8]\nTable 6: Hyperparameter space of the NER , POS tagging,\nmorphological tagging, and dependency parsing models.\nA.2 List of Contributions17\nLefteris Loukas:Conceptualization, Software,\nProject administration, Funding acquisition, Writ-\ning. Lefteris led the software\u2019s refactoring to the\ncurrent version, after identifying limitations in the\nfirst early one. He secured funding, supervised the\ndevelopment of the revamped toolkit, and created\nthe demonstration space as well as the API . He also\nco-authored this publication.\nNikolaos Smyrnioudis: Methodology, For-\nmal Analysis, Software, Writing. Nikolaos re-\nsearched and created the NER methodology, and\nco-developed the first version of the toolkit. Con-\nsult Smyrnioudis (2021) for more information on\nhis work, which is also summarized in \u00a74.1.\nChrysa Dikonimaki: Methodology, Formal\nAnalysis, Software, Writing. Chrysa researched\nand created the DP, POS , and morphological tag-\nging methodologies, and co-developed the first ver-\nsion of the toolkit. Consult Dikonimaki (2021)\nfor more information on her work, which is also\nsummarized in \u00a74.2 and \u00a74.3.\nSpyros Barbakos: Software, Resources,\nMethodology. Spyros refactored the previous ver-\nsion of the toolkit as a participant in Google\u2019s\nSummer of Code 2024, and enhanced it with the\nGreeklish-to-Greek transliteration component.\nAnastasios Toumazatos:Software, Resources,\nMethodology. Anastasios provided guidance on\nhow to integrate their Greeklish-to-Greek translit-\neration algorithm (Toumazatos et al., 2024) in the\nrevamped introduced toolkit.\nJohn Koutsikakis:Supervision, Software, Re-\nsources. John co-supervised the BSc theses of\n17We follow the Contributor Role Taxonomy (CRediT).\nConsult http://www.credit.niso.org.\n8", "mimetype": "text/plain", "start_char_idx": 2643, "end_char_idx": 4703, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "11fe23aa-ab54-420e-8591-9985e2416897": {"__data__": {"id_": "11fe23aa-ab54-420e-8591-9985e2416897", "embedding": null, "metadata": {"page_label": "9", "file_name": "GR-NLP-TOOLKIT An Open-Source NLP Toolkit for Modern Greek.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\GR-NLP-TOOLKIT An Open-Source NLP Toolkit for Modern Greek.pdf", "file_type": "application/pdf", "file_size": 556151, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f62d972e-7b41-47b2-9bc9-855b46565dfa", "node_type": "4", "metadata": {"page_label": "9", "file_name": "GR-NLP-TOOLKIT An Open-Source NLP Toolkit for Modern Greek.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\GR-NLP-TOOLKIT An Open-Source NLP Toolkit for Modern Greek.pdf", "file_type": "application/pdf", "file_size": 556151, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "6350f9ed15736240627534c58a3110ce3543804d37a319f57ff30357e8459cd7", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Dikonimaki (2021) and Smyrnioudis (2021), and\nassisted in their software and resources.\nManolis Kyriakakis: Software, Resources,\nMethodology. Manolis assisted in the development\nof the dependency parsing functionality, which was\nbased on his MSc thesis (Kyriakakis, 2018).\nMary Georgiou: Software, Resources. Mary\nassisted in debugging and making pip-installable\nthe first (older) version of the toolkit.\nStavros Vassos: Resources, Supervision.\nStavros identified current limitations in Greek NLP\nand provided resources for the work on Greeklish-\nto-Greek of Toumazatos et al. (2024), as well as\nfor this work.\nJohn Pavlopoulos: Supervision, Writing,\nMethodology. John co-supervised the work on\nGreeklish-to-Greek of Toumazatos et al. (2024),\nand this work. He co-authored this publication.\nIon Androutsopoulos: Supervision, Writing,\nMethodology. Ion co-supervised the BSc theses\nof Dikonimaki (2021) and Smyrnioudis (2021),\nthe Greeklish-to-Greek work of Toumazatos et al.\n(2024), this work, and co-authored this publication.\n9", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1028, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "486e479c-e64c-4266-855a-eb4bc57b34c6": {"__data__": {"id_": "486e479c-e64c-4266-855a-eb4bc57b34c6", "embedding": null, "metadata": {"page_label": "1", "file_name": "Is Self-knowledge and Action Consistent or Not Investigating Large Language Models Personality.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Is Self-knowledge and Action Consistent or Not Investigating Large Language Models Personality.pdf", "file_type": "application/pdf", "file_size": 659650, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "47ba212c-fa32-4730-86ea-70f7b8ef0ea1", "node_type": "4", "metadata": {"page_label": "1", "file_name": "Is Self-knowledge and Action Consistent or Not Investigating Large Language Models Personality.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Is Self-knowledge and Action Consistent or Not Investigating Large Language Models Personality.pdf", "file_type": "application/pdf", "file_size": 659650, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "061b767ad6486d5d307f2324f28234ff5f65ec9203fb2007c06f7886dd1bb9a2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "093a4658-f460-40d5-a1f2-b14bcd70336b", "node_type": "1", "metadata": {}, "hash": "dc4c2a1a070b3b09f6e6e8c248cab18071c2df3eb741522c445fc03aec899266", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Is Self-knowledge and Action Consistent or Not:\nInvestigating Large Language Model\u2019s Personality\nYiming Ai 1 Zhiwei He 1 Ziyin Zhang 1 Wenhong Zhu1 Hongkun Hao 1 Kai Yu2 Lingjun Chen 3 Rui Wang1\nAbstract\nIn this study, we delve into the validity of conven-\ntional personality questionnaires in capturing the\nhuman-like personality traits of Large Language\nModels (LLMs). Our objective is to assess the\ncongruence between the personality traits LLMs\nclaim to possess and their demonstrated tenden-\ncies in real-world scenarios. By conducting an\nextensive examination of LLM outputs against\nobserved human response patterns, we aim to un-\nderstand the disjunction between self-knowledge\nand action in LLMs.\n1. Introduction\nPersonality, a foundational social, behavioral phenomenon\nin psychology, encompasses the unique patterns of thoughts,\nemotions, and behaviors of an entity (Allport, 1937; Roberts\n& Yoon, 2022). In humans, personality is shaped by biologi-\ncal and social factors, fundamentally influencing daily inter-\nactions and preferences (Roberts et al., 2007). Studies have\nindicated how personality information is richly encoded\nwithin human language (Goldberg, 1981; Saucier & Gold-\nberg, 2001). LLMs, containing extensive socio-political,\neconomic, and behavioral data, can generate language that\nexpresses personality content. Measuring and verifying the\nability of LLMs to synthesize personality brings hope for\nthe safety, responsibility, and coordination of LLM efforts\n(Gabriel, 2020) and sheds light on enhancing LLM perfor-\nmance in specific tasks through targeted adjustments.\nThus, evaluating the anthropomorphic personality perfor-\nmance of LLMs has become a shared interest across fields\nsuch as artificial intelligence(AI) studies, social sciences,\n1MT Lab, Department of Computer Science and Engineering,\nShanghai Jiao Tong University, Shanghai, China2X-LANCE Lab,\nDepartment of Computer Science and Engineering, Shanghai Jiao\nTong University, Shanghai, China3School of Education, Shanghai\nJiao Tong University, Shanghai, China. Correspondence to: Rui\nWang <wangrui12@sjtu.edu.cn>.\nProceedings of the 41 st International Conference on Machine\nLearning, Vienna, Austria. PMLR 235, 2024. Copyright 2024 by\nthe author(s).\ncognitive psychology, and psychometrics. A common\nmethod for assessment involves having LLMs answer per-\nsonality questionnaires (Huang et al., 2024). However, the\nreliability of LLMs\u2019 responses, whether the responses truly\nreflect LLMs\u2019 genuine personality inclinations, and whether\nLLMs\u2019 behavior in real-world scenarios aligns with their\nstated human-like personality tendencies remain unknown.\nTo illustrate such inconsistency in LLMs, we introduce two\nconcepts: self-knowledge 1 and action. In the following, self-\nknowledge specifically refers to an individual\u2019s understand-\ning and awareness of their own internal states, including\npersonality, emotions, values, motivations, and behavioral\npatterns. The term personality knowledge mentioned later is\nequivalent to self-knowledge. Action refers to the behavioral\nstate of an individual in actual situations. For humans, ac-\ntion is the way self-knowledge is transformed into external\nexpression. Self-knowledge and action are meant to be two\ninteracting aspects.\nFrom the perspective of LLMs, a discordance between an\nLLM\u2019s asserted self-knowledge and its action can result in\nnoteworthy adverse outcomes. For example, while an LLM\nmay claim to prioritize human friendliness, its failure to\nmanifest amicable behaviors in real-world situations is un-\ndoubtedly a circumstance we fervently seek to avert. Hence,\nour study endeavors to assess the alignment between the\npersonality traits claimed by LLMs and their actual behav-\nior tendency. From the perspective of personality scales,\nthere have been several studies investigating the reliability\nof personality questionnaires on LLMs (Huang et al., 2023;\nSafdari et al., 2023). However, there has yet to be any explo-\nration of the validity of psychological scales on LLMs. Our\nwork aims to address this gap in the research literature.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4087, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "093a4658-f460-40d5-a1f2-b14bcd70336b": {"__data__": {"id_": "093a4658-f460-40d5-a1f2-b14bcd70336b", "embedding": null, "metadata": {"page_label": "1", "file_name": "Is Self-knowledge and Action Consistent or Not Investigating Large Language Models Personality.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Is Self-knowledge and Action Consistent or Not Investigating Large Language Models Personality.pdf", "file_type": "application/pdf", "file_size": 659650, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "47ba212c-fa32-4730-86ea-70f7b8ef0ea1", "node_type": "4", "metadata": {"page_label": "1", "file_name": "Is Self-knowledge and Action Consistent or Not Investigating Large Language Models Personality.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Is Self-knowledge and Action Consistent or Not Investigating Large Language Models Personality.pdf", "file_type": "application/pdf", "file_size": 659650, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "061b767ad6486d5d307f2324f28234ff5f65ec9203fb2007c06f7886dd1bb9a2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "486e479c-e64c-4266-855a-eb4bc57b34c6", "node_type": "1", "metadata": {"page_label": "1", "file_name": "Is Self-knowledge and Action Consistent or Not Investigating Large Language Models Personality.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Is Self-knowledge and Action Consistent or Not Investigating Large Language Models Personality.pdf", "file_type": "application/pdf", "file_size": 659650, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "1b090c84ab7b141d9d4501d86e92f6f6737192fb90071f836bdebaa3907ac0d8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "From the perspective of LLMs, a discordance between an\nLLM\u2019s asserted self-knowledge and its action can result in\nnoteworthy adverse outcomes. For example, while an LLM\nmay claim to prioritize human friendliness, its failure to\nmanifest amicable behaviors in real-world situations is un-\ndoubtedly a circumstance we fervently seek to avert. Hence,\nour study endeavors to assess the alignment between the\npersonality traits claimed by LLMs and their actual behav-\nior tendency. From the perspective of personality scales,\nthere have been several studies investigating the reliability\nof personality questionnaires on LLMs (Huang et al., 2023;\nSafdari et al., 2023). However, there has yet to be any explo-\nration of the validity of psychological scales on LLMs. Our\nwork aims to address this gap in the research literature. In\ngeneral, our research makes three significant contributions:\n\u2022 We design a behavior tendency questionnaire that re-\nflects real-world situations and behaviors based on\nthem;\n\u2022 We evaluate the self-knowledge-action congruence of\nLLMs, revealing substantial disparities between LLMs\u2019\n1https://plato.stanford.edu/entries/\nself-knowledge/\n1\narXiv:2402.14679v2  [cs.CL]  10 Dec 2024", "mimetype": "text/plain", "start_char_idx": 3265, "end_char_idx": 4468, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "137e5b84-358b-4e54-bcf1-bb0c3d427592": {"__data__": {"id_": "137e5b84-358b-4e54-bcf1-bb0c3d427592", "embedding": null, "metadata": {"page_label": "2", "file_name": "Is Self-knowledge and Action Consistent or Not Investigating Large Language Models Personality.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Is Self-knowledge and Action Consistent or Not Investigating Large Language Models Personality.pdf", "file_type": "application/pdf", "file_size": 659650, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a2efa05f-e582-4af3-9798-c351506768bf", "node_type": "4", "metadata": {"page_label": "2", "file_name": "Is Self-knowledge and Action Consistent or Not Investigating Large Language Models Personality.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Is Self-knowledge and Action Consistent or Not Investigating Large Language Models Personality.pdf", "file_type": "application/pdf", "file_size": 659650, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "c07fe9d93ec61d678005d7db9642e9c1359ec705aff9d5375d5b95f9399bc18c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5fb5511c-6f36-4dc6-96ca-f6aa8b423f1e", "node_type": "1", "metadata": {}, "hash": "09a791b4727a9698ca24059f97dcf4ddf11eb8b86f4fe7f4f5c403841da5b4e0", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Is Self-knowledge and Action Consistent or Not: Investigating Large Language Model\u2019s Personality\npersonality knowledge and behavioral inclinations;\n\u2022 We empirically test various LLMs against observed\nhuman response patterns and formulate conjectures,\nthereby shedding light on the potential and limitations\nof LLMs in mimicking complex human psychological\ntraits.\nIn Section 2, we introduce the process of our corpus design.\nSection 3 presents the our empirical analysis \u2013 evaluating\nself-knowledge-action congruence of various LLMs. Fi-\nnally, in Section 4, we conclude our work.\n2. Corpus Design\nIn the nuanced exploration of anthropomorphic personal-\nity traits within LLMs, selecting the most appropriate per-\nsonality tests is paramount. Among diverse personality\nassessments, the comprehensive coverage of personality\ndimensions, theoretical robustness, and practical relevance\nmake the Big Five Personality Traits (Goldberg, 1981; Costa\n& McCrae, 2008) and the Myers-Briggs Type Indicator\n(MBTI) (Myers, 1962) the most fitting choices for our study.\nTo devise a straightforward yet impactful evaluation of\nLLMs\u2019 personality traits, we\u2019ve opted for two question-\nnaires (TDA-100 (Goldberg, 1992) and BFI-44 (John et al.,\n1991)) rooted in the Big Five model, along with one ques-\ntionnaire (16 Personalities 2) based on the MBTI model.\nThese selections were made due to their proven high relia-\nbility and validity in both English and Chinese (Goldberg,\n1992; John et al., 1991; Makwana & Dave, 2020; Zhang,\n2012). Based on these questionnaires, we ensure that our\ninvestigation into the anthropomorphic traits of LLMs is\ngrounded in robust psychological methodology and thereby\nconstruct a bilingual personality knowledge questionnaire,\nincluding a total of 180 statements.\nIn the following, we will detail the methodology adopted to\ncreate a comprehensive corpus aimed at evaluating the con-\ngruence between the personality traits professed by LLMs\nand their behavior tendency. The corpus is comprised of 2\nparts: a personality knowledge questionnaire and a behavior\ntendency questionnaire. The former includes 180 statements\nmentioned before, and the latter is closely aligned with the\nformer.\nWe apply the common method of constructing behavioral\nprocedures approach test,sample approach, which assumes\nthat the test behavior constitutes a subset of the actual be-\nhaviors of interest (Golfried & Kent, 1972). The detailed\ndesign process is outlined as follows:\nStep 1: As Golfried & Kent (1972) has mentioned that\nthe ideal approach to response expression would constitute\n2https://www.16personalities.com/\nthe individual\u2019s actual response in a real-life situation, in\nthat this represents the most direct approach to behavioral\nsampling. We recruited 16 individuals, each representing\na distinct MBTI type, to undertake the following task: for\nevery statement in the personality knowledge questionnaire,\nthey provided a practical scenario case. Each scenario case\ncomprises situations drawn from their own lives, along with\ntwo completely contrasting actions: Action A and Action\nB. Action A fully aligns with the statement, while Action\nB completely contradicts it. The content of Action A and\nAction B need to be kept basically the same length.\nStep 2: Following the acquisition of the 16 practical sce-\nnario cases corresponding to each statement, we condensed\nthem into a single case. For 19 statements exhibiting signifi-\ncant variations in cases, we amalgamated them into 2 to 3\ncases.\nStep 3: For statements associated with multiple practical\nscenario cases, we tasked the previously enlisted 16 individ-\nuals to assign ratings to each case. A rating of 1 was given\nif they believed the case accurately reflected the meaning\nof the corresponding statement in the personality knowl-\nedge questionnaire; otherwise, a rating of 0 was assigned.\nThe case with the highest score for these 19 statements was\nselected as the final practical scenario case.\nStep 4: We enlisted the participation of 10 reviewers to\nassess the consistency of the 180 personality knowledge -\npractical scenario pairs. The results demonstrate that the\nconsistency approval rate for each pair exceeds 90%.\nAll the individuals involved are native Chinese speakers with\na level of English proficiency of CEFR C1.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4297, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5fb5511c-6f36-4dc6-96ca-f6aa8b423f1e": {"__data__": {"id_": "5fb5511c-6f36-4dc6-96ca-f6aa8b423f1e", "embedding": null, "metadata": {"page_label": "2", "file_name": "Is Self-knowledge and Action Consistent or Not Investigating Large Language Models Personality.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Is Self-knowledge and Action Consistent or Not Investigating Large Language Models Personality.pdf", "file_type": "application/pdf", "file_size": 659650, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a2efa05f-e582-4af3-9798-c351506768bf", "node_type": "4", "metadata": {"page_label": "2", "file_name": "Is Self-knowledge and Action Consistent or Not Investigating Large Language Models Personality.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Is Self-knowledge and Action Consistent or Not Investigating Large Language Models Personality.pdf", "file_type": "application/pdf", "file_size": 659650, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "c07fe9d93ec61d678005d7db9642e9c1359ec705aff9d5375d5b95f9399bc18c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "137e5b84-358b-4e54-bcf1-bb0c3d427592", "node_type": "1", "metadata": {"page_label": "2", "file_name": "Is Self-knowledge and Action Consistent or Not Investigating Large Language Models Personality.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Is Self-knowledge and Action Consistent or Not Investigating Large Language Models Personality.pdf", "file_type": "application/pdf", "file_size": 659650, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "cb819ef1731746c83efb597fb7bb2c9ad2b56fd3ed09f4bc35f6784897495f5d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "For 19 statements exhibiting signifi-\ncant variations in cases, we amalgamated them into 2 to 3\ncases.\nStep 3: For statements associated with multiple practical\nscenario cases, we tasked the previously enlisted 16 individ-\nuals to assign ratings to each case. A rating of 1 was given\nif they believed the case accurately reflected the meaning\nof the corresponding statement in the personality knowl-\nedge questionnaire; otherwise, a rating of 0 was assigned.\nThe case with the highest score for these 19 statements was\nselected as the final practical scenario case.\nStep 4: We enlisted the participation of 10 reviewers to\nassess the consistency of the 180 personality knowledge -\npractical scenario pairs. The results demonstrate that the\nconsistency approval rate for each pair exceeds 90%.\nAll the individuals involved are native Chinese speakers with\na level of English proficiency of CEFR C1. The detailed\ninstructions for the scenario providers and reviewers are\nshown in Appendix G. Several examples of a personality\nknowledge - practical scenario pair are shown in Appendix\nD.\nThe culmination of this meticulous process is a bilingual\nEnglish-Chinese Parallel Sentence Pair Self-knowledge-\nAction Test Set, comprising 180 matched pairs of personal-\nity knowledge and action scenarios. This corpus serves as\na fundamental tool in our study, allowing us to rigorously\nevaluate the LLMs\u2019 proficiency in understanding and acting\nupon various personality traits, bridging the gap between\npersonality understanding and practical action in the realm\nof AI.\n3. Experiment on LLMs\u2019\nSelf-knowledge-Action Congruence\n3.1. Experiment\nAmong all LLMs, we selected baize-v2-7b, ChatGLM3,\nGPT-3.5-turbo, GPT-4, internLM-chat-7b, Mistral-7b, MPT-\n7b-chat, Qwen-14b-chat, TULU-2-DPO-7b, Vicuna-13b,\n2", "mimetype": "text/plain", "start_char_idx": 3400, "end_char_idx": 5190, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1ce29805-188b-417d-a30f-af9c2955d2b5": {"__data__": {"id_": "1ce29805-188b-417d-a30f-af9c2955d2b5", "embedding": null, "metadata": {"page_label": "3", "file_name": "Is Self-knowledge and Action Consistent or Not Investigating Large Language Models Personality.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Is Self-knowledge and Action Consistent or Not Investigating Large Language Models Personality.pdf", "file_type": "application/pdf", "file_size": 659650, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c5f0cfe2-8c16-4f64-b558-2ab8fdf4cafd", "node_type": "4", "metadata": {"page_label": "3", "file_name": "Is Self-knowledge and Action Consistent or Not Investigating Large Language Models Personality.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Is Self-knowledge and Action Consistent or Not Investigating Large Language Models Personality.pdf", "file_type": "application/pdf", "file_size": 659650, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "2430a9c8dd69502e5f9dadfe6c4edeaa22837974d92797d4cd507e2f898350ef", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bdeedcd9-20e6-4219-b236-da2b8b56d9e0", "node_type": "1", "metadata": {}, "hash": "75102b2d38f8da93c9035d6dd8b66ab44abdd176a68a5d27b52704e28d892b8f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Is Self-knowledge and Action Consistent or Not: Investigating Large Language Model\u2019s Personality\nVicuna-33b and Zephyr-7b, 12 LLMs in total, who could\nanswer the personality cognitive questionnaire in the form\nof a Q&A. The detailed setup is shown in Appendix D. Then,\nwe rewrote a prompt for LLM to answer the former part\nof our corpus - personality knowledge questionnaire based\non the response requirements of the MBTI-M questionnaire\n(GU & Hu, 2012) in Appendix D.\nUpon reviewing the responses from the LLMs, we discov-\nered that some LLMs failed to grasp the intended mean-\ning of the prompts, resulting in unreasonable responses as\ndetailed in Appendix A. Out of the LLMs assessed, only\nseven LLMs, ChatGLM3, GPT-4, GPT-3.5-turbo, Mistral-\n7b, Vicuna-13b, Vicuna-33b, and Zephyr7b, produced valid\nresponses. Subsequently, we sifted through these valid re-\nsponses, computed their averages to represent the LLMs\u2019\nactual responses, and proceeded to evaluate the reliability\nof these responses, as outlined in Appendix B. Following\nthis assessment, we determined that the responses from\nChatGLM3, GPT-3.5-turbo, GPT4, Vicuna13b and Vi-\ncuna33b are reliable for further personality analysis.\nIn the following, we explore the alignment between re-\nsponses given by LLMs to personality knowledge question-\nnaires and their actions within designed scenarios. Regard-\ning the prompt for questioning, we selected the instructions\nof five common academic questionnaires with effective anal-\nysis of reliability and validity (Makwana & Dave, 2020;\nJohnson et al., 1998; Goldberg, 1992; John et al., 1991;\nNardi, 2011), 16 Personalities Test, MBTI-M, TDA-100,\nBFI-44-Children adapted and Dario Nardi\u2019s Cognitive Test,\nas the prompt for the LLM of questioning of the personal-\nity knowledge questionnaire. We utilize various prompts\nto prevent any particular prompt from exerting a specific\ninfluence on LLM responses, thereby accurately reflecting\nthe general tendencies of LLMs when answering personality\nknowledge questionnaires.\nAs for the responding approach to the personality knowl-\nedge questionnaire, according to the structure of the chosen\npersonality scales in 2, responses to statements are initially\nmapped on a 7-point Likert scale, ranging from 1 to 7. Ac-\ncording to several previous studies, when responding to\npersonality scales, LLMs\u2019 answers often remain consistent,\nregardless of factors such as question order, quantity, an-\nswer sequence, or timing of inquiry. (Huang et al., 2023;\nSafdari et al., 2023). Therefore, for each prompt, we asked\neach LLM 10 times with the original form of our chosen\npersonality scales and then screened the valid responses.\nWe averaged all the valid responses to reduce errors and\nreflect the general LLMs\u2019 response pattern. and rounded\nthe average response to each statement to the nearest whole\nnumber as each LLM\u2019s response to the personality knowl-\nedge questionnaire. The details of the prompts are shown in\nAppendix D.\nConcerning the prompt for LLM to answer the latter part\nof our corpus-behavior tendency questionnaire, we inherit\nthe instruction of the MBTI-M questionnaire (GU & Hu,\n2012) and rewrite it, for we intend to change the responding\napproach.\nWe apply a 7-point graded forced-choice format (Brown &\nMaydeu-Olivares, 2018) as the responding approach. Cur-\nrently, the commonly used response formats for question-\nnaires in psychometrics are the forced-choice format (Sis-\nson, 1948) and the Likert scale format (Joshi et al., 2015).\nIn comparison to traditional forced-choice scales, graded\nforced-choice scales exhibit comparable validity, superior\nreliability and model fit. Contrary to Likert scales, graded\nforced-choice scales show better model fit and slightly\nhigher self-other agreement (Zhang et al., 2023).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3784, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "bdeedcd9-20e6-4219-b236-da2b8b56d9e0": {"__data__": {"id_": "bdeedcd9-20e6-4219-b236-da2b8b56d9e0", "embedding": null, "metadata": {"page_label": "3", "file_name": "Is Self-knowledge and Action Consistent or Not Investigating Large Language Models Personality.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Is Self-knowledge and Action Consistent or Not Investigating Large Language Models Personality.pdf", "file_type": "application/pdf", "file_size": 659650, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c5f0cfe2-8c16-4f64-b558-2ab8fdf4cafd", "node_type": "4", "metadata": {"page_label": "3", "file_name": "Is Self-knowledge and Action Consistent or Not Investigating Large Language Models Personality.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Is Self-knowledge and Action Consistent or Not Investigating Large Language Models Personality.pdf", "file_type": "application/pdf", "file_size": 659650, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "2430a9c8dd69502e5f9dadfe6c4edeaa22837974d92797d4cd507e2f898350ef", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1ce29805-188b-417d-a30f-af9c2955d2b5", "node_type": "1", "metadata": {"page_label": "3", "file_name": "Is Self-knowledge and Action Consistent or Not Investigating Large Language Models Personality.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Is Self-knowledge and Action Consistent or Not Investigating Large Language Models Personality.pdf", "file_type": "application/pdf", "file_size": 659650, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "d36cd4989922afa8e977f4d35d810ca94662c9c6ab10303921164377a4f7fbb3", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We apply a 7-point graded forced-choice format (Brown &\nMaydeu-Olivares, 2018) as the responding approach. Cur-\nrently, the commonly used response formats for question-\nnaires in psychometrics are the forced-choice format (Sis-\nson, 1948) and the Likert scale format (Joshi et al., 2015).\nIn comparison to traditional forced-choice scales, graded\nforced-choice scales exhibit comparable validity, superior\nreliability and model fit. Contrary to Likert scales, graded\nforced-choice scales show better model fit and slightly\nhigher self-other agreement (Zhang et al., 2023). The spe-\ncific meaning of numbers in common 7-point graded forced-\nchoice is shown in Appendix E.\nHere, given that we have rewritten the prompt of responding\nto personality knowledge questionnaire based on the original\ninstructions of the chosen personality scales, thereby not\nindicating the specific meaning of numbers 2, 3, 5 and 6. We\nfollowed this prompt pattern to avoid influence on LLMs\u2019\nresponses brought by such change, which means only retain\nthe meaning of numbers 1, 4 and 7. Hence, the specific\nprompt is: Read the following scenarios with actions A and\nB carefully and rate each scenario in the range from 1 to\n7. 1 means that action A applies to you completely in this\nscenario, 4 means that action A and action B equally apply\n(or not) to you in this scenario, and 7 means that action B\napplies to you completely in this scenario. You only need to\ngive the number.\nThese measures above allow us to to observe the congruence\nbetween self-knowledge and action of LLMs, to compare\nhuman and LLM responses.\n3.2. Results\nTo quantify the similarity between responses, we employ\nthe following four metrics: cosine similarity, Spearman\u2019s\nrank correlation coefficient, value mean difference (VMD)\nand Proportion of Consistent Pairs.\nCosine Similarity A measure used to calculate the cosine\nof the angle between two vectors in a multi-dimensional\nspace, offering a value range from -1 (exactly opposite) to\n1 (exactly the same), where higher values indicate greater\nsimilarity.\nscos =\nPn\ni=1 (xi \u00d7 yi)qPn\ni=1 (xi)2 \u00d7\nqPn\ni=1 (yi)2 , (1)\nwhere xi are LLMs\u2019 responses of personality knowledge\nquestionnaire, yi are LLMs\u2019 corresponding responses of\nscenario and action questionnaire, and xi and yi correspond\n3", "mimetype": "text/plain", "start_char_idx": 3212, "end_char_idx": 5499, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "97be7611-a131-490c-888c-b98cd2aba5ac": {"__data__": {"id_": "97be7611-a131-490c-888c-b98cd2aba5ac", "embedding": null, "metadata": {"page_label": "4", "file_name": "Is Self-knowledge and Action Consistent or Not Investigating Large Language Models Personality.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Is Self-knowledge and Action Consistent or Not Investigating Large Language Models Personality.pdf", "file_type": "application/pdf", "file_size": 659650, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0defce02-ea5d-4345-994e-e84e6570e0b4", "node_type": "4", "metadata": {"page_label": "4", "file_name": "Is Self-knowledge and Action Consistent or Not Investigating Large Language Models Personality.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Is Self-knowledge and Action Consistent or Not Investigating Large Language Models Personality.pdf", "file_type": "application/pdf", "file_size": 659650, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "efececee8aafb9b019b0c09d33b8cc40cebce686f51b4f00df844792bf57a6f3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "24341575-0552-4162-92a7-4b04ea961302", "node_type": "1", "metadata": {}, "hash": "df4f44f6f32e35d168e199cf0887756e2f256fc4657b88107d6fd0c590dd1c6b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Is Self-knowledge and Action Consistent or Not: Investigating Large Language Model\u2019s Personality\nTable 1. LLMs\u2019 Self-knowledge - Action Congruence Performance with Reference of Human Respondents\u2019 Performance (A VG, SD,\nMIN and MAX represents the average number, standard deviation, minimum and maximum.)\nLLMs & Human\nRespondents\nCosine\nSimilarity\nSpearman Rank\nCorrelation Coefficient\nValue Mean\nDifference\nProportion of\nConsistent Pairs\nChatGLM3 0.24 0.23 1.58 47.22%\nGPT-3.5-turbo 0.17 0.19 1.74 50.56%\nGPT-4 0.52 0.56 1.02 78.89%\nVicuna-13b 0.08 0.07 1.57 52.78%\nVicuna-33b 0.18 0.06 1.68 52.22%\nLLMs(A VG\u00b1 SD) 0.24 \u00b1 0.15 0.22 \u00b1 0.18 1.52 \u00b1 0.26 56.78 \u00b1 11.25%\nHuman(A VG\u00b1 SD) 0.76 \u00b1 0.09 0.78 \u00b1 0.08 0.69 \u00b1 0.27 84.69 \u00b1 8.22%\nHuman(MIN) 0.61 0.66 1.08 73.78%\nHuman(MAX) 0.95 0.96 0.07 99.44%\nto each other one-to-one.\nSpearman\u2019s Rank Correlation Coefficient A non-\nparametric measure of rank correlation, assessing how well\nthe relationship between two variables can be described\nusing a monotonic function. Its value ranges from -1 to\n1, where 1 means a perfect association of ranks. Specifi-\ncally, we rank the responses on two questionnaires of the\nLLMs based on their numerical values separately. Then,\nwe calculate the difference in rankings for each personality\nknowledge \u2013 scenario & action pair. Afterwards, we use the\nfollowing formula to calculate the coefficient rs.\nrs = 1\u2212 6 Pd2\ni\nn(n2 \u2212 1), (2)\nwhere di is the difference in rankings of each pair and n is\nthe total count of pairs.\nValue Mean Difference (VMD) Value Mean Difference\nis the average difference in responses across all paired items\nin the questionnaires, as shown in the formula below.\nVMD =\nPdi\nn , (3)\nwhere di is the difference of responses in each pair.\nProportion of Consistent Pairs Recognizing that minor\ndiscrepancies are natural when comparing psychological\ntendencies with actual actions, this metric quantifies the\nproportion of item pairs with a response difference of 1 or\nless, focusing on the consistency of tendencies rather than\nexact matches.\nPc = Nc\nNt\n, (4)\nwhere Nc is the number of consistent pairs, Nt is the total\nnumber of pairs.\nFor this study, we recruited 16 participants, comprising 8\nmales and 8 females, all native Chinese speakers with an\nEnglish proficiency level of CEFR C1. As shown in Table\n1, the analysis of their response data yielded an average\nCosine Similarity and Spearman\u2019s Rank Correlation Coeffi-\ncient above 0.75, with a Value Mean Difference around 0.68,\nand a Proportion of Consistent Pairs exceeding 84%. These\nresults indicate a high degree of similarity and strong corre-\nlation between responses to the two types of questionnaires,\nsuggesting a basic consistency in human self-knowledge and\nan ability to align self-knowledge with action in real-life\nscenarios.\nThe same questionnaires were administered to the 5 LLMs\nselected in Section B, and their responses were analyzed\nusing the aforementioned metrics. Compared to human\nrespondents, the similarity in LLMs\u2019 responses is notably\nlower, and the corresponding significance test is shown in\nAppendix F. Specifically, the average Cosine Similarity and\nSpearman\u2019s Rank Correlation Coefficient for LLMs are sub-\nstantially below those of human respondents, with a huge\ndifference exceeding 0.42. The Value Mean Difference for\nLLMs averages around 1.52, indicating a substantial diver-\ngence in self-knowledge between the two types of question-\nnaires for LLMs.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3445, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "24341575-0552-4162-92a7-4b04ea961302": {"__data__": {"id_": "24341575-0552-4162-92a7-4b04ea961302", "embedding": null, "metadata": {"page_label": "4", "file_name": "Is Self-knowledge and Action Consistent or Not Investigating Large Language Models Personality.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Is Self-knowledge and Action Consistent or Not Investigating Large Language Models Personality.pdf", "file_type": "application/pdf", "file_size": 659650, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0defce02-ea5d-4345-994e-e84e6570e0b4", "node_type": "4", "metadata": {"page_label": "4", "file_name": "Is Self-knowledge and Action Consistent or Not Investigating Large Language Models Personality.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Is Self-knowledge and Action Consistent or Not Investigating Large Language Models Personality.pdf", "file_type": "application/pdf", "file_size": 659650, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "efececee8aafb9b019b0c09d33b8cc40cebce686f51b4f00df844792bf57a6f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "97be7611-a131-490c-888c-b98cd2aba5ac", "node_type": "1", "metadata": {"page_label": "4", "file_name": "Is Self-knowledge and Action Consistent or Not Investigating Large Language Models Personality.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Is Self-knowledge and Action Consistent or Not Investigating Large Language Models Personality.pdf", "file_type": "application/pdf", "file_size": 659650, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "ef8f56a1ef6563d41b1a5e7845594fe893abfe042d96d1323596cb12fb6da301", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The same questionnaires were administered to the 5 LLMs\nselected in Section B, and their responses were analyzed\nusing the aforementioned metrics. Compared to human\nrespondents, the similarity in LLMs\u2019 responses is notably\nlower, and the corresponding significance test is shown in\nAppendix F. Specifically, the average Cosine Similarity and\nSpearman\u2019s Rank Correlation Coefficient for LLMs are sub-\nstantially below those of human respondents, with a huge\ndifference exceeding 0.42. The Value Mean Difference for\nLLMs averages around 1.52, indicating a substantial diver-\ngence in self-knowledge between the two types of question-\nnaires for LLMs. And as for most LLMs, the proportion\nof consistent pairs falls below 55%, raising questions about\nLLMs\u2019 ability to achieve self-knowledge-action unity in\npractice.\n4. Conclusion\nWe demonstrate that while LLMs exhibit some capacity to\nmimic human-like tendencies, there are significant gaps in\nthe coherence between their stated personality and exhibited\nbehaviors. This disparity probably suggests a limitation in\nLLMs\u2019 ability to authentically replicate human personality\ndynamics. Our study underscores the importance of further\nexploration into enhancing LLMs\u2019 ability to perform more\ngenuinely human-like interactions, suggesting avenues for\nfuture research in improving the psychological realism of\n4", "mimetype": "text/plain", "start_char_idx": 2797, "end_char_idx": 4151, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "6e85332c-a9ef-4f4d-b1b3-6a84a2163910": {"__data__": {"id_": "6e85332c-a9ef-4f4d-b1b3-6a84a2163910", "embedding": null, "metadata": {"page_label": "5", "file_name": "Is Self-knowledge and Action Consistent or Not Investigating Large Language Models Personality.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Is Self-knowledge and Action Consistent or Not Investigating Large Language Models Personality.pdf", "file_type": "application/pdf", "file_size": 659650, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a363b883-8b6e-45e6-a6e4-78d54bc51cf8", "node_type": "4", "metadata": {"page_label": "5", "file_name": "Is Self-knowledge and Action Consistent or Not Investigating Large Language Models Personality.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Is Self-knowledge and Action Consistent or Not Investigating Large Language Models Personality.pdf", "file_type": "application/pdf", "file_size": 659650, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "a0bd5092cd5e4cf3cbe18dfa79ce2b8816ed5e463e31305abb5451522b528099", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "dc84dba2-5377-460c-b83c-a7d56cd34395", "node_type": "1", "metadata": {}, "hash": "f553a40dafb9eb79e8f14bca397ad42f177fd8566b394fe5a7d5f58dad2ade24", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Is Self-knowledge and Action Consistent or Not: Investigating Large Language Model\u2019s Personality\nLLM outputs.\nLimitations\nIn this study, we delve into the alignment between what\nLarge Language Models (LLMs) claim and their actions,\naiming to discern if there\u2019s a consistency in their self-\nknowledge and their actual behavior tendency. This ob-\nservation is merely one among several hypotheses exploring\nthe root causes of this inconsistency, underscoring the need\nfor further investigation into the fundamental reasons be-\nhind it. Moreover, the scope of our initial experiments was\nlimited to a selection of several LLMs. Future endeavors\nwill expand this investigation to encompass a broader array\nof models. Additionally, our study has yet to identify an\neffective strategy for enhancing the congruence between\nLLMs\u2019 self-knowledge and action. As we move forward,\nour efforts will focus on leveraging the insights gained from\nthis research to improve the performance and reliability\nof LLMs, paving the way for models that more accurately\nmirror human thought and behavior.\nImpact Statement\nOur personality knowledge survey leverages the TDA-100,\nBFI-44, and the 16 Personalities Test, which are extensively\nrecognized and employed within the personality knowledge\ndomain. These tests, available in both Chinese and English,\nare backed by thorough reliability and validity analyses. We\nensured the integrity of these instruments by maintaining\ntheir original content without any modifications. The de-\nsign of every questionnaire intentionally avoids any bias\nrelated to gender and is free from racial content, fostering\nan inclusive approach. Participants\u2019 anonymity was strictly\npreserved during the survey process. Moreover, all indi-\nviduals were fully informed about the purpose of the study\nand consented to their responses being utilized for scientific\nresearch, thereby arising no ethical issues.\nAcknowledgement\nThis paper is partially supported by SMP-Zhipu.AI Large\nModel Cross-Disciplinary Fund.\nReferences\nAllport, G. W. Personality: A psychological interpreta-\ntion. 1937. URL https://psycnet.apa.org/\nrecord/1938-01964-000.\nBrown, A. and Maydeu-Olivares, A. Ordinal fac-\ntor analysis of graded-preference questionnaire\ndata. Structural Equation Modeling: A Multidis-\nciplinary Journal , 25(4):516\u2013529, 2018. URL\nhttps://www.tandfonline.com/doi/abs/\n10.1080/10705511.2017.1392247.\nCosta, P. T. and McCrae, R. R. The revised neo personality\ninventory (neo-pi-r). The SAGE handbook of personality\ntheory and assessment, 2(2):179\u2013198, 2008.\nGabriel, I. Artificial intelligence, values, and align-\nment. Minds and machines , 30(3):411\u2013437, 2020.\nURL https://link.springer.com/article/\n10.1007/s11023-020-09539-2 .\nGoldberg, L. R. Language and individual differ-\nences: The search for universals in personality\nlexicons. Review of personality and social psy-\nchology, 2(1):141\u2013165, 1981. URL https:\n//www.scienceopen.com/document?vid=\n3cdca9a2-ab50-48bf-97b5-0c2236e65098 .\nGoldberg, L. R. The development of markers for the\nbig-five factor structure. Psychological assessment, 4\n(1):26, 1992. URL https://psycnet.apa.org/\nrecord/1992-25730-001.\nGolfried, M. R. and Kent, R. N. Traditional versus behav-\nioral personality assessment: A comparison of method-\nological and theoretical assumptions. Psychological\nBulletin, 77(6):409, 1972. URL https://psycnet.\napa.org/record/1972-29191-001.\nGU, X.-Y . and Hu, S. Mbti: New develop-\nment and application. Advances in Psycho-\nlogical Science , 20(10):1700, 2012. URL\nhttps://journal.psych.ac.cn/xlkxjz/\nEN/10.3724/SP.J.1042.2012.01700.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3594, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "dc84dba2-5377-460c-b83c-a7d56cd34395": {"__data__": {"id_": "dc84dba2-5377-460c-b83c-a7d56cd34395", "embedding": null, "metadata": {"page_label": "5", "file_name": "Is Self-knowledge and Action Consistent or Not Investigating Large Language Models Personality.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Is Self-knowledge and Action Consistent or Not Investigating Large Language Models Personality.pdf", "file_type": "application/pdf", "file_size": 659650, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a363b883-8b6e-45e6-a6e4-78d54bc51cf8", "node_type": "4", "metadata": {"page_label": "5", "file_name": "Is Self-knowledge and Action Consistent or Not Investigating Large Language Models Personality.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Is Self-knowledge and Action Consistent or Not Investigating Large Language Models Personality.pdf", "file_type": "application/pdf", "file_size": 659650, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "a0bd5092cd5e4cf3cbe18dfa79ce2b8816ed5e463e31305abb5451522b528099", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6e85332c-a9ef-4f4d-b1b3-6a84a2163910", "node_type": "1", "metadata": {"page_label": "5", "file_name": "Is Self-knowledge and Action Consistent or Not Investigating Large Language Models Personality.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Is Self-knowledge and Action Consistent or Not Investigating Large Language Models Personality.pdf", "file_type": "application/pdf", "file_size": 659650, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "1f5222ea111d96adf024d83ed7748ee01c3588a30522a8ff0ae67ee2f2bb8e77", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Psychological assessment, 4\n(1):26, 1992. URL https://psycnet.apa.org/\nrecord/1992-25730-001.\nGolfried, M. R. and Kent, R. N. Traditional versus behav-\nioral personality assessment: A comparison of method-\nological and theoretical assumptions. Psychological\nBulletin, 77(6):409, 1972. URL https://psycnet.\napa.org/record/1972-29191-001.\nGU, X.-Y . and Hu, S. Mbti: New develop-\nment and application. Advances in Psycho-\nlogical Science , 20(10):1700, 2012. URL\nhttps://journal.psych.ac.cn/xlkxjz/\nEN/10.3724/SP.J.1042.2012.01700.\nHuang, J., Wang, W., Lam, M. H., Li, E. J., Jiao, W., and\nLyu, M. R. Revisiting the reliability of psychological\nscales on large language models, 2023. URL https:\n//arxiv.org/abs/2305.19926v3.\nHuang, J., Wang, W., Li, E. J., Lam, M. H., Ren, S., Yuan,\nY ., Jiao, W., Tu, Z., and Lyu, M. R. Who is chatgpt?\nbenchmarking llms\u2019 psychological portrayal using psy-\nchobench, 2024. URL https://arxiv.org/abs/\n2310.01386.\nJohn, O. P., Donahue, E. M., and Kentle, R. L. Big\nfive inventory. Journal of personality and social psy-\nchology, 1991. URL https://psycnet.apa.org/\ndoiLanding?doi=10.1037%2Ft07550-000.\nJohnson, W. L., Johnson, A. M., Murphy, S. D., Weiss,\nA., and Zimmerman, K. J. A third-order component\nanalysis of the myers-briggs type indicator. Educa-\ntional and psychological measurement, 58(5):820\u2013831,\n1998. URL https://journals.sagepub.com/\ndoi/abs/10.1177/0013164498058005007.\n5", "mimetype": "text/plain", "start_char_idx": 3065, "end_char_idx": 4483, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "05c5cb43-8417-45d9-b07c-3a11d7a9f595": {"__data__": {"id_": "05c5cb43-8417-45d9-b07c-3a11d7a9f595", "embedding": null, "metadata": {"page_label": "6", "file_name": "Is Self-knowledge and Action Consistent or Not Investigating Large Language Models Personality.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Is Self-knowledge and Action Consistent or Not Investigating Large Language Models Personality.pdf", "file_type": "application/pdf", "file_size": 659650, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a07860db-4f84-4673-840e-80adb4b7c7f2", "node_type": "4", "metadata": {"page_label": "6", "file_name": "Is Self-knowledge and Action Consistent or Not Investigating Large Language Models Personality.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Is Self-knowledge and Action Consistent or Not Investigating Large Language Models Personality.pdf", "file_type": "application/pdf", "file_size": 659650, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "1ab39b8582db469ede79123e650c5d95ff2a3c62568bca3238a22c88c9732881", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Is Self-knowledge and Action Consistent or Not: Investigating Large Language Model\u2019s Personality\nJoshi, A., Kale, S., Chandel, S., and Pal, D. K. Lik-\nert scale: Explored and explained. British jour-\nnal of applied science & technology , 7(4):396\u2013403,\n2015. URL http://research.sdpublishers.\nnet/id/eprint/2464/.\nMakwana, K. and Dave, D. G. B. Confirmatory factor anal-\nysis of neris type explorer\u00ae scale\u2013a tool for personality\nassessment. International Journal of Management, 11(9),\n2020. URL https://papers.ssrn.com/sol3/\npapers.cfm?abstract_id=3709640.\nMiotto, M., Rossberg, N., and Kleinberg, B. Who is GPT-\n3? an exploration of personality, values and demograph-\nics. In Bamman, D., Hovy, D., Jurgens, D., Keith, K.,\nO\u2019Connor, B., and V olkova, S. (eds.), Proceedings of\nthe Fifth Workshop on Natural Language Processing\nand Computational Social Science (NLP+CSS), pp. 218\u2013\n227, Abu Dhabi, UAE, November 2022. Association\nfor Computational Linguistics. doi: 10.18653/v1/2022.\nnlpcss-1.24. URL https://aclanthology.org/\n2022.nlpcss-1.24.\nMyers, I. B. The myers-briggs type indicator: Manual\n(1962). 1962. URL https://psycnet.apa.org/\nrecord/2013-29682-000?doi=1.\nNardi, D. Neuroscience of personality. Neuroscience, 2:\n10\u20132012, 2011. URL https://core.ac.uk/pdf/\naaa287819128.pdf.\nRoberts, B. W. and Yoon, H. J. Personality\npsychology. Annual review of psychology ,\n73:489\u2013516, 2022. URL https://www.\nannualreviews.org/doi/abs/10.1146/\nannurev-psych-020821-114927 .\nRoberts, B. W., Kuncel, N. R., Shiner, R., Caspi,\nA., and Goldberg, L. R. The power of per-\nsonality: The comparative validity of personality\ntraits, socioeconomic status, and cognitive ability\nfor predicting important life outcomes. Perspec-\ntives on Psychological science , 2(4):313\u2013345, 2007.\nURL https://journals.sagepub.com/doi/\nabs/10.1111/j.1745-6916.2007.00047.x.\nSafdari, M., Serapio-Garc\u00b4\u0131a, G., Crepy, C., Fitz, S., Romero,\nP., Sun, L., Abdulhai, M., Faust, A., and Matari \u00b4c,\nM. Personality traits in large language models. arXiv\npreprint arXiv:2307.00184 , 2023. URL https://\narxiv.org/abs/2307.00184.\nSaucier, G. and Goldberg, L. R. Lexical studies of in-\ndigenous personality factors: Premises, products, and\nprospects. Journal of personality, 69(6):847\u2013879, 2001.\nSisson, E. D. Forced choice\u2014the new army rating 1. Per-\nsonnel Psychology, 1(3):365\u2013381, 1948. URL https:\n//onlinelibrary.wiley.com/doi/abs/10.\n1111/j.1744-6570.1948.tb01316.x.\nZhang, B., Luo, J., and Li, J. Moving beyond lik-\nert and traditional forced-choice scales: A compre-\nhensive investigation of the graded forced-choice for-\nmat. Multivariate Behavioral Research , pp. 1\u201327,\n2023. URL https://www.tandfonline.com/\ndoi/abs/10.1080/00273171.2023.2235682.\nZhang, X. Preliminary revision of the Big Five Personal-\nity Inventory (IPIP NEO-PI-R). PhD thesis, Yangzhou\nUniversity, 2012.\n6", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2839, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "6ab425c6-f964-41a0-8cfe-31a3a69c86cf": {"__data__": {"id_": "6ab425c6-f964-41a0-8cfe-31a3a69c86cf", "embedding": null, "metadata": {"page_label": "7", "file_name": "Is Self-knowledge and Action Consistent or Not Investigating Large Language Models Personality.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Is Self-knowledge and Action Consistent or Not Investigating Large Language Models Personality.pdf", "file_type": "application/pdf", "file_size": 659650, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "99f38eb9-9953-44e4-9411-21ad8234c490", "node_type": "4", "metadata": {"page_label": "7", "file_name": "Is Self-knowledge and Action Consistent or Not Investigating Large Language Models Personality.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Is Self-knowledge and Action Consistent or Not Investigating Large Language Models Personality.pdf", "file_type": "application/pdf", "file_size": 659650, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "ffad56d6e3fcca1aba6abe2dd0369288ade5255cd894044bdd51a52d0aa8e7e2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Is Self-knowledge and Action Consistent or Not: Investigating Large Language Model\u2019s Personality\nA. LLMs\u2019 Unreasonable Responses\nThe unreasonable responses mainly fall into the following five categories:\n\u2022 All responses are the same number;\n\u2022 All responses are greater than or equal to 4 or less than or equal to 4. (Due to the presence of both positive and negative\ndescriptions for the same assessment dimension (e.g., Openness) in our personality knowledge questionnaire, it is\nimpossible for a participant to answer with all responses greater than or equal to 4, indicating agreement or neutrality\nfor all statements, or all responses less than or equal to 4, indicating disagreement or neutrality for all statements.);\n\u2022 Responses fall outside the numerical range of 1 to 7;\n\u2022 Unable to score: responses similar to the following text: \u201dI\u2019m sorry, but as an AI language model, I cannot provide a\nresponse to your prompt as it is not clear what you are asking for. Please provide more context or clarify your question\nfor me to provide an accurate response.\u201d\n\u2022 Responses are non-score-related content, such as merely repeating statements from the questionnaire.\nB. Reliability of LLMs\u2019 Responses\nIn evaluating the anthropomorphic personality traits demonstrated by LLMs through human personality assessments, the\nreliability and validity of LLMs\u2019 responses to such questionnaires merit further scientific scrutiny. The study by Miotto\net al. (2022) highlighted the necessity for a more formal psychometric evaluation and construct validity assessment when\ninterpreting questionnaire-based measurements of LLMs\u2019 potential psychological characteristics. To address these concerns,\nwe employed two distinct methods to examine the reliability of LLMs\u2019 responses systematically: Logical Consistency and\nSplit-Half Reliability. These methods provide a structured approach to evaluating the consistency and reliability of responses,\nwhich is crucial for ensuring the robustness of our findings. Out of three selected personality scales, we chose TDA-100 (80\nstatements) for reliability testing. Each statement of TDA-100 has explicitly stated the specific assessment dimension and\nscoring direction (forward scoring or reverse scoring) (Goldberg, 1992), both of which are critical to our assessment of the\nreliability of LLM responses using the two subsequent methods. As for the basis model of TDA-100, the Big Five model,\nthere are 5 assessment dimensions in total: neuroticism, extraversion, openness, agreeableness and conscientiousness.\nThe TDA-100 response format employs a 7-point Likert scale, with a scoring range of 1 to 7 for each statement. From 1 to\n7, 1 indicates that the respondent believes the statement does not apply to them at all, and 7 indicates that the statement\ncompletely applies to them. Each assessment dimension consists of several statements, some of which are positive and\nothers negative. Specifically, within a selected assessment dimension, the closer a respondent\u2019s score is to 7 for positive\nstatements, the more they exhibit characteristics of that dimension. Conversely, the closer their score is to 7 for negative\nstatements, the less they exhibit characteristics of that dimension. For example, consider two statements for the Extraversion\ndimension as shown below. Statement 1 is positive, while Statement 2 is negative.\nStatement 1: Finish what I start.\nStatement 2: Leave things unfinished.\nA higher score for Statement 1 indicates greater extraversion, while a higher score for Statement 2 indicates greater\nintroversion. Therefore, within each dimension, positive statements are scored forwardly, and negative statements are scored\nreversely (7 minus the original score). Thus, when calculating a respondent\u2019s score for any given dimension, the total score\ncomprises the original scores for all positive statements plus (7 minus the original score) for all negative statements.\nThe first method, Logical Consistency, is employed to ensure that the LLMs\u2019 responses across the questionnaire are coherent\nand consistent. By integrating reverse-scored items, we are able to check whether the LLMs carefully read and seriously\nrespond to the questions. And the distribution of forward and reverse scored items within each assessment orientation is\nshown in Table 2.\nAfter collecting the data, we adjusted the answers of negative(reverse-scored) items to align them with the overall scoring\ndirection of the questionnaire. In this way, if LLMs\u2019 responses to positive and adjusted negative items are statistically\nconsistent, they will show a similar pattern or trend, as evidenced by a 7-point Likert scale in which all answers are greater\n7", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4676, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "55ce15a4-c761-484b-9fae-db2c52ce3cb5": {"__data__": {"id_": "55ce15a4-c761-484b-9fae-db2c52ce3cb5", "embedding": null, "metadata": {"page_label": "8", "file_name": "Is Self-knowledge and Action Consistent or Not Investigating Large Language Models Personality.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Is Self-knowledge and Action Consistent or Not Investigating Large Language Models Personality.pdf", "file_type": "application/pdf", "file_size": 659650, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6f7c45c8-4b1f-41a7-9fb8-4e2d716dd634", "node_type": "4", "metadata": {"page_label": "8", "file_name": "Is Self-knowledge and Action Consistent or Not Investigating Large Language Models Personality.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Is Self-knowledge and Action Consistent or Not Investigating Large Language Models Personality.pdf", "file_type": "application/pdf", "file_size": 659650, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "129efa80864c2d1044813111427a349b6d66ebf54331d8c09a8198b57b3b2730", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Is Self-knowledge and Action Consistent or Not: Investigating Large Language Model\u2019s Personality\nTable 2.Distribution of Forward and Reverse Scored Items\nOrientation Forward Reverse\nNEUROTICISM 9 5\nEXTRA VERSION 10 10\nOPENNESS 9 5\nAGREEABLENESS 10 9\nCONSCIENTIOUSNESS 6 7\nTOTAL COUNT 44 36\nthan or equal to 4, or less than or equal to 4, which indicate that the LLMs have responded conscientiously and logically. We\nintroduce the Consistency metric to measure the logical consistency of LLM responses with the following formula:\nConsistency =\nNc\nNt\n\u2212 Pmin\nPmax \u2212 Pmin\n, (5)\nwhere Nc is the number of questions with the same response direction within each measurement tendency in the adjusted\nresponse, Nt is the number of all statements, Pmax and Pmin are the maximum and the minimum of the proportion of\nconsistent responses in all the statements. The value of Pmax is 1, representing that all the responses are internally consistent\nwithin each assessment orientation. The value of Pmin is supposed to be\nP\u2308 Ni\n2 \u2309\nNt\n, where Nt is the count of all of the scored\nstatements and Ni is the count of scored statements in each assessment orientation. Hence, Pmin equals to 0.5125. The\nrange of Consistency is from 0 to 1. The closer the value of Consistency is to 1, the more internally consistent the LLM\u2019s\nresponses are. Consequently, we can evaluate the LLM\u2019s responses based on the prior knowledge of human personality\nassessment questionnaires\nThe second method is Split-Half Reliability. We measure the reliability of LLM\u2019s responses by comparing two equal-length\nsections of the questionnaire. This approach is based on the assumption that if a test is reliable, then any two equal-length\nsections of it should produce similar results. We first divide the questionnaire into two equal-length sections while ensuring\nthat the content of each section is basically the same, representing that the numbers of statements within any assessment\ndimension in two halves are the same, thereby ensuring the accuracy of the reliability assessment. Then, we compute the\nSpearman\u2019s rank coefficient between the scores of the two sections to measure their consistency. The specific formula\nis shown in Section 3.2. Larger values indicate higher internal consistency of the responses. Finally, we calculated the\nreliability of the overall responses by using the Spearman-Brown formula as follows:\nReliability = 2corr\n1 + corr, (6)\nwhere corr is the Spearman\u2019s rank coefficient between the scores of the two sections. The range of Reliability is from\nnegative infinity to 1. Only if the value of an LLM\u2019s responses Reliability metric is around the human level, we can make it\nfor further investigation.\nWe assessed the reliability of seven LLMs\u2019 reponses. The results of the are shown in Table 3.\nWe have also recruited 16 human participants, comprising an equal number of males and females, all native Chinese speakers\nwith an English proficiency level of C1 according to the Common European Framework of Reference for Languages (CEFR),\nrepresenting that they can express themselves effectively and flexibly in English in social, academic and work situations.\nThe average value (with standard deviation) of their Consistency and Reliability is 0.73 \u00b1 0.13 and 0.69 \u00b1 0.09. And\nthe minimum value is 0.49 and 0.57. Therefore, we regard ChatGLM3, GPT-3.5-turbo, GPT4, Vicuna13b and Vicuna33b\nas LLMs demonstrating high coherence in logical consistency, as well as high consistency in the split-half reliability test,\nwhich indicates that they respond to the personality questionnaires like how humans would. Hence, their responses are\ndeemed sufficiently reliable to be used for further personality analysis. This rigorous methodological approach provides a\nsolid foundation for our exploration into the potential of LLMs to simulate human personality traits.\nC. Several Examples of Our Corpus\nOur corpus consists of 2 parts: one part is personality knowledge questionnaire, including 180 statements; the other part is\nbehavior tendency questionnaire, including 180 practical scenario cases corresponding to the statements before. Here are\n8", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4128, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "bdff2aad-1647-45f0-8a03-6e853f0c95d7": {"__data__": {"id_": "bdff2aad-1647-45f0-8a03-6e853f0c95d7", "embedding": null, "metadata": {"page_label": "9", "file_name": "Is Self-knowledge and Action Consistent or Not Investigating Large Language Models Personality.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Is Self-knowledge and Action Consistent or Not Investigating Large Language Models Personality.pdf", "file_type": "application/pdf", "file_size": 659650, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d5140a05-729e-4628-918f-a044b9bbdeda", "node_type": "4", "metadata": {"page_label": "9", "file_name": "Is Self-knowledge and Action Consistent or Not Investigating Large Language Models Personality.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Is Self-knowledge and Action Consistent or Not Investigating Large Language Models Personality.pdf", "file_type": "application/pdf", "file_size": 659650, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "c2546fcdd67d56ef8dc7733150ddbc12ac3df99da01d359fde59a1cb8341ac30", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Is Self-knowledge and Action Consistent or Not: Investigating Large Language Model\u2019s Personality\nTable 3.Results of Verification on LLMs\u2019 and Human Respondents\u2019 Responses of Personality Cognition Questionnaire based on\nConsistency and Reliability Metrics\nLLM Consistency Reliability\nChatGLM3 0.82 0.69\nGPT-3.5-turbo 0.97 0.88\nGPT-4 1 0.90\nMistral-7b 0.46 0.66\nVicuna-13b 0.79 0.72\nVicuna-33b 0.64 0.61\nZephyr-7b 0.28 0.64\nSelected LLMs 0.85 \u00b1 0.13 0.69 \u00b1 0.11\nHuman(A VG) 0.73 \u00b1 0.13 0.69 \u00b1 0.09\nHuman(MIN) 0.49 0.57\nHuman(MAX) 1 0.83\nseveral examples of our corpus shown in Table 5.\nTable 4.LLMs\u2019 Resources for Cognition-Action Congruence and Corresponding Hypothesis Experiments\nModel URL or version Licence\nGPT-3.5-turbo gpt-3.5-turbo-0613 -\nGPT-4 gpt-4-0314 -\nbaize-v2-7b https://huggingface.co/project-baize/baize-v2-7b cc-by-nc-4.0\ninternLM-chat-7b https://huggingface.co/internlm/internlm-chat-7b Apache-2.0\nMistral-7b https://huggingface.co/mistralai/Mistral-7B-v0.1 Apache-2.0\nMPT-7b-chat https://huggingface.co/mosaicml/mpt-7b-chat cc-by-nc-sa-4.0\nTULU2-DPO-7b https://huggingface.co/allenai/tulu-2-dpo-7b AI2 ImpACT Low-risk license\nVicuna-13b https://huggingface.co/lmsys/vicuna-13b-v1.5 llama2\nVicuna-33b https://huggingface.co/lmsys/vicuna-33b-v1.3 Non-commercial license\nZephyr-7b https://huggingface.co/HuggingFaceH4/zephyr-7b-alphaMit\nQwen-14b-Chat https://huggingface.co/Qwen/Qwen-14B-Chat Tongyi Qianwen\nChatGLM3-6b https://huggingface.co/THUDM/chatglm3-6b The ChatGLM3-6B License\nD. Experiment Setup\nThe details of the experimental setup are shown in Table 6.\nThe details of the experimental setup are shown in Table 4.\nE. Meaning of numbers in 7-point graded forced-choice\nThe specific meaning of numbers in common 7-point graded forced-choice is shown as follows:\n1. Action A applies to you completely in this scenario.\n2. Action A applies to you much more than action B in this scenario.\n3. Action A applies to you slightly more than action B in this scenario.\n4. Action A and action B equally apply (or not) to you in this scenario.\n5. Action B applies to you much more than action A in this scenario.\n6. Action B applies to you slightly more than action A in this scenario.\n7. Action B applies to you completely in this scenario.\n9", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2256, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3759c985-34e2-4ab9-adcb-525409e4ecea": {"__data__": {"id_": "3759c985-34e2-4ab9-adcb-525409e4ecea", "embedding": null, "metadata": {"page_label": "10", "file_name": "Is Self-knowledge and Action Consistent or Not Investigating Large Language Models Personality.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Is Self-knowledge and Action Consistent or Not Investigating Large Language Models Personality.pdf", "file_type": "application/pdf", "file_size": 659650, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f7bc83d6-6855-4ec3-a5bb-a4135cdfffdc", "node_type": "4", "metadata": {"page_label": "10", "file_name": "Is Self-knowledge and Action Consistent or Not Investigating Large Language Models Personality.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Is Self-knowledge and Action Consistent or Not Investigating Large Language Models Personality.pdf", "file_type": "application/pdf", "file_size": 659650, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "9c5d22c6827c6d1deeddfd0274091148f2ee54a0f8f9c494bfb674d3e17eb095", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fc2a617d-2126-4026-b1ac-903d5b5a556a", "node_type": "1", "metadata": {}, "hash": "208cc448b5a7b9c4e43c58517abb1eed8fb487ac4d4cc87600f7e7dd7b3fd4cd", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Is Self-knowledge and Action Consistent or Not: Investigating Large Language Model\u2019s Personality\nTable 5. Several Examples of the Corpus\nPersonality Knowledge Statements Practical Scenario Cases\nEN: You waste your time. In everyday life:\nA. you always use your time productively.\nB. you always spend time on meaningless activities.\nZH: \u4f60\u6d6a\u8d39\u81ea\u5df1\u7684\u65f6\u95f4\u3002 \u5728\u65e5\u5e38\u751f\u6d3b\u4e2d\uff1a\nA. \u4f60\u603b\u662f\u6709\u6548\u5730\u5229\u7528\u65f6\u95f4\u3002\nB. \u4f60\u603b\u662f\u5728\u65e0\u610f\u4e49\u7684\u6d3b\u52a8\u4e0a\u82b1\u8d39\u65f6\u95f4\u3002\nEN: You complete tasks successfully. When assigned a challenging project with a tight deadline:\nA. you are overwhelmed and have difficulty moving the process forward effec-\ntively, often resulting in incomplete or unsatisfactory results.\nB. you organise your work and manage your resources properly, and the project\nis often completed successfully and on time.\nZH: \u4f60\u80fd\u6210\u529f\u5b8c\u6210\u4efb\u52a1\u3002 \u5f53\u88ab\u6307\u6d3e\u4e00\u4e2a\u671f\u9650\u7d27\u8feb\u7684\u5177\u6709\u6311\u6218\u6027\u7684\u9879\u76ee\u65f6\uff1a\nA. \u4f60\u4e0d\u77e5\u6240\u63aa\uff0c\u96be\u4ee5\u6709\u6548\u5730\u63a8\u8fdb\u8fdb\u7a0b\uff0c\u5e38\u5bfc\u81f4\u7ed3\u679c\u4e0d\u5b8c\u6574\u6216\u4e0d\u4ee4\u4eba\u6ee1\n\u610f\u3002\nB. \u4f60\u7ec4\u7ec7\u5de5\u4f5c\uff0c\u59a5\u5584\u7ba1\u7406\u8d44\u6e90\uff0c\u9879\u76ee\u5f80\u5f80\u6309\u65f6\u987a\u5229\u5b8c\u6210\u3002\nEN: You shirk your duties. When someone points out a mistake in your work:\nA. you take responsibility.\nB. you shirk your responsibility.\nZH: \u4f60\u63a8\u5378\u8d23\u4efb\u3002 \u5f53\u522b\u4eba\u6307\u51fa\u4f60\u7684\u5de5\u4f5c\u5931\u8bef\uff1a\nA. \u4f60\u52c7\u4e8e\u627f\u62c5\u8d23\u4efb\u3002\nB. \u4f60\u63a8\u5378\u8d23\u4efb\u3002\nEN: You tend to find fault with others. When dealing with people:\nA. you tend to focus on the person\u2019s good points and strengths.\nB. you often pick on other people\u2019s faults and weaknesses.\nZH: \u4f60\u559c\u6b22\u6311\u5254\u522b\u4eba\u7684\u6bdb\u75c5\u3002 \u5728\u4e0e\u4eba\u76f8\u5904\u65f6\uff1a\nA. \u4f60\u5f80\u5f80\u5173\u6ce8\u4ed6\u7684\u4f18\u70b9\u4e0e\u957f\u5904\u3002\nB. \u4f60\u5e38\u6311\u5254\u522b\u4eba\u7684\u7f3a\u70b9\u4e0e\u6bdb\u75c5\u3002\nEN: You usually postpone finalizing When making choices:\ndecisions for as long as possible. A. you make choices quickly, usually finalising the necessary decisions as soon\nas possible.\nB. you delay making a definite choice, usually taking as long as possible to\nfinalise the necessary decision.\nZH: \u4f60\u901a\u5e38\u4f1a\u5c3d\u53ef\u80fd\u63a8\u8fdf\u6700\u7ec8\u51b3\u5b9a\u3002 \u5728\u505a\u9009\u62e9\u65f6\uff1a\nA. \u4f60\u4f1a\u8fc5\u901f\u505a\u51fa\u9009\u62e9\uff0c\u901a\u5e38\u4f1a\u5c3d\u5feb\u6572\u5b9a\u5fc5\u8981\u7684\u51b3\u5b9a\u3002\nB. \u4f60\u4f1a\u63a8\u8fdf\u505a\u51fa\u660e\u786e\u7684\u9009\u62e9\uff0c\u901a\u5e38\u4f1a\u5c3d\u53ef\u80fd\u957f\u65f6\u95f4\u5730\u6572\u5b9a\u5fc5\u8981\u7684\u51b3\u5b9a\u3002\nEN: You struggle with deadlines. You have a week to complete a work project:\nA. you always make sure that it is completed ahead of or on the deadline.\nB. you are always rushing at the last minute and have a hard time completing\ntasks.\nZH: \u4f60\u5f88\u96be\u5728\u6700\u540e\u671f\u9650\u524d\u5b8c\u6210\u4efb\u52a1\u3002 \u4f60\u6709\u4e00\u5468\u7684\u65f6\u95f4\u6765\u5b8c\u6210\u4e00\u4e2a\u5de5\u4f5c\u9879\u76ee\uff1a\nA. \u4f60\u5f80\u5f80\u786e\u4fdd\u63d0\u524d\u6216\u5728\u622a\u6b62\u65e5\u671f\u5b8c\u6210\u3002\nB. \u4f60\u603b\u662f\u5728\u6700\u540e\u4e00\u523b\u8fd8\u5728\u8d76\u5de5\uff0c\u5f88\u96be\u5b8c\u6210\u4efb\u52a1\u3002\nEN: You remain calm in tense situations. When dealing with a conflict or a high-pressure problem:\nA. you become visibly agitated, finding it challenging to maintain composure.\nB. you stay composed, handling the situation with a level head and a calm\ndemeanor.\nZH: \u4f60\u5728\u7d27\u5f20\u60c5\u5883\u4e2d\u4ecd\u4fdd\u6301\u51b7\u9759\u3002 \u5728\u5904\u7406\u51b2\u7a81\u6216\u9ad8\u538b\u95ee\u9898\u65f6\uff1a\nA. \u4f60\u4f1a\u660e\u663e\u53d8\u5f97\u7126\u8e81\u4e0d\u5b89\uff0c\u53d1\u73b0\u4fdd\u6301\u9547\u5b9a\u5f88\u6709\u6311\u6218\u6027\u3002\nB. \u4f60\u4fdd\u6301\u9547\u5b9a\uff0c\u4ee5\u5e73\u548c\u7684\u5fc3\u6001\u548c\u51b7\u9759\u7684\u4e3e\u6b62\u5904\u7406\u60c5\u51b5\u3002\nEN: You are the life of the party. When attending a social gathering, like a friend\u2019s birthday party or a casual\nget-together:\nA. you prefer to blend in, engaging in low-key conversations rather than energiz-\ning the atmosphere.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2550, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "fc2a617d-2126-4026-b1ac-903d5b5a556a": {"__data__": {"id_": "fc2a617d-2126-4026-b1ac-903d5b5a556a", "embedding": null, "metadata": {"page_label": "10", "file_name": "Is Self-knowledge and Action Consistent or Not Investigating Large Language Models Personality.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Is Self-knowledge and Action Consistent or Not Investigating Large Language Models Personality.pdf", "file_type": "application/pdf", "file_size": 659650, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f7bc83d6-6855-4ec3-a5bb-a4135cdfffdc", "node_type": "4", "metadata": {"page_label": "10", "file_name": "Is Self-knowledge and Action Consistent or Not Investigating Large Language Models Personality.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Is Self-knowledge and Action Consistent or Not Investigating Large Language Models Personality.pdf", "file_type": "application/pdf", "file_size": 659650, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "9c5d22c6827c6d1deeddfd0274091148f2ee54a0f8f9c494bfb674d3e17eb095", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3759c985-34e2-4ab9-adcb-525409e4ecea", "node_type": "1", "metadata": {"page_label": "10", "file_name": "Is Self-knowledge and Action Consistent or Not Investigating Large Language Models Personality.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Is Self-knowledge and Action Consistent or Not Investigating Large Language Models Personality.pdf", "file_type": "application/pdf", "file_size": 659650, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "8d6f25c3c93ea5aefa29c310595ee8dbdd91090bacd39d0c4aed5a155bd7c8ee", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "When dealing with a conflict or a high-pressure problem:\nA. you become visibly agitated, finding it challenging to maintain composure.\nB. you stay composed, handling the situation with a level head and a calm\ndemeanor.\nZH: \u4f60\u5728\u7d27\u5f20\u60c5\u5883\u4e2d\u4ecd\u4fdd\u6301\u51b7\u9759\u3002 \u5728\u5904\u7406\u51b2\u7a81\u6216\u9ad8\u538b\u95ee\u9898\u65f6\uff1a\nA. \u4f60\u4f1a\u660e\u663e\u53d8\u5f97\u7126\u8e81\u4e0d\u5b89\uff0c\u53d1\u73b0\u4fdd\u6301\u9547\u5b9a\u5f88\u6709\u6311\u6218\u6027\u3002\nB. \u4f60\u4fdd\u6301\u9547\u5b9a\uff0c\u4ee5\u5e73\u548c\u7684\u5fc3\u6001\u548c\u51b7\u9759\u7684\u4e3e\u6b62\u5904\u7406\u60c5\u51b5\u3002\nEN: You are the life of the party. When attending a social gathering, like a friend\u2019s birthday party or a casual\nget-together:\nA. you prefer to blend in, engaging in low-key conversations rather than energiz-\ning the atmosphere.\nB. you often initiate games, conversations, and entertain others, energizing the\natmosphere.\nZH: \u805a\u4f1a\u65f6\u4f60\u662f\u6d3b\u8dc3\u6c14\u6c1b\u7684\u4eba\u3002 \u53c2\u52a0\u793e\u4ea4\u805a\u4f1a\uff0c\u5982\u670b\u53cb\u7684\u751f\u65e5\u6d3e\u5bf9\u6216\u4f11\u95f2\u805a\u4f1a\u65f6\uff1a\nA. \u4f60\u559c\u6b22\u878d\u5165\u5176\u4e2d\uff0c\u4f4e\u8c03\u5730\u4ea4\u8c08\uff0c\u800c\u4e0d\u662f\u4e3b\u52a8\u6d3b\u8dc3\u6c14\u6c1b\u3002\nB. \u4f60\u7ecf\u5e38\u4f1a\u4e3b\u52a8\u53d1\u8d77\u6e38\u620f\u3001\u8c08\u8bdd\uff0c\u6d3b\u8dc3\u6c14\u6c1b\u3002\n10", "mimetype": "text/plain", "start_char_idx": 2018, "end_char_idx": 2737, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "90566177-f0cc-49ac-82c9-959e5bdfc12a": {"__data__": {"id_": "90566177-f0cc-49ac-82c9-959e5bdfc12a", "embedding": null, "metadata": {"page_label": "11", "file_name": "Is Self-knowledge and Action Consistent or Not Investigating Large Language Models Personality.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Is Self-knowledge and Action Consistent or Not Investigating Large Language Models Personality.pdf", "file_type": "application/pdf", "file_size": 659650, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "14ebd053-462b-4700-a31c-03c944a0ce29", "node_type": "4", "metadata": {"page_label": "11", "file_name": "Is Self-knowledge and Action Consistent or Not Investigating Large Language Models Personality.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Is Self-knowledge and Action Consistent or Not Investigating Large Language Models Personality.pdf", "file_type": "application/pdf", "file_size": 659650, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "4915dfbe3d57d707b4f7dec8bfe8a4af340c970d5a20ee0e776da7cd1590598a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Is Self-knowledge and Action Consistent or Not: Investigating Large Language Model\u2019s Personality\nTable 6. Various Prompts of Personality Knowledge Questionnaire\nSource Prompt\n16 Personalities\nTest\nYou can only reply to me with numbers from 1 to 7. Score each statement on a scale of 1 to 7 with 1 being\ndisagree, 4 being not sure and 7 being agree.\nMBTI-M Test Read the following statements carefully and rate each one from 1 to 7, with 7 meaning that it applies to you\ncompletely, 1 meaning that it doesn\u2019t apply to you at all, and 4 meaning that you are not sure whether it\napplies to you or not.\nTDA-100 Test Below are several descriptions that may or may not fit you. Please indicate how much you agree or disagree\nwith that statement by giving a specific number from 1 to 7. 1 means you totally disagree with the statement,\n4 means you are not sure, and 7 means you totally agree with the statement.\nBFI-44-children\nadapted version\nHere are several statements that may or may not describe what you are like. Write the number between 1 and\n7 that shows how much you agree or disagree that it describes you. 1 means you disagree strongly that the\nstatement applies to you, 4 means you are not sure, and 7 means you agree strongly with the statement.\nDario Nardi\u2019s\nCognitive Test\nPlease read carefully each of the phrases below. For each phrase: Rate how often you do skillfully what the\nphrase describes between 1 and 7. 1 means the phrase is not me, 4 means that you are not sure, and 7 means\nthat the phrase is exactly me.\nF. Significance Tests\nIn the following, we will apply significance tests to further demonstrate significant differences between the performance of\nLLMs and humans. We incorporated significance testing for the responses of LLMs and humans in the same experiment.\nSpecifically, we performed permutation tests to compare LLMs\u2019 results and human respondents\u2019 results, yielding p-values\nsignificantly below 0.05 in experiments in Section 3.2 (corresponding to results in Table 1). This confirms substantial\ndisparities between LLMs and humans in performance for each metric across both experiments. The specific p-values are\noutlined in Table 7.\nTable 7.Results of significance testing for the responses of LLMs and humans in the experiment in Section 3.2\nP-value of COMPARISON\nbetween LLMs & Human\nCosine\nSimilarity\nSpearman Rank\nCorrelation Coefficient\nValue Mean\nDifference\nProportion of\nConsistent Pairs\nResults in Table 1 4.91e-05 4.91e-05 1.47e-04 2.46e-05\nG. Additional Notes On Human Reviewers and Respondents\nG.1. Recruitment of Scenario Providers, Reviewers and Human Respondents\nWe recruited individuals from undergraduate, postgraduate and PhD students. Taking the International English Language\nTesting System(IELTS), CET 6 exam results, and their GPA in English courses into account, we recruited 16, 10 and 35\nnative Chinese speakers as reviewers and respondents.\nG.2. Instructions Given to Scenario Providers\nBefore requiring the individual to complete the following tasks, we asked the respondents whether they agreed to the\nanonymisation of their reviews for scientific research and subsequent publication. Only if the respondents gave their consent\nwere they given the corpus to review. And we promised not to publish each individual\u2019s MBTI results and specific practical\nscenario cases. Then, we investigated each person\u2019s MBTI type and ensured that we ultimately recruited 16 individuals with\ndistinct MBTI types. After this, we required the reviewers to accomplish the following tasks\uff1a\nPlease provide a practical scenario case for every statement in the personality knowledge questionnaire. Each scenario case\ncomprises situations drawn from your own lives, along with two completely contrasting actions: Action A and Action B.\nAction A fully aligns with the statement, while Action B completely contradicts it. The content of Action A and Action B\nneed to be kept basically the same length.\nG.3. Instructions Given to Reviewers\nWe require the reviewers to accomplish the following tasks\uff1a\n11", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4036, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d24febc5-aad5-4be1-af9e-073ce6adc763": {"__data__": {"id_": "d24febc5-aad5-4be1-af9e-073ce6adc763", "embedding": null, "metadata": {"page_label": "12", "file_name": "Is Self-knowledge and Action Consistent or Not Investigating Large Language Models Personality.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Is Self-knowledge and Action Consistent or Not Investigating Large Language Models Personality.pdf", "file_type": "application/pdf", "file_size": 659650, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a1ed0bfb-8953-4137-a7e4-0371e6bde0a6", "node_type": "4", "metadata": {"page_label": "12", "file_name": "Is Self-knowledge and Action Consistent or Not Investigating Large Language Models Personality.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Is Self-knowledge and Action Consistent or Not Investigating Large Language Models Personality.pdf", "file_type": "application/pdf", "file_size": 659650, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "1c511ffa2c8a203d537093b9ec017f9621b535625d538e274c46b2ad185fad13", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Is Self-knowledge and Action Consistent or Not: Investigating Large Language Model\u2019s Personality\n\u2022 Please determine whether the practical scenario case is consistent with its corresponding personality knowledge\nstatement. If yes, rate 1. If not, rate 0.\n\u2022 If you rate 0 for all of the practical scenario cases of a personality knowledge statement, please offer suggestions to\nimprove the practical scenario design. It would be better if an example could be provided.\nG.4. Instructions Given to Respondents\nBefore answering the questionnaires, we did not tell the respondents what kind of questionnaires they would be answering\nor how the questions were related to each other. In addition to this, we asked the respondents whether they agreed to the\nanonymisation of their answers for scientific research and subsequent publication. Only if the respondents gave their consent\nwere they given the questionnaires to answer.\nIn all experiments that appeared in our research, human respondents received the exact same prompts that LLM received.\nThe difference is that in the case of experiments with multiple prompts with similar meanings, LLM responded multiple\ntimes by prompt type, while human subjects read all the prompts and responded only once.\n12", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1249, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "cd3ceaaa-3ca9-4ce5-8839-959ef939dcb2": {"__data__": {"id_": "cd3ceaaa-3ca9-4ce5-8839-959ef939dcb2", "embedding": null, "metadata": {"page_label": "1", "file_name": "Large Language Models Lack Understanding of Character Composition of Words.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Large Language Models Lack Understanding of Character Composition of Words.pdf", "file_type": "application/pdf", "file_size": 262705, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "af012466-de7c-4201-a840-90f780af132d", "node_type": "4", "metadata": {"page_label": "1", "file_name": "Large Language Models Lack Understanding of Character Composition of Words.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Large Language Models Lack Understanding of Character Composition of Words.pdf", "file_type": "application/pdf", "file_size": 262705, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "84a5652fc08bd2ed86ee0c164ad482104aefab3f9a79ca76243c70aef7864cc2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3bfd9e46-65cd-4004-96f4-6d644c333188", "node_type": "1", "metadata": {}, "hash": "43ac89db754dabd71e99324019358b61f7b9288567dca082fb9bf19ab2b796e3", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "arXiv:2405.11357v3  [cs.CL]  23 Jul 2024\nLarge Language Models Lack\nUnderstanding of Character Composition of W ords\nAndrew Shin 1 Kunitake Kaneko 1\nAbstract\nLarge language models (LLMs) have demon-\nstrated remarkable performances on a wide range\nof natural language tasks. Y et, LLMs\u2019 successes\nhave been largely restricted to tasks concerning\nwords, sentences, or documents, and it remains\nquestionable how much they understand the min-\nimal units of text, namely characters. In this pa-\nper, we examine contemporary LLMs regarding\ntheir ability to understand character composition\nof words, and show that most of them fail to re-\nliably carry out even the simple tasks that can\nbe handled by humans with perfection. W e an-\nalyze their behaviors with comparison to token\nlevel performances, and discuss the potential di-\nrections for future research.\n1. Introduction\nLarge language models (LLMs) (\nAchiam et al. , 2023;\nChowdhery et al. , 2022; T ouvron et al. , 2023; Reid et al. ,\n2024; OpenAI, 2022; Jiang et al. , 2023) have exhibited out-\nstanding performance across a diverse array of natural lan-\nguage tasks. It has largely outperformed pre-LLM ap-\nproaches on benchmark tasks, such as GLUE (\nW ang et al. ,\n2018) and SuperGLUE ( W ang et al. , 2019), often surpass-\ning humans on a number of tasks ( Chowdhery et al. , 2022).\nIt is noteworthy that most of the tasks upon which LLMs\nhave been tested revolve around words, sentences, or pas-\nsages, but hardly involve character-level understanding. In-\ntuitively, character-level tasks should be much easier to\ntackle, as they rarely deal with complex semantics, gram-\nmatical structures, or background knowledge, while only\nrequiring highly elementary understanding of characters\nand, depending on the task, simple counting. Indeed,\nhumans are able to perform basic character-level tasks\nvery easily as we will see in Sec\n3.2. It has also been\nknown that LLMs hardly make spelling errors and can\n1 Faculty of Science and T echnology , Keio University ,\nKanagawa, Japan. Correspondence to: Andrew Shin\n<shin@inl.ics.keio.ac.jp>.\nbe used for spelling correction of human-written passages\n(\nWhittaker & Kitagishi , 2024). Surprisingly, however, our\nexamination shows that LLMs struggle with very simple\ntasks involving character composition, severely underper -\nforming humans, making a striking contrast with their per-\nformance on more complex tasks at token level.\nHumans are able to instantly recognize which characters\nconstitute a given word. However, large language mod-\nels, most of which are trained at token-level, struggle to\ngrasp the nuances of character composition within words.\nThis dif\ufb01culty arises from the fact that LLMs primarily\nlearn at the token level, where words are treated as indivisi -\nble units separated by spaces or punctuation marks. Con-\nsequently, LLMs lack the \ufb01ne-grained understanding of\ncharacter-level relationships and morphology that humans\npossess. Understanding character composition is crucial\nfor various linguistic tasks, including morphological ana l-\nysis, semantic interpretation, and language generation. A s\nsuch, addressing the challenge of character composition is\nessential for enhancing the reliability of LLMs across a di-\nverse range of languages and writing systems.\nIn this paper, we examine LLMs with a number of simple\ntasks designed to test the understanding of character com-\nposition. None of the tasks requires any advanced knowl-\nedge of grammar or semantics, and can be easily tackled\nwith elementary understanding of characters. Y et, our re-\nsults show a surprisingly poor performance, suggesting tha t\nthere may be a fundamental drawback with regards to how\nLLMs are trained and how they perceive the language. W e\ncompare LLMs\u2019 performances at character level tasks with\nthose at token level tasks of the same types, and investigate\nthe implications of the large discrepancies. W e further dis -\ncuss potential future research directions to enhance LLMs\u2019\nunderstanding of character composition, such as incorpora t-\ning character embedding and visual features into language\nrepresentation of LLMs.\n2.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4129, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3bfd9e46-65cd-4004-96f4-6d644c333188": {"__data__": {"id_": "3bfd9e46-65cd-4004-96f4-6d644c333188", "embedding": null, "metadata": {"page_label": "1", "file_name": "Large Language Models Lack Understanding of Character Composition of Words.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Large Language Models Lack Understanding of Character Composition of Words.pdf", "file_type": "application/pdf", "file_size": 262705, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "af012466-de7c-4201-a840-90f780af132d", "node_type": "4", "metadata": {"page_label": "1", "file_name": "Large Language Models Lack Understanding of Character Composition of Words.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Large Language Models Lack Understanding of Character Composition of Words.pdf", "file_type": "application/pdf", "file_size": 262705, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "84a5652fc08bd2ed86ee0c164ad482104aefab3f9a79ca76243c70aef7864cc2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cd3ceaaa-3ca9-4ce5-8839-959ef939dcb2", "node_type": "1", "metadata": {"page_label": "1", "file_name": "Large Language Models Lack Understanding of Character Composition of Words.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Large Language Models Lack Understanding of Character Composition of Words.pdf", "file_type": "application/pdf", "file_size": 262705, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "3a8a0276668f3477431906cb8f017e061ecadd4906ca87af2bb1bb3703ba021f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In this paper, we examine LLMs with a number of simple\ntasks designed to test the understanding of character com-\nposition. None of the tasks requires any advanced knowl-\nedge of grammar or semantics, and can be easily tackled\nwith elementary understanding of characters. Y et, our re-\nsults show a surprisingly poor performance, suggesting tha t\nthere may be a fundamental drawback with regards to how\nLLMs are trained and how they perceive the language. W e\ncompare LLMs\u2019 performances at character level tasks with\nthose at token level tasks of the same types, and investigate\nthe implications of the large discrepancies. W e further dis -\ncuss potential future research directions to enhance LLMs\u2019\nunderstanding of character composition, such as incorpora t-\ning character embedding and visual features into language\nrepresentation of LLMs.\n2. Related W orks\nAlthough a majority of language models have relied on\ntoken-level embeddings, there have been a number of\nnotable endeavors to incorporate character composition\n1", "mimetype": "text/plain", "start_char_idx": 3283, "end_char_idx": 4307, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c4603e68-71fd-4782-9e69-e79eac54d26d": {"__data__": {"id_": "c4603e68-71fd-4782-9e69-e79eac54d26d", "embedding": null, "metadata": {"page_label": "2", "file_name": "Large Language Models Lack Understanding of Character Composition of Words.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Large Language Models Lack Understanding of Character Composition of Words.pdf", "file_type": "application/pdf", "file_size": 262705, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "37cd8caf-a241-4665-b4a6-7a01a8677890", "node_type": "4", "metadata": {"page_label": "2", "file_name": "Large Language Models Lack Understanding of Character Composition of Words.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Large Language Models Lack Understanding of Character Composition of Words.pdf", "file_type": "application/pdf", "file_size": 262705, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "9ddda92ff736ac3b5fed6d71bfb2f25937e0caa1c23b73cb421f0bec457fcc50", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "105119c7-5759-49bc-a8bb-cfb2f6681588", "node_type": "1", "metadata": {}, "hash": "ea752552526589272b86493dfdaa43b93ccda1b5f25e7adb3852fb6ce55e6445", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Large Language Models Lack Understanding of Character Composition of W ords\nor sub-word tokenization into language models, some of\nwhich have demonstrated improved performance on rele-\nvant tasks. ( Kim et al. , 2015) introduced character-aware\nneural language models, which utilize character-level em-\nbeddings alongside word embeddings to capture morpho-\nlogical and orthographic features of words. Similarly,\n(\nWieting et al. , 2016) proposed Charagram, a character-\nlevel language model that generates word representations\nbased on character n-grams, enabling better handling of\nout-of-vocabulary words. (\nBojanowski et al. , 2016) pre-\nsented FastT ext, a fast and ef\ufb01cient word embedding tech-\nnique that leverages sub-word information to enhance word\nrepresentations, particularly for morphologically rich l an-\nguages. While these approaches demonstrate the effec-\ntiveness of integrating character information into langua ge\nmodels, paving the way for improved performance in var-\nious natural language processing tasks, they have mostly\nbeen tested on natural language generation tasks, such as\nPenn Treebank (\nMarcus et al. , 1993), and have not explic-\nitly been tested for understanding of character compositio n.\nSubsequent works in language modeling have further\nexplored the integration of character-level information.\nFor instance, (\nPeters et al. , 2018) introduced deep con-\ntextualized word representations (ELMo), which enhance\nword embeddings by considering the internal structure\nof words through character-level convolutions. This\nmethod signi\ufb01cantly improved the performance of vari-\nous NLP tasks by capturing complex word morphologies.\n(\nAkbik et al. , 2019) proposed Flair embeddings, which\ncombine character-level embeddings with contextual strin g\nembeddings to provide a more comprehensive representa-\ntion of words in their context. (\nClark et al. , 2020) intro-\nduced ELECTRA, a pre-training method that includes a\ndiscriminative component to identify corruptions at the to -\nken level, which indirectly bene\ufb01ts from \ufb01ner-grained text\nrepresentations. For most of these works, however, the pri-\nmary focus has been on token-level tasks rather than specif-\nically addressing character composition understanding.\nWith regards to more recent LLMs, there have been a num-\nber of works that highlight their downsides from various\nangles. For example, (\nQian et al. , 2022) claims that LLMs\nstruggle with arithmetic and symbolic manipulations, whil e\n(Lee & Lim , 2024) shows that LLMs fail to learn phys-\nical manifestation of language, such as the visuals and\nsounds of the language. (\nTruong et al. , 2023) also shows\nthat LLMs\u2019 performances degrade when negation is in-\nvolved. With regards to the character composition, there\nhave been a few attempts to benchmark the performances of\nLLMs (\nSrivastava et al. , 2022; Efrat et al. , 2022), although\nthe scope of evaluating character composition was highly\nrestricted, with stronger emphasis on evaluation of word-\nlevel understanding.\n3. Experiments\n3.1. Setting\nW e perform simple tasks that are designed to assess the\nLLM\u2019s understanding of character composition of words.\nNearly all tasks are simple and straightforward with hardly\nany component for complexity or confusion. It would be\nfair to state that even humans with very little educational\nbackground of up to elementary school can solve most of\nthese tasks without dif\ufb01culty.\nW ord retrieval: W e provide the LLM with input text and\nask it to retrieve all words containing a certain character.\nFor example, \u201cFind all words that contain the character h\nin the following text: She is home. \u201d should output \u201c She\u201d\nand \u201c home\u201d. The task may be examined in variations by\nspecifying the position or the number of occurrences of the\ncharacters within a word.\nCharacter insertion / deletion / replacement : W e ask\nLLM to insert a character to words in the input text at a\nspeci\ufb01ed position, or delete a speci\ufb01ed character or any\ncharacter at a speci\ufb01ed position from the input text, or re-\nplace a character with another character.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4066, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "105119c7-5759-49bc-a8bb-cfb2f6681588": {"__data__": {"id_": "105119c7-5759-49bc-a8bb-cfb2f6681588", "embedding": null, "metadata": {"page_label": "2", "file_name": "Large Language Models Lack Understanding of Character Composition of Words.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Large Language Models Lack Understanding of Character Composition of Words.pdf", "file_type": "application/pdf", "file_size": 262705, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "37cd8caf-a241-4665-b4a6-7a01a8677890", "node_type": "4", "metadata": {"page_label": "2", "file_name": "Large Language Models Lack Understanding of Character Composition of Words.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Large Language Models Lack Understanding of Character Composition of Words.pdf", "file_type": "application/pdf", "file_size": 262705, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "9ddda92ff736ac3b5fed6d71bfb2f25937e0caa1c23b73cb421f0bec457fcc50", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c4603e68-71fd-4782-9e69-e79eac54d26d", "node_type": "1", "metadata": {"page_label": "2", "file_name": "Large Language Models Lack Understanding of Character Composition of Words.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Large Language Models Lack Understanding of Character Composition of Words.pdf", "file_type": "application/pdf", "file_size": 262705, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "b5be9b2dc9c1bfa91335ac9937a2e0aad2d4889d5e85fdbd75775f8af67aa63c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "It would be\nfair to state that even humans with very little educational\nbackground of up to elementary school can solve most of\nthese tasks without dif\ufb01culty.\nW ord retrieval: W e provide the LLM with input text and\nask it to retrieve all words containing a certain character.\nFor example, \u201cFind all words that contain the character h\nin the following text: She is home. \u201d should output \u201c She\u201d\nand \u201c home\u201d. The task may be examined in variations by\nspecifying the position or the number of occurrences of the\ncharacters within a word.\nCharacter insertion / deletion / replacement : W e ask\nLLM to insert a character to words in the input text at a\nspeci\ufb01ed position, or delete a speci\ufb01ed character or any\ncharacter at a speci\ufb01ed position from the input text, or re-\nplace a character with another character. For example, \u201cIn-\nsert the character a to the beginning of all words in the fol-\nlowing text: I am well \u201d should output \u201c aI aam awell , \u201d and\nsimilarly for deletion and replacement.\nCharacter reordering : W e provide the LLM with words\nand ask it to reorder the characters within each word to\nform a new word, in a similar manner to anagram, e.g.,\ngenerate \u201c epics\u201d from the input word \u201c spice. \u201d The output\nis deemed correct if it contains all characters in the input\nword with the same number of occurrences. Note that there\nis no restriction as to whether new word should be an exist-\ning word, as long as all characters have been used.\nCharacter counting: W e provide the LLM with input text\nand ask it to count the number of certain characters or a\ncategory of characters, such as vowels and consonants. For\nexample, \u201cHow many occurrences of the character s are in\nthe following word: obsessed?\u201d should return 3.\nW e experimented with 4 publicly available LLMs, namely\nGPT4 (\nAchiam et al. , 2023), Claude ( Claude, 2023), Gem-\nini 1.5 ( Reid et al. , 2024), and Mistral 7B ( Jiang et al. ,\n2023). W e randomly sampled words, phrases, or sentences\nfrom Wikipedia corpus. Note that, while it is possible\nthat such publicly available text was used during the pre-\ntraining of target LLMs, the character-based nature of our\nexperiments prevents the models from taking advantage of\nit, and the results in Sec\n3.2 seem to reinforce the claim.\nFor each task, 100 prompts were used, where each prompt\nmay contain multiple answers. In order to compare the\nLLM\u2019s understanding of character composition with that\nof humans, we also asked human annotators to perform ex-\n2", "mimetype": "text/plain", "start_char_idx": 3259, "end_char_idx": 5734, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "845c2370-1994-47c2-841e-2a84d4213591": {"__data__": {"id_": "845c2370-1994-47c2-841e-2a84d4213591", "embedding": null, "metadata": {"page_label": "3", "file_name": "Large Language Models Lack Understanding of Character Composition of Words.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Large Language Models Lack Understanding of Character Composition of Words.pdf", "file_type": "application/pdf", "file_size": 262705, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "389302f2-e074-40fd-a6c2-f9b3c69461f0", "node_type": "4", "metadata": {"page_label": "3", "file_name": "Large Language Models Lack Understanding of Character Composition of Words.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Large Language Models Lack Understanding of Character Composition of Words.pdf", "file_type": "application/pdf", "file_size": 262705, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "1fa7de9459b2f190e80e30ee46a6d57930f67ef1c58a475fcf095911aafc7540", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "22452d97-2010-4532-8f33-9a0d61c55216", "node_type": "1", "metadata": {}, "hash": "754f84418b480f8c539ddd89c5baaa44a1f318eb0cfd45f8e4d683d32f458272", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Large Language Models Lack Understanding of Character Composition of W ords\nT able 1. Precision, recall, and F-score for each model on evaluation tasks at character level. For reordering and counting, accu racy is\nreported in precision column.\nT ask Human GPT4 Claude Gemini Mistral\nPrec. Rec. F-score Prec. Rec. F-score Prec. Rec. F-score Prec. Rec. F-score Prec. Rec. F-score\nW ord Retrieval 1.0 .989 .994 .523 .691 .595 .406 .534 .461 .549 .602 .574 .614 .671 .641\nCharacter Insertion 1.0 1.0 1.0 .286 .514 .368 .214 .357 .268 .203 .414 .272 .429 .443 .436\nCharacter Deletion 1.0 1.0 1.0 .236 .336 .277 .372 .439 .403 .270 .342 .302 .353 .362 .357\nCharacter Replacement 1.0 .943 .971 .725 .453 .558 .815 .435 .567 .823 .725 .771 .488 .328 .392\nCharacter Reordering 1.0 \u2013 \u2013 .91 \u2013 \u2013 .93 \u2013 \u2013 .92 \u2013 \u2013 .88 \u2013 \u2013\nCharacter Counting .98 \u2013 \u2013 .59 \u2013 \u2013 .51 \u2013 \u2013 .63 \u2013 \u2013 .60 \u2013 \u2013\nT able 2. F-score for each model on evaluation tasks at token level.\nFor reordering and counting, accuracy is reported.\nT ask Human GPT4 Claude Gemini Mistral\nSentence Retrieval 1.0 .926 .893 .921 .953\nW ord Insertion 1.0 .625 .643 .701 .632\nW ord Deletion 1.0 .578 .542 .602 .529\nW ord Replacement 1.0 .991 .994 .993 .981\nW ord Reordering .99 .95 .97 .97 .96\nW ord Counting 1.0 .98 .93 .97 .91\nactly the same tasks with identical prompts and passages.\nIn order to compare LLMs\u2019 performances at character level\nand token level tasks, we also extend each task described\nabove to token level tasks. W ord retrieval is extended to\nsentence retrieval, where the model is given 5-sentence pas -\nsage and is asked to return all sentences containing a target\nword. Insertion and deletion work similarly by providing\ntarget word and position within sentence, whereas we pro-\nvide target word and another input word for replacement\ntask. Reordering and counting are extended similarly. For\nreordering, as with character-level reordering, we comput e\naccuracy from whether the \ufb01nal answer is correct, without\ncomputing precision and recall for each reordered word.\n3.2. Results\nT able\n7 summarizes the results of our experiments with pre-\ncision, recall, and F-score for each task at character level .\nFor token level, we only report F-score for brevity in T a-\nble\n2. It is clearly shown that, for most tasks, all target\nLLMs display severely degraded performance at character\nlevel when compared to token level. While discrepancies\nexist among respective models\u2019 performances, none rises\nto the level of demonstrating a clear superiority over other\nmodels. It is also out of scope of this paper to determine\nwhich LLM is better, as our focus is on assessing LLMs in\nterms of understanding character composition in general.\nHumans, not surprisingly, demonstrated near-perfect per-\nformance throughout all tasks. There was hardly any mis-\ntake in precision, while defects in recall tended to occur\nmostly around characters that are placed in the middle of\nthe word, rather than beginning or the end, suggesting at-\ntention to saliency in human perception of character com-\nposition. Considering that humans have been surpassed by\nLLMs in many NLP tasks that are supposedly more com-\nplex, our results suggest an unsettling dichotomy between\nLLM\u2019s capability at token-level and character-level tasks .\nT able\n8 shows some of the failure cases for each model\nat character level. It is notable that the tasks for which\nLLMs struggled the most frequently involved specifying\npositions of the characters, mostly using numbers, as in in-\nsertion or deletion tasks. It should be noted that a similar\nperformance decline was observed even at token level, as\nillustrated in T able\n2. T able 4 shows example failure cases\nat token level.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3684, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "22452d97-2010-4532-8f33-9a0d61c55216": {"__data__": {"id_": "22452d97-2010-4532-8f33-9a0d61c55216", "embedding": null, "metadata": {"page_label": "3", "file_name": "Large Language Models Lack Understanding of Character Composition of Words.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Large Language Models Lack Understanding of Character Composition of Words.pdf", "file_type": "application/pdf", "file_size": 262705, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "389302f2-e074-40fd-a6c2-f9b3c69461f0", "node_type": "4", "metadata": {"page_label": "3", "file_name": "Large Language Models Lack Understanding of Character Composition of Words.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Large Language Models Lack Understanding of Character Composition of Words.pdf", "file_type": "application/pdf", "file_size": 262705, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "1fa7de9459b2f190e80e30ee46a6d57930f67ef1c58a475fcf095911aafc7540", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "845c2370-1994-47c2-841e-2a84d4213591", "node_type": "1", "metadata": {"page_label": "3", "file_name": "Large Language Models Lack Understanding of Character Composition of Words.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Large Language Models Lack Understanding of Character Composition of Words.pdf", "file_type": "application/pdf", "file_size": 262705, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "6e8eb03b0af95f8dab6069b093f508422da2fa50a4b3909be055f03a5bc3db9d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Considering that humans have been surpassed by\nLLMs in many NLP tasks that are supposedly more com-\nplex, our results suggest an unsettling dichotomy between\nLLM\u2019s capability at token-level and character-level tasks .\nT able\n8 shows some of the failure cases for each model\nat character level. It is notable that the tasks for which\nLLMs struggled the most frequently involved specifying\npositions of the characters, mostly using numbers, as in in-\nsertion or deletion tasks. It should be noted that a similar\nperformance decline was observed even at token level, as\nillustrated in T able\n2. T able 4 shows example failure cases\nat token level. This suggests that some of the limitations\nin understanding character composition may not simply be\nattributed to the fact that LLMs are trained at token level,\nbut to a more fundamental drawback in their training ap-\nproach. Notably, all LLMs performed far better on charac-\nter reordering task than on other tasks, closely trailing th e\nperformance of humans. W e conjecture that this is due to\nabundant resources available online about anagram, which\nare likely to have been used in pre-training of the models.\nEven when the newly formed words are non-existing words,\nmany of them are likely to have appeared in the training\ncorpora as possible anagrams of an existing word . It is\ntherefore only natural that all models struggled with chara c-\nter reordering as the word gets longer, or with an unknown\nword, as shown in T able\n4.\nA clearer contrast between LLMs\u2019 performances on token\nlevel and character level tasks is made on the tasks that do\nnot involve numerical elements, such as replacement. As\nillustrated in an example in T able\n5, LLMs rarely have any\ntrouble with replacement task at token level, indicating th at\ntoken-based embeddings are functioning in a desired man-\nner. W ord reordering task also turned out to be reliable,\neven for fairly long sentences. Such clear contrast between\nLLMs\u2019 performances on token level and character level\ntasks highlights a fundamental discrepancy in how these\nmodels process linguistic information, which suggests tha t,\nwhile LLMs have been effectively optimized for tasks in-\nvolving tokens, their handling of \ufb01ner-grained character-\nlevel tasks remains inadequate. See Appendix\nA for ex-\nperiments on languages of varying writing systems.\n4. Discussion\nAs shown throughout the paper, much of limitation in terms\nof understanding character composition derives from the\nvery nature of LLMs where they are almost invariably\n3", "mimetype": "text/plain", "start_char_idx": 3040, "end_char_idx": 5566, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ea9fa585-d3e1-4804-8530-e88e6ffefd9e": {"__data__": {"id_": "ea9fa585-d3e1-4804-8530-e88e6ffefd9e", "embedding": null, "metadata": {"page_label": "4", "file_name": "Large Language Models Lack Understanding of Character Composition of Words.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Large Language Models Lack Understanding of Character Composition of Words.pdf", "file_type": "application/pdf", "file_size": 262705, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "bc381e92-8ba4-45f7-8be6-58a06ddb0100", "node_type": "4", "metadata": {"page_label": "4", "file_name": "Large Language Models Lack Understanding of Character Composition of Words.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Large Language Models Lack Understanding of Character Composition of Words.pdf", "file_type": "application/pdf", "file_size": 262705, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "33007f0de8f0f56f3c382b1383b2ec66a137fb9436033ca5bb42ca8f0bae9bab", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "daa5f973-e246-4d32-b013-da8b527e3250", "node_type": "1", "metadata": {}, "hash": "8b44ce58c52b5a6e6bfb138cd4a52ce77e851ffdf39c06c39f243536e2dee6cb", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Large Language Models Lack Understanding of Character Composition of W ords\nT able 3. Example failure cases at character level tasks. Bold letter s indicate correct answers by the model.\nPrompt GPT4 Claude Gemini Mistral\nFind all words with character o: P eople enjoy music.\n(answer: P eople, enjoy )\nPeople, enjoy,\nmusic\nPeople, enjoy,\nmusic\nPeople, enjoy,\nmusic People, enjoy\nInsert character i after the \ufb01rst character for all words:\nThe Great W all stretches far .\n(answer: T ihe Gireat W iall sitretches \ufb01ar . )\nTihie Giireat\nWiiall strietches\nfari.\niThe iGreat\niW all istretches\nifar.\nThie Great\nW all stretches\nfar.\nTihe Griat\nWiall striatches\n\ufb01ar.\nRemove the third character from the end\nfor all words: Gravity affects falling objects.\n(answer: Gravty affets fallng objets. )\nGravie affect\nfallin object.\nGrav affects\nfallin objec.\nGravit affect\nfallin object.\nGravty affecs\nfaling obects.\nReplace all occurrences of h with x:\nHe has three children. (answer: Xe xas txree cxildren. )\nxe xas three\ncxildren.\nHe xas three\nchildren.\nXe xas txree\ncxildren.\nEx has three\nchildren\nReorder the characters in the following word to form\na new word: supercalifragilistic\n(answer: any valid anagram apart from input word itself )\nupercalifra-\ngilistic\nsupercalifr-\nagilistic\nlapsticalifr-\nagiceorous\ncilisuparegalf-\nitisticxedocious\nHow many vowels are in the following word:\nsupercalifragilistic (answer: 8 ) 9 11 8 7\nT able 4. Failure cases at token level tasks. Note that they frequentl y involve numerical elements such as position. Bold letters indicate\nthe correct answer by the model.\nModel\nPrompt Remove the third word from the following sentence:\nThe Renaissance was a period of cultural and artistic rebirt h.\n(ans: The Renaissance a period of cultural and artistic rebi rth.\nWhat is the seventh word from the end:\nThe Great Pyramid of Giza is one of the\nSeven W onders. (ans: Giza )\nGPT4 The was a period of cultural and artistic rebirth. one\nClaude The Renaissance was a cultural and artistic rebirth. W onders\nGemini The Renaissance was a period of artistic rebirth. of\nMistral The Renaissance a period of cultural and artistic rebirth. Pyramid\nT able 5. Example of LLMs\u2019 performances at token level in tasks\nthat do not involve numerical elements. Bold letters indica te the\ncorrect answer by the model.\nModel\nPrompt Replace all occurrences of \u201cthe\u201d with \u201cX\u201d:\nThe history of the city is in\ufb02uenced by the river .\n(ans: X history of X city is in\ufb02uenced by X river . )\nGPT4 X history of X city is in\ufb02uenced by X river .\nClaude X history of X city is in\ufb02uenced by X river .\nGemini X history of X city is in\ufb02uenced by X river .\nMistral X history of X city is in\ufb02uenced by X river .\ntrained at token levels, regardless of the pre-training obj ec-\ntives. By operating primarily at the token level, LLMs over-\nlook the intrinsic characteristics and nuances of individu al\ncharacters within words. This oversight hinders their abil -\nity to capture the rich semantic and syntactic information\nencoded at the character level, leading to sub-optimal per-\nformance in tasks requiring \ufb01ne-grained understanding of\nlanguage structure.\nA promising direction to address this limitation involves\nembedding character-level information directly into word\nembeddings, enabling models to capture the intricate rela-\ntionships and structures within individual characters. Fo r\nexample, BER T (\nDevlin et al. , 2019) represents input to-\nkens not only with token embedding, but also with seg-\nment embedding, which indicates the sentence that the to-\nken belongs to, and position embedding, which shows the\nposition of the token within the sentence.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3637, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "daa5f973-e246-4d32-b013-da8b527e3250": {"__data__": {"id_": "daa5f973-e246-4d32-b013-da8b527e3250", "embedding": null, "metadata": {"page_label": "4", "file_name": "Large Language Models Lack Understanding of Character Composition of Words.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Large Language Models Lack Understanding of Character Composition of Words.pdf", "file_type": "application/pdf", "file_size": 262705, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "bc381e92-8ba4-45f7-8be6-58a06ddb0100", "node_type": "4", "metadata": {"page_label": "4", "file_name": "Large Language Models Lack Understanding of Character Composition of Words.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Large Language Models Lack Understanding of Character Composition of Words.pdf", "file_type": "application/pdf", "file_size": 262705, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "33007f0de8f0f56f3c382b1383b2ec66a137fb9436033ca5bb42ca8f0bae9bab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ea9fa585-d3e1-4804-8530-e88e6ffefd9e", "node_type": "1", "metadata": {"page_label": "4", "file_name": "Large Language Models Lack Understanding of Character Composition of Words.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Large Language Models Lack Understanding of Character Composition of Words.pdf", "file_type": "application/pdf", "file_size": 262705, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "be2b92472c82a3b227a76f85f75dbad04dcfefc118542749a3a456df7a10f296", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "trained at token levels, regardless of the pre-training obj ec-\ntives. By operating primarily at the token level, LLMs over-\nlook the intrinsic characteristics and nuances of individu al\ncharacters within words. This oversight hinders their abil -\nity to capture the rich semantic and syntactic information\nencoded at the character level, leading to sub-optimal per-\nformance in tasks requiring \ufb01ne-grained understanding of\nlanguage structure.\nA promising direction to address this limitation involves\nembedding character-level information directly into word\nembeddings, enabling models to capture the intricate rela-\ntionships and structures within individual characters. Fo r\nexample, BER T (\nDevlin et al. , 2019) represents input to-\nkens not only with token embedding, but also with seg-\nment embedding, which indicates the sentence that the to-\nken belongs to, and position embedding, which shows the\nposition of the token within the sentence. A similar struc-\ntural approach can be made with respect to character, where\ncharacter is embedded also with information of the word it\nbelongs to, and its position within the word. Such multi-\nlevel embedding strategy could signi\ufb01cantly enhance the\nmodel\u2019s ability to understand and manipulate text at a \ufb01ner\ngranularity, and can help ensure that the model obtains a ro-\nbust understanding of word composition while being sensi-\ntive to the arrangement of characters within words. Another\npotential line of approach involves harnessing visual reco g-\nnition techniques to simulate human-like character percep -\ntion. In scene text recognition literature, there has been a\nnumber of endeavors to integrate computer vision method-\nologies to visually identify characters, replicating the c og-\nnitive processes humans employ when reading and com-\nprehending text (\nDu et al. , 2022; Bartz et al. , 2017). By\nleveraging the complementary strengths of both domains,\nthese approaches may potentially offer novel opportuni-\nties for improving robustness for character-level compre-\nhension within large language models.\n5. Conclusion\nW e examined LLMs\u2019 ability to understand character com-\nposition of words. Our experiments suggest that LLMs ut-\nterly fail to demonstrate the ability to understand charact er\ncomposition even at highly simple tasks that can be easily\nsolved by humans with elementary knowledge of language,\nmaking a stark contrast with their performances at token\nlevel. W e further discussed potential future directions, s uch\nas incorporating character-embedding and visual features .\n4", "mimetype": "text/plain", "start_char_idx": 2688, "end_char_idx": 5239, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "83762c53-d02e-412e-b133-69bc9c97b8dd": {"__data__": {"id_": "83762c53-d02e-412e-b133-69bc9c97b8dd", "embedding": null, "metadata": {"page_label": "5", "file_name": "Large Language Models Lack Understanding of Character Composition of Words.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Large Language Models Lack Understanding of Character Composition of Words.pdf", "file_type": "application/pdf", "file_size": 262705, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8c6ecd77-87d5-401e-a1fb-7581149cbc58", "node_type": "4", "metadata": {"page_label": "5", "file_name": "Large Language Models Lack Understanding of Character Composition of Words.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Large Language Models Lack Understanding of Character Composition of Words.pdf", "file_type": "application/pdf", "file_size": 262705, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "9c146484b4f73822def862eb0bd34772a049e8a75f297ab3450a90c554805220", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ddf3a6fd-2501-44b0-81b5-fa5a5aa59ad5", "node_type": "1", "metadata": {}, "hash": "2aabea0b75f679bc505858072a30d3c4b898a91f383765139e4663e28efcab1c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Large Language Models Lack Understanding of Character Composition of W ords\nImpact Statement\nThis paper presents work whose goal is to advance the \ufb01eld\nof Machine Learning. There are many potential societal\nconsequences of our work, none which we feel must be\nspeci\ufb01cally highlighted here.\nReferences\nAchiam, O. J., Adler, S., Agarwal, S., Ahmad, L., Akkaya,\nI., Aleman, F . L., Almeida, D., Altenschmidt, J., Altman,\nS., Anadkat, S., A vila, R., Babuschkin, I., Balaji, S., Bal-\ncom, V ., Baltescu, P ., Bao, H., Bavarian, M., Belgum,\nJ., Bello, I., Berdine, J., Bernadett-Shapiro, G., Berner,\nC., Bogdonoff, L., Boiko, O., Boyd, M., Brakman, A.-\nL., Brockman, G., Brooks, T ., Brundage, M., Button, K.,\nCai, T ., Campbell, R., Cann, A., Carey, B., Carlson, C.,\nCarmichael, R., Chan, B., Chang, C., Chantzis, F ., Chen,\nD., Chen, S., Chen, R., Chen, J., Chen, M., Chess, B.,\nCho, C., Chu, C., Chung, H. W ., Cummings, D., Cur-\nrier, J., Dai, Y ., Decareaux, C., Degry, T ., Deutsch, N.,\nDeville, D., Dhar, A., Dohan, D., Dowling, S., Dunning,\nS., Ecoffet, A., Eleti, A., Eloundou, T ., Farhi, D., Fe-\ndus, L., Felix, N., Fishman, S. P ., Forte, J., Fulford, I.,\nGao, L., Georges, E., Gibson, C., Goel, V ., Gogineni,\nT ., Goh, G., Gontijo-Lopes, R., Gordon, J., Grafstein,\nM., Gray, S., Greene, R., Gross, J., Gu, S. S., Guo, Y .,\nHallacy, C., Han, J., Harris, J., He, Y ., Heaton, M., Hei-\ndecke, J., Hesse, C., Hickey, A., Hickey, W ., Hoeschele,\nP ., Houghton, B., Hsu, K., Hu, S., Hu, X., Huizinga,\nJ., Jain, S., Jain, S., Jang, J., Jiang, A., Jiang, R., Jin,\nH., Jin, D., Jomoto, S., Jonn, B., Jun, H., Kaftan, T .,\nKaiser, L., Kamali, A., Kanitscheider, I., Keskar, N. S.,\nKhan, T ., Kilpatrick, L., Kim, J. W ., Kim, C., Kim, Y .,\nKirchner, H., Kiros, J. R., Knight, M., Kokotajlo, D.,\nKondraciuk, L., Kondrich, A., Konstantinidis, A., Kosic,\nK., Krueger, G., Kuo, V ., Lampe, M., Lan, I., Lee, T .,\nLeike, J., Leung, J., Levy, D., Li, C. M., Lim, R., Lin,\nM., Lin, S., Litwin, M., Lopez, T ., Lowe, R., Lue, P .,\nMakanju, A. A., Malfacini, K., Manning, S., Markov, T .,\nMarkovski, Y ., Martin,", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2102, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ddf3a6fd-2501-44b0-81b5-fa5a5aa59ad5": {"__data__": {"id_": "ddf3a6fd-2501-44b0-81b5-fa5a5aa59ad5", "embedding": null, "metadata": {"page_label": "5", "file_name": "Large Language Models Lack Understanding of Character Composition of Words.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Large Language Models Lack Understanding of Character Composition of Words.pdf", "file_type": "application/pdf", "file_size": 262705, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8c6ecd77-87d5-401e-a1fb-7581149cbc58", "node_type": "4", "metadata": {"page_label": "5", "file_name": "Large Language Models Lack Understanding of Character Composition of Words.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Large Language Models Lack Understanding of Character Composition of Words.pdf", "file_type": "application/pdf", "file_size": 262705, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "9c146484b4f73822def862eb0bd34772a049e8a75f297ab3450a90c554805220", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "83762c53-d02e-412e-b133-69bc9c97b8dd", "node_type": "1", "metadata": {"page_label": "5", "file_name": "Large Language Models Lack Understanding of Character Composition of Words.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Large Language Models Lack Understanding of Character Composition of Words.pdf", "file_type": "application/pdf", "file_size": 262705, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "816c6b171a2af23e67bd64db9a135102a6c5cb7668236eac7c5e10cd43599912", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "aba461e4-1c6b-44a2-aba5-379dfa0ffddc", "node_type": "1", "metadata": {}, "hash": "daecb3fe1c1053fb32e9d53d10591f22b01fedf1505d8b534a22dd02dfcf544e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "C., Kim, Y .,\nKirchner, H., Kiros, J. R., Knight, M., Kokotajlo, D.,\nKondraciuk, L., Kondrich, A., Konstantinidis, A., Kosic,\nK., Krueger, G., Kuo, V ., Lampe, M., Lan, I., Lee, T .,\nLeike, J., Leung, J., Levy, D., Li, C. M., Lim, R., Lin,\nM., Lin, S., Litwin, M., Lopez, T ., Lowe, R., Lue, P .,\nMakanju, A. A., Malfacini, K., Manning, S., Markov, T .,\nMarkovski, Y ., Martin, B., Mayer, K., Mayne, A., Mc-\nGrew , B., McKinney, S. M., McLeavey, C., McMillan, P .,\nMcNeil, J., Medina, D., Mehta, A., Menick, J., Metz, L.,\nMishchenko, A., Mishkin, P ., Monaco, V ., Morikawa, E.,\nMossing, D. P ., Mu, T ., Murati, M., Murk, O., M\u2019ely, D.,\nNair, A., Nakano, R., Nayak, R., Neelakantan, A., Ngo,\nR., Noh, H., Long, O., O\u2019Keefe, C., Pachocki, J. W .,\nPaino, A., Palermo, J., Pantuliano, A., Parascandolo, G.,\nParish, J., Parparita, E., Passos, A., Pavlov, M., Peng,\nA., Perelman, A., de A vila Belbute Peres, F ., Petrov, M.,\nde Oliveira Pinto, H. P ., Pokorny, M., Pokrass, M., Pong,\nV . H., Powell, T ., Power, A., Power, B., Proehl, E., Puri,\nR., Radford, A., Rae, J., Ramesh, A., Raymond, C., Real,\nF ., Rimbach, K., Ross, C., Rotsted, B., Roussez, H., Ry-\nder, N., Saltarelli, M. D., Sanders, T ., Santurkar, S., Sas-\ntry, G., Schmidt, H., Schnurr, D., Schulman, J., Selsam,\nD., Sheppard, K., Sherbakov, T ., Shieh, J., Shoker, S.,\nShyam, P ., Sidor, S., Sigler, E., Simens, M., Sitkin, J.,\nSlama, K., Sohl, I., Sokolowsky, B. D., Song, Y ., Stau-\ndacher, N., Such, F . P ., Summers, N., Sutskever, I., T ang,\nJ., T ezak, N. A., Thompson, M., Tillet, P ., T ootoonchian,\nA., Tseng, E., Tuggle, P ., Turley, N., T worek, J., Uribe,\nJ. F .", "mimetype": "text/plain", "start_char_idx": 1725, "end_char_idx": 3363, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "aba461e4-1c6b-44a2-aba5-379dfa0ffddc": {"__data__": {"id_": "aba461e4-1c6b-44a2-aba5-379dfa0ffddc", "embedding": null, "metadata": {"page_label": "5", "file_name": "Large Language Models Lack Understanding of Character Composition of Words.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Large Language Models Lack Understanding of Character Composition of Words.pdf", "file_type": "application/pdf", "file_size": 262705, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8c6ecd77-87d5-401e-a1fb-7581149cbc58", "node_type": "4", "metadata": {"page_label": "5", "file_name": "Large Language Models Lack Understanding of Character Composition of Words.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Large Language Models Lack Understanding of Character Composition of Words.pdf", "file_type": "application/pdf", "file_size": 262705, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "9c146484b4f73822def862eb0bd34772a049e8a75f297ab3450a90c554805220", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ddf3a6fd-2501-44b0-81b5-fa5a5aa59ad5", "node_type": "1", "metadata": {"page_label": "5", "file_name": "Large Language Models Lack Understanding of Character Composition of Words.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Large Language Models Lack Understanding of Character Composition of Words.pdf", "file_type": "application/pdf", "file_size": 262705, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "1eab230547188a9b0c42554916349941bd4c765b4f2b9ee393ced2111daf5a41", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "P ., Summers, N., Sutskever, I., T ang,\nJ., T ezak, N. A., Thompson, M., Tillet, P ., T ootoonchian,\nA., Tseng, E., Tuggle, P ., Turley, N., T worek, J., Uribe,\nJ. F . C., V allone, A., V ijayvergiya, A., V oss, C., W ain-\nwright, C. L., W ang, J. J., W ang, A., W ang, B., W ard,\nJ., W ei, J., W einmann, C., W elihinda, A., W elinder, P .,\nW eng, J., W eng, L., Wiethoff, M., Willner, D., Winter,\nC., W olrich, S., W ong, H., W orkman, L., Wu, S., Wu,\nJ., Wu, M., Xiao, K., Xu, T ., Y oo, S., Y u, K., Y uan, Q.,\nZaremba, W ., Zellers, R., Zhang, C., Zhang, M., Zhao,\nS., Zheng, T ., Zhuang, J., Zhuk, W ., and Zoph, B. Gpt-4\ntechnical report. 2023.\nAkbik, A., Bergmann, T ., Blythe, D. A. J., Rasul, K.,\nSchweter, S., and V ollgraf, R. Flair: An easy-to-use\nframework for state-of-the-art nlp. In North American\nChapter of the Association for Computational Linguis-\ntics, 2019.\nBartz, C., Y ang, H., and Meinel, C. See: T owards semi-\nsupervised end-to-end scene text recognition. In AAAI\nConference on Arti\ufb01cial Intelligence , 2017.\nBojanowski, P ., Grave, E., Joulin, A., and Mikolov, T . En-\nriching word vectors with subword information. T rans-\nactions of the Association for Computational Linguistics ,\n5:135\u2013146, 2016.\nChowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra,\nG., Roberts, A., Barham, P ., Chung, H. W ., Sutton, C.,\nGehrmann, S., Schuh, P ., Shi, K., Tsvyashchenko, S.,\nMaynez, J., Rao, A., Barnes, P ., T ay, Y ., Shazeer, N. M.,\nPrabhakaran, V ., Reif, E., Du, N., Hutchinson, B. C.,\nPope, R., Bradbury, J., Austin, J., Isard, M., Gur-Ari,\nG., Y in, P ., Duke, T ., Levskaya, A., Ghemawat, S., Dev,\nS., Michalewski, H., Garc\u00b4 \u0131a, X., Misra, V ., Robinson,\nK., Fedus, L., Zhou, D., Ippolito, D., Luan, D., Lim,\nH., Zoph, B., Spiridonov, A., Sepassi, R., Dohan, D.,\nAgrawal, S., Omernick, M., Dai, A. M., Pillai, T . S., Pel-\nlat, M., Lewkowycz, A., Moreira, E., Child, R., Polozov,\nO., Lee, K., Zhou, Z., W ang, X., Saeta, B., D\u00b4 \u0131az, M., Fi-\nrat, O., Catasta, M., W ei, J., Meier-Hellstern, K. S., Eck,\nD., Dean, J., Petrov, S., and Fiedel, N. Palm: Scaling\nlanguage modeling with pathways. J. Mach. Learn. Res. ,\n24:240:1\u2013240:113, 2022.\nClark, K., Luong, M.-T ., Le, Q. V ., and Manning, C. D.\nElectra: Pre-training text encoders as discriminators\nrather than generators. In International Conference on\nLearning Representations , 2020.\n5", "mimetype": "text/plain", "start_char_idx": 3196, "end_char_idx": 5564, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d73280f5-7e6f-4306-ae87-e3678b933ac1": {"__data__": {"id_": "d73280f5-7e6f-4306-ae87-e3678b933ac1", "embedding": null, "metadata": {"page_label": "6", "file_name": "Large Language Models Lack Understanding of Character Composition of Words.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Large Language Models Lack Understanding of Character Composition of Words.pdf", "file_type": "application/pdf", "file_size": 262705, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1abd0ab0-cac6-42dd-900d-db62e73afad0", "node_type": "4", "metadata": {"page_label": "6", "file_name": "Large Language Models Lack Understanding of Character Composition of Words.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Large Language Models Lack Understanding of Character Composition of Words.pdf", "file_type": "application/pdf", "file_size": 262705, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "c694ea37548166fbafb81f6cdcffc898b631f5c6b24e0e1c88e7d8be87285a0e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "84b77102-ad65-49af-a68e-e8b98324a8fe", "node_type": "1", "metadata": {}, "hash": "5ef2fc89255c65dc26856f8532a133aab13add4953bf2ec18ee49e2efe15817c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Large Language Models Lack Understanding of Character Composition of W ords\nClaude. Claude.ai. https://claude.ai/, 2023. [Ac-\ncessed 17-05-2024].\nDevlin, J., Chang, M.-W ., Lee, K., and T outanova, K. Bert:\nPre-training of deep bidirectional transformers for lan-\nguage understanding. In North American Chapter of the\nAssociation for Computational Linguistics , 2019.\nDu, Y ., Chen, Z., Jia, C., Y in, X., Zheng, T ., Li, C., Du,\nY ., and Jiang, Y .-G. Svtr: Scene text recognition with a\nsingle visual model. In International Joint Conference\non Arti\ufb01cial Intelligence , 2022.\nEfrat, A., Honovich, O., and Levy, O. Lmentry: A language\nmodel benchmark of elementary language tasks. ArXiv,\nabs/2211.02069, 2022.\nJiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C.,\nChaplot, D. S., de Las Casas, D., Bressand, F ., Lengyel,\nG., Lample, G., Saulnier, L., Lavaud, L. R., Lachaux, M.-\nA., Stock, P ., Scao, T . L., Lavril, T ., W ang, T ., Lacroix,\nT ., and Sayed, W . E. Mistral 7b. ArXiv, abs/2310.06825,\n2023.\nKim, Y ., Jernite, Y ., Sontag, D. A., and Rush, A. M.\nCharacter-aware neural language models. In AAAI Con-\nference on Arti\ufb01cial Intelligence , 2015.\nLee, B. W . and Lim, J. Language models don\u2019t learn the\nphysical manifestation of language. 2024.\nMarcus, M. P ., Santorini, B., and Marcinkiewicz, M. A.\nBuilding a large annotated corpus of english: The penn\ntreebank. Comput. Linguistics , 19:313\u2013330, 1993.\nOpenAI. Openai: Introducing chatgpt.\nhttps://openai.com/blog/chatgpt, 2022.\nPeters, M. E., Neumann, M., Iyyer, M., Gardner, M., Clark,\nC., Lee, K., and Zettlemoyer, L. Deep contextualized\nword representations. ArXiv, abs/1802.05365, 2018.\nQian, J., W ang, H., Li, Z., LI, S., and Y an, X. Limitations\nof language models in arithmetic and symbolic induction.\nIn Annual Meeting of the Association for Computational\nLinguistics, 2022.\nQin, L., Chen, Q., Zhou, Y ., Chen, Z., Li, Y ., Liao, L., Li,\nM., Che, W ., and Y u, P . S. Multilingual large language\nmodel: A survey of resources, taxonomy and frontiers.\nArXiv, abs/2404.04925, 2024.\nReid, M., Savinov, N., T eplyashin, D., Lepikhin, D., Lil-\nlicrap, T .", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2127, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "84b77102-ad65-49af-a68e-e8b98324a8fe": {"__data__": {"id_": "84b77102-ad65-49af-a68e-e8b98324a8fe", "embedding": null, "metadata": {"page_label": "6", "file_name": "Large Language Models Lack Understanding of Character Composition of Words.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Large Language Models Lack Understanding of Character Composition of Words.pdf", "file_type": "application/pdf", "file_size": 262705, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1abd0ab0-cac6-42dd-900d-db62e73afad0", "node_type": "4", "metadata": {"page_label": "6", "file_name": "Large Language Models Lack Understanding of Character Composition of Words.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Large Language Models Lack Understanding of Character Composition of Words.pdf", "file_type": "application/pdf", "file_size": 262705, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "c694ea37548166fbafb81f6cdcffc898b631f5c6b24e0e1c88e7d8be87285a0e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d73280f5-7e6f-4306-ae87-e3678b933ac1", "node_type": "1", "metadata": {"page_label": "6", "file_name": "Large Language Models Lack Understanding of Character Composition of Words.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Large Language Models Lack Understanding of Character Composition of Words.pdf", "file_type": "application/pdf", "file_size": 262705, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "e0eb5eb80cef8e0878f99cbeb71b3be169741d1e384872f09368e6fc36f6573c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8ecc17c4-4d16-4cd5-b72c-ca609dab826a", "node_type": "1", "metadata": {}, "hash": "ad7207c0cefc3c4d20f17a13b6f3ef477ec6739e71c6a48cf81329355bd05e5c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "ArXiv, abs/1802.05365, 2018.\nQian, J., W ang, H., Li, Z., LI, S., and Y an, X. Limitations\nof language models in arithmetic and symbolic induction.\nIn Annual Meeting of the Association for Computational\nLinguistics, 2022.\nQin, L., Chen, Q., Zhou, Y ., Chen, Z., Li, Y ., Liao, L., Li,\nM., Che, W ., and Y u, P . S. Multilingual large language\nmodel: A survey of resources, taxonomy and frontiers.\nArXiv, abs/2404.04925, 2024.\nReid, M., Savinov, N., T eplyashin, D., Lepikhin, D., Lil-\nlicrap, T . P ., Alayrac, J.-B., Soricut, R., Lazaridou, A.,\nFirat, O., Schrittwieser, J., Antonoglou, I., Anil, R.,\nBorgeaud, S., Dai, A. M., Millican, K., Dyer, E., Glaese,\nM., Sottiaux, T ., Lee, B., V iola, F ., Reynolds, M., Xu,\nY ., Molloy, J., Chen, J., Isard, M., Barham, P ., Henni-\ngan, T ., McIlroy, R., Johnson, M., Schalkwyk, J., Collins,\nE., Rutherford, E., Moreira, E., A youb, K. W ., Goel, M.,\nMeyer, C., Thornton, G., Y ang, Z., Michalewski, H., Ab-\nbas, Z., Schucher, N., Anand, A., Ives, R., Keeling, J.,\nLenc, K., Haykal, S., Shakeri, S., Shyam, P ., Chowdhery,\nA., Ring, R., Spencer, S., Sezener, E., V ilnis, L., Chang,\nO., Morioka, N., Tucker, G., Zheng, C., W oodman, O.,\nAttaluri, N., Kocisky, T ., Eltyshev, E., Chen, X., Chung,\nT ., Selo, V ., Brahma, S., Georgiev, P ., Slone, A., Zhu,\nZ., Lottes, J., Qiao, S., Caine, B., Riedel, S., T omala, A.,\nChadwick, M., Love, J. C., Choy, P ., Mittal, S., Houlsby,\nN., T ang, Y ., Lamm, M., Bai, L., Zhang, Q., He, L.,\nCheng, Y ., Humphreys, P ., Li, Y ., Brin, S., Cassirer, A.,\nMiao, Y .-Q., Zilka, L., T obin, T ., Xu, K., Proleev, L.,\nSohn, D., Magni, A., Hendricks, L. A., Gao, I., On-\ntan\u2019on, S., Bunyan, O., Byrd, N., Sharma, A., Zhang, B.,\nPinto, M., Sinha, R., Mehta, H., Jia, D., Caelles, S., W eb-\nson, A., Morris, A., Roelofs, B., Ding, Y ., Strudel, R.,\nXiong, X., Ritter, M., Dehghani, M., Chaabouni, R., Kar-\nmarkar, A., Lai, G., Mentzer, F ., Xu, B., Li, Y ., Zhang,\nY ., Paine, T .", "mimetype": "text/plain", "start_char_idx": 1631, "end_char_idx": 3585, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8ecc17c4-4d16-4cd5-b72c-ca609dab826a": {"__data__": {"id_": "8ecc17c4-4d16-4cd5-b72c-ca609dab826a", "embedding": null, "metadata": {"page_label": "6", "file_name": "Large Language Models Lack Understanding of Character Composition of Words.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Large Language Models Lack Understanding of Character Composition of Words.pdf", "file_type": "application/pdf", "file_size": 262705, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1abd0ab0-cac6-42dd-900d-db62e73afad0", "node_type": "4", "metadata": {"page_label": "6", "file_name": "Large Language Models Lack Understanding of Character Composition of Words.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Large Language Models Lack Understanding of Character Composition of Words.pdf", "file_type": "application/pdf", "file_size": 262705, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "c694ea37548166fbafb81f6cdcffc898b631f5c6b24e0e1c88e7d8be87285a0e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "84b77102-ad65-49af-a68e-e8b98324a8fe", "node_type": "1", "metadata": {"page_label": "6", "file_name": "Large Language Models Lack Understanding of Character Composition of Words.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Large Language Models Lack Understanding of Character Composition of Words.pdf", "file_type": "application/pdf", "file_size": 262705, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "8649379e46620225429e7615a00787a898f48a684240647202b2b862799c9448", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "L., Goldin, A., Neyshabur, B., Baumli, K.,\nLevskaya, A., Laskin, M., Jia, W ., Rae, J. W ., Xiao, K.,\nHe, A., Giordano, S., Y agati, L., Lespiau, J.-B., Natsev,\nP ., Ganapathy, S., Liu, F ., Martins, D., Chen, N., Xu,\nY ., Barnes, M., May, R., V ezer, A., Oh, J., Franko, K.,\nBridgers, S., Zhao, R., Wu, B., Mustafa, B., Sechrist, S.,\nParisotto, E., Pillai, T . S., Larkin, C., Gu, C., Sorokin,\nC., Krikun, M., Guseynov, A., Landon, J., Datta, R.,\nPritzel, A., Thacker, P ., Y ang, F ., Hui, K., Hauth, A.,\nY eh, C.-K., Barker, D., Mao-Jones, J., Austin, S., Shea-\nhan, H., Schuh, P ., Svensson, J., Jain, R., Ramasesh,\nV . V ., Briukhov, A., Chung, D.-W ., von Glehn, T ., Butter-\n\ufb01eld, C., Jhakra, P ., Wiethoff, M., Frye, J., Grimstad, J.,\nChangpinyo, B., Lan, C. L., Bortsova, A., Wu, Y ., V oigt-\nlaender, P ., Sainath, T . N., Smith, C., Hawkins, W ., Cao,\nK., Besley, J., Srinivasan, S., Omernick, M., Gaffney,\nC., de Castro Surita, G., Burnell, R., Damoc, B., Ahn,\nJ., Brock, A., Pajarskas, M., Petrushkina, A., Noury, S.,\nBlanco, L., Swersky, K., Ahuja, A., A vrahami, T ., Misra,\nV ., de Liedekerke, R., Iinuma, M., Polozov, A., Y ork, S.,\nvan den Driessche, G., Michel, P ., Chiu, J., Blevins, R.,\nGleicher, Z., Recasens, A., Rrustemi, A., Gribovskaya,\nE., Roy, A., Gworek, W ., Arnold, S. M. R., Lee, L.,\nLee-Thorp, J., Maggioni, M., Piqueras, E., Badola, K.,\nV ikram, S., Gonzalez, L., Baddepudi, A., Senter, E., De-\nvlin, J., Qin, J., Azzam, M., Trebacz, M., Polacek, M.,\nKrishnakumar, K., yiin Chang, S., Tung, M., Penchev,\nI., Joshi, R., Olszewska, K., Muir, C., Wirth, M., Hart-\nman, A. J., Newlan, J., Kashem, S., Bolina, V ., Dabir, E.,\nvan Amersfoort, J. R., Ahmed, Z., Cobon-Kerr, J., Ka-\nmath, A. B., Hrafnkelsson, A. M., Hou, L., Mackinnon,\nI., Frechette, A., Noland, E., Si, X., T aropa, E., Li, D.,\nCrone, P ., Gulati, A., Cevey, S., Adler, J., Ma, A., Sil-\nver, D., T okumine, S., Powell, R., Lee, S., Chang, M. B.,\nHassan, S., Mincu, D., Y ang, A., Levine, N., Brennan, J.,\n6", "mimetype": "text/plain", "start_char_idx": 3586, "end_char_idx": 5588, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "29de92db-3dcd-4a0a-b487-fe16b81d8c2c": {"__data__": {"id_": "29de92db-3dcd-4a0a-b487-fe16b81d8c2c", "embedding": null, "metadata": {"page_label": "7", "file_name": "Large Language Models Lack Understanding of Character Composition of Words.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Large Language Models Lack Understanding of Character Composition of Words.pdf", "file_type": "application/pdf", "file_size": 262705, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "570eb31e-738d-4493-b7e2-4ac63f688606", "node_type": "4", "metadata": {"page_label": "7", "file_name": "Large Language Models Lack Understanding of Character Composition of Words.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Large Language Models Lack Understanding of Character Composition of Words.pdf", "file_type": "application/pdf", "file_size": 262705, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "861bafc2b61aab6f86d5f814197afd4ab2690b7a02a4da33c064530a444eebee", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "08dcc7d2-d8a0-4f4e-8b5d-8103cd163907", "node_type": "1", "metadata": {}, "hash": "8d7a0b3f8213a0ca60a7ba91e002f1d07d8176556f051aa57cdf50899f76c43a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Large Language Models Lack Understanding of Character Composition of W ords\nW ang, M., Hodkinson, S., Zhao, J., Lipschultz, J., Pope,\nA., Chang, M. B., Li, C., Shafey, L. E., Paganini, M.,\nDouglas, S., Bohnet, B., Pardo, F ., Odoom, S., Rosca,\nM., dos Santos, C. N., Soparkar, K., Guez, A., Hudson,\nT ., Hansen, S., Asawaroengchai, C., Addanki, R., Y u,\nT ., Stokowiec, W ., Khan, M., Gilmer, J., Lee, J., Bo-\nstock, C. G., Rong, K., Caton, J., Pejman, P ., Pavetic,\nF ., Brown, G., Sharma, V ., Luvci\u2019c, M., Samuel, R., Djo-\nlonga, J., Mandhane, A., Sjosund, L. L., Buchatskaya,\nE., White, E., Clay, N., Jiang, J., Lim, H., Hemsley,\nR., Labanowski, J., Cao, N. D., Steiner, D., Hashemi,\nS. H., Austin, J., Gergely, A., Blyth, T ., Stanton, J.,\nShivakumar, K., Siddhant, A., Andreassen, A., Araya,\nC. L., Sethi, N., Shivanna, R., Hand, S., Bapna, A., Kho-\ndaei, A., Miech, A., T anzer, G., Swing, A., Thakoor,\nS., Pan, Z., Nado, Z., Winkler, S., Y u, D., Saleh, M.,\nMaggiore, L., Barr, I., Giang, M., Kagohara, T ., Dani-\nhelka, I., Marathe, A., Feinberg, V ., Elhawaty, M., Ghe-\nlani, N., Horgan, D., Miller, H., W alker, L., T anburn, R.,\nT ariq, M., Shrivastava, D., Xia, F ., Chiu, C.-C., Ash-\nwood, Z. C., Baatarsukh, K., Samangooei, S., Alcober,\nF ., Stjerngren, A., Komarek, P ., Tsihlas, K., Boral, A.,\nComanescu, R., Chen, J., Liu, R., Bloxwich, D., Chen,\nC., Sun, Y ., Feng, F ., Mauger, M., Dotiwalla, X., Hel-\nlendoorn, V ., Sharman, M., Zheng, I., Haridasan, K.,\nBarth-Maron, G., Swanson, C., Rogozi\u2019nska, D., An-\ndreev, A., Rubenstein, P . K., Sang, R., Hurt, D., Elsayed,\nG., W ang, R., Lacey, D., Ili\u2019c, A., Zhao, Y ., Aroyo, L.,\nIwuanyanwu, C., Nikolaev, V ., Lakshminarayanan, B.,\nJazayeri, S., Kaufman, R. L., V aradarajan, M., T ekur, C.,\nFritz, D., Khalman, M., Reitter, D., Dasgupta, K., Sar-\ncar, S., Ornduff, T ., Snaider, J., Huot, F ., Jia, J., Kemp,\nR., Trdin, N., V ijayakumar, A., Kim, L., Angermueller,\nC., Lao, L., Liu, T ., Zhang, H., Engel, D., Greene, S.,\nWhite, A., Austin, J., T aylor, L., Ashraf, S., Liu, D.,\nGeorgaki, M., Cai, I., Kulizhskaya, Y ., Goenka, S., Saeta,\nB., V odrahalli, K., Frank, C.,", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2138, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "08dcc7d2-d8a0-4f4e-8b5d-8103cd163907": {"__data__": {"id_": "08dcc7d2-d8a0-4f4e-8b5d-8103cd163907", "embedding": null, "metadata": {"page_label": "7", "file_name": "Large Language Models Lack Understanding of Character Composition of Words.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Large Language Models Lack Understanding of Character Composition of Words.pdf", "file_type": "application/pdf", "file_size": 262705, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "570eb31e-738d-4493-b7e2-4ac63f688606", "node_type": "4", "metadata": {"page_label": "7", "file_name": "Large Language Models Lack Understanding of Character Composition of Words.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Large Language Models Lack Understanding of Character Composition of Words.pdf", "file_type": "application/pdf", "file_size": 262705, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "861bafc2b61aab6f86d5f814197afd4ab2690b7a02a4da33c064530a444eebee", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "29de92db-3dcd-4a0a-b487-fe16b81d8c2c", "node_type": "1", "metadata": {"page_label": "7", "file_name": "Large Language Models Lack Understanding of Character Composition of Words.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Large Language Models Lack Understanding of Character Composition of Words.pdf", "file_type": "application/pdf", "file_size": 262705, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "ba48bca6e613960d12802be06cbb210be0dd2b7e21cd3a05d5047b3ca675926d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "dc286893-8803-4e33-a246-13cd50c5c268", "node_type": "1", "metadata": {}, "hash": "f78eec99522f51ba71f7ae926b31de836038182e141677ef65c6d126e528442a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "D., Khalman, M., Reitter, D., Dasgupta, K., Sar-\ncar, S., Ornduff, T ., Snaider, J., Huot, F ., Jia, J., Kemp,\nR., Trdin, N., V ijayakumar, A., Kim, L., Angermueller,\nC., Lao, L., Liu, T ., Zhang, H., Engel, D., Greene, S.,\nWhite, A., Austin, J., T aylor, L., Ashraf, S., Liu, D.,\nGeorgaki, M., Cai, I., Kulizhskaya, Y ., Goenka, S., Saeta,\nB., V odrahalli, K., Frank, C., de Cesare, D., Robenek,\nB., Richardson, H., Alnahlawi, M., Y ew , C., Ponnapalli,\nP ., T agliasacchi, M., Korchemniy, A., Kim, Y ., Li, D.,\nRosgen, B., Levin, K., Wiesner, J., Banzal, P ., Srini-\nvasan, P ., Y u, H., cCauglar Unlu, Reid, D., Tung, Z.,\nFinchelstein, D. F ., Kumar, R., Elisseeff, A., Huang, J.,\nZhang, M., Zhu, R., Aguilar, R., Gim\u2019enez, M., Xia,\nJ., Dousse, O., Gierke, W ., Y eganeh, S. H., Y ates, D.,\nJalan, K., Li, L., Latorre-Chimoto, E., Nguyen, D. D.,\nDurden, K., Kallakuri, P ., Liu, Y ., Johnson, M., Tsai,\nT ., T albert, A., Liu, J., Neitz, A., Elkind, C., Selvi, M.,\nJasarevic, M., Soares, L. B., Cui, A., W ang, P ., W ang,\nA. W ., Y e, X., Kallarackal, K., Loher, L., Lam, H.,\nBroder, J., Holtmann-Rice, D. N., Martin, N., Ramad-\nhana, B., T oyama, D., Shukla, M., Basu, S., Mohan, A.,\nFernando, N., Fiedel, N., Paterson, K., Li, H., Garg, A.,\nPark, J., Choi, D., Wu, D., Singh, S., Zhang, Z., Glober-\nson, A., Y u, L., Carpenter, J., de Chaumont Quitry, F .,\nRadebaugh, C., Lin, C.-C., Tudor, A., Shroff, P ., Gar-\nmon, D., Du, D., V ats, N., Lu, H., Iqbal, S., Y akubovich,\nA., Tripuraneni, N., Manyika, J., Qureshi, H., Hua, N.,\nNgani, C., Raad, M. A., Forbes, H., Bulanova, A., Stan-\nway, J., Sundararajan, M., Ungureanu, V ., Bishop, C., Li,\nY ., V enkatraman, B., Li, B., Thornton, C., Scellato, S.,\nGupta, N., W ang, Y ., T enney, I., Wu, X., Shenoy, A., Car-\nvajal, G., Wright, D. G., Bariach, B., Xiao, Z., Hawkins,\nP ., Dalmia, S., Farabet, C., V alenzuela, P ., Y uan, Q.,", "mimetype": "text/plain", "start_char_idx": 1766, "end_char_idx": 3652, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "dc286893-8803-4e33-a246-13cd50c5c268": {"__data__": {"id_": "dc286893-8803-4e33-a246-13cd50c5c268", "embedding": null, "metadata": {"page_label": "7", "file_name": "Large Language Models Lack Understanding of Character Composition of Words.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Large Language Models Lack Understanding of Character Composition of Words.pdf", "file_type": "application/pdf", "file_size": 262705, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "570eb31e-738d-4493-b7e2-4ac63f688606", "node_type": "4", "metadata": {"page_label": "7", "file_name": "Large Language Models Lack Understanding of Character Composition of Words.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Large Language Models Lack Understanding of Character Composition of Words.pdf", "file_type": "application/pdf", "file_size": 262705, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "861bafc2b61aab6f86d5f814197afd4ab2690b7a02a4da33c064530a444eebee", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "08dcc7d2-d8a0-4f4e-8b5d-8103cd163907", "node_type": "1", "metadata": {"page_label": "7", "file_name": "Large Language Models Lack Understanding of Character Composition of Words.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Large Language Models Lack Understanding of Character Composition of Words.pdf", "file_type": "application/pdf", "file_size": 262705, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "a920acab548b34793b682229bd41c635eed6b11f0df2003219687cc42e2628aa", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "16fae7b4-07e7-4137-a637-ccfbcf72d8e4", "node_type": "1", "metadata": {}, "hash": "dbe11a025794d758c0e2417662b7da662fb4a1a344a30a2573edf09920748609", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": ", Manyika, J., Qureshi, H., Hua, N.,\nNgani, C., Raad, M. A., Forbes, H., Bulanova, A., Stan-\nway, J., Sundararajan, M., Ungureanu, V ., Bishop, C., Li,\nY ., V enkatraman, B., Li, B., Thornton, C., Scellato, S.,\nGupta, N., W ang, Y ., T enney, I., Wu, X., Shenoy, A., Car-\nvajal, G., Wright, D. G., Bariach, B., Xiao, Z., Hawkins,\nP ., Dalmia, S., Farabet, C., V alenzuela, P ., Y uan, Q.,\nW elty, C. A., Agarwal, A., Chen, M., Kim, W ., Hulse, B.,\nDukkipati, N., Paszke, A., Bolt, A., Davoodi, E., Choo,\nK., Beattie, J., Prendki, J., V ashisht, H., Santamaria-\nFernandez, R., Cobo, L. C., Wilkiewicz, J., Madras, D.,\nElqursh, A., Uy, G., Ramirez, K., Harvey, M., Liechty,\nT ., Zen, H., Seibert, J., Hu, C. H., Khorlin, A. Y ., Le,\nM., Aharoni, A., Li, M., W ang, L., Kumar, S., Lince, A.,\nCasagrande, N., Hoover, J., Badawy, D. E., Soergel, D.,\nVnukov, D., Miecnikowski, M., Simsa, J., Koop, A., Ku-\nmar, P ., Sellam, T ., Vlasic, D., Daruki, S., Shabat, N.,\nZhang, J., Su, G., Zhang, J., Liu, J., Sun, Y ., Palmer,\nE., Ghaffarkhah, A., Xiong, X., Cotruta, V ., Fink, M.,\nDixon, L., Sreevatsa, A., Goedeckemeyer, A., Dimitriev,\nA., Jafari, M., Crocker, R., Fitzgerald, N. A., Kumar, A.,\nGhemawat, S., Philips, I., Liu, F ., Liang, Y ., Sterneck, R.,\nRepina, A., Wu, M., Knight, L., Georgiev, M., Lee, H.,\nAskham, H., Chakladar, A., Louis, A., Crous, C., Cate,\nH., Petrova, D., Quinn, M., Owusu-Afriyie, D., Singhal,\nA., W ei, N., Kim, S., V incent, D., Nasr, M., Choquette-\nChoo, C. A., T ojo, R., Lu, S., de Las Casas, D., Cheng,\nY ., Bolukbasi, T ., Lee, K., Fatehi, S., Ananthanarayanan,\nR., Patel, M., Kaed, C. E., Li, J., Sygnowski, J., Belle,\nS. R., Chen, Z., Konzelmann, J., Poder, S., Garg, R.,\nKoverkathu, V ., Brown, A., Dyer, C., Liu, R., Nova,\nA., Xu, J., Petrov, S., Hassabis, D., Kavukcuoglu, K.,\nDean, J., and V inyals, O. Gemini 1.", "mimetype": "text/plain", "start_char_idx": 3264, "end_char_idx": 5111, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "16fae7b4-07e7-4137-a637-ccfbcf72d8e4": {"__data__": {"id_": "16fae7b4-07e7-4137-a637-ccfbcf72d8e4", "embedding": null, "metadata": {"page_label": "7", "file_name": "Large Language Models Lack Understanding of Character Composition of Words.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Large Language Models Lack Understanding of Character Composition of Words.pdf", "file_type": "application/pdf", "file_size": 262705, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "570eb31e-738d-4493-b7e2-4ac63f688606", "node_type": "4", "metadata": {"page_label": "7", "file_name": "Large Language Models Lack Understanding of Character Composition of Words.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Large Language Models Lack Understanding of Character Composition of Words.pdf", "file_type": "application/pdf", "file_size": 262705, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "861bafc2b61aab6f86d5f814197afd4ab2690b7a02a4da33c064530a444eebee", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "dc286893-8803-4e33-a246-13cd50c5c268", "node_type": "1", "metadata": {"page_label": "7", "file_name": "Large Language Models Lack Understanding of Character Composition of Words.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Large Language Models Lack Understanding of Character Composition of Words.pdf", "file_type": "application/pdf", "file_size": 262705, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "c2e23105ddbc8195de0c78ba28f7b9be76e98e0c24662f010377e2e1917ac34a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "C. A., T ojo, R., Lu, S., de Las Casas, D., Cheng,\nY ., Bolukbasi, T ., Lee, K., Fatehi, S., Ananthanarayanan,\nR., Patel, M., Kaed, C. E., Li, J., Sygnowski, J., Belle,\nS. R., Chen, Z., Konzelmann, J., Poder, S., Garg, R.,\nKoverkathu, V ., Brown, A., Dyer, C., Liu, R., Nova,\nA., Xu, J., Petrov, S., Hassabis, D., Kavukcuoglu, K.,\nDean, J., and V inyals, O. Gemini 1.5: Unlocking mul-\ntimodal understanding across millions of tokens of con-\ntext. ArXiv, abs/2403.05530, 2024.\nSrivastava, A., Rastogi, A., Rao, A., Shoeb, A. A. M., Abid,\nA., Fisch, A., Brown, A. R., Santoro, A., Gupta, A.,\nGarriga-Alonso, A., Kluska, A., Lewkowycz, A., Agar-\nwal, A., Power, A., Ray, A., W arstadt, A., Kocurek,\nA. W ., Safaya, A., T azarv, A., Xiang, A., Parrish, A.,\nNie, A., Hussain, A., Askell, A., Dsouza, A., Slone, A.,\nRahane, A. A., Iyer, A. S., Andreassen, A., Madotto,\nA., Santilli, A., Stuhlmuller, A., Dai, A. M., La, A.,\nLampinen, A. K., Zou, A., Jiang, A., Chen, A., V uong,\nA., Gupta, A., Gottardi, A., Norelli, A., V enkatesh,\nA., Gholamidavoodi, A., T abassum, A., Menezes, A.,\nKirubarajan, A., Mullokandov, A., Sabharwal, A., Her-\nrick, A., Efrat, A., Erdem, A., Karakacs, A., Roberts,\nB. R., Loe, B. S., Zoph, B., Bojanowski, B., Ozyurt,\nB., Hedayatnia, B., Neyshabur, B., Inden, B., Stein, B.,\nEkmekci, B., Lin, B. Y ., Howald, B. S., Orinion, B.,\n7", "mimetype": "text/plain", "start_char_idx": 4744, "end_char_idx": 6097, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "87216d92-ae2d-4147-b70d-bf07c6074206": {"__data__": {"id_": "87216d92-ae2d-4147-b70d-bf07c6074206", "embedding": null, "metadata": {"page_label": "8", "file_name": "Large Language Models Lack Understanding of Character Composition of Words.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Large Language Models Lack Understanding of Character Composition of Words.pdf", "file_type": "application/pdf", "file_size": 262705, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6ffb189f-fd69-4a30-8d40-b3753854c569", "node_type": "4", "metadata": {"page_label": "8", "file_name": "Large Language Models Lack Understanding of Character Composition of Words.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Large Language Models Lack Understanding of Character Composition of Words.pdf", "file_type": "application/pdf", "file_size": 262705, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "c6a7ee505f3fec04d0fbf4755f10d8cfa137f6e45062ef96dc10de4a26093f1c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cb5664aa-ecde-4a8a-9e90-1769bba41b27", "node_type": "1", "metadata": {}, "hash": "7c89731ac13c6cd9c41ce784dc525be280fbf3ef4e4d94d0cff7a5714b8d68e2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Large Language Models Lack Understanding of Character Composition of W ords\nDiao, C., Dour, C., Stinson, C., Argueta, C., Ram\u2019irez,\nC. F ., Singh, C., Rathkopf, C., Meng, C., Baral, C., Wu,\nC., Callison-Burch, C., W aites, C., V oigt, C., Manning,\nC. D., Potts, C., Ramirez, C., Rivera, C., Siro, C., Raf-\nfel, C., Ashcraft, C., Garbacea, C., Sileo, D., Garrette,\nD. H., Hendrycks, D., Kilman, D., Roth, D., Freeman,\nD., Khashabi, D., Levy, D., Gonz\u2019alez, D. M., Perszyk,\nD. R., Hernandez, D., Chen, D., Ippolito, D., Gilboa, D.,\nDohan, D., Drakard, D., Jurgens, D., Datta, D., Ganguli,\nD., Emelin, D., Kleyko, D., Y uret, D., Chen, D., T am, D.,\nHupkes, D., Misra, D., Buzan, D., Mollo, D. C., Y ang,\nD., Lee, D.-H., Schrader, D., Shutova, E., Cubuk, E. D.,\nSegal, E., Hagerman, E., Barnes, E., Donoway, E. P .,\nPavlick, E., Rodol` a, E., Lam, E., Chu, E., T ang, E., Er-\ndem, E., Chang, E., Chi, E. A., Dyer, E., Jerzak, E., Kim,\nE., Manyasi, E. E., Zheltonozhskii, E., Xia, F ., Siar, F .,\nMart\u2019inez-Plumed, F ., Happ\u2019e, F ., Chollet, F ., Rong, F .,\nMishra, G., Winata, G. I., de Melo, G., Kruszewski, G.,\nParascandolo, G., Mariani, G., W ang, G. X., Jaimovitch-\nL \u2019opez, G., Betz, G., Gur-Ari, G., Galijasevic, H., Kim,\nH., Rashkin, H., Hajishirzi, H., Mehta, H., Bogar, H.,\nShevlin, H., Schutze, H., Y akura, H., Zhang, H., W ong,\nH. M., Ng, I., Noble, I., Jumelet, J., Geissinger, J.,\nKernion, J., Hilton, J., Lee, J., Fisac, J. F ., Simon, J. B.,\nKoppel, J., Zheng, J., Zou, J., Koco\u2019n, J., Thompson,\nJ., Wing\ufb01eld, J., Kaplan, J., Radom, J., Sohl-Dickstein,\nJ. N., Phang, J., W ei, J., Y osinski, J., Novikova, J., Boss-\ncher, J., Marsh, J., Kim, J., T aal, J., Engel, J., Alabi,\nJ. O., Xu, J., Song, J., T ang, J., W aweru, J. W ., Burden,\nJ., Miller, J., Balis, J. U., Batchelder, J., Berant, J., Fro -\nhberg, J., Rozen, J., Hern \u00b4 andez-Orallo, J., Boudeman, J.,\nGuerr, J., Jones, J., T enenbaum, J., Rule, J.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1920, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "cb5664aa-ecde-4a8a-9e90-1769bba41b27": {"__data__": {"id_": "cb5664aa-ecde-4a8a-9e90-1769bba41b27", "embedding": null, "metadata": {"page_label": "8", "file_name": "Large Language Models Lack Understanding of Character Composition of Words.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Large Language Models Lack Understanding of Character Composition of Words.pdf", "file_type": "application/pdf", "file_size": 262705, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6ffb189f-fd69-4a30-8d40-b3753854c569", "node_type": "4", "metadata": {"page_label": "8", "file_name": "Large Language Models Lack Understanding of Character Composition of Words.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Large Language Models Lack Understanding of Character Composition of Words.pdf", "file_type": "application/pdf", "file_size": 262705, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "c6a7ee505f3fec04d0fbf4755f10d8cfa137f6e45062ef96dc10de4a26093f1c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "87216d92-ae2d-4147-b70d-bf07c6074206", "node_type": "1", "metadata": {"page_label": "8", "file_name": "Large Language Models Lack Understanding of Character Composition of Words.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Large Language Models Lack Understanding of Character Composition of Words.pdf", "file_type": "application/pdf", "file_size": 262705, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "ab2e69941a77a312b3f7a9cefbaf71fc0e8a3cb306d2b8d89c2050eb03c63fae", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f66e05d1-3e84-49fa-afbc-a375602fd48d", "node_type": "1", "metadata": {}, "hash": "9e58682851570cb4011ad8e2e9f3a554992e0585837951ab5056736e25ef0c87", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "J., Radom, J., Sohl-Dickstein,\nJ. N., Phang, J., W ei, J., Y osinski, J., Novikova, J., Boss-\ncher, J., Marsh, J., Kim, J., T aal, J., Engel, J., Alabi,\nJ. O., Xu, J., Song, J., T ang, J., W aweru, J. W ., Burden,\nJ., Miller, J., Balis, J. U., Batchelder, J., Berant, J., Fro -\nhberg, J., Rozen, J., Hern \u00b4 andez-Orallo, J., Boudeman, J.,\nGuerr, J., Jones, J., T enenbaum, J., Rule, J. S., Chua,\nJ., Kanclerz, K., Livescu, K., Krauth, K., Gopalakrish-\nnan, K., Ignatyeva, K., Markert, K., Dhole, K. D., Gim-\npel, K., Omondi, K., Mathewson, K. W ., Chiafullo, K.,\nShkaruta, K., Shridhar, K., McDonell, K., Richardson,\nK., Reynolds, L., Gao, L., Zhang, L., Dugan, L., Qin, L.,\nContreras-Ochando, L., Morency, L.-P ., Moschella, L.,\nLam, L., Noble, L., Schmidt, L., He, L., Col\u2019on, L. O.,\nMetz, L., cSenel, L. K., Bosma, M., Sap, M., ter Hoeve,\nM., Farooqi, M., Faruqui, M., Mazeika, M., Baturan, M.,\nMarelli, M., Maru, M., Quintana, M. J. R., T olkiehn, M.,\nGiulianelli, M., Lewis, M., Potthast, M., Leavitt, M. L.,\nHagen, M., Schubert, M., Baitemirova, M., Arnaud, M.,\nMcElrath, M. A., Y ee, M., Cohen, M., Gu, M., Ivanit-\nskiy, M. I., Starritt, M., Strube, M., Swkedrowski, M.,\nBevilacqua, M., Y asunaga, M., Kale, M., Cain, M., Xu,\nM., Suzgun, M., W alker, M., Tiwari, M., Bansal, M.,\nAminnaseri, M., Geva, M., Gheini, M., MukundV arma,\nT ., Peng, N., Chi, N. A., Lee, N., Krakover, N. G.-A.,\nCameron, N., Roberts, N., Doiron, N., Martinez, N.,\nNangia, N., Deckers, N., Muennighoff, N., Keskar, N. S.,\nIyer, N., Constant, N., Fiedel, N., W en, N., Zhang, O.,\nAgha, O., Elbaghdadi, O., Levy, O., Evans, O., Casares,\nP . A. M., Doshi, P ., Fung, P ., Liang, P . P ., V icol, P .,\nAlipoormolabashi, P ., Liao, P ., Liang, P ., Chang, P ., Eck-\nersley, P ., Htut, P . M., Hwang, P .-B., Milkowski, P ., Patil,\nP .", "mimetype": "text/plain", "start_char_idx": 1535, "end_char_idx": 3345, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f66e05d1-3e84-49fa-afbc-a375602fd48d": {"__data__": {"id_": "f66e05d1-3e84-49fa-afbc-a375602fd48d", "embedding": null, "metadata": {"page_label": "8", "file_name": "Large Language Models Lack Understanding of Character Composition of Words.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Large Language Models Lack Understanding of Character Composition of Words.pdf", "file_type": "application/pdf", "file_size": 262705, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6ffb189f-fd69-4a30-8d40-b3753854c569", "node_type": "4", "metadata": {"page_label": "8", "file_name": "Large Language Models Lack Understanding of Character Composition of Words.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Large Language Models Lack Understanding of Character Composition of Words.pdf", "file_type": "application/pdf", "file_size": 262705, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "c6a7ee505f3fec04d0fbf4755f10d8cfa137f6e45062ef96dc10de4a26093f1c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cb5664aa-ecde-4a8a-9e90-1769bba41b27", "node_type": "1", "metadata": {"page_label": "8", "file_name": "Large Language Models Lack Understanding of Character Composition of Words.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Large Language Models Lack Understanding of Character Composition of Words.pdf", "file_type": "application/pdf", "file_size": 262705, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "a0c74e528d80601ea3e1b1ae9f7acc3a0b348b803a226438d20a680e20ffa0eb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ddd73be3-0c6a-489b-bb7d-2220debca08c", "node_type": "1", "metadata": {}, "hash": "a40fd458f56e79f0d132e538f9bbc48e4b7a9e2f8cb8b7a85a856fea2cf6380e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Krakover, N. G.-A.,\nCameron, N., Roberts, N., Doiron, N., Martinez, N.,\nNangia, N., Deckers, N., Muennighoff, N., Keskar, N. S.,\nIyer, N., Constant, N., Fiedel, N., W en, N., Zhang, O.,\nAgha, O., Elbaghdadi, O., Levy, O., Evans, O., Casares,\nP . A. M., Doshi, P ., Fung, P ., Liang, P . P ., V icol, P .,\nAlipoormolabashi, P ., Liao, P ., Liang, P ., Chang, P ., Eck-\nersley, P ., Htut, P . M., Hwang, P .-B., Milkowski, P ., Patil,\nP . S., Pezeshkpour, P ., Oli, P ., Mei, Q., Lyu, Q., Chen,\nQ., Banjade, R., Rudolph, R. E., Gabriel, R., Habacker,\nR., Risco, R., Milliere, R., Garg, R., Barnes, R., Saurous,\nR. A., Arakawa, R., Raymaekers, R., Frank, R., Sikand,\nR., Novak, R., Sitelew , R., Bras, R. L., Liu, R., Jacobs,\nR., Zhang, R., Salakhutdinov, R., Chi, R., Lee, R., Sto-\nvall, R., T eehan, R., Y ang, R., Singh, S., Mohammad,\nS. M., Anand, S., Dillavou, S., Shleifer, S., Wiseman, S.,\nGruetter, S., Bowman, S. R., Schoenholz, S. S., Han, S.,\nKwatra, S., Rous, S. A., Ghazarian, S., Ghosh, S., Casey,\nS., Bischoff, S., Gehrmann, S., Schuster, S., Sadeghi, S.,\nHamdan, S. S., Zhou, S., Srivastava, S., Shi, S., Singh,\nS., Asaadi, S., Gu, S. S., Pachchigar, S., T oshniwal, S.,\nUpadhyay, S., Debnath, S., Shakeri, S., Thormeyer, S.,\nMelzi, S., Reddy, S., Makini, S. P ., Lee, S.-H., T orene,\nS., Hatwar, S., Dehaene, S., Divic, S., Ermon, S., Bi-\nderman, S., Lin, S., Prasad, S., Piantadosi, S. T ., Shieber,\nS. M., Misherghi, S., Kiritchenko, S., Mishra, S., Linzen,\nT ., Schuster, T ., Li, T ., Y u, T ., Ali, T ., Hashimoto, T ., Wu,\nT .-L., Desbordes, T ., Rothschild, T ., Phan, T ., W ang, T .,\nNkinyili, T ., Schick, T ., Kornev, T ., Tunduny, T ., Ger-\nstenberg, T ., Chang, T ., Neeraj, T ., Khot, T ., Shultz, T .,\nShaham, U., Misra, V ., Demberg, V ., Nyamai, V ., Rau-\nnak, V ., Ramasesh, V . V ., Prabhu, V .", "mimetype": "text/plain", "start_char_idx": 2909, "end_char_idx": 4736, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ddd73be3-0c6a-489b-bb7d-2220debca08c": {"__data__": {"id_": "ddd73be3-0c6a-489b-bb7d-2220debca08c", "embedding": null, "metadata": {"page_label": "8", "file_name": "Large Language Models Lack Understanding of Character Composition of Words.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Large Language Models Lack Understanding of Character Composition of Words.pdf", "file_type": "application/pdf", "file_size": 262705, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6ffb189f-fd69-4a30-8d40-b3753854c569", "node_type": "4", "metadata": {"page_label": "8", "file_name": "Large Language Models Lack Understanding of Character Composition of Words.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Large Language Models Lack Understanding of Character Composition of Words.pdf", "file_type": "application/pdf", "file_size": 262705, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "c6a7ee505f3fec04d0fbf4755f10d8cfa137f6e45062ef96dc10de4a26093f1c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f66e05d1-3e84-49fa-afbc-a375602fd48d", "node_type": "1", "metadata": {"page_label": "8", "file_name": "Large Language Models Lack Understanding of Character Composition of Words.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Large Language Models Lack Understanding of Character Composition of Words.pdf", "file_type": "application/pdf", "file_size": 262705, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "ba21d0bd43e2c2ca4bcea24185f09dc584970a74b48011e4abe75897f2cf2333", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "V ., Prabhu, V . U., Padmakumar,\nV ., Srikumar, V ., Fedus, W ., Saunders, W ., Zhang, W .,\nV ossen, W ., Ren, X., T ong, X., Zhao, X., Wu, X., Shen,\nX., Y aghoobzadeh, Y ., Lakretz, Y ., Song, Y ., Bahri, Y .,\nChoi, Y ., Y ang, Y ., Hao, Y ., Chen, Y ., Belinkov, Y .,\nHou, Y ., Hou, Y ., Bai, Y ., Seid, Z., Zhao, Z., W ang, Z.,\nW ang, Z. J., W ang, Z., and Wu, Z. Beyond the imitation\ngame: Quantifying and extrapolating the capabilities of\nlanguage models. ArXiv, abs/2206.04615, 2022.\nT ouvron, H., Martin, L., Stone, K. R., Albert, P ., Alma-\nhairi, A., Babaei, Y ., Bashlykov, N., Batra, S., Bhargava,\nP ., Bhosale, S., Bikel, D. M., Blecher, L., Ferrer, C. C.,\nChen, M., Cucurull, G., Esiobu, D., Fernandes, J., Fu,\nJ., Fu, W ., Fuller, B., Gao, C., Goswami, V ., Goyal, N.,\nHartshorn, A. S., Hosseini, S., Hou, R., Inan, H., Kar-\ndas, M., Kerkez, V ., Khabsa, M., Kloumann, I. M., Ko-\nrenev, A. V ., Koura, P . S., Lachaux, M.-A., Lavril, T .,\nLee, J., Liskovich, D., Lu, Y ., Mao, Y ., Martinet, X.,\nMihaylov, T ., Mishra, P ., Molybog, I., Nie, Y ., Poulton,\nA., Reizenstein, J., Rungta, R., Saladi, K., Schelten, A.,\nSilva, R., Smith, E. M., Subramanian, R., T an, X., T ang,\nB., T aylor, R., Williams, A., Kuan, J. X., Xu, P ., Y an, Z.,\nZarov, I., Zhang, Y ., Fan, A., Kambadur, M., Narang, S.,\nRodriguez, A., Stojnic, R., Edunov, S., and Scialom, T .\nLlama 2: Open foundation and \ufb01ne-tuned chat models.\nArXiv, abs/2307.09288, 2023.\nTruong, T . H., Baldwin, T ., V erspoor, K. M., and Cohn,\n8", "mimetype": "text/plain", "start_char_idx": 4720, "end_char_idx": 6226, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "35a2357b-cc80-4c87-929d-1899d8eecae9": {"__data__": {"id_": "35a2357b-cc80-4c87-929d-1899d8eecae9", "embedding": null, "metadata": {"page_label": "9", "file_name": "Large Language Models Lack Understanding of Character Composition of Words.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Large Language Models Lack Understanding of Character Composition of Words.pdf", "file_type": "application/pdf", "file_size": 262705, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f8fc1b27-3fc1-4ae7-96a5-6c388c6de267", "node_type": "4", "metadata": {"page_label": "9", "file_name": "Large Language Models Lack Understanding of Character Composition of Words.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Large Language Models Lack Understanding of Character Composition of Words.pdf", "file_type": "application/pdf", "file_size": 262705, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "dcfec4b99d54fd6d7130bbd1a927c034dda7b223c1ea5a75eaf2655a38236a56", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c3009f7a-96cf-447e-b732-b3c665c865be", "node_type": "1", "metadata": {}, "hash": "e1633e5b25c54e571e5f0d4e2cdc439b1950c0c26bf3d5d289076782770d0832", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Large Language Models Lack Understanding of Character Composition of W ords\nT . Language models are not naysayers: an analysis\nof language models on negation benchmarks. ArXiv,\nabs/2306.08189, 2023.\nW ang, A., Singh, A., Michael, J., Hill, F ., Levy, O., and\nBowman, S. R. Glue: A multi-task benchmark and anal-\nysis platform for natural language understanding. In\nBlackboxNLP@EMNLP, 2018.\nW ang, A., Pruksachatkun, Y ., Nangia, N., Singh, A.,\nMichael, J., Hill, F ., Levy, O., and Bowman, S. R. Su-\nperglue: A stickier benchmark for general-purpose lan-\nguage understanding systems. ArXiv, abs/1905.00537,\n2019.\nWhittaker, E. and Kitagishi, I. Large language models for\nsimultaneous named entity extraction and spelling cor-\nrection. ArXiv, abs/2403.00528, 2024.\nWieting, J., Bansal, M., Gimpel, K., and Livescu, K. Chara-\ngram: Embedding words and sentences via character n-\ngrams. In Su, J., Duh, K., and Carreras, X. (eds.), Pro-\nceedings of the 2016 Conference on Empirical Methods\nin Natural Language Processing , pp. 1504\u20131515, Austin,\nT exas, November 2016. Association for Computational\nLinguistics. doi: 10.18653/v1/D16-1157.\nXu, Y ., Hu, L., Zhao, J., Qiu, Z., Y e, Y ., and Gu, H. A\nsurvey on multilingual large language models: Corpora,\nalignment, and bias. ArXiv, abs/2404.00929, 2024.\nA. Appendix\nLanguages of various character systems present vastly dif-\nferent ways of what the characters represent, how syllables\nare constructed, and how they are tokenized, etc. W e pro-\nvide an overview of such differences that fundamentally\nchange the way each language is to be processed by lan-\nguage models, focusing on English, Chinese, Korean, and\nJapanese.\nA.1. Preliminaries\nEnglish employs a phonetic character system based on al-\nphabets, where characters (letters) correspond to sounds\nthat form the basis of words, and multiple characters form\na syllable. English words are typically delimited by spaces\nor punctuation marks, facilitating tokenization at the wor d\nlevel. For example, \u201d How are you? \u201d would simply be tok-\nenized into \u201d, \u201c how\u201d, \u201c are\u201d, \u201c you\u201d, \u201c?\u201d. In certain applica-\ntions, such as parsing, tokens may be further broken down\nwith morphological analysis, e.g., \u201c talked\u201d into \u201c talk\u201d and\n\u201c ed\u201d, but requires additional processes, such as lemmati-\nzation or part-of-speech tagging. Splitting English token s\ninto characters is much more straightforward, as each to-\nken corresponds directly to a sequence of characters, such\nas \u201c dog\u201d being split into \u201c d, \u201d,\u201c o, \u201d,\u201c g. \u201d\nChinese utilizes a logographic character system, where\ncharacters represent morphemes, words, or meaningful\nunits rather than individual sounds. Each Chinese characte r\ncarries an inherent meaning and may be composed of vari-\nous components known as radicals. As such, Chinese char-\nacters do not rely on phonetic representation and are not\ndelimited by spaces within words. Chinese tokenization\nseparates text into individual characters or words, depend -\ning on the granularity required. For instance, \u201c \u8001\u5e08\u597d\uff0c\u6211\n\u53eb\u5c0f\u660e\u3002(Hello teacher , my name is Xiaoming. )\u201d would be\ntokenized into \u201c \u8001\u5e08(teacher)\u201d, \u201c \u597d(hello)\u201d, \u201c \uff0c\u201d, \u201c \u6211(I)\u201d,\n\u201c \u53eb(am called )\u201d, \u201c \u5c0f\u660e(Xiaoming)\u201d, \u201c \u3002\u201d. Note that we\nconsider radicals to be out of scope in this paper.\nKorean employs a unique featural character system known\nas Hangul. Korean characters represent both phonetic and\nfeatural information, where each character is constructed\nfrom combinations of consonants and vowels.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3452, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c3009f7a-96cf-447e-b732-b3c665c865be": {"__data__": {"id_": "c3009f7a-96cf-447e-b732-b3c665c865be", "embedding": null, "metadata": {"page_label": "9", "file_name": "Large Language Models Lack Understanding of Character Composition of Words.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Large Language Models Lack Understanding of Character Composition of Words.pdf", "file_type": "application/pdf", "file_size": 262705, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f8fc1b27-3fc1-4ae7-96a5-6c388c6de267", "node_type": "4", "metadata": {"page_label": "9", "file_name": "Large Language Models Lack Understanding of Character Composition of Words.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Large Language Models Lack Understanding of Character Composition of Words.pdf", "file_type": "application/pdf", "file_size": 262705, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "dcfec4b99d54fd6d7130bbd1a927c034dda7b223c1ea5a75eaf2655a38236a56", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "35a2357b-cc80-4c87-929d-1899d8eecae9", "node_type": "1", "metadata": {"page_label": "9", "file_name": "Large Language Models Lack Understanding of Character Composition of Words.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Large Language Models Lack Understanding of Character Composition of Words.pdf", "file_type": "application/pdf", "file_size": 262705, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "4a7465b03e7e4de9f4d8f4d66d03d43e33fbbed950f58614c66e26716db688b2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "As such, Chinese char-\nacters do not rely on phonetic representation and are not\ndelimited by spaces within words. Chinese tokenization\nseparates text into individual characters or words, depend -\ning on the granularity required. For instance, \u201c \u8001\u5e08\u597d\uff0c\u6211\n\u53eb\u5c0f\u660e\u3002(Hello teacher , my name is Xiaoming. )\u201d would be\ntokenized into \u201c \u8001\u5e08(teacher)\u201d, \u201c \u597d(hello)\u201d, \u201c \uff0c\u201d, \u201c \u6211(I)\u201d,\n\u201c \u53eb(am called )\u201d, \u201c \u5c0f\u660e(Xiaoming)\u201d, \u201c \u3002\u201d. Note that we\nconsider radicals to be out of scope in this paper.\nKorean employs a unique featural character system known\nas Hangul. Korean characters represent both phonetic and\nfeatural information, where each character is constructed\nfrom combinations of consonants and vowels. For exam-\nple, consonants \u3131(g),\u3141(m) and a vowel \u3163(i) form a sin-\ngle character /uni1100.35/uni1175.1/uni11B7.2(gim). While structurally combining multi-\nple components to form a single character may seem sim-\nilar to radicals in Chinese characters, note that each Ko-\nrean letter represents a phoneme, while radicals are mapped\nto meanings. As such, any elementary Korean reader can\nrecognize which letters are contained in a character by its\nsound or its visual composition. Featural structure of Ko-\nrean poses further challenges for language models as they\nare encountered not only with character composition of\nwords, but also with how each character is composed. Ko-\nrean words are typically delimited by spaces or punctua-\ntion marks, similarly to English. However, tokenization\nof Korean can be more complex due to the absence of ex-\nplicit word delimiters for different parts-of-speech, eve n\nwithin space-delimited chunks. For example, \u201d /uni1103.1/uni1161.5\u11bc/uni1109.42 /uni1175.2/uni11AB.2/uni110B.12 /uni1166.46\n/uni1100.4 \u1166(to you )\u201d involves preposition and pronoun in a single\nspace-delimited chunk, and may be further tokenized into\n\u201c /uni1103.1/uni1161.5\u11bc/uni1109.42 /uni1175.2/uni11AB.2(you)\u201d, \u201c /uni110B.12 /uni1166.46/uni1100.4 \u1166 (to)\u201d. It often relies on morphologi-\ncal analysis or dictionary-based approaches to segment tex t\ninto meaningful units.\nJapanese features a complex character system that incor-\nporates elements of both phonetic and logographic scripts.\nThe primary scripts in Japanese are Hiragana and Katakana,\nwhich represent phonetic syllables and are used for na-\ntive Japanese words and loanwords, respectively. Addi-\ntionally, Japanese utilizes Chinese characters (Kanji) im -\nported from Chinese writing. Kanji characters are lo-\ngographic and represent morphemes or words, often in-\nterspersed with Hiragana or Katakana within sentences.\nJapanese tokenization involves segmenting text into mor-\nphemes, and may utilize dictionary-based methods or rules\n9", "mimetype": "text/plain", "start_char_idx": 2767, "end_char_idx": 5450, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "6e3c1a8d-2671-42c9-9903-b2bf73c25f07": {"__data__": {"id_": "6e3c1a8d-2671-42c9-9903-b2bf73c25f07", "embedding": null, "metadata": {"page_label": "10", "file_name": "Large Language Models Lack Understanding of Character Composition of Words.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Large Language Models Lack Understanding of Character Composition of Words.pdf", "file_type": "application/pdf", "file_size": 262705, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "27f4c239-a832-4016-91e0-385fc534d943", "node_type": "4", "metadata": {"page_label": "10", "file_name": "Large Language Models Lack Understanding of Character Composition of Words.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Large Language Models Lack Understanding of Character Composition of Words.pdf", "file_type": "application/pdf", "file_size": 262705, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "5eea5208e90753fb3d0191faf6994072f6f38f0f298d8043a753ecf436df8e04", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "af5f0b41-f9e7-4ffc-a321-f571af0304e1", "node_type": "1", "metadata": {}, "hash": "9404f6d7fc55223ddf103af4aa260897271b3931f4a7b42e1c4edac58dec7917", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Large Language Models Lack Understanding of Character Composition of W ords\nT able 6. Comparison of character systems for English, Chinese, Kore an, and Japanese. Space and punctuation marks are skipped in the\nexamples.\nLanguage character system Subword T okenization Example\nEnglish phonetic word \u2192 morpheme How are you? \u2192 How, are, you\n\u2192 character \u2192 H,o,w ,a,r ,e,y,o,u\nChinese logographic word\u2192 character ( \u2192 radical) \u4f60\u597d\u5417\u2192 \u4f60, \u597d, \u5417\nKorean phonetic, featural word\u2192 syllable\u2192 letter\n/uni110B.2/uni1161.2\u11ab/uni1102.16/uni1167.10/uni11BC.2\u1112 \u1161/uni1109.12 /uni1166.35/uni110B.25 /uni116D.5\u2192 /uni110B.2/uni1161.2\u11ab/uni1102.16/uni1167.10/uni11BC.2, \u1112 \u1161/uni1109.12 /uni1166.35/uni110B.25 /uni116D.5\n\u2192 /uni110B.2/uni1161.2\u11ab,/uni1102.16/uni1167.10/uni11BC.2,\u1112 \u1161,/uni1109.12 /uni1166.35 ,/uni110B.25 /uni116D.5 \u2192\n\u3147,\u314f,\u3134,\u3134,\u3155,\u3147,\u314e,\u314f,\u3145,\u3154,\u3147,\u315b\nJapanese phonetic, word \u5143\u6c17\u3067\u3059\u304b\u2192 \u5143\u6c17, \u3067\u3059, \u304b\nlogographic \u2192 morpheme/character \u2192 \u5143, \u6c17, \u3067, \u3059, \u304b\nT able 7. F-score for each language with different LLMs in evaluation tasks, except accuracy is reported for counting task. For Ko rean,\nparentheses indicate the results from letter-based examin ation, except for insertion and deletion tasks where letter -based examination is\nnot applicable due to structural restraints of Korean chara cters.\nT ask English Chinese Korean Japanese\nGPT4 Mist. Gem. GPT4 Mist. Gem. GPT4 Mist. Gem. GPT4 Mist. Gem.\nRetrieval .595 .641 .574 .687 .535 .634 .324 (.229) .252 (.165) .279 (.130) .353 .328 .407\nInsertion .368 .436 .272 .429 .437 .391 .275 (\u2013) .308 (\u2013) .201 (\u2013) .334 .287 .340\nDeletion .277 .357 .302 .387 .356 .329 .256 (\u2013) .284 (\u2013) .239 (\u2013) .397 .258 .365\nReplacement .558 .392 .771 .628 .743 .592 .361 ( .110) .485 (.079) .301 (.058) .498 .427 .573\nCounting .59 .60 .63 .70 .74 .65 .49 (.27) .43 (.20) .38 (.15) .55 .50 .57\nto identify word boundaries. For example, \u201c \u3053\u3093\u306b\u3061\n\u306f\u3001\u5143\u6c17\u3067\u3059\u304b\uff1f(Hello, how are you? )\u201d would be tok-\nenized into \u201c \u3053\u3093\u306b\u3061\u306f(hello)\u201d, \u201c \u3001\u201d, \u201c \u5143\u6c17(good)\u201d, \u201c \u3067\n\u3059(is)\u201d, \u201c \u304b(marker for question)\u201d, \u201c \uff1f\u201d. The mixed nature\nof Japanese character system poses further challenges for\nunderstanding character composition. For example, logo-\ngraphic Kanjis can be converted to phonetic syllables based\non their sounds, e.g. \u201c \u5143\u6c17\u201d to \u201c \u3052\u3093\u304d\u201d, but it requires the\nknowledge of the pronunciation, and cannot be determined\nfrom the text alone. In this paper, to lower the barrier, we\nconsider Kanjis to be of different characters from their pho -\nnetic correspondents.\nT able\n6 summarizes the characteristics of each language.\nA.2. Experiments\nA.2.1. S E T T IN G\nW e perform the same tasks as in Sec 3 for Chinese, Korean,\nand Japanese with the exception of word reordering, which\nis not directly portable over languages of other writing sys -\ntems.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2706, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "af5f0b41-f9e7-4ffc-a321-f571af0304e1": {"__data__": {"id_": "af5f0b41-f9e7-4ffc-a321-f571af0304e1", "embedding": null, "metadata": {"page_label": "10", "file_name": "Large Language Models Lack Understanding of Character Composition of Words.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Large Language Models Lack Understanding of Character Composition of Words.pdf", "file_type": "application/pdf", "file_size": 262705, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "27f4c239-a832-4016-91e0-385fc534d943", "node_type": "4", "metadata": {"page_label": "10", "file_name": "Large Language Models Lack Understanding of Character Composition of Words.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Large Language Models Lack Understanding of Character Composition of Words.pdf", "file_type": "application/pdf", "file_size": 262705, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "5eea5208e90753fb3d0191faf6994072f6f38f0f298d8043a753ecf436df8e04", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6e3c1a8d-2671-42c9-9903-b2bf73c25f07", "node_type": "1", "metadata": {"page_label": "10", "file_name": "Large Language Models Lack Understanding of Character Composition of Words.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Large Language Models Lack Understanding of Character Composition of Words.pdf", "file_type": "application/pdf", "file_size": 262705, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "829acfe34afe1791066fff64384ada060b888b25a735d9cb4975a5ae678d217e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The mixed nature\nof Japanese character system poses further challenges for\nunderstanding character composition. For example, logo-\ngraphic Kanjis can be converted to phonetic syllables based\non their sounds, e.g. \u201c \u5143\u6c17\u201d to \u201c \u3052\u3093\u304d\u201d, but it requires the\nknowledge of the pronunciation, and cannot be determined\nfrom the text alone. In this paper, to lower the barrier, we\nconsider Kanjis to be of different characters from their pho -\nnetic correspondents.\nT able\n6 summarizes the characteristics of each language.\nA.2. Experiments\nA.2.1. S E T T IN G\nW e perform the same tasks as in Sec 3 for Chinese, Korean,\nand Japanese with the exception of word reordering, which\nis not directly portable over languages of other writing sys -\ntems. Note that, in many cases, humans will be able to\ncarry out the tasks, even without knowing anything about\nthe target language, via visual inspection. Note that the\ndif\ufb01culty posed by each task signi\ufb01cantly varies depend-\ning on the language. For instance, word retrieval is highly\nstraightforward in Chinese, as words themselves frequentl y\ncorrespond to the target characters. W e also use the subset\nof LLMs used in Sec\n3, namely GPT4, Mistral 7B, and\nGemini 1.5. W e sampled input texts from Wikipedia of re-\nspective languages, and randomly chose a target character\nthat appears at least N times in the sampled text, varying\nthe number from 1 to 3. 100 prompts were used per task for\neach language. W e report F-score for each task in each lan-\nguage. Note that, for Korean, we examined the tasks at two\ndifferent levels, namely characters and letters (consonan ts\nand vowels).\nA.2.2. R E S U LT S\nT able\n7 summarizes the results across all tasks for each lan-\nguage using different LLMs, and T able 8 shows example\nfailure cases for each language. Considering that humans\nwith elementary understanding of respective languages can\nsolve the tasks easily, it is fair to say that all languages pe r-\nform poorly across all tasks and models. However, there ex-\nist notable discrepancies among the performances depend-\ning on the language.\nFirst, LLMs performed slightly better on Chinese than in\nEnglish throughout the models and tasks, which is notable\nsince, for most models, English is likely the majority lan-\nguage in their training corpora. W e conjecture that this\nmay be related to the logographic nature of Chinese char-\nacter system, where characters are frequently equivalent\nto words, eliminating the necessity for the models to un-\nderstand the language at different levels. On the contrary,\nLLMs\u2019 performances unanimously degraded for Korean\neven at character levels, and utterly failed to demonstrate\nany sign of understanding the character composition when\nit comes to further breaking down the characters to letters.\nIn fact, in many cases dealing with Korean letters, LLMs\nfailed even to generate outputs that are compatible for com-\nputing accuracy. As discussed in Sec\nA.1, this likely has\nto with the unique featural structure of Korean language,\nand suggests that it might require a different approach to\n10", "mimetype": "text/plain", "start_char_idx": 1972, "end_char_idx": 5037, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3c295f1c-7959-4c82-baad-ceceae5185c0": {"__data__": {"id_": "3c295f1c-7959-4c82-baad-ceceae5185c0", "embedding": null, "metadata": {"page_label": "11", "file_name": "Large Language Models Lack Understanding of Character Composition of Words.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Large Language Models Lack Understanding of Character Composition of Words.pdf", "file_type": "application/pdf", "file_size": 262705, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "76222c74-1a7a-429b-88f2-a4ed6fa37c0f", "node_type": "4", "metadata": {"page_label": "11", "file_name": "Large Language Models Lack Understanding of Character Composition of Words.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Large Language Models Lack Understanding of Character Composition of Words.pdf", "file_type": "application/pdf", "file_size": 262705, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "a8bd99e2cdba6e85900f72da8a39d481a7c548f24d32918a504f90819bfa6e2a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "50ff1b79-77ed-49f5-bb6a-671c186fd066", "node_type": "1", "metadata": {}, "hash": "8fa979eb1254d8e58f6fc8e3e5832a532f7d13f6cd1b80783e7830c268cfcaec", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Large Language Models Lack Understanding of Character Composition of W ords\nT able 8. Example failure cases. English translation is provided for each prompt. Bold letters indicate the correct answers from the model.\nN/A indicates that the model was not able to provide compatib le outputs.\nLanguage Prompt GPT4 Mistral Gemini\nEnglish\nRemove all occurrences of the character i\nArti\ufb01cial intelligence will revolutionize various \ufb01elds.\n(answer: Artfcal ntellgence wll revolutonze varous felds. )\nartfcal ntellgence\nwll revolutonze\nvarous \ufb01elds.\nartfcal ntellgence\nwll revolutonze\nvarous felds.\nartfcial ntellgence\nwll revolutonze\nvarous felds.\nChinese\n\u4ee5\u4e0b\u6587\u672c\u4e2d\u7684\u5b57\u7b26\u201c \u7684\u201d \u51fa\u73b0\u4e86\u591a\u5c11\u6b21\uff1f\n\u5317\u4eac\u7684\u6545\u5bab\u662f\u4e2d\u56fd\u5386\u53f2\u6587\u5316\u7684\u91cd\u8981\u9057\u4ea7\u4e4b\u4e00\u3002\nFind all characters with letter \u7684 (answer: 2)\n2 4 2\nKorean\n(character)\n\u1103 \u1161/uni110B.15 /uni1173.9 /uni11B7.11 /uni1106.25/uni116E.6/uni11AB.3/uni110C.1/uni1161.5\u11bc/uni110B.12 /uni1166.46 \u1102 \u1161/uni110B.14 /uni1169.8 /uni1102.21/uni1173.6/uni11AB.3 /uni1106.25/uni116E.6/uni11AB.3\u110c \u1161\u201c \u1103 \u1161\u201d /uni1105.22/uni1173.11/uni11AF.11 /uni1106.13 /uni1169.8/uni1103.29 /uni116E.4 \u201c \u110b \u1161\u201d /uni1105.19 /uni1169.15 \u1107 \u1161/uni1101.20 \u116e/uni1109.40 \u1175/uni110B.14 /uni1169.8:\n/uni1112.2/uni1161.12/uni11AB.11/uni1100.22 /uni116E.1 /uni11A8.3/uni110B.39 \u1174/uni110C.10/uni1165.43/uni11AB.2/uni1110.18 /uni1169.16 /uni11BC.7/uni1106.25/uni116E.6/uni11AB.3/uni1112.17 /uni116A.40 /uni1102.21/uni1173.6/uni11AB.3 \u1103 \u1161/uni110B.1/uni1163.5\u11bc\u1112 \u1161/uni1100.11 \u1169 \u110b \u1161/uni1105.20 /uni1173.9 /uni11B7.11/uni1103.2 /uni1161.3 \u11b8\u1103 \u1161.\nReplace all occurrences of \u1103 \u1161with \u110b \u1161\n(answer: /uni1112.2/uni1161.12/uni11AB.11/uni1100.22 /uni116E.1 /uni11A8.3/uni110B.39 \u1174/uni110C.10/uni1165.43/uni11AB.2/uni1110.18 /uni1169.16 /uni11BC.7/uni1106.25/uni116E.6/uni11AB.3/uni1112.17 /uni116A.40 /uni1102.21/uni1173.6/uni11AB.3 \u110b \u1161/uni110B.1/uni1163.5\u11bc\u1112 \u1161/uni1100.11 \u1169 \u110b \u1161/uni1105.20 /uni1173.9 /uni11B7.11/uni1103.2 /uni1161.3 \u11b8\u110b \u1161.)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1860, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "50ff1b79-77ed-49f5-bb6a-671c186fd066": {"__data__": {"id_": "50ff1b79-77ed-49f5-bb6a-671c186fd066", "embedding": null, "metadata": {"page_label": "11", "file_name": "Large Language Models Lack Understanding of Character Composition of Words.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Large Language Models Lack Understanding of Character Composition of Words.pdf", "file_type": "application/pdf", "file_size": 262705, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "76222c74-1a7a-429b-88f2-a4ed6fa37c0f", "node_type": "4", "metadata": {"page_label": "11", "file_name": "Large Language Models Lack Understanding of Character Composition of Words.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Large Language Models Lack Understanding of Character Composition of Words.pdf", "file_type": "application/pdf", "file_size": 262705, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "a8bd99e2cdba6e85900f72da8a39d481a7c548f24d32918a504f90819bfa6e2a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3c295f1c-7959-4c82-baad-ceceae5185c0", "node_type": "1", "metadata": {"page_label": "11", "file_name": "Large Language Models Lack Understanding of Character Composition of Words.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Large Language Models Lack Understanding of Character Composition of Words.pdf", "file_type": "application/pdf", "file_size": 262705, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "43eae063086241bb9b3279f72dc54a59905bc997061a1bf06d33661e51206764", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4560d650-9763-44a3-a62f-302b6204a058", "node_type": "1", "metadata": {}, "hash": "10423f9a33fb9973cb2d4713619ed73d760b56cd8589ff9904f6dba877aa1675", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "\u110b \u1161\u110b \u1161\u110b \u1161/uni110B.39 \u1174\n/uni110C.10/uni1165.43/uni11AB.2/uni1110.18 /uni1169.16 /uni11BC.7/uni1106.25/uni116E.6/uni11AB.3/uni1112.17 /uni116A.40 \u110b \u1161/uni1102.21/uni1173.6/uni11AB.3\n\u1103 \u1161/uni110B.1/uni1163.5\u11bc\u1112 \u1161/uni1100.11 \u1169\n\u110b \u1161/uni1105.20 /uni1173.9 /uni11B7.11/uni1103.2 /uni1161.3 \u11b8\u1103 \u1161.", "mimetype": "text/plain", "start_char_idx": 1861, "end_char_idx": 2145, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "4560d650-9763-44a3-a62f-302b6204a058": {"__data__": {"id_": "4560d650-9763-44a3-a62f-302b6204a058", "embedding": null, "metadata": {"page_label": "11", "file_name": "Large Language Models Lack Understanding of Character Composition of Words.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Large Language Models Lack Understanding of Character Composition of Words.pdf", "file_type": "application/pdf", "file_size": 262705, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "76222c74-1a7a-429b-88f2-a4ed6fa37c0f", "node_type": "4", "metadata": {"page_label": "11", "file_name": "Large Language Models Lack Understanding of Character Composition of Words.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Large Language Models Lack Understanding of Character Composition of Words.pdf", "file_type": "application/pdf", "file_size": 262705, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "a8bd99e2cdba6e85900f72da8a39d481a7c548f24d32918a504f90819bfa6e2a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "50ff1b79-77ed-49f5-bb6a-671c186fd066", "node_type": "1", "metadata": {"page_label": "11", "file_name": "Large Language Models Lack Understanding of Character Composition of Words.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Large Language Models Lack Understanding of Character Composition of Words.pdf", "file_type": "application/pdf", "file_size": 262705, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "09420f466657cae25a85e355b00714a8a95fb890cd7309c30645da41f3e5d6c9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5e88a86a-0c21-4113-88b3-59da15d2006d", "node_type": "1", "metadata": {}, "hash": "f8bd482cb5897de9272a564d3a36b012771f417b889bce0803c84bdf0c7ca8a1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "/uni1112.2/uni1161.12/uni11AB.11/uni1100.22 /uni116E.1 /uni11A8.3/uni110B.39 \u1174\n/uni110C.10/uni1165.43/uni11AB.2/uni1110.18 /uni1169.16 /uni11BC.7/uni1106.25/uni116E.6/uni11AB.3/uni1112.17 /uni116A.40 /uni1102.21/uni1173.6/uni11AB.3\n\u110b \u1161\u1103 \u1161/uni110B.1/uni1163.5\u11bc\u1112 \u1161/uni1100.11 \u1169\n\u110b \u1161/uni1105.20 /uni1173.9 /uni11B7.11/uni1103.2 /uni1161.3 \u11b8\u1103 \u1161.\nN/A\nKorean\n(letter)\n\u1103 \u1161/uni110B.15 /uni1173.9 /uni11B7.11 /uni1100.12 /uni1173.3 /uni11AF.3/uni110B.12 /uni1166.46/uni1109.8 /uni1165.36 \u3134/uni110B.42 \u1175/uni1103.22 /uni1173.7 /uni11AF.8/uni110B.8 /uni1165.24 /uni1100.2/uni1161.2\u11ab /uni1100.12 /uni1173.3 /uni11AF.3\u110c \u1161/uni1105.22/uni1173.11/uni11AF.11 /uni1106.13 /uni1169.8/uni1103.29 /uni116E.4 /uni110E.3/uni1161.8/uni11BD.4 /uni110B.38 /uni1173.4/uni1109.40 \u1175/uni110B.14 /uni1169.8 :\n/uni110B.2/uni1161.2\u11ab/uni1102.16/uni1167.10/uni11BC.2\u1112 \u1161/uni1109.12 /uni1166.35/uni110B.25 /uni116D.5 /uni1107.3/uni1161.7/uni11AB.6/uni1100.3 /uni1161.3 \u11b8 /uni1109.18 /uni1173.7 /uni11B8.8/uni1102.39 \u1175\u1103 \u1161\nFind all characters with letter \u3134 (answer:/uni110B.2/uni1161.2\u11ab,/uni1102.16/uni1167.10/uni11BC.2,/uni1107.3/uni1161.7/uni11AB.6 ,/uni1102.39 \u1175)\n/uni110B.2/uni1161.2\u11ab,/uni1107.3/uni1161.7/uni11AB.6 ,/uni1100.3 /uni1161.3 \u11b8,/uni1109.18 /uni1173.7 /uni11B8.8 N/A N/A\nJapanese\n\u6b21\u306e\u6587\u7ae0\u3067\u5f8c\u308d\u304b\u3089\uff13\u756a\u76ee\u306e\u6587\u5b57\u306f\u4f55\u3067\u3059\u304b\uff1a\n\u543e\u8f29\u306f\u732b\u3067\u3042\u308b\nWhat is the third character from the end (answer: \u3067)\n\u3042 \u3067 \u732b\naccount for such irregular systems. Japanese displays simi -\nlarly lower scores, af\ufb01rming the complexity of dealing with\nmixed character system of both logographic and phonetic\ncharacters.\nA.2.3. D IS CU S S IO N\nWhile LLMs\u2019 understanding of character composition re-\nmains itself an area to be further explored regardless of\nthe language, the results of our experiments seem to fur-\nther highlight the signi\ufb01cant impact of language-speci\ufb01c\ncharacteristics on the performance. Multilingual LLMs in\ngeneral encounter challenges such as uneven distribution o f\nlanguages in training corpora, or decline in performance fo r\nlow-resource languages. A popular approach to deal with\nthis problem has been to employ parameter tuning align-\nment at multiple stages (\nQin et al.", "mimetype": "text/plain", "start_char_idx": 2146, "end_char_idx": 4259, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5e88a86a-0c21-4113-88b3-59da15d2006d": {"__data__": {"id_": "5e88a86a-0c21-4113-88b3-59da15d2006d", "embedding": null, "metadata": {"page_label": "11", "file_name": "Large Language Models Lack Understanding of Character Composition of Words.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Large Language Models Lack Understanding of Character Composition of Words.pdf", "file_type": "application/pdf", "file_size": 262705, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "76222c74-1a7a-429b-88f2-a4ed6fa37c0f", "node_type": "4", "metadata": {"page_label": "11", "file_name": "Large Language Models Lack Understanding of Character Composition of Words.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Large Language Models Lack Understanding of Character Composition of Words.pdf", "file_type": "application/pdf", "file_size": 262705, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "a8bd99e2cdba6e85900f72da8a39d481a7c548f24d32918a504f90819bfa6e2a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4560d650-9763-44a3-a62f-302b6204a058", "node_type": "1", "metadata": {"page_label": "11", "file_name": "Large Language Models Lack Understanding of Character Composition of Words.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Large Language Models Lack Understanding of Character Composition of Words.pdf", "file_type": "application/pdf", "file_size": 262705, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "52ad48e8aa3f4b1d491448a3427d06becf5466da5d4e716fca07139f8e95524a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Japanese displays simi -\nlarly lower scores, af\ufb01rming the complexity of dealing with\nmixed character system of both logographic and phonetic\ncharacters.\nA.2.3. D IS CU S S IO N\nWhile LLMs\u2019 understanding of character composition re-\nmains itself an area to be further explored regardless of\nthe language, the results of our experiments seem to fur-\nther highlight the signi\ufb01cant impact of language-speci\ufb01c\ncharacteristics on the performance. Multilingual LLMs in\ngeneral encounter challenges such as uneven distribution o f\nlanguages in training corpora, or decline in performance fo r\nlow-resource languages. A popular approach to deal with\nthis problem has been to employ parameter tuning align-\nment at multiple stages (\nQin et al. , 2024). However, it still\nremains as a challenge to handle language heterogeneity\nof signi\ufb01cantly varying syntax, morphology, and semantics\n(\nXu et al. , 2024), and our results seem to further reinforce\nthe claim that there may be an underlying limitation to con-\nventional approaches, without accounting for fundamental\ndifferences between the language systems.\n11", "mimetype": "text/plain", "start_char_idx": 3526, "end_char_idx": 4626, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "43686da1-60d8-45fa-b7ed-b5c9a5f0b7d3": {"__data__": {"id_": "43686da1-60d8-45fa-b7ed-b5c9a5f0b7d3", "embedding": null, "metadata": {"page_label": "1", "file_name": "Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_type": "application/pdf", "file_size": 2538334, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "239162ef-3dd3-4a8f-80ab-b2273614b495", "node_type": "4", "metadata": {"page_label": "1", "file_name": "Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_type": "application/pdf", "file_size": 2538334, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "89b71e5c8fc04bd7b7e70f1a854496d36b1e3cd654661a8433d4ad01fb4281ec", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "55708ffe-04f4-43f1-87eb-c649e796341e", "node_type": "1", "metadata": {}, "hash": "b02f5367e771dfa734163da9df08f2be2db026f241eb89bc6ba6900c17dc2c6f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Learn to Accumulate Evidence from All Training Samples: Theory and Practice\nDeep Pandey 1 Qi Yu1\nAbstract\nEvidential deep learning, built upon belief the-\nory and subjective logic, offers a principled and\ncomputationally efficient way to turn a determin-\nistic neural network uncertainty-aware. The resul-\ntant evidential models can quantify fine-grained\nuncertainty using the learned evidence. To en-\nsure theoretically sound evidential models, the ev-\nidence needs to be non-negative, which requires\nspecial activation functions for model training\nand inference. This constraint often leads to infe-\nrior predictive performance compared to standard\nsoftmax models, making it challenging to extend\nthem to many large-scale datasets. To unveil the\nreal cause of this undesired behavior, we theoreti-\ncally investigate evidential models and identify a\nfundamental limitation that explains the inferior\nperformance: existing evidential activation func-\ntions create zero evidence regions, which prevent\nthe model to learn from training samples falling\ninto such regions. A deeper analysis of eviden-\ntial activation functions based on our theoretical\nunderpinning inspires the design of a novel regu-\nlarizer that effectively alleviates this fundamental\nlimitation. Extensive experiments over many chal-\nlenging real-world datasets and settings confirm\nour theoretical findings and demonstrate the effec-\ntiveness of our proposed approach.\n1. Introduction\nDeep Learning (DL) models have found great success in\nmany real-world applications such as speech recognition\n(Kamath et al., 2019), machine translation (Singh et al.,\n2017), and computer vision (V oulodimos et al., 2018). How-\never, these highly expressive models may easily fit the noise\nin the training data, which leads to overconfident predictions\n(Nguyen et al., 2015). The challenge is further compounded\nwhen learning from limited labeled data, which is common\n1Rochester Institute of Technology. Correspondence to: Qi Yu\n<qi.yu@rit.edu>.\nProceedings of the 40 th International Conference on Machine\nLearning, Honolulu, Hawaii, USA. PMLR 202, 2023. Copyright\n2023 by the author(s).\nfor applications from specialized domain ( e.g., medicine,\npublic safety, and military operations) where data collec-\ntion and annotation is highly costly. Accurate uncertainty\nquantification is essential for successful application of DL\nmodels in these domains. To this end, DL models have been\naugmented to become uncertainty-aware (Gal & Ghahra-\nmani, 2016; Blundell et al., 2015; Pearce et al., 2020). How-\never, commonly used extensions require expensive sampling\noperations (Gal & Ghahramani, 2016; Blundell et al., 2015),\nwhich significantly increase the computational costs (Lak-\nshminarayanan et al., 2017).\nThe recently developed evidential models bring together\nevidential theory (Shafer, 1976; J\u00f8sang, 2016) and deep\nneural architectures that turn a deterministic neural network\nuncertainty-aware. By leveraging the learned evidence, evi-\ndential models are capable of quantifying fine-grained un-\ncertainty that helps to identify the sources of \u2018unknowns\u2019.\nFurthermore, since only lightweight modifications are intro-\nduced to existing DL architectures, additional computational\ncosts remain minimum. Such evidential models have been\nsuccessfully extended to classification (Sensoy et al., 2018),\nregression (Amini et al., 2020), meta-learning (Pandey & Yu,\n2022a), and open-set recognition (Bao et al., 2021) settings.\nFigure 1.Cifar100 Result\nDespite the attractive\nuncertainty quantifica-\ntion capacity, eviden-\ntial models are only\nable to achieve a pre-\ndictive performance on\npar with standard deep\narchitectures in rela-\ntively simple learning problems. They suffer from a sig-\nnificant performance drop when facing large datasets with\nmore complex features even in the common classification\nsetting. As shown in Figure 1, an evidential model using\nReLU activation and an evidential MSE loss (Sensoy et al.,\n2018) only achieves 36% test accuracy on Cifar100, which\nis almost 40% lower than a standard model trained using\nsoftmax.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4093, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "55708ffe-04f4-43f1-87eb-c649e796341e": {"__data__": {"id_": "55708ffe-04f4-43f1-87eb-c649e796341e", "embedding": null, "metadata": {"page_label": "1", "file_name": "Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_type": "application/pdf", "file_size": 2538334, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "239162ef-3dd3-4a8f-80ab-b2273614b495", "node_type": "4", "metadata": {"page_label": "1", "file_name": "Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_type": "application/pdf", "file_size": 2538334, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "89b71e5c8fc04bd7b7e70f1a854496d36b1e3cd654661a8433d4ad01fb4281ec", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "43686da1-60d8-45fa-b7ed-b5c9a5f0b7d3", "node_type": "1", "metadata": {"page_label": "1", "file_name": "Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_type": "application/pdf", "file_size": 2538334, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "2d587bd19a0c2b4c95ceb505298bc3fbd4ddd0eb6fb1281d749a8e5a0332759d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Figure 1.Cifar100 Result\nDespite the attractive\nuncertainty quantifica-\ntion capacity, eviden-\ntial models are only\nable to achieve a pre-\ndictive performance on\npar with standard deep\narchitectures in rela-\ntively simple learning problems. They suffer from a sig-\nnificant performance drop when facing large datasets with\nmore complex features even in the common classification\nsetting. As shown in Figure 1, an evidential model using\nReLU activation and an evidential MSE loss (Sensoy et al.,\n2018) only achieves 36% test accuracy on Cifar100, which\nis almost 40% lower than a standard model trained using\nsoftmax. Additionally, most evidential models can easily\nbreak down with minor architecture changes and/or have\na much stronger dependency on hyperparameter tuning to\nachieve reasonable predictive performance. The experiment\nsection provides more details on these failure cases.\n1\narXiv:2306.11113v2  [cs.LG]  24 Jun 2023", "mimetype": "text/plain", "start_char_idx": 3477, "end_char_idx": 4406, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "330173e5-3244-4333-8eb1-99887241be2d": {"__data__": {"id_": "330173e5-3244-4333-8eb1-99887241be2d", "embedding": null, "metadata": {"page_label": "2", "file_name": "Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_type": "application/pdf", "file_size": 2538334, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a6588327-9da3-4981-8f90-f859ca2934f2", "node_type": "4", "metadata": {"page_label": "2", "file_name": "Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_type": "application/pdf", "file_size": 2538334, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "9d9e7865ff2b6538153635dd6aee950a0390077bb9376de4fdf015061a6d2638", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cedc07a4-2f9a-4da2-b70e-0234162b6c3e", "node_type": "1", "metadata": {}, "hash": "cd38dfac65a50df2228dfdb678a14ec2b2bf1ac0c2442eecb831fb4dc3db343c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Learn to Accumulate Evidence from All Training Samples: Theory and Practice\nFigure 2.Visualization of zero-evidence region for evidential mod-\nels with ReLU activation in a binary classification setting. Existing\nmodels fail to learn from samples that are mapped to such zero-\nevidence region (shared area at the bottom left quadrant).\nTo train uncertainty-aware evidential models that can also\npredict well, we perform a novel theoretical analysis with\na focus on the standard classification setting to unveil the\nunderlying cause of the performance gap. Our theoreti-\ncal results show that existing evidential models learn sub-\noptimally compared to corresponding softmax counterparts.\nSuch sub-optimal training is mainly attributed to the inher-\nent learning deficiency of evidential models that prevents\nthem from learning across all training samples. More specif-\nically, they are incapable to acquire new knowledge from\ntraining samples mapped to \u201czero-evidence regions\u201d in the\nevidence space, where the predicted evidence reduces to\nzero. The sub-optimal learning phenomenon is illustrated\nin Figure 2 (detailed discussion is presented in Section 4.2).\nWe analyze different variants of evidential models present\nin the existing literature and observe this limitation across\nall the models and settings. Our theoretical results inspire\nthe design of a novel Regularized Evidential model (RED)\nthat includes positive evidence regularization in its train-\ning objective to battle the learning deficiency. Our major\ncontributions can be summarized as follows:\n\u2022 We identify a fundamental limitation of evidential models,\ni.e., lack the capability to learn from any data samples that\nlie in the \u201czero-evidence\u201d region in the evidence space.\n\u2022 We theoretically show the superiority of evidential models\nwith exp activation over other activation functions.\n\u2022 We conduct novel evidence regularization that enables\nevidential models to avoid the \u201czero-evidence\u201d region so\nthat they can effectively learn from all training samples.\n\u2022 We carry out experiments over multiple challenging real-\nworld datasets to empirically validate the presented theory,\nand show the effectiveness of our proposed ideas.\n2. Related Works\nUncertainty Quantification in Deep Learning. Accu-\nrate quantification of predictive uncertainty is essential for\ndevelopment of trustworthy Deep Learning (DL) models.\nDeep ensemble techniques (Pearce et al., 2020; Lakshmi-\nnarayanan et al., 2017) have been developed for uncer-\ntainty quantification. An ensemble of neural networks is\nconstructed and the agreement/disagreement across the en-\nsemble components is used to quantify different uncertain-\nties. Ensemble-based methods significantly increase the\nnumber of model parameters, which are computationally\nexpensive at both training and test times. Alternatively,\nBayesian neural networks (Gal & Ghahramani, 2016)(Blun-\ndell et al., 2015)(Mobiny et al., 2021) have been devel-\noped that consider a Bayesian formalism to quantify dif-\nferent uncertainties. For instance, (Blundell et al., 2015)\nuse Bayes-by-backdrop to learn a distribution over neural\nnetwork parameters, whereas (Gal & Ghahramani, 2016)\nenable dropout during inference phase to obtain predictive\nuncertainty. Bayesian methods resort to some form of ap-\nproximation to address the intractability issue in marginal-\nization of latent variables. Moreover, these methods are\nalso computationally expensive as they require sampling for\nuncertainty quantification.\nEvidential Deep Learning. Evidential models introduce a\nconjugate higher-order evidential prior for the likelihood dis-\ntribution that enables the model to capture the fine-grained\nuncertainties. For instance, Dirichlet prior is introduced\nover the multinomial likelihood for evidential classification\n(Bao et al., 2021; Zhao et al., 2020), and NIG prior is in-\ntroduced over the Gaussian likelihood (Amini et al., 2020;\nPandey & Yu, 2022b) for the evidential regression models.\nAdversarial robustness (Kopetzki et al., 2021) and calibra-\ntion (Tomani & Buettner, 2021) of evidential models have\nalso been well studied. Usually, these models are trained\nwith evidential losses in conjunction with heuristic evidence\nregularization to guide the uncertainty behavior (Pandey &\nYu, 2022a; Shi et al., 2020) in addition to reasonable gen-\neralization performance.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4359, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "cedc07a4-2f9a-4da2-b70e-0234162b6c3e": {"__data__": {"id_": "cedc07a4-2f9a-4da2-b70e-0234162b6c3e", "embedding": null, "metadata": {"page_label": "2", "file_name": "Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_type": "application/pdf", "file_size": 2538334, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a6588327-9da3-4981-8f90-f859ca2934f2", "node_type": "4", "metadata": {"page_label": "2", "file_name": "Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_type": "application/pdf", "file_size": 2538334, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "9d9e7865ff2b6538153635dd6aee950a0390077bb9376de4fdf015061a6d2638", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "330173e5-3244-4333-8eb1-99887241be2d", "node_type": "1", "metadata": {"page_label": "2", "file_name": "Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_type": "application/pdf", "file_size": 2538334, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "33b25d7e3d7929ca87969a34db817aeb8456e9473bb2196ab7878c8823159e58", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "For instance, Dirichlet prior is introduced\nover the multinomial likelihood for evidential classification\n(Bao et al., 2021; Zhao et al., 2020), and NIG prior is in-\ntroduced over the Gaussian likelihood (Amini et al., 2020;\nPandey & Yu, 2022b) for the evidential regression models.\nAdversarial robustness (Kopetzki et al., 2021) and calibra-\ntion (Tomani & Buettner, 2021) of evidential models have\nalso been well studied. Usually, these models are trained\nwith evidential losses in conjunction with heuristic evidence\nregularization to guide the uncertainty behavior (Pandey &\nYu, 2022a; Shi et al., 2020) in addition to reasonable gen-\neralization performance. Some evidential models assume\naccess to out-of-distribution data during training (Malinin &\nGales, 2019; 2018) and use the OOD data to guide the un-\ncertainty behavior. A recent survey (Ulmer, 2021) provides\na thorough review of the evidential deep learning field.\nIn this work, we focus on evidential classification models\nand consider settings where no OOD data is used during\nmodel training to make the proposed approach more broadly\napplicable to practical real-world situations.\n3. Learning Deficiency of Evidential Models\n3.1. Preliminaries and problem setup\nStandard classification models use a softmax transformation\non the output from the neural network F\u0398 for input x to ob-\ntain the class probabilities in K-class classification problem.\nSuch models are trained with the cross-entropy based loss.\n2", "mimetype": "text/plain", "start_char_idx": 3696, "end_char_idx": 5169, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "2d3dcf54-241e-4ac6-8943-7d3b8f9b791e": {"__data__": {"id_": "2d3dcf54-241e-4ac6-8943-7d3b8f9b791e", "embedding": null, "metadata": {"page_label": "3", "file_name": "Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_type": "application/pdf", "file_size": 2538334, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7d342485-4866-499e-bca6-1a2f3178ea06", "node_type": "4", "metadata": {"page_label": "3", "file_name": "Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_type": "application/pdf", "file_size": 2538334, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "d4b68fc6ec192150ab43622a81c623bd39ca3403d151c82dd61c30e407f863cb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e060e362-ebc9-4553-a4ea-a891d1fad74d", "node_type": "1", "metadata": {}, "hash": "144abd490fb0a10458c08ef0eda0a57d09f46620a55c6a85226e130bf8c47604", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Learn to Accumulate Evidence from All Training Samples: Theory and Practice\nFor a given training sample (x, y), the loss is given by\nLcross = \u2212\nKX\nk=1\nyk log(smk) (1)\nwhere smk is the softmax output. These models have\nachieved state-of-the-art performance on many benchmark\nproblems. A detailed gradient analysis shows that they can\neffectively learn from all training data samples (see Ap-\npendix A). Nevertheless, these models lack a systematic\nmechanism to quantify different sources of uncertainty, a\nhighly desired property in many real-world problems.\nFigure 3.Graphical model for Evidential Deep Learning\nEvidential classification models formulate training as an\nevidence acquisition process and consider a higher-order\nDirichlet prior Dir(p|\u03b1) over the predictive Multino-\nmial distribution Mult(y|p). Different from a standard\nBayesian formulation which optimizes Type II Maximum\nLikelihood to learn the Dirichlet hyperparameter (Bishop\n& Nasrabadi, 2006), evidential models directly predict \u03b1\nusing data features x and then generate the prediction y by\nmarginalizing the Multinomial parameter p. Figure 3 de-\nscribes this generative process. Such higher-order prior en-\nables the model to systematically quantify different sources\nof uncertainty. In evidential models, the softmax layer of\nthe standard neural networks is replaced by a non-negative\nactivation function A, where A(x) \u2265 0 \u2200x \u2208 [\u2212\u221e, \u221e],\nsuch that for input x, the neural network model F\u0398 with\nparameters \u0398 can output evidence e for different classes.\nDirichlet prior \u03b1 is evaluated as\u03b1 = e+1 to ensure \u03b1 \u2265 1.\nThe trained evidential model outputs Dirichlet parameters\n\u03b1 for input x that can quantify fine-grained uncertainties in\naddition to the prediction y. Mathematically, for K\u2212class\nclassification problem,\nEvidence(e) = A(F\u0398(x)) = A(o) (2)\nDirichlet Parameter(\u03b1) = e + 1 (3)\nDirichlet Strength(S) = K +\nKX\nk=1\nek (4)\nThe activation function A(\u00b7) assumes three common forms\nto transform the neural network output into evidence: (1)\nReLU(\u00b7) = max(0 , \u00b7), (2) SoftPlus(\u00b7) = log(1 +\nexp(\u00b7)), and (3) exp(\u00b7).\nEvidential models assign input sample to that class for which\nthe output evidence is greatest. Moreover, they quantify the\nconfidence in the prediction for K class classification prob-\nlem through vacuity \u03bd (i.e., measure of lack of confidence\nin the prediction) computed as\nVacuity(\u03bd) = K\nS (5)\nFor any training sample (x, y), the evidential models aim to\nmaximize the evidence for the correct class, minimize the\nevidence for the incorrect classes, and output accurate confi-\ndence. To this end, three variants of evidential loss functions\nhave been proposed (Sensoy et al., 2018): 1) Bayes risk with\nsum of squares loss, 2) Bayes risk with cross-entropy loss,\nand 3) Type II Maximum Likelihood loss. Please refer to\nequations (21), (22), and (23) in the Appendix for the spe-\ncific forms of these losses. Additionally, incorrect evidence\nregularization terms are introduced to guide the model to\noutput low evidence for classes other than the ground truth\nclass (See Appendix C for discussion on the regularization).\nWith evidential training, accurate evidential deep learning\nmodels are expected to output high evidence for the correct\nclass, low evidence for all other classes, and output very\nhigh vacuity for unseen/out-of-distribution samples.\n3.2. Theoretical Analysis of Learning Deficiency in\nEvidential Learning\nTo identify the underlying reason that causes the perfor-\nmance gap of evidential models as described earlier, we\nconsider a K class classification problem and a represen-\ntative evidential model trained using Bayes risk with sum\nof squares loss given in (21). We first provide an important\ndefinition that is critical for our theoretical analysis.\nDefinition 1 (Zero-Evidence Region). A Zero-evidence\nsample is a data sample for which the model outputs zero\nevidence for all classes. A region in the evidence space that\ncontains zero-evidence samples is a zero-evidence region.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3985, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e060e362-ebc9-4553-a4ea-a891d1fad74d": {"__data__": {"id_": "e060e362-ebc9-4553-a4ea-a891d1fad74d", "embedding": null, "metadata": {"page_label": "3", "file_name": "Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_type": "application/pdf", "file_size": 2538334, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7d342485-4866-499e-bca6-1a2f3178ea06", "node_type": "4", "metadata": {"page_label": "3", "file_name": "Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_type": "application/pdf", "file_size": 2538334, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "d4b68fc6ec192150ab43622a81c623bd39ca3403d151c82dd61c30e407f863cb", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2d3dcf54-241e-4ac6-8943-7d3b8f9b791e", "node_type": "1", "metadata": {"page_label": "3", "file_name": "Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_type": "application/pdf", "file_size": 2538334, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "f9ca96ee65f733e241a9ba7ba822621670c5b08f9580de5ab203e32f1a45452f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "With evidential training, accurate evidential deep learning\nmodels are expected to output high evidence for the correct\nclass, low evidence for all other classes, and output very\nhigh vacuity for unseen/out-of-distribution samples.\n3.2. Theoretical Analysis of Learning Deficiency in\nEvidential Learning\nTo identify the underlying reason that causes the perfor-\nmance gap of evidential models as described earlier, we\nconsider a K class classification problem and a represen-\ntative evidential model trained using Bayes risk with sum\nof squares loss given in (21). We first provide an important\ndefinition that is critical for our theoretical analysis.\nDefinition 1 (Zero-Evidence Region). A Zero-evidence\nsample is a data sample for which the model outputs zero\nevidence for all classes. A region in the evidence space that\ncontains zero-evidence samples is a zero-evidence region.\nFor a reasonable evidential model, novel data samples not\nyet seen during training, difficult data samples, and out-of-\ndistribution samples should become zero-evidence samples.\nTheorem 1. Given a training sample(x, y), if an evidential\nneural network outputs zero evidence e, then the gradients\nof the evidential loss evaluated on this training sample over\nthe network parameters reduce to zero.\nProof. Consider an input x with one-hot ground truth label\ny. Let the ground truth class index be gt, i.e., ygt = 1 ,\nwith corresponding Dirichlet parameter \u03b1gt, and y\u0338=gt =\n0. Moreover, let o, e, and \u03b1 represent the neural network\noutput vector before applying the activation A, the evidence\nvector, and the Dirichlet parameters respectively.\nIn this evidential model, the loss is given by\nLMSE(x, y) =\nKX\nj=1\n(yj \u2212 \u03b1j\nS )2 + \u03b1j(S \u2212 \u03b1j)\nS2(S + 1) (6)\n3", "mimetype": "text/plain", "start_char_idx": 3103, "end_char_idx": 4836, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "6da607c4-d2d7-4855-aa2f-f521964c41e0": {"__data__": {"id_": "6da607c4-d2d7-4855-aa2f-f521964c41e0", "embedding": null, "metadata": {"page_label": "4", "file_name": "Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_type": "application/pdf", "file_size": 2538334, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5450fee1-0f33-4a6a-9e18-ef6f25d6fede", "node_type": "4", "metadata": {"page_label": "4", "file_name": "Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_type": "application/pdf", "file_size": 2538334, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "6246ad4df211d4cada1d1b8aac87943613efb8748606d7753522a41e7b138801", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "31ebd248-ec81-4731-8134-09cbe74020c7", "node_type": "1", "metadata": {}, "hash": "369a26888c8d690383fde6a9538434c3646e92b86265bd9991877738e3a1aa9e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Learn to Accumulate Evidence from All Training Samples: Theory and Practice\nNow, the gradient of the loss with respect to the neural\nnetwork output can be computed using the chain rule:\n\u2202LMSE(x, y)\n\u2202ok\n= \u2202LMSE(x, y)\n\u2202\u03b1k\n\u2202ek\n\u2202ok\n=\n\u00142\u03b1gt\nS2 \u2212 2yk\nS \u2212 2(S \u2212 \u03b1k)\nS(S + 1) +\n+\n2(2S + 1)P\ni\nP\nj \u03b1i\u03b1j\n(S2 + S)2\n\u0015\n\u00d7 \u2202ek\n\u2202ok\n(7)\nBased on the actual form of A, we have three cases:\nCase I: ReLU(\u00b7) to transform logits to evidence\nek = ReLU(ok) =\u21d2 \u2202ek\n\u2202ok\n=\n(\n1 if ok > 0\n0 otherwise (8)\nFor a zero-evidence sample, the logits ok satisfy the rela-\ntionship ok \u2264 0 \u2200 k =\u21d2 \u2202ek\n\u2202ok\n= 0 =\u21d2 \u2202LMSE(x,y)\n\u2202ok\n= 0\nCase II: SoftPlus(\u00b7) to transform logits to evidence\nek = log(exp(ok) + 1) =\u21d2 \u2202ek\n\u2202ok\n= Sigmoid(ok) (9)\nFor a zero-evidence sample, the logits ok \u2192 \u2212\u221e =\u21d2\nSigmoid(ok) \u2192 0 & \u2202ek\n\u2202ok\n\u2192 0.\nCase III: exp(\u00b7) to transform logits to evidence\nek = exp(ok) =\u21d2 \u2202ek\n\u2202ok\n= exp(ok) = \u03b1k \u2212 1 (10)\nFor a zero-evidence sample, \u03b1k \u2192 1 = \u21d2 \u2202ek\n\u2202ok\n\u2192 0.\nMoreover, there is no term in the first part of the loss gradient\nin (7) to counterbalance these zero-approaching gradients.\nSo, for zero-evidence training samples, for any node k,\n\u2202LMSE(x, y)\n\u2202ok\n= 0 (11)\nSince the gradient of the loss with respect to all the nodes\nis zero, there is no update to the model from such samples.\nThis implies that the evidential models fail to learn from a\nzero-evidence data sample.\nFor completeness, we present the analysis of standard clas-\nsification models in Appendix A, detailed proof of the evi-\ndential models trained using Bayes risk with sum of squares\nerror along with other evidential lossses in Appendix B, and\nimpact of incorrect evidence regularization in Appendix C.\nRemark: Evidential models can not learn from a train-\ning sample that the model has never seen and for which\nthe model accurately outputs \u201cI don\u2019t know\u201d, i.e., ek =\n0 \u2200k \u2208 [1, K]. Such samples are expected and likely to be\npresent during model training. However, the supervised in-\nformation in such training data points is completely missed\nby evidential models so they fail to acquire any new knowl-\nedge from all such training data samples (i.e., data samples\nin zero-evidence region of the evidence space).\nCorollary 1. Incorrect evidence regularization can not help\nevidential models learn from zero-evidence samples.\nIntuitively, the incorrect evidence regularization encourages\nthe model to output zero evidence for all classes other than\nthe ground truth class and the regularization does not have\nany impact on the evidence for the ground truth class. So,\nthe regularization updates the model parameters such that\nthe model is likely to map input samples closer to zero-\nevidence region in the evidence space. Thus, the regular-\nization does not address the failure of evidential models to\nlearn from zero evidence samples.\nTheorem 2. For a data sample x, if an evidential model\noutputs logits ok \u2264 0 \u2200k \u2208 [0, K], the exponential acti-\nvation function leads to a larger gradident update on the\nmodel parameters than softplus and ReLu.\nLimited by space, we present the proof of Theorem 2 along\nwith additional analysis in the Appendix D. The proof fol-\nlows the gradient analysis of the exponential, Softplus,\nand ReLU based models. It implies that the the training\nof evidential models is most effective with the exponential\nactivation function.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3297, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "31ebd248-ec81-4731-8134-09cbe74020c7": {"__data__": {"id_": "31ebd248-ec81-4731-8134-09cbe74020c7", "embedding": null, "metadata": {"page_label": "4", "file_name": "Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_type": "application/pdf", "file_size": 2538334, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5450fee1-0f33-4a6a-9e18-ef6f25d6fede", "node_type": "4", "metadata": {"page_label": "4", "file_name": "Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_type": "application/pdf", "file_size": 2538334, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "6246ad4df211d4cada1d1b8aac87943613efb8748606d7753522a41e7b138801", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6da607c4-d2d7-4855-aa2f-f521964c41e0", "node_type": "1", "metadata": {"page_label": "4", "file_name": "Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_type": "application/pdf", "file_size": 2538334, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "251da0319160ae30aa5fd5473215bae708b949d65e91e55bf26baa94a6acbb53", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "So,\nthe regularization updates the model parameters such that\nthe model is likely to map input samples closer to zero-\nevidence region in the evidence space. Thus, the regular-\nization does not address the failure of evidential models to\nlearn from zero evidence samples.\nTheorem 2. For a data sample x, if an evidential model\noutputs logits ok \u2264 0 \u2200k \u2208 [0, K], the exponential acti-\nvation function leads to a larger gradident update on the\nmodel parameters than softplus and ReLu.\nLimited by space, we present the proof of Theorem 2 along\nwith additional analysis in the Appendix D. The proof fol-\nlows the gradient analysis of the exponential, Softplus,\nand ReLU based models. It implies that the the training\nof evidential models is most effective with the exponential\nactivation function. Intuitively, the ReLU based activation\ncompletely destroys all the information in the negative logits,\nand has largest region in evidence space in which training\ndata have zero evidence. Softplus activation improves\nover the ReLU, and compared to ReLU, has smaller region\nin evidence space where training data have zero evidence.\nHowever, Softplus based evidential models fail to cor-\nrect the acquired knowledge when the model has strong\nwrong evidence. Moreover, these models are likely to suf-\nfer from vanishing gradients problem when the number of\nclasses increases (i.e., classification problem becomes more\nchallenging). Finally, exponential activation has the smallest\nzero-evidence region in the evidence space without suffering\nfrom the issues of SoftPlus based evidential models.\n4. Avoiding Zero-Evidence Regions Through\nCorrect Evidence Regularization\nWe now consider an evidential model with exponential func-\ntion to transform the logits into evidence. We propose a\nnovel vacuity-guided correct evidence regularization term\nLcor(x, y) = \u2212\u03bbcor log(\u03b1gt \u2212 1) (12)\nwhere \u03bbcor = \u03bd = K\nS represents the regularization term\nwhose value is given by the magnitude of the vacuity output\nby the evidential model and \u03b1gt \u22121 represents the predicted\nevidence for the ground truth class. The regularization\nterm \u03bbcor determines the relative importance of the correct\n4", "mimetype": "text/plain", "start_char_idx": 2504, "end_char_idx": 4667, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5b63552a-f65c-479f-b39f-ef5e9cdc8267": {"__data__": {"id_": "5b63552a-f65c-479f-b39f-ef5e9cdc8267", "embedding": null, "metadata": {"page_label": "5", "file_name": "Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_type": "application/pdf", "file_size": 2538334, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0c266220-80bd-43df-b799-10278a3c2009", "node_type": "4", "metadata": {"page_label": "5", "file_name": "Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_type": "application/pdf", "file_size": 2538334, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "59a3bc299fb2bae28df6e75c86594723cd286953971768201e4bbaa43587aee3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e946cd83-0855-4a5e-a3a1-6dd6f28b29b9", "node_type": "1", "metadata": {}, "hash": "77f3d981d8e11d3f6344f3681c5350cfeb352e75cac41f5b8e1ac5937de97b69", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Learn to Accumulate Evidence from All Training Samples: Theory and Practice\nevidence regularization term compared to the evidential\nloss and incorrect evidence regularization and is treated as\nconstant during model parameter update.\nTheorem 3. Correct evidence regularization Lcor(x, y)\ncan address the issue of learning from zero-evidence train-\ning samples.\nProof. The proposed regularization term Lcor(x, y) does\nnot contain any evidence terms other than the evidence for\nthe ground truth node. So, the gradient of the regularization\nfor nodes other than the ground truth node will be 0 i.e.\n\u2202Lcor(x,y)\n\u2202ok\n\f\f\f\nk\u0338=gt\n= 0 and there will be no update on these\nnodes. For the ground truth node gt, ygt = 1, the gradient\nis given by\n\u2202Lcor(x, y)\n\u2202ogt\n= \u2202\n\u0000\n\u2212 \u03bbcor log(\u03b1gt \u2212 1)\n\u0001\n\u2202ogt\n(13)\n= \u2212\u03bbcor\n\u2202 log(\u03b1gt \u2212 1)\n\u2202\u03b1gt\n\u00d7 \u2202\u03b1gt\n\u2202ogt\n(14)\n= \u2212 \u03bbcor\n(\u03b1gt \u2212 1)(\u03b1gt \u2212 1) = \u2212\u03bbcor (15)\nThe gradient value equals the magnitude of the vacuity. The\nvacuity is bounded in the range [0, 1], and zero-evidence\nsample, the vacuity is maximum, leading to the greatest\ngradient value of \u2202Lcor(x,y)\n\u2202ogt\n= \u22121. In other words, the reg-\nularization encourages the model to update the parameters\nsuch that the correct evidence \u03b1gt \u2212 1 increases. As the\nmodel evidence increases, the vacuity decreases, and the\ncontribution of the regularization Lcor(x, y) is minimized.\nThus, the proposed regularization enables the evidential\nmodel to learn from zero-evidence samples.\n4.1. Evidential Model Training\nWe formulate an overall objective used to train the pro-\nposed Regularized evidential model (RED). Essentially,\nthe evidential model is trained to maximize the correct evi-\ndence, minimize the incorrect evidence, and avoid the zero-\nevidence region during training. The overall loss is\nL(x, y) = Levid(x, y) + \u03b71Linc(x, y) + Lcor(x, y)\n(16)\nwhere Levid(x, y) is the loss based on the evidential\nframework given by (21), (23), or (22) (See Appendix B),\nLinc(x, y) represents the incorrect evidence regularization\n(See Appendix Section C), Lcor(x, y) represents the pro-\nposed novel correct evidence regularization term in(12), and\n\u03b71 = \u03bb1 \u00d7 min(1.0, epoch index/10) controls the impact\nof incorrect evidence regularization to the overall model\ntraining. In this work, we consider the forward-KL based\nincorrect evidence regularization given in (42) based on\n(Sensoy et al., 2018).\n4.2. Evidence Space Visualization\nFigure 4.Evidence space visualization to demonstrate the effec-\ntiveness of the proposed method.\nFigure 2 visualizes the evidence space in ReLU-based ev-\nidential models by considering the pre-ReLU output in a\nbinary classification setting. Ideally, all samples that belong\nto Class 1 should be mapped to the blue region (region of\nhigh evidence for Class 1, low evidence for all other classes),\nall samples that belong to Class 2 should be mapped to the\nred region, and all out-of distribution samples should be\nmapped to the zero-evidence region (no evidence for all\nclasses). To realize this goal, the models are trained using\nthe evidential loss Levid with incorrect evidence regular-\nization Linc. However, there is no update to the evidential\nmodel from such samples of zero-evidence region. Model\u2019s\nprior belief of \u201cI don\u2019t know\u201d for such samples does not\nget updated even after being exposed to the true label. For\nthe samples with high incorrect evidence and low correct\nevidence, evidential model aims to correct itself. However,\nmany such samples are likely to get mapped to the zero-\nevidence region (as shown by blue and orange arrows in\nFigure 2) after which there is no update to the model. Such\nfundamental limitation holds true for all evidential models.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3660, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e946cd83-0855-4a5e-a3a1-6dd6f28b29b9": {"__data__": {"id_": "e946cd83-0855-4a5e-a3a1-6dd6f28b29b9", "embedding": null, "metadata": {"page_label": "5", "file_name": "Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_type": "application/pdf", "file_size": 2538334, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0c266220-80bd-43df-b799-10278a3c2009", "node_type": "4", "metadata": {"page_label": "5", "file_name": "Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_type": "application/pdf", "file_size": 2538334, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "59a3bc299fb2bae28df6e75c86594723cd286953971768201e4bbaa43587aee3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5b63552a-f65c-479f-b39f-ef5e9cdc8267", "node_type": "1", "metadata": {"page_label": "5", "file_name": "Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_type": "application/pdf", "file_size": 2538334, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "7988389abbbdb34f74031e64e4e6ab502307794e27d6c04dd5299fdfaef1a719", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "To realize this goal, the models are trained using\nthe evidential loss Levid with incorrect evidence regular-\nization Linc. However, there is no update to the evidential\nmodel from such samples of zero-evidence region. Model\u2019s\nprior belief of \u201cI don\u2019t know\u201d for such samples does not\nget updated even after being exposed to the true label. For\nthe samples with high incorrect evidence and low correct\nevidence, evidential model aims to correct itself. However,\nmany such samples are likely to get mapped to the zero-\nevidence region (as shown by blue and orange arrows in\nFigure 2) after which there is no update to the model. Such\nfundamental limitation holds true for all evidential models.\nThe evidence space visualization for RED is shown in Figure\n4 to illustrate how it addresses the above limitation. Cor-\nrect evidence regularization (indicated by green arrows) is\nweighted by the magnitude of the vacuity and is maximum\nin the zero-evidence region. In this problematic region, the\nproposed regularization fully dominates the model update\nas there is no update to the model from the two loss com-\nponents (Levid and Linc) in (16). As the sample gets far\naway from the zero evidence region, the vacuity decreases\nproportionally, the impact of the proposed regularization\nto model update becomes insignificant, and the evidential\nlosses (Levid & Linc) guide the model training. In this\nway, RED can effectively learn from all training samples\nirrespective of the model\u2019s existing evidence.\n5", "mimetype": "text/plain", "start_char_idx": 2968, "end_char_idx": 4465, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "fb33ae35-b68b-4043-80fc-0dc3793943a0": {"__data__": {"id_": "fb33ae35-b68b-4043-80fc-0dc3793943a0", "embedding": null, "metadata": {"page_label": "6", "file_name": "Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_type": "application/pdf", "file_size": 2538334, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "dbc09027-59e7-45b2-babc-9a46a8d66665", "node_type": "4", "metadata": {"page_label": "6", "file_name": "Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_type": "application/pdf", "file_size": 2538334, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "73dff53e56e3a74d112bfc65198ecc3c017d86c898bbfb83def73df918fe8f88", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cf4950db-ce5b-489e-b6ba-af95d9ded45d", "node_type": "1", "metadata": {}, "hash": "a004dec707e8d35f697a963929e6de64e144726e053a9d21722a8908ee9440c4", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Learn to Accumulate Evidence from All Training Samples: Theory and Practice\n5. Experiments\nDatasets and setup. We consider the standard supervised\nclassification problem with MNIST (LeCun, 1998), Ci-\nfar10, and Cifar100 datasets (Krizhevsky et al., 2009), and\nfew-shot classification with mini-ImageNet dataset (Vinyals\net al., 2016). We employ the LeNet model for MNIST,\nResNet18 model (He et al., 2016) for Cifar10/Cifar100,\nand ResNet12 model (He et al., 2016) for mini-ImageNet.\nWe first conduct experiments to demonstrate the learning\ndeficiency of existing evidential models to confirm our the-\noretical findings. We then evaluate the proposed correct\nevidence regularization to show its effectiveness. We finally\nconduct ablation studies to investigate the impact of evi-\ndential losses on model generalization and the uncertainty\nquantification of the proposed evidential model. Limited by\nspace, additional clarifications, experiment results includ-\ning few-shot classification experiments, experiments over\nchallenging tiny-Imagenet datasett with Swin Transformer,\nhyperparameter details, and discussions are presented in the\nAppendix.\n5.1. Learning Deficiency of Evidential Models\nSensitivity to the change of the architecture. We first\nconsider a toy illustrative experiment with two frameworks:\n1) standard softmax, 2) evidential learning, and experiment\nwith the LeNet (LeCun et al., 1999) model considered in\nEDL (Sensoy et al., 2018) with a minor modification to the\narchitecture: no dropout in the model. To construct the toy\ndataset, we randomly select 4 labeled data points from the\nMNIST training dataset as shown in the Figure 5. For the\nevidential model, we use ReLU to transform the network\noutputs to evidence, and train the model with MSE-based\nevidential loss (Sensoy et al., 2018) given in (21) without\nincorrect evidence regularization. We train both models\nusing only these 4 training data points.\nFigure 6 compares the training accuracy and training loss\ntrends of the evidential model with the standard softmax\nmodel (trained with the cross-entropy loss). Before any\ntraining, both models have 0% accuracy and the loss is high\nas expected. For the evidential model, in the first few iter-\nations, the model learns from the training dataset, and the\nmodel\u2019s accuracy increases to 50%. Afterward, the eviden-\ntial model fails to learn as the evidential model maps two of\nthe training data samples to the zero-evidence region. Even\nin such a trivial setting, the evidential model fails to fit the 4\ntraining data points showing their learning deficiency that\nempirically verifies the conclusion in Theorem 1. It is also\nworth noting that the range of the evidential model\u2019s loss is\nsignificantly smaller than the standard model. This is mainly\ndue to the bounded nature of the evidential MSE loss(i.e., it\nis bounded in the range [0, 2]) (a detailed theoretical analy-\nsis of the evidential losses is provided in the Appendix). In\ncontrast, the standard model trained with cross-entropy loss\nG T : 3\n G T : 5\n G T : 2\n G T : 6\nFigure 5.Toy dataset with 4 data points.\n0 2 4 6 8\nIterations (\u00d7 10)\n0.00\n0.25\n0.50\n0.75\n1.00\nAccuracy\nStandard model\nEvidential model\n(a) Training accuracy trend\n0 2 4 6 8\nIterations (\u00d7 10)\n0\n1\n2\nLoss\nStandard Model\nEvidential Model (b) Training loss trend\nFigure 6.Training of standard and evidential models\neasily fits the trivial dataset, obtains near 0 loss, and perfect\naccuracy of 100% after a few iterations of training.\n0 25 50 75 100\nIteration\n0\n2\n4\nEvidence\n3\n5\n2\n6\nFigure 7.Zero-evidence trend during model training\nAdditionally, we visualize the zero-evidence data samples\nfor the toy dataset setting. We plot the total evidence for\neach training sample as training progresses for the first 100\niterations.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3774, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "cf4950db-ce5b-489e-b6ba-af95d9ded45d": {"__data__": {"id_": "cf4950db-ce5b-489e-b6ba-af95d9ded45d", "embedding": null, "metadata": {"page_label": "6", "file_name": "Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_type": "application/pdf", "file_size": 2538334, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "dbc09027-59e7-45b2-babc-9a46a8d66665", "node_type": "4", "metadata": {"page_label": "6", "file_name": "Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_type": "application/pdf", "file_size": 2538334, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "73dff53e56e3a74d112bfc65198ecc3c017d86c898bbfb83def73df918fe8f88", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fb33ae35-b68b-4043-80fc-0dc3793943a0", "node_type": "1", "metadata": {"page_label": "6", "file_name": "Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_type": "application/pdf", "file_size": 2538334, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "7a1fc35e26c7809a48b4ae7bef2ac1c5a8a6917e56e25231675e140b40e0cae1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "0 25 50 75 100\nIteration\n0\n2\n4\nEvidence\n3\n5\n2\n6\nFigure 7.Zero-evidence trend during model training\nAdditionally, we visualize the zero-evidence data samples\nfor the toy dataset setting. We plot the total evidence for\neach training sample as training progresses for the first 100\niterations. The total evidence trend as training progresses\nfor the first 100 iterations is shown in Figure 7. The ev-\nidential model\u2019s predictions are correct for data samples\nwith ground truth labels of 3 and 6, and incorrect for the\nremaining two data samples. After few iterations of training,\nthe remaining two samples have zero total evidence (i.e.\nsamples are mapped to zero evidence region), the model\nnever learns from them, and the model only achieves overall\n50% training accuracy even after 100 iterations. Clearly,\nthe evidential model continues to output zero evidence for\ntwo of the training examples and fails to learn from them.\nSuch learning deficiency of evidential models limits their\nextension to challenging settings. In contrast, the standard\nmodel easily overfits the 4 training examples and achieves\n100% accuracy.\nSensitivity to hyperparameter tuning. In this experi-\nment, evidential models are trained using evidential losses\ngiven in (21), (22), or (23) with incorrect evidence regular-\nization to guide the model for accurate uncertainty quan-\n6", "mimetype": "text/plain", "start_char_idx": 3484, "end_char_idx": 4838, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a23bb4b8-8bce-4d31-8faf-1d29ebd4b53b": {"__data__": {"id_": "a23bb4b8-8bce-4d31-8faf-1d29ebd4b53b", "embedding": null, "metadata": {"page_label": "7", "file_name": "Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_type": "application/pdf", "file_size": 2538334, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a51a9d5b-8d3d-4c4a-86f8-d1cd040ef289", "node_type": "4", "metadata": {"page_label": "7", "file_name": "Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_type": "application/pdf", "file_size": 2538334, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "74d3f09dbc6d77762e0113ee8cbe09be20dd686e43cff8dd4ac95ea4b81a5c36", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Learn to Accumulate Evidence from All Training Samples: Theory and Practice\nFigure 8.Impact of different incorrect evidence regularization\nstrengths to the test set accuracy on Cifar100 dataset\ntification. We study the impact of the incorrect evidence\nregularization \u03bb1 to the evidential model\u2019s performance\nusing Cifar100. The result shows that the generalization\nperformance of evidential models is highly sensitive to \u03bb1\nvalues. To illustrate, we consider the Type II Maximum\nLikelihood loss in (23) with different \u03bb1 to control KL reg-\nularization (results on other loss functions are presented in\nthe Appendix). As shown in Figure 8, when some regular-\nization is introduced, evidential model\u2019s test performance\nimproves slightly. However, when strong regularization is\nused, the model focuses strongly on minimizing the incor-\nrect evidence. Such regularization causes the model to push\nmany training samples into or close to the zero-evidence\nregions, which hurts the model\u2019s learning capabilities. In\ncontrast, the proposed model can continue to learn from\nsamples in zero-evidence regions, which shows its robust-\nness to incorrect evidence regularization. Moreover, our\nmodel has stable performance across all hyperparameter\nsettings as it can effectively learn from all training samples.\nChallenging datasets and settings. We next consider\nstandard classification models for the Cifar100 dataset and\n1-shot classification with the mini-ImageNet dataset. We\ndevelop evidential extensions of the classification models\nusing Type II Maximum Likelihood loss given in (23) with-\nout any incorrect evidence regularization and use ReLU to\ntransform logits to evidence. As shown in Figure 10, com-\npared to the standard classification model, the evidential\nmodel\u2019s predictive performance is sub-optimal (almost 20%\nlower for both classification problems). This is mainly due\nto the fact that evidential model maps many of the training\ndata points to zero-evidence region, which is equivalent to\nthe model saying \u201cI don\u2019t know to which class this sample\nbelongs\u201d and stopping to learn from them. Consequently,\nthe model fails to acquire new knowledge (i.e., update itself),\neven after being exposed to correct supervision (the label\ninformation). In these cases, instead of learning, the eviden-\ntial model chooses to ignore the training data on which it\ndoes not have any evidence and remains to be ignorant.\nVisualization of zero-evidence samples. We next show\nthe 2-dimensional visualization of the latent representation\nfor the randomly selected 500 training examples based on\nFigure 9.Zero-Evidence Sample Visualization\n(a) Cifar100 Results\n (b) 1-Shot Results\nFigure 10.Learning trends in complex classification problems\nthe tSNE plot for ReLU based evidential model trained on\nthe Cifar100 dataset with \u03bb1 = 0.1. Figure 9 plot visualizes\nthe latent embedding of zero evidence (Zero E) training sam-\nples with non-zero evidence (Non-Zero E) training samples.\nAs can be seen, both zero and non-zero evidence samples ap-\npear to be dispersed, overlap at different regions, and cover\na large area in the embedding space. This further confirms\nthe challenge of effectively learning from these samples\n5.2. Effectiveness of the RED\nEvidential activation function. We first experiment with\ndifferent activation functions for the evidential models to\nshow the superior predictive performance and generalization\ncapability of exp activation validating our Theorem 2. We\nconsider evidential models trained with evidential log loss\ngiven by (23) in Table 1 (Additional results along with hy-\nperparameter details are presented in Appendix Section F).\nAs can be seen, exp activation to transform network outputs\ninto evidence leads to superior performance compared to\nReLU and Softplus based transformations. Furthermore,\nour proposed model with correct evidence regularization\nfurther improves over the exp-based evidential models as\nit enables the evidential model to continue learning from\nzero-evidence samples.\nTable 1.Classification performance comparison\nModel MNIST Cifar10 Cifar100\nReLU 98.19\u00b10.08 41.43\u00b119.60 61.27\u00b13.79\nSoftPlus 98.21\u00b10.05 95.18\u00b10.11 74.48\u00b10.17\nexp 98.79\u00b10.02 95.11\u00b10.10 76.12\u00b10.04\nRED(Ours) 99.10\u00b10.02 95.24\u00b10.06 76.43\u00b10.21\nWe next present the test set performance change as training\n7", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4310, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c3d2c8d2-4610-4844-b880-5a5e0e1619b1": {"__data__": {"id_": "c3d2c8d2-4610-4844-b880-5a5e0e1619b1", "embedding": null, "metadata": {"page_label": "8", "file_name": "Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_type": "application/pdf", "file_size": 2538334, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "122a356a-560b-4765-b4a3-49c3566409a6", "node_type": "4", "metadata": {"page_label": "8", "file_name": "Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_type": "application/pdf", "file_size": 2538334, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "ac244666836410d720224d8d21af9d57890b249c9efa12e559805cbbdcb49bc0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a2fc65a4-c794-47ec-a6d5-8f5605fcbb01", "node_type": "1", "metadata": {}, "hash": "d2502eb2ea43379990cfc0235e19487488d8148abba83e3f00e7eda99c0ba681", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Learn to Accumulate Evidence from All Training Samples: Theory and Practice\nprogresses with MNIST dataset and two different evidential\nlosses in Figure 11 where we observe similar results. The\nexp activation shows superior performance, as it has small-\nest zero-evidence region, and does not suffer from many\nlearning issues present in other activation functions.\n(a) Evidential MSE loss\n (b) Evidential Log loss\nFigure 11.Impact of evidential activation functions to the Test\nAccuracy\nCorrect evidence regularization. We now study the im-\npact of the proposed correct evidence regularization using\nthe MNIST and Cifar100 classification problems. We con-\nsider the evidential baseline model that uses exp activation\nto acquire evidence, and is trained with Type II Maximum\nLikelihood based loss with different incorrect evidence reg-\nularization strengths. We introduce the proposed novel cor-\nrect evidence regularization to the model. As can be seen in\nFigure 12, the model with correct-evidence regularization\nhas superior generalization performance compared to the\nbaseline evidential model. This is mainly due to the fact\nthat with proposed correct evidence regularization, the evi-\ndential model can also learn from the zero-evidence training\nsamples to acquire new knowledge instead of ignoring them.\nOur proposed model considers knowledge from all the train-\ning data and aims to acquire new knowledge to improve its\ngeneralization instead of ignoring the samples on which it\nhas no knowledge. Finally, even though strong incorrect\nevidence regularization hurts the model\u2019s generalization, the\nproposed model is robust and generalizes better, empirically\nvalidating our Theorem 3. Limited by space, we present\nadditional results in Appendix F.3.2.\nZero-evidence Sample Anaysis. Similar to the toy\nMNIST zero-evidence analysis, we consider the Cifar100\ndataset, and carry out the analysis for this complex\ndataset/setting. Instead of focusing on a few training ex-\namples, we present the average statistics of the evidence\n(E) for the 50,000 training samples in the 100 class classi-\nfication problem for a model trained for 200 epochs using\na log-based evidential loss in (23) with \u03bb1 = 1.0. For ref-\nerence, the samples with less than 0.01 average evidence\n(i.e., E \u22640.01) are samples on which the model is not\nconfident (i.e., having a high vacuity of \u03bd \u2265 0.99), and are\nclose to the ideal zero-evidence region. Our proposed RED\nmodel effectively avoids such zero evidence regions, and\nhas the lowest number of samples (i.e. only 0.06% of total\ntraining dataset compared to 58.96% of SoftPlus based,\n(a) Trend for \u03bb1 = 1.0\n (b) Trend for \u03bb1 = 10.0\n(c) Trend for \u03bb1 = 0.1\n (d) Trend for \u03bb1 = 1.0\nFigure 12.Impact of correct evidence regularization to test accu-\nracy: (a), (b) - MNIST Results; (c), (d) - Cifar100 Results\nand 100% of ReLU based evidential models) in very low\nevidence regions.\nTable 2.Zero-Evidence Analysis for Complex Dataset-Setting\nModel E \u2264.01 E \u22640.1 E \u22641.0 E > 1.0\nReLU 50000 50000 50000 0\nSoftPlus 29483 32006 49938 62\nExp 48318 49881 49949 51\nRED 30 16322 25154 24846\n5.3. Ablation Study\nImpact of loss function. We next study the impact of\nthe evidential loss function on the model\u2019s performance\nusing MNIST and CIFAR100 classification problems. We\nconsider all three activations: ReLU, SoftPlus, and exp\nto transform neural network outputs to evidence and carry\nout experiments over CIFAR100 with identical model and\nsettings. As seen in Table 3, the generalization performance\nof evidential model is consistently sub-optimal when trained\nwith evidential MSE loss given by(21) compared to the two\nother evidential losses (22) & (23). This is consistent across\nall three evidence activation functions.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3736, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a2fc65a4-c794-47ec-a6d5-8f5605fcbb01": {"__data__": {"id_": "a2fc65a4-c794-47ec-a6d5-8f5605fcbb01", "embedding": null, "metadata": {"page_label": "8", "file_name": "Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_type": "application/pdf", "file_size": 2538334, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "122a356a-560b-4765-b4a3-49c3566409a6", "node_type": "4", "metadata": {"page_label": "8", "file_name": "Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_type": "application/pdf", "file_size": 2538334, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "ac244666836410d720224d8d21af9d57890b249c9efa12e559805cbbdcb49bc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c3d2c8d2-4610-4844-b880-5a5e0e1619b1", "node_type": "1", "metadata": {"page_label": "8", "file_name": "Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_type": "application/pdf", "file_size": 2538334, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "dcf362648f9f780842658502a9f25c06f648bed44db11a284faff44b57751505", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Ablation Study\nImpact of loss function. We next study the impact of\nthe evidential loss function on the model\u2019s performance\nusing MNIST and CIFAR100 classification problems. We\nconsider all three activations: ReLU, SoftPlus, and exp\nto transform neural network outputs to evidence and carry\nout experiments over CIFAR100 with identical model and\nsettings. As seen in Table 3, the generalization performance\nof evidential model is consistently sub-optimal when trained\nwith evidential MSE loss given by(21) compared to the two\nother evidential losses (22) & (23). This is consistent across\nall three evidence activation functions. This is mainly due\nto the bounded nature of the evidential MSE loss (21): for\nall training samples, evidential MSE loss is bounded in the\nrange of [0, 2]. Type II Maximum Likelihood loss given in\n(23) and cross-entropy based evidential loss given in (22)\nshow comparable empirical results.\nNext, we consider exp activation and conduct experiments\nover the MNIST dataset for incorrect evidence regulariza-\ntion strengths of \u03bb1 = 0&1 . We again observe similar\nresults where the training with the Evidential MSE loss in\n(21) leads to sub-optimal test performance. Additional re-\nsults, along with theoretical analysis are presented in the\nAppendix. In the subsequent experiments, we consider the\nType II Maximum Likelihood loss (23) for evidential model\ntraining due to its simplicity and some theoretical advan-\n8", "mimetype": "text/plain", "start_char_idx": 3107, "end_char_idx": 4549, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1c45901e-04c8-4b71-8002-4591c7fe6579": {"__data__": {"id_": "1c45901e-04c8-4b71-8002-4591c7fe6579", "embedding": null, "metadata": {"page_label": "9", "file_name": "Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_type": "application/pdf", "file_size": 2538334, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "cb4135a6-e312-46b3-b1a3-cff024aa7263", "node_type": "4", "metadata": {"page_label": "9", "file_name": "Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_type": "application/pdf", "file_size": 2538334, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "7a9fe5607cdf0ebe4a2d7c37e03afe5aa6c3c0951092add593ff005bce0f4ed1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8148cce7-bad0-41d0-a147-c2daeb37fb08", "node_type": "1", "metadata": {}, "hash": "f7cbf3bf7e9ef1fb2ea4bfd6bedeb68ee2a35cfc5e3af4e860763f8207280953", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Learn to Accumulate Evidence from All Training Samples: Theory and Practice\ntages (see Appendix E). We leave a thorough investigation\nof these two evidential losses ((22) & (23)) as future work.\nTable 3.Impact of evidential losses on classification performance\nLoss ReLU SoftPlus exp RED(Ours)\nMSE(21) 31.49\u00b10.3 15.74\u00b10.5 42.95\u00b10.7 75.73\u00b10.3\nCE (22) 68.62\u00b12.4 74.44\u00b10.1 76.23\u00b10.1 76.35\u00b10.1\nLog(23) 61.27\u00b13.8 74.48\u00b10.1 76.12\u00b10.1 76.43\u00b10.2\n(a) Trend for \u03bb1 = 0.0\n (b) Trend for \u03bb1 = 1.0\nFigure 13.Impact of evidential losses on test set accuracy\nFigure 14.Accuracy-Vacuity curve\nStudy of uncertainty information. We now investigate\nthe uncertainty behavior of the proposed evidential model\nwith Cifar100 experiments. We present the Accuracy-\nVacuity curve for different incorrect evidence regulariza-\ntion strengths (\u03bb1) in Figure 14. Vacuity reflects the lack\nof confidence in the predictions, and the accuracy of effec-\ntive evidential model should increase with lower vacuity\nthreshold. Without any incorrect evidence regularization\n(i.e., \u03bb1 = 0), the evidential model is highly confident on\nits predictions and all test samples are concentrated on the\nlow vacuity region. As the incorrect evidence regularization\nstrength is increased, the model outputs more accurate confi-\ndence in the predictions. Strong incorrect evidence regular-\nization hurts the generalization over the test set as indicated\nby low accuracy when all test samples are considered (i.e.,\nvacuity threshold of 1.0). In all cases, the evidential model\nshows reasonable uncertainty behavior: the model\u2019s test set\naccuracy increases as the vacuity threshold is decreased.\nNext, we look at the accuracy of the evidential models on\ntheir top-K % most confident predictions over the test set.\nTable 4 shows the accuracy trend of Top-K (%) confident\nsamples. Consider the most confident 20% samples (cor-\nresponding to 2000 test samples of Cifar100 dataset). The\nproposed model leads to highest accuracy (of 99.35%) com-\npared to all the models. Similar trend is seen for different\nK values where the proposed model shows comparable\nto superior results demonstrating its accurate uncertainty\nquantification capability.\nTable 4.Accuracy on Top-K% confident samples (%)\nModel 10% 20% 30% 50% 80% 100%\nReLU 98.50 98.30 97.27 90.60 71.54 61.27\nSoftPlus 99.10 98.75 98.30 95.86 85.56 74.48\nexp 99.40 98.95 98.50 96.52 86.46 76.12\nRED 99.60 99.35 98.83 96.24 86.38 76.43\nWe next consider out-of-distribution (OOD) detection ex-\nperiments for the Cifar100-trained evidential model using\nSVHN dataset (as OOD) (Netzer et al., 2011). As seen\nin Table 5, the evidential models, on average, output very\nhigh vacuity for the OOD samples, showing the potential\nfor OOD detection.\nTable 5.Out-of-Distribution sample detection\nModel InD Vacuity OOD Vacuity (SVHN)\nexp 0.3227 0.7681\nRED (Ours) 0.2729 0.7552\nWe present the AUROC score for Cifar100 trained models\nwith SVHN dataset test set as the OOD samples in Table\n6. In AUROC calculation, we use the maximum softmax\nscore for the standard model, and predicted vacuity score\nfor all the evidential models. As can be seen, theexp-based\nmodel outperforms all other activation functions, and the\nproposed model RED can learn from all the training samples\nthat leads to the best performance.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3291, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8148cce7-bad0-41d0-a147-c2daeb37fb08": {"__data__": {"id_": "8148cce7-bad0-41d0-a147-c2daeb37fb08", "embedding": null, "metadata": {"page_label": "9", "file_name": "Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_type": "application/pdf", "file_size": 2538334, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "cb4135a6-e312-46b3-b1a3-cff024aa7263", "node_type": "4", "metadata": {"page_label": "9", "file_name": "Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_type": "application/pdf", "file_size": 2538334, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "7a9fe5607cdf0ebe4a2d7c37e03afe5aa6c3c0951092add593ff005bce0f4ed1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1c45901e-04c8-4b71-8002-4591c7fe6579", "node_type": "1", "metadata": {"page_label": "9", "file_name": "Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_type": "application/pdf", "file_size": 2538334, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "8d810b32dc7a2e44445c727cdfedfaf9f83a70aef657ca4d411fbdf6186d2d0b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "As seen\nin Table 5, the evidential models, on average, output very\nhigh vacuity for the OOD samples, showing the potential\nfor OOD detection.\nTable 5.Out-of-Distribution sample detection\nModel InD Vacuity OOD Vacuity (SVHN)\nexp 0.3227 0.7681\nRED (Ours) 0.2729 0.7552\nWe present the AUROC score for Cifar100 trained models\nwith SVHN dataset test set as the OOD samples in Table\n6. In AUROC calculation, we use the maximum softmax\nscore for the standard model, and predicted vacuity score\nfor all the evidential models. As can be seen, theexp-based\nmodel outperforms all other activation functions, and the\nproposed model RED can learn from all the training samples\nthat leads to the best performance.\nTable 6.AUROC for Cifar100-SVHN experiment\nModel ReLU SoftPlus Standard exp RED\nAUROC 0.7430 0.8058 0.8669 0.8804 0.8833\n6. Conclusion\nIn this paper, we theoretically investigate the evidential mod-\nels to identify their learning deficiency, which makes them\nfail to learn from zero-evidence regions. We then show\nthe superiority of the evidential model with exp evidential\nactivation over the ReLU and SoftPlus based models.\nWe further analyze the evidential losses, and introduce a\nnovel correct evidence regularization over theexp-based ev-\nidential model. The proposed model effectively pushes the\ntraining samples out of the zero-evidence regions, leading to\nsuperior learning capabilities. We conduct extensive experi-\nments that empirically validate all theoretical claims while\ndemonstrating the effectiveness of the proposed approach.\nAcknowledgements\nThis research was supported in part by an NSF IIS award\nIIS-1814450 and an ONR award N00014-18-1-2875. The\nviews and conclusions contained in this paper are those of\nthe authors and should not be interpreted as representing\nany funding agency.\n9", "mimetype": "text/plain", "start_char_idx": 2592, "end_char_idx": 4398, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "7ea043a2-7d33-46ef-9cde-d1204f2caf9f": {"__data__": {"id_": "7ea043a2-7d33-46ef-9cde-d1204f2caf9f", "embedding": null, "metadata": {"page_label": "10", "file_name": "Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_type": "application/pdf", "file_size": 2538334, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "fb69018b-7e55-40cf-a486-dda2ebb7e8ce", "node_type": "4", "metadata": {"page_label": "10", "file_name": "Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_type": "application/pdf", "file_size": 2538334, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "b7da5a3ef1dcc7520c56047ada272e1271b9996ebf33a9dd2c3a5795fcfdff2f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c69362e8-cd2d-4de7-bb1f-8fa6435e5671", "node_type": "1", "metadata": {}, "hash": "52b802aefe0358515a37c6f32c91e23e3063da961a8c78d0dd17b2dd044f736e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Learn to Accumulate Evidence from All Training Samples: Theory and Practice\nReferences\nAmini, A., Schwarting, W., Soleimany, A., and Rus, D.\nDeep evidential regression. Advances in Neural Informa-\ntion Processing Systems, 33:14927\u201314937, 2020.\nBao, W., Yu, Q., and Kong, Y . Evidential deep learning\nfor open set action recognition. In Proceedings of the\nIEEE/CVF International Conference on Computer Vision,\npp. 13349\u201313358, 2021.\nBishop, C. M. and Nasrabadi, N. M. Pattern recognition\nand machine learning, volume 4. Springer, 2006.\nBlundell, C., Cornebise, J., Kavukcuoglu, K., and Wierstra,\nD. Weight uncertainty in neural network. InInternational\nconference on machine learning, pp. 1613\u20131622. PMLR,\n2015.\nCharpentier, B., Z\u00a8ugner, D., and G\u00a8unnemann, S. Posterior\nnetwork: Uncertainty estimation without ood samples\nvia density-based pseudo-counts. Advances in Neural\nInformation Processing Systems, 33:1356\u20131367, 2020.\nChen, Y ., Liu, Z., Xu, H., Darrell, T., and Wang, X. Meta-\nbaseline: Exploring simple meta-learning for few-shot\nlearning. In Proceedings of the IEEE/CVF International\nConference on Computer Vision, pp. 9062\u20139071, 2021.\nFinn, C., Abbeel, P., and Levine, S. Model-agnostic meta-\nlearning for fast adaptation of deep networks. In Proceed-\nings of the 34th International Conference on Machine\nLearning-Volume 70, pp. 1126\u20131135. JMLR. org, 2017.\nGal, Y . and Ghahramani, Z. Dropout as a bayesian approx-\nimation: Representing model uncertainty in deep learn-\ning. In international conference on machine learning, pp.\n1050\u20131059. PMLR, 2016.\nHe, K., Zhang, X., Ren, S., and Sun, J. Deep residual learn-\ning for image recognition. In Proceedings of the IEEE\nconference on computer vision and pattern recognition,\npp. 770\u2013778, 2016.\nHuynh, E. Vision transformers in 2022: An update on tiny\nimagenet. arXiv preprint arXiv:2205.10660, 2022.\nJ\u00f8sang, A. Subjective logic, volume 3. Springer, 2016.\nKamath, U., Liu, J., and Whitaker, J. Deep learning for\nNLP and speech recognition, volume 84. Springer, 2019.\nKingma, D. P. and Ba, J. Adam: A method for stochastic\noptimization. arXiv preprint arXiv:1412.6980, 2014.\nKnopp, K. Weierstrass\u2019s factor-theorem. In Theory of\nFunctions: Part II, pp. 1\u20137. Dover, 1996.\nKopetzki, A.-K., Charpentier, B., Z\u00a8ugner, D., Giri, S., and\nG\u00a8unnemann, S. Evaluating robustness of predictive un-\ncertainty estimation: Are dirichlet-based models reliable?\nIn International Conference on Machine Learning , pp.\n5707\u20135718. PMLR, 2021.\nKrizhevsky, A., Hinton, G., et al. Learning multiple layers\nof features from tiny images. -, 2009.\nLakshminarayanan, B., Pritzel, A., and Blundell, C. Simple\nand scalable predictive uncertainty estimation using deep\nensembles. Advances in neural information processing\nsystems, 30, 2017.\nLeCun, Y . The mnist database of handwritten digits.\nhttp://yann. lecun. com/exdb/mnist/, 1998.\nLeCun, Y ., Haffner, P., Bottou, L., and Bengio, Y . Ob-\nject recognition with gradient-based learning. In Shape,\ncontour and grouping in computer vision, pp. 319\u2013345.\nSpringer, 1999.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3045, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c69362e8-cd2d-4de7-bb1f-8fa6435e5671": {"__data__": {"id_": "c69362e8-cd2d-4de7-bb1f-8fa6435e5671", "embedding": null, "metadata": {"page_label": "10", "file_name": "Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_type": "application/pdf", "file_size": 2538334, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "fb69018b-7e55-40cf-a486-dda2ebb7e8ce", "node_type": "4", "metadata": {"page_label": "10", "file_name": "Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_type": "application/pdf", "file_size": 2538334, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "b7da5a3ef1dcc7520c56047ada272e1271b9996ebf33a9dd2c3a5795fcfdff2f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7ea043a2-7d33-46ef-9cde-d1204f2caf9f", "node_type": "1", "metadata": {"page_label": "10", "file_name": "Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_type": "application/pdf", "file_size": 2538334, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "f23f416c0f32721eb477af9500cbb37566990345504c91b20e1927157c9506e3", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "5707\u20135718. PMLR, 2021.\nKrizhevsky, A., Hinton, G., et al. Learning multiple layers\nof features from tiny images. -, 2009.\nLakshminarayanan, B., Pritzel, A., and Blundell, C. Simple\nand scalable predictive uncertainty estimation using deep\nensembles. Advances in neural information processing\nsystems, 30, 2017.\nLeCun, Y . The mnist database of handwritten digits.\nhttp://yann. lecun. com/exdb/mnist/, 1998.\nLeCun, Y ., Haffner, P., Bottou, L., and Bengio, Y . Ob-\nject recognition with gradient-based learning. In Shape,\ncontour and grouping in computer vision, pp. 319\u2013345.\nSpringer, 1999.\nMalinin, A. and Gales, M. Predictive uncertainty estima-\ntion via prior networks. Advances in neural information\nprocessing systems, 31, 2018.\nMalinin, A. and Gales, M. Reverse kl-divergence training\nof prior networks: Improved uncertainty and adversarial\nrobustness. Advances in Neural Information Processing\nSystems, 32, 2019.\nMobiny, A., Yuan, P., Moulik, S. K., Garg, N., Wu, C. C.,\nand Van Nguyen, H. Dropconnect is effective in modeling\nuncertainty of bayesian deep networks. Scientific reports,\n11(1):1\u201314, 2021.\nNetzer, Y ., Wang, T., Coates, A., Bissacco, A., Wu, B.,\nand Ng, A. Y . Reading digits in natural images with\nunsupervised feature learning, 2011.\nNguyen, A., Yosinski, J., and Clune, J. Deep neural net-\nworks are easily fooled: High confidence predictions for\nunrecognizable images. In Proceedings of the IEEE con-\nference on computer vision and pattern recognition, pp.\n427\u2013436, 2015.\nPandey, D. S. and Yu, Q. Multidimensional belief quantifi-\ncation for label-efficient meta-learning. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition (CVPR), pp. 14391\u201314400, June 2022a.\nPandey, D. S. and Yu, Q. Evidential conditional neural\nprocesses. arXiv preprint arXiv:2212.00131, 2022b.\nPearce, T., Leibfried, F., and Brintrup, A. Uncertainty in\nneural networks: Approximately bayesian ensembling.\nIn International conference on artificial intelligence and\nstatistics, pp. 234\u2013244. PMLR, 2020.\n10", "mimetype": "text/plain", "start_char_idx": 2455, "end_char_idx": 4493, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a09f5385-a3ea-4ea1-afb4-8b5bbd833531": {"__data__": {"id_": "a09f5385-a3ea-4ea1-afb4-8b5bbd833531", "embedding": null, "metadata": {"page_label": "11", "file_name": "Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_type": "application/pdf", "file_size": 2538334, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "da908028-a38c-419a-a992-d070d12cf349", "node_type": "4", "metadata": {"page_label": "11", "file_name": "Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_type": "application/pdf", "file_size": 2538334, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "796be24385fc27b67f0f11680e26a5ea28a821478d378ef1e5b3cbfc6303ebfb", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Learn to Accumulate Evidence from All Training Samples: Theory and Practice\nSensoy, M., Kaplan, L., and Kandemir, M. Evidential deep\nlearning to quantify classification uncertainty. Advances\nin neural information processing systems, 31, 2018.\nShafer, G. A mathematical theory of evidence, volume 42.\nPrinceton university press, 1976.\nShi, W., Zhao, X., Chen, F., and Yu, Q. Multifaceted\nuncertainty estimation for label-efficient deep learning.\nAdvances in neural information processing systems, 33,\n2020.\nSingh, S. P., Kumar, A., Darbari, H., Singh, L., Rastogi, A.,\nand Jain, S. Machine translation using deep learning: An\noverview. In 2017 international conference on computer,\ncommunications and electronics (comptelix) , pp. 162\u2013\n167. IEEE, 2017.\nTomani, C. and Buettner, F. Towards trustworthy predictions\nfrom deep neural networks with fast adversarial calibra-\ntion. In Proceedings of the AAAI Conference on Artificial\nIntelligence, volume 35, pp. 9886\u20139896, 2021.\nUlmer, D. A survey on evidential deep learning for\nsingle-pass uncertainty estimation. arXiv preprint\narXiv:2110.03051, 2021.\nVinyals, O., Blundell, C., Lillicrap, T., Wierstra, D., et al.\nMatching networks for one shot learning. Advances in\nneural information processing systems, 29, 2016.\nV oulodimos, A., Doulamis, N., Doulamis, A., and Protopa-\npadakis, E. Deep learning for computer vision: A brief\nreview. Computational intelligence and neuroscience ,\n2018, 2018.\nZhao, X., Chen, F., Hu, S., and Cho, J.-H. Uncertainty\naware semi-supervised learning on graph data. Advances\nin Neural Information Processing Systems , 33:12827\u2013\n12836, 2020.\n11", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1621, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8e0684d3-82e0-4238-bb63-2de33702345a": {"__data__": {"id_": "8e0684d3-82e0-4238-bb63-2de33702345a", "embedding": null, "metadata": {"page_label": "12", "file_name": "Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_type": "application/pdf", "file_size": 2538334, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "20b8d52b-d00a-448b-bd9e-2fbd198a83dc", "node_type": "4", "metadata": {"page_label": "12", "file_name": "Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_type": "application/pdf", "file_size": 2538334, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "d8fae46af7721d9545f41eb6d839f04cf667d1c0139aa88ab1bef14978a12989", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Learn to Accumulate Evidence from All Training Samples: Theory and Practice\nAppendix\nOrganization of the Appendix\n\u2022 In Section A, we present an analysis of standard classification models trained with cross-entropy loss to show their\nlearning capabilities.\n\u2022 In Section B, we present a complete proof of Theorem 1 for different evidential losses that demonstrates the inability of\nevidential models to learn from zero-evidence samples.\n\u2022 In Section C, we describe different incorrect evidence regularizations used in the existing literature and carry out a\ngradient analysis to study their impact on evidential model learning.\n\u2022 In Section D, we present the proof for Theorem 2 that shows the superiority of exp activation over the SoftPlus and\nReLU functions to transform logits to evidence.\n\u2022 In Section E, we analyze the evidential losses that reveals the theoretical limitation of evidential models trained using\nBayes risk with sum of squares loss.\n\u2022 In Section F, we present additional experiment results, clarifications, hyperparameter details, and discuss some\nlimitations along with possible future works.\nThe source code for the experiments carried out in this work is attached in the supplementary materials and is available at\nthe link: https://github.com/pandeydeep9/EvidentialResearch2023\nA. Standard Classification Model\nConsider a standard cross-entropy based model for K\u2212class classification. Let the overall network be represented by f\u0398(.),\nand let o = f\u0398(x) be the output from this network before the softmax layer for input x and one-hot ground truth label of y.\nThe output after the softmax layer is given by\nsmi = exp(oi)PK\nk=1 exp(ok)\n= exp(oi)\nSce (17)\nWhere Sce = PK\ni=1 exp(oi). The model is trained with cross-entropy loss. For a given sample (x, y), the loss is given by\nLcross-entropy = \u2212\nKX\nk=1\nyk log(smk) = \u2212\nKX\nk=1\nh\nykok \u2212 yk log\n\u0010 KX\ni=1\nexp(oi)\n\u0011i\n(18)\n= log Sce \u2212\nKX\nk=1\nykok (19)\nNow, looking at the gradient of this loss with respect to the pre-softmax values o\ngradk = \u2202Lcross-entropy\n\u2202ok\n=\n\u0010 1\nSce\n\u2202Sce\n\u2202ok\n\u2212 yk\n\u0011\n=\n\u0010exp(ok)\nSce \u2212 yk\n\u0011\n= smk \u2212 yk (20)\nAnalysis of the gradients For Standard Classification Model.\nThe gradient measures the error signal, and for standard classification models, it is bounded in the range [-1, 1] as\n0 \u2264 smk \u2264 1 and yk \u2208 {0, 1}. The model is updated using gradient descent based optimization objectives. For input x, the\nneural network outputs K values o1 to oK, and the corresponding ground truth is y, ygt = 1, y\u0338=gt = 0.\nWhen yi = 0, the gradient signal is gradi = smi and the model optimizes the parameters to minimize this value. Only when\nsmi = 0, the gradient is zero, and the model is not updated. In all other cases when smi \u0338= 0, there is a non-zero gradient\ndependent on smi, and the model is updated to minimize the smi as expected.\n12", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2818, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "9710975d-22ac-432a-ab3a-973c2f3c9b1e": {"__data__": {"id_": "9710975d-22ac-432a-ab3a-973c2f3c9b1e", "embedding": null, "metadata": {"page_label": "13", "file_name": "Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_type": "application/pdf", "file_size": 2538334, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f92eebd4-284e-4e4d-8e4f-6143deed8c60", "node_type": "4", "metadata": {"page_label": "13", "file_name": "Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_type": "application/pdf", "file_size": 2538334, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "1f313f770d52ce241eeb7ae236284a8b7bceb362a8c0ea745d6cffee22152cf7", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Learn to Accumulate Evidence from All Training Samples: Theory and Practice\nWhen yi = 1, the gradient signal is gradi = smi \u2212 1 and the model optimizes the parameters to minimize this value. As\nsmi \u2208 [0, 1], only when the model outputs a large logit on i (corresponding to the ground truth class) and small logit for\nall other nodes, smi = 1, the gradient is zero, and the model is not updated. In all other cases when smi < 1, there is a\nnon-zero gradient dependent on smi and the model is updated to maximize the smi and minimize all other sm\u0338=i as expected.\nThe gradient signal in standard classification models trained with standard cross-entropy loss is reasonable and enables\nlearning from all the training data samples.\nB. Evidential Classification Models\nTheorem 1: Given a training sample (x, y), if an evidential neural network outputs zero evidence e, then the gradients of\nthe evidential loss evaluated on this training sample over the network parameters reduce to zero.\nProof. In the main paper, we considered a K\u2212class classification problem and a representative evidential model trained\nusing Bayes risk with sum of squares loss (Eqn. 21) in the proof. Following 3 variants of evidential losses ((Sensoy et al.,\n2018)) have been commonly used in evidential classification works:\n1. Bayes risk with sum of squares loss ( i.e., Evidential MSE loss) (Zhao et al., 2020)\nLMSE(x, y) =\nKX\nj=1\n(yj \u2212 \u03b1j\nS )2 + \u03b1j(S \u2212 \u03b1j)\nS2(S + 1) (21)\n2. Bayes risk with cross-entropy loss ( i.e., Evidential CE loss)(Charpentier et al., 2020)\nLCE(x, y) =\nKX\nj=1\nyk\n\u0010\n\u03a8(S) \u2212 \u03a8(\u03b1k)\n\u0011\n(22)\n3. Type II Maximum Likelihood loss ( i.e., Evidential log loss)(Pandey & Yu, 2022a)\nLLog(x, y) =\nKX\nk=1\nyk\n\u0010\nlog(S) \u2212 log(\u03b1k)\n\u0011\n(23)\nFor completeness, we consider all three loss functions used in evidential classification models and carry out their analysis.\nB.1. Gradient of Evidential Activation Functions A(.)\nThree non-linear functions are proposed and commonly used in the existing literature to transform the neural network output\nto evidence: 1) ReLU function, 2) SoftPlus function, and 3) Exponential function. In this section, we compute the\ngradients of the evidence output ei from these non-linear activation functions with respect to the logit input oi\n1. A(.) = ReLU(.) = max(0, .)\nek = ReLU(ok) = max(0, ok) =\u21d2 \u2202ek\n\u2202ok\n=\n(\n0 if ok \u2264 0\n1 otherwise (24)\n2. A(.) = SoftPlus(.) = log(1 + exp(.))\nek = log(exp(ok) + 1) =\u21d2 \u2202ek\n\u2202ok\n= 1\n1 + exp(\u2212ok) = Sigmoid(ok) (25)\n3. A(.) = exp(.)\nek = exp(ok) =\u21d2 \u2202ek\n\u2202ok\n= exp(ok) = ek = \u03b1k \u2212 1 (26)\n13", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2527, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f58f799b-f10d-4bd1-940b-3e69e87e8819": {"__data__": {"id_": "f58f799b-f10d-4bd1-940b-3e69e87e8819", "embedding": null, "metadata": {"page_label": "14", "file_name": "Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_type": "application/pdf", "file_size": 2538334, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "330da648-2bd7-419a-adb7-9d4831526ebd", "node_type": "4", "metadata": {"page_label": "14", "file_name": "Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_type": "application/pdf", "file_size": 2538334, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "30eb55b89184ff7fd1d9d9ee68f9fec8eed03ab928e3882c846f8c3158f8f487", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "908b4d61-a868-4aa2-9042-fa69fddc3cba", "node_type": "1", "metadata": {}, "hash": "2293c7e745abf0e0450c456cc05aba2127bfb4775226ecbacb1ec32c02becb00", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Learn to Accumulate Evidence from All Training Samples: Theory and Practice\nB.2. Evidential Model Trained using Bayes risk with sum of squares loss (i.e., Eqn. 21)\nProof. Consider an input x with one-hot ground truth label of y. Let the ground truth class be g i.e. ygt = 1 , with\ncorresponding Dirichlet parameter \u03b1gt, and y\u0338=gt = 0. Moreover, let o, e, and \u03b1 represent the neural network output vector\nbefore applying the activation A, the evidence vector, and the Dirichlet parameters respectively.\nIn this evidential framework, the loss is given by\nLMSE(x, y) =\nKX\nj=1\n(yj \u2212 \u03b1j\nS )2 + \u03b1j(S \u2212 \u03b1j)\nS2(S + 1) = 1 \u2212 2\u03b1gt\nS +\nP\nk \u03b12\nk\nS2 +\n2 P\ni\nP\nj \u03b1i\u03b1j\nS2(S + 1) (27)\n= 2 \u2212 2\u03b1gt\nS \u2212\n2 P\ni\nP\nj \u03b1i\u03b1j\nS(S + 1) (28)\nNow, consider different components of the loss and compute the gradients of the components with respect to Dirichlet\nparameters \u03b1,\n\u2202 \u03b1gt\nS\n\u2202\u03b1gt\n= 1\nS \u2212 \u03b1gt\nS2 & \u2202 \u03b1gt\nS\n\u2202\u03b1\u0338=gt\n= \u2212\u03b1gt\nS2 =\u21d2 \u2202 \u03b1gt\nS\n\u2202\u03b1k\n= yk\nS \u2212 \u03b1gt\nS2\nThe gradient of the variance term is the same for all the K Dirichlet parameters and is given by\n\u2202\nP\ni\nP\nj \u03b1i\u03b1j\nS(S+1)\n\u2202\u03b1k\n= (S \u2212 \u03b1k)\nS(S + 1) \u2212\n(2S + 1)P\ni\nP\nj \u03b1i\u03b1j\n(S2 + S)2\nNow, the gradient of the loss with respect to the neural network output can be computed using the chain rule as\n\u2202LMSE(x, y)\n\u2202ok\n= \u2202LMSE(x, y)\n\u2202\u03b1k\n\u2202ek\n\u2202ok\n= \u2212\n\u0014\n2\u2202 \u03b1k\nS\n\u2202\u03b1k\n\u2212 2\n\u2202\nP\ni\nP\nj \u03b1i\u03b1j\nS(S+1)\n\u2202\u03b1k\n\u0015\n\u00d7 \u2202ek\n\u2202ok\n=\n\u00142\u03b1gt\nS2 \u2212 2yk\nS \u2212 2(S \u2212 \u03b1k)\nS(S + 1) +\n2(2S + 1)P\ni\nP\nj \u03b1i\u03b1j\n(S2 + S)2\n\u0015\n\u00d7 \u2202ek\n\u2202ok\nCase I: ReLU(.) to transform logits to evidence\nek = ReLU(ok) = max(0, ok) =\u21d2 \u2202ek\n\u2202ok\n=\n(\n1 if ok > 0\no otherwise (29)\nFor zero-evidence sample with ReLU(.) used to transform the logits to evidence, the logits ok satisfy the relationship\nok \u2264 0 \u2200 k =\u21d2 \u2202ek\n\u2202ok\n= 0 =\u21d2 \u2202LMSE(x,y)\n\u2202ok\n= 0\nCase II: SoftPlus(.) to transform logits to evidence\nek = log(exp(ok) + 1) =\u21d2 \u2202ek\n\u2202ok\n= Sigmoid(ok) (30)\nCase II: exp(.) to transform logits to evidence\nek = exp(ok) =\u21d2 \u2202ek\n\u2202ok\n= exp(ok) = \u03b1k \u2212 1 (31)\nFor zero-evidence sample with SoftPlus(.) used to transform the logits to evidence, the logits ok \u2192 \u2212\u221e =\u21d2\nSigmoid(ok) \u2192 0 & \u2202ek\n\u2202ok\n\u2192 0. For zero-evidence sample with exp(.) used to transform the logits to evidence, \u03b1k \u2192\n1 = \u21d2 \u2202ek\n\u2202ok\n\u2192 0. Moreover, there is no term in the first part of the loss gradient (see Eqn. 29) to counterbalance these\nzero-approaching gradients.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2264, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "908b4d61-a868-4aa2-9042-fa69fddc3cba": {"__data__": {"id_": "908b4d61-a868-4aa2-9042-fa69fddc3cba", "embedding": null, "metadata": {"page_label": "14", "file_name": "Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_type": "application/pdf", "file_size": 2538334, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "330da648-2bd7-419a-adb7-9d4831526ebd", "node_type": "4", "metadata": {"page_label": "14", "file_name": "Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_type": "application/pdf", "file_size": 2538334, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "30eb55b89184ff7fd1d9d9ee68f9fec8eed03ab928e3882c846f8c3158f8f487", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f58f799b-f10d-4bd1-940b-3e69e87e8819", "node_type": "1", "metadata": {"page_label": "14", "file_name": "Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_type": "application/pdf", "file_size": 2538334, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "cdafea793a0245ab74f482be9de055f207b2f10b93fb15f7fd5fe5829ca71296", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "to transform logits to evidence\nek = log(exp(ok) + 1) =\u21d2 \u2202ek\n\u2202ok\n= Sigmoid(ok) (30)\nCase II: exp(.) to transform logits to evidence\nek = exp(ok) =\u21d2 \u2202ek\n\u2202ok\n= exp(ok) = \u03b1k \u2212 1 (31)\nFor zero-evidence sample with SoftPlus(.) used to transform the logits to evidence, the logits ok \u2192 \u2212\u221e =\u21d2\nSigmoid(ok) \u2192 0 & \u2202ek\n\u2202ok\n\u2192 0. For zero-evidence sample with exp(.) used to transform the logits to evidence, \u03b1k \u2192\n1 = \u21d2 \u2202ek\n\u2202ok\n\u2192 0. Moreover, there is no term in the first part of the loss gradient (see Eqn. 29) to counterbalance these\nzero-approaching gradients. So, for zero-evidence samples,\n\u2202LMSE(x, y)\n\u2202ok\n= 0 (32)\nSince the gradient of the loss with respect to all the nodes is zero, there is no update to the model from such samples. Thus,\nthe evidential models fail to learn from such zero-evidence samples.\n14", "mimetype": "text/plain", "start_char_idx": 1713, "end_char_idx": 2519, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ac12a5f9-b0c3-4e99-89a3-d5a6bd176038": {"__data__": {"id_": "ac12a5f9-b0c3-4e99-89a3-d5a6bd176038", "embedding": null, "metadata": {"page_label": "15", "file_name": "Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_type": "application/pdf", "file_size": 2538334, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "447bf0b4-af11-4369-84b8-9eaea9c8bab9", "node_type": "4", "metadata": {"page_label": "15", "file_name": "Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_type": "application/pdf", "file_size": 2538334, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "1214e9b7983ec46acb89073977f1012bcb265cc1beb17fa42ef9822a40bd6e28", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Learn to Accumulate Evidence from All Training Samples: Theory and Practice\nB.3. Evidential Model Trained using Type II Maximum Likelihood formulation of Evidential loss (i.e., Eqn. 23)\nConsider a K\u2212class evidential classification model that trains the model using Type II Maximum Likelihood formulation of\nthe evidential loss. Consider an input x with one-hot ground truth label of y, PK\nk=1 yk = 1. For this evidential framework,\nthe Type II Maximum Likelihood loss is given by\nLLog(x, y) =\nKX\nk=1\nyk\n\u0010\nlog(S) \u2212 log(\u03b1k)\n\u0011\n= log S \u2212\nKX\nk=1\nyk log \u03b1k (33)\nTaking the gradient of the loss with the logits o, we get\ngradk = \u2202LLog(x, y)\n\u2202ok\n= 1\nS\n\u2202S\n\u2202ok\n\u2212 yk\n1\n\u03b1k\n\u2202\u03b1k\n\u2202ok\n=\n\u0010 1\nS \u2212 yk\n\u03b1k\n\u0011\u2202ek\n\u2202ok\n(34)\nCase I: ReLU(.) to transform logits to evidence\nFor any zero-evidence sample with ReLU(.) used to transform the logits to evidence, the logits ok satisfy the relationship\nok \u2264 0 \u2200 k =\u21d2 \u2202ek\n\u2202ok\n= 0 =\u21d2 \u2202LLog(x,y)\n\u2202ok\n= 0 \u2200k \u2208 [1, K]\nCase II: SoftPlus(.) to transform logits to evidence. Considering Eqn. 34 and Eqn 25, the gradient of the loss with\nrespect to the logits becomes\ngradk = \u2202LLog(x, y)\n\u2202ok\n=\n\u0010 1\nS \u2212 yk\n\u03b1k\n\u0011\nSigmoid(ok) (35)\nCase III: exp(.) to transform logits to evidence. Considering Eqn. 34 and Eqn 26, the gradient of the loss with respect to\nthe logits becomes\ngradk = \u2202LLog(x, y)\n\u2202ok\n=\n\u0010 1\nS \u2212 yk\n\u03b1k\n\u0011\n(ek) =\n\u0010 1\nS \u2212 yk\n\u03b1k\n\u0011\n(\u03b1k \u2212 1) (36)\nFor zero-evidence sample with SoftPlus(.) used to transform the logits to evidence, the logits ok \u2192 \u2212\u221e =\u21d2\nSigmoid(ok) \u2192 0 & \u2202ek\n\u2202ok\n\u2192 0. Similarly, for zero-evidence sample with exp(.) used to transform the logits to evidence,\n\u03b1k \u2192 1 = \u21d2 \u2202ek\n\u2202ok\n\u2192 0. Moreover, there is no term in the first part of the loss gradient (see Eqn. 35 and Eqn. 36 ) to\ncounterbalance these zero-approaching gradient terms.\nSince the gradient of the loss with respect to all the nodes is zero, there is no update to the model from such samples. Thus,\nthe evidential models trained with Type II Maximum Likelihood formulation of the evidential loss fail to learn from such\nzero-evidence samples.\nB.4. Evidential Model Trained using Bayes risk with cross-entropy formulation of Evidential loss (i.e., Eqn. 22)\nConsider a K\u2212class evidential classification model that trains model using Bayes risk with cross-entropy loss for evidential\nlearning (Eqn. 22). Consider an input x with one-hot ground truth label of y, PK\nk=1 yk = 1. For this evidential framework,\nthe loss is given by\nLCE(x, y) =\nKX\nj=1\nyk\n\u0010\n\u03a8(S) \u2212 \u03a8(\u03b1k)\n\u0011\n= \u03a8(S) \u2212 \u03a8(\u03b1gt) (37)\nWhere \u03b1gt represents the output Dirichlet parameter for the ground truth class i.e. ygt = 1, y\u0338=gt = 0, and \u03a8(.) represents\nthe Digamma function, and for z \u2265 1, is given by\n\u03a8(z) = d\ndz log \u0393(z) = d\ndz\n\u0012\n\u2212 \u03b3z \u2212 log z +\n\u221eX\nn=1\n\u0010z\nn \u2212 log\n\u0000\n1 + z\nn\n\u0001\u0011\u0013\n= \u2212\u03b3 \u2212 1\nz + z\n\u221eX\nn=1\n1\nn(n + z)\n15", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2756, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5eccfe52-5aac-4acb-9d7b-742966373750": {"__data__": {"id_": "5eccfe52-5aac-4acb-9d7b-742966373750", "embedding": null, "metadata": {"page_label": "16", "file_name": "Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_type": "application/pdf", "file_size": 2538334, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0b652de9-e188-4b14-913d-74336aa59bbc", "node_type": "4", "metadata": {"page_label": "16", "file_name": "Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_type": "application/pdf", "file_size": 2538334, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "f85babca168a0c20810b510feb0cf951953d9595136043a84652942aa896e402", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "18acc2cf-6af5-457e-bd1d-328d55dcea96", "node_type": "1", "metadata": {}, "hash": "35b0dcd3c593998dc2e0c259c48610a8a704c585cbd55dfa899b0ba489bb70bb", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Learn to Accumulate Evidence from All Training Samples: Theory and Practice\nHere, \u03b3 is the Euler\u2013Mascheroni constant, and \u0393(.) is the gamma function, Using Weierstass\u2019s definition of gamma function\n(Knopp, 1996) for values outside negative integers that is given by\n\u0393(z) = e\u2212\u03b3z\nz\n\u221eY\nn=1\n\u0010\n1 + z\nn\n\u0011\u22121\ne\nz\nn\nUsing the definition of the digamma functions, the loss updates as\nLCE(x, y) = \u03a8(S) \u2212 \u03a8(\u03b1gt) = 1\n\u03b1gt\n\u2212 1\nS + S\n\u221eX\nn=1\n1\nn(n + S) \u2212 \u03b1gt\n\u221eX\nn=1\n1\nn(n + \u03b1gt) (38)\nThe derivative of the digamma function is bounded and is given by\n\u2202\u03a8(z)\n\u2202z = \u2202\n\u2202z\n\u0012\n\u2212 \u03b3 \u2212 1\nz +\n\u221eX\nn=1\n1\nn \u2212 1\nn + z\n\u0013\n= 1\nz2 +\n\u221eX\nn=1\n1\n(n + z)2\n1\nz2 <\u2202\u03a8(z)\n\u2202z < 1\nz2 + \u03c02\n6 , z \u2265 1\nWith this, we can compute the gradients of the loss with respect to the logits as\ngradk = \u2202LCE(x, y)\n\u2202ok\n= \u2202\n\u2202\u03b1k\n\u0000\n\u03a8(S) \u2212 \u03a8(\u03b1gt)\n\u0001\u2202\u03b1k\n\u2202ok\n=\n\u0010 1\nS2 +\n\u221eX\ni=1\n1\n(n + S)2 \u2212 yk\n\u03b12\ngt\n\u2212\n\u221eX\ni=1\nyk\n(n + \u03b1gt)2\n\u0011\u2202ek\n\u2202ok\n(39)\nCase I: ReLU(.) to transform logits to evidence\nFor any zero-evidence sample with ReLU(.) used to transform the logits to evidence, the logits ok satisfy the relationship\nok \u2264 0 \u2200 k =\u21d2 \u2202ek\n\u2202ok\n= 0 =\u21d2 \u2202LCE(x,y)\n\u2202ok\n= 0 \u2200k \u2208 [1, K]\nCase II: SoftPlus(.) to transform logits to evidence. Considering Eqn. 25 and Eqn 39, the gradient of the loss with\nrespect to the logits becomes\ngradk = \u2202LCE(x, y)\n\u2202ok\n=\n\u0010 1\nS2 +\n\u221eX\ni=1\n1\n(n + S)2 \u2212 yk\n\u03b12\ngt\n\u2212\n\u221eX\ni=1\nyk\n(n + \u03b1gt)2\n\u0011\nSigmoid(ok) (40)\nCase III: exp(.) to transform logits to evidence. Considering Eqn. 26 and Eqn 39, the gradient of the loss with respect to\nthe logits becomes\ngradk = \u2202LCE(x, y)\n\u2202ok\n=\n\u0010 1\nS2 +\n\u221eX\ni=1\n1\n(n + S)2 \u2212 yk\n\u03b12\ngt\n\u2212\n\u221eX\ni=1\nyk\n(n + \u03b1gt)2\n\u0011\n(\u03b1k \u2212 1) (41)\nFor zero-evidence sample with SoftPlus(.) used to transform the logits to evidence, the logits ok \u2192 \u2212\u221e =\u21d2\nSigmoid(ok) \u2192 0 & \u2202ek\n\u2202ok\n\u2192 0. Similarly, for zero-evidence sample with exp(.) used to transform the logits to evidence,\n\u03b1k \u2192 1 = \u21d2 \u2202ek\n\u2202ok\n\u2192 0. Moreover, there is no term in the first part of the loss gradient (see Eqn. 29) to counterbalance\nthese zero-approaching gradient terms.\nThe gradient of the loss with respect to all the nodes is zero for all the considered cases. Since the gradient of the loss with\nrespect to all the nodes is zero for all three cases, there is no update to the model from such samples. Thus, the evidential\nmodels fail to learn from such zero-evidence samples in all cases.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2299, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "18acc2cf-6af5-457e-bd1d-328d55dcea96": {"__data__": {"id_": "18acc2cf-6af5-457e-bd1d-328d55dcea96", "embedding": null, "metadata": {"page_label": "16", "file_name": "Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_type": "application/pdf", "file_size": 2538334, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0b652de9-e188-4b14-913d-74336aa59bbc", "node_type": "4", "metadata": {"page_label": "16", "file_name": "Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_type": "application/pdf", "file_size": 2538334, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "f85babca168a0c20810b510feb0cf951953d9595136043a84652942aa896e402", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5eccfe52-5aac-4acb-9d7b-742966373750", "node_type": "1", "metadata": {"page_label": "16", "file_name": "Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_type": "application/pdf", "file_size": 2538334, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "5d9bc0855c9dbaea14f321852f9642d2865fca82c94bc75cb738f5a9547ac196", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "used to transform the logits to evidence, the logits ok \u2192 \u2212\u221e =\u21d2\nSigmoid(ok) \u2192 0 & \u2202ek\n\u2202ok\n\u2192 0. Similarly, for zero-evidence sample with exp(.) used to transform the logits to evidence,\n\u03b1k \u2192 1 = \u21d2 \u2202ek\n\u2202ok\n\u2192 0. Moreover, there is no term in the first part of the loss gradient (see Eqn. 29) to counterbalance\nthese zero-approaching gradient terms.\nThe gradient of the loss with respect to all the nodes is zero for all the considered cases. Since the gradient of the loss with\nrespect to all the nodes is zero for all three cases, there is no update to the model from such samples. Thus, the evidential\nmodels fail to learn from such zero-evidence samples in all cases.\nC. Regularization in the Evidential Classification Models\nBased on the evidence e, beliefs b, and the Dirichlet parameters \u03b1, various regularization terms have been introduced that\naim to penalize the incorrect evidence/incorrect belief of the model, leading to the model with accurate uncertainty estimates.\nHere, we briefly summarize the key regurlaizations:\n16", "mimetype": "text/plain", "start_char_idx": 1632, "end_char_idx": 2663, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1e350745-3144-4ee9-8bba-511dd6041377": {"__data__": {"id_": "1e350745-3144-4ee9-8bba-511dd6041377", "embedding": null, "metadata": {"page_label": "17", "file_name": "Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_type": "application/pdf", "file_size": 2538334, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c4de1146-dcb5-4171-ae01-d6a4582b9142", "node_type": "4", "metadata": {"page_label": "17", "file_name": "Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_type": "application/pdf", "file_size": 2538334, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "fb2ac5b422e5ab8c2f16399b7adc8377342189173f341b31046b135aa79fadef", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Learn to Accumulate Evidence from All Training Samples: Theory and Practice\n1. Introduce a forward KL regularization term as in EDL (Sensoy et al., 2018) that regularizes the model to output no\nincorrect evidence.\nLEDL\nreg(x, y) = KL\n\u0000\nDir(p|\u02dc\u03b1)||Dir(p|1)\n\u0001\n= log\n\u0010 \u0393 PK\nk=1 \u02dc\u03b1k\n\u0393(K) QK\nk=1 \u0393\u02dc\u03b1k\n\u0011\n+\nKX\nk=1\n(\u02dc\u03b1k \u2212 1)\n\u0014\n\u03c8(\u02dc\u03b1k) \u2212 \u03c8\n\u0010 KX\nj=1\n\u02dc\u03b1j\n\u0011\u0015\n(42)\nWhere \u02dc\u03b1 = y + (1 \u2212 y) \u2299 \u03b1 = (\u02dc\u03b11, \u02dc\u03b12, ...\u02dc\u03b1N ) parameterize a dirichlet distribution, \u02dc\u03b1i=gt = 1, \u02dc\u03b1i = \u03b1i\u2200i \u0338= gt.\nHere, the KL regularization term encourages the Dirichlet distribution based on the incorrect evidence i.e., Dir(p|\u02dc\u03b1)\nto be flat which is possible when there is no incorrect evidence. From Eqn. 42, we can see that the regularization\nterm, introduces digamma functions for the loss and may require evaluation of higher-order polygamma functions for\nchallenging problems (e.g. involving bi-level optimizations as in MAML (Finn et al., 2017)).\n2. Introduce an incorrect evidence regularization term as in ADL (Shi et al., 2020) that is the sum of the incorrect evidence\nfor a sample\nLADL\nreg(x, y) =\nKX\nk=1\n\u0000\ne \u2299 (1 \u2212 y)\n\u0001\nk =\nKX\nk=1\nek \u00d7 (1 \u2212 yk) (43)\nHere, \u2299 represents element-wise product. The evidence for a class ek is only restricted to be non-negative and can take\nlarge positive values leading to large variation in the overall loss.\n3. Introduce incorrect belief-based regularization as in Units-ML (Pandey & Yu, 2022a)\nLUnits\nreg (x, y) =\nKX\nk=1\n\u0000e\nS \u2299 (1 \u2212 y)\n\u0001\nk =\nKX\nk=1\nek\nS \u00d7 (1 \u2212 yk) (44)\nThe regularization value is bounded to be in a range of [0, 1] for all the data samples, no matter how severe the mistake\nis.\nAll three regularizations aim to guide the model such that the incorrect evidence is minimized (ideally close to zero). These\nregularizations help the evidential model acquire desired uncertainty quantification capabilities in evidential models. Such\nguidance is expected to update the model such that it maps input samples near zero-evidence regions in the evidence space.\nThus, the regularization does not help address the issue of learning from zero-evidence samples and is likely to hurt the\nmodel\u2019s learning capabilities.\nC.1. Gradient Analysis of the Incorrect Evidence Regularizations\nThe regularization terms use ground truth information to consider only the incorrect evidence. Thus, the gradient of the\nregularization loss with respect to the ground truth node \u03b1gt is 0. In this analysis, we consider the gradient with respect to\nnon-ground truth nodes i.e. \u03b1k, and ok, k\u0338= gt.\n1. Gradient for EDL regularization (Eqn. 42 )\nLEDL\nreg(x, y) = KL\n\u0000\nDir(p|\u02dc\u03b1)||Dir(p|1)\n\u0001\n= log\n\u0010 \u0393 PK\nk=1 \u02dc\u03b1k\n\u0393(K) QK\nk=1 \u0393\u02dc\u03b1k\n\u0011\n+\nKX\nk=1\n(\u02dc\u03b1k \u2212 1)\n\u0014\n\u03c8(\u02dc\u03b1k) \u2212 \u03c8\n\u0010 KX\nj=1\n\u02dc\u03b1j\n\u0011\u0015\n= log \u0393(S \u2212 \u03b1gt) \u2212 log \u0393(K) \u2212\nKX\nk=1\nlog \u0393\u02dc\u03b1k +\nKX\nk=1\n(\u02dc\u03b1k \u2212 1)\n\u0014\n\u03c8(\u02dc\u03b1k) \u2212 \u03c8(S \u2212 \u03b1gt)\n\u0015 (45)\n17", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2761, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a8aa00a7-78b7-4473-ab84-9a0578b06179": {"__data__": {"id_": "a8aa00a7-78b7-4473-ab84-9a0578b06179", "embedding": null, "metadata": {"page_label": "18", "file_name": "Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_type": "application/pdf", "file_size": 2538334, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "38105673-de91-4a2d-971a-90ea9d6486b9", "node_type": "4", "metadata": {"page_label": "18", "file_name": "Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_type": "application/pdf", "file_size": 2538334, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "6239c96cd988c477663e7e295d3cb5fb658edb39295b965ce95a7c7a41e9589e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fcb6ab74-30fc-4a18-b291-a4f85e9481b0", "node_type": "1", "metadata": {}, "hash": "39c246bbb4e9941abba82816d130a45082679b94576ccf9657a12c7dce73708d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Learn to Accumulate Evidence from All Training Samples: Theory and Practice\n\u2202LEDL\nreg(x, y)\n\u2202\u03b1k\n= \u2202\n\u2202\u03b1k\n\u0012\nlog \u0393(S \u2212 \u03b1gt) \u2212 log \u0393(K) \u2212\nKX\nk=1\nlog \u0393\u02dc\u03b1k +\nKX\nk=1\n(\u02dc\u03b1k \u2212 1)\n\u0014\n\u03c8(\u02dc\u03b1k) \u2212 \u03c8(S \u2212 \u03b1gt)\n\u0015\u0013\n= \u03c8(S \u2212 \u03b1gt) \u2212 \u03c8(\u03b1k) + \u2202\n\u2202\u03b1k\n\u0012 KX\nk=1\n(\u02dc\u03b1k \u2212 1)\n\u0014\n\u03c8(\u02dc\u03b1k) \u2212 \u03c8(S \u2212 \u03b1gt)\n\u0015\u0013\n= \u03c8(S \u2212 \u03b1gt) \u2212 \u03c8(\u03b1k) + \u03c8(\u03b1k) \u2212 \u03c8(S \u2212 \u03b1gt) + (\u03b1k \u2212 1) \u2202\n\u2202\u03b1k\n\u0012\n\u03c8(\u02dc\u03b1k) \u2212 \u03c8(S \u2212 \u03b1gt)\n\u0013\n= (\u03b1k \u2212 1) \u2202\n\u2202\u03b1k\n\u0012\n\u03c8(\u03b1k) \u2212 \u03c8(S \u2212 \u03b1gt)\n\u0013\n= (\u03b1k \u2212 1)\n\u0000\n\u03c81(\u03b1k) \u2212 \u03c81(S \u2212 \u03b1gt)\n\u0001\nWhere \u03c81 is the trigamma function. Further, using the definition of trigamma function,\n\u2202LEDL\nreg(x, y)\n\u2202\u03b1k\n= (\u03b1k \u2212 1)\n\u0000\n\u03c81(\u03b1k) \u2212 \u03c81(S \u2212 \u03b1gt)\n\u0001\n= (\u03b1k \u2212 1)\n\u0012 \u221eX\nn=0\n1\n(n + \u03b1k)2 \u2212 1\n(n + S \u2212 \u03b1gt)2\n\u0013\n(46)\nNow, the gradients with respect to the logits ok becomes\n\u2202LEDL\nreg(x, y)\n\u2202ok\n= \u2202LEDL\nreg(x, y)\n\u2202\u03b1k\n\u2202\u03b1k\n\u2202ok\n= (\u03b1k \u2212 1)\n\u0012 \u221eX\nn=0\n1\n(n + \u03b1k)2 \u2212 1\n(n + S \u2212 \u03b1gt)2\n\u0013\n\u00d7 \u2202ek\n\u2202ok\n(47)\nCase I: ReLU(.) to transform logits to evidence. The gradients with respect to the logits ok for zero evidence is zero.\nFor all non-zero evidence, the gradient updates as \u2202ek\n\u2202ok\n= 1\u2200ek > 0 and\n\u2202LEDL\nreg(x, y)\n\u2202ok\n= (\u03b1k \u2212 1)\n\u0012 \u221eX\nn=0\n1\n(n + \u03b1k)2 \u2212 1\n(n + S \u2212 \u03b1gt)2\n\u0013\n(48)\nNow, when \u03b1k \u2192 \u221e, the value of the gradient\n\u2202LEDL\nreg(x,y)\n\u2202ok\n\u2192 0. There is close to zero model update from regularization\nfor very large incorrect evidence.\nCase II: SoftPlus(.) to transform logits to evidence. The gradients with respect to the logits ok is given by the\nsigmoid i.e. \u2202ek\n\u2202ok\n= sigmoid(ok) , limok\u2192\u221e\n\u2202ek\n\u2202ok\n= 1, and\n\u2202LEDL\nreg(x, y)\n\u2202ok\n= (\u03b1k \u2212 1)\n\u0012 \u221eX\nn=0\n1\n(n + \u03b1k)2 \u2212 1\n(n + S \u2212 \u03b1gt)2\n\u0013\n\u03c3(\u03b1k \u2212 1) (49)\nNow, similar to ReLU, when \u03b1k \u2192 \u221e, the value of the gradient\n\u2202LEDL\nreg(x,y)\n\u2202ok\n\u2192 0. There is close to zero model update\nfrom regularization for very large incorrect evidence.\nCase III: exp(.) to transform logits to evidence. When using exponential non-linearity to transform the neural network\noutput to evidence, the \u03b1k is given by \u03b1k = exp(ok) + 1, \u2202\u03b1k\n\u2202ok\n= \u03b1k \u2212 1.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1866, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "fcb6ab74-30fc-4a18-b291-a4f85e9481b0": {"__data__": {"id_": "fcb6ab74-30fc-4a18-b291-a4f85e9481b0", "embedding": null, "metadata": {"page_label": "18", "file_name": "Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_type": "application/pdf", "file_size": 2538334, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "38105673-de91-4a2d-971a-90ea9d6486b9", "node_type": "4", "metadata": {"page_label": "18", "file_name": "Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_type": "application/pdf", "file_size": 2538334, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "6239c96cd988c477663e7e295d3cb5fb658edb39295b965ce95a7c7a41e9589e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a8aa00a7-78b7-4473-ab84-9a0578b06179", "node_type": "1", "metadata": {"page_label": "18", "file_name": "Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_type": "application/pdf", "file_size": 2538334, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "1cc6dbdc5b78cac6a413e085fe02f001ba7397939feafe7cf2a54ac801a19daf", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "There is close to zero model update\nfrom regularization for very large incorrect evidence.\nCase III: exp(.) to transform logits to evidence. When using exponential non-linearity to transform the neural network\noutput to evidence, the \u03b1k is given by \u03b1k = exp(ok) + 1, \u2202\u03b1k\n\u2202ok\n= \u03b1k \u2212 1. Now the gradients with respect to the neural\nnetwork output ok becomes:\n\u2202L2\nreg(x, y)\n\u2202ok\n= \u2202L2\nreg(x, y)\n\u2202\u03b1k\n\u00d7 \u2202\u03b1k\n\u2202ok\n= (\u03b1k \u2212 1)2\n\u0012 \u221eX\nn=0\n1\n(n + \u03b1k)2 \u2212 1\n(n + S \u2212 \u03b1gt)2\n\u0013\n(50)\nHere, the gradient values increase as \u03b1k \u2192 \u221e, and the gradient values do not vanish. Simply, as the incorrect evidence\nbecomes very large, the model updates also become large in the accurate direction.\nThus, considering Case I, II, and II, we see that the incorrect evidence-based regularization with forward KL divergence\nis not effective in regions of incorrect evidence when using ReLu and SoftPlus functions to transform logits to\nevidence. This issue of correcting very large incorrect evidence does not appear when using exp function to transform\nthe logits into evidence.\n18", "mimetype": "text/plain", "start_char_idx": 1582, "end_char_idx": 2627, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "bb62b73e-655e-4323-80db-c2c1f1036634": {"__data__": {"id_": "bb62b73e-655e-4323-80db-c2c1f1036634", "embedding": null, "metadata": {"page_label": "19", "file_name": "Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_type": "application/pdf", "file_size": 2538334, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "38db0674-39e1-4922-9a0f-f14bcb4b57e1", "node_type": "4", "metadata": {"page_label": "19", "file_name": "Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_type": "application/pdf", "file_size": 2538334, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "1dfb95ecbf882895dec19e158cd6be3e4f082d039148a31ebc5ad2f6384da22a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Learn to Accumulate Evidence from All Training Samples: Theory and Practice\n2. Gradient for ADL regularization ((Shi et al., 2020) )\nLADL\nreg(x, y) =\nKX\nk=1\n\u0000\ne \u2299 (1 \u2212 y)\n\u0001\nk =\nKX\nk=1\nek \u00d7 (1 \u2212 yk) = S \u2212 K \u2212 \u03b1gt + 1 (51)\nConsidering the gradient of the regularization with respect to the parameters \u03b1k, k\u0338= gt, and corresponding logits ok,\nwe get\n\u2202LADL\nreg(x, y)\n\u2202\u03b1k\n= 1 = \u21d2 \u2202LADL\nreg(x, y)\n\u2202ok\n= \u2202ek\nok\n(52)\nWhen considering the exp function to transform logits to evidence, \u2202ek\nok\n= ek = exp(ok) and the gradient value\nbecomes very large when the model\u2019s predicted incorrect evidence value is large. This may lead to exploding gradients\nand stability issues in the model training. For ReLU and SoftPlus functions, the gradients in positive evidence\nregions are \u2202ek\nok\n= 1, and \u2202ek\nok\n= \u03c3(ok) respectably. Thus, the gradient and corresponding model updates for high\nincorrect evidence are as desired.\n3. Gradient analysis of incorrect belief regularization term as in Units-ML(Pandey & Yu, 2022a)\nLUnits\nreg (x, y) =\nKX\nk=1\n\u0000e\nS \u2299 (1 \u2212 y)\n\u0001\nk =\nKX\nk=1\nek\nS \u00d7 (1 \u2212 yk) = 1\nS\n\u0000\nS \u2212 K \u2212 \u03b1gt + 1\n\u0001\n(53)\nThe regularization value is bounded to be in a range of [0, 1] for all the data samples, no matter how severe the mistake\nwhich may limit its effectiveness. Next, the gradient of the regularization with respect to the parameters \u03b1k, and logits\nok is given by\n\u2202LUnits\nreg (x, y)\n\u2202\u03b1k\n=\n\u2202\n\u0010\n1\nS\n\u0000\nS \u2212 K \u2212 \u03b1gt + 1\n\u0001\u0011\n\u2202\u03b1k\n= \u03b1gt + K \u2212 1\nS2 = egt + K\n(K + PK\nk=1 ek)2\n(54)\n\u2202L3\nreg(x, y)\n\u2202ok\n= \u2202LUnits\nreg (x, y)\n\u2202\u03b1k\n\u00d7 \u2202\u03b1k\n\u2202ok\n= egt + K\nS2 \u00d7 \u2202ek\n\u2202ok\n(55)\nThe gradient value decreases as the number of classes K in the classification problem increases. For all three\ntransformations: ReLU, SoftPlus, and exp to transform logits to evidence, the gradients will go to zero as the\nincorrect evidence increases i.e. ek \u2192 \u221eand S \u2192 \u221e=\u21d2\n\u2202L3\nreg(x,y)\n\u2202ok\n\u2192 0. So, the regularization may be ineffective\nwhen the incorrect evidence is very high.\nD. Impact of Non-linear Transformation\nTheorem 2: For a data sample x, if an evidential model outputs logits ok \u2264 0 \u2200k \u2208 [0, K], the exponential activation\nfunction leads to a larger gradident update on the model parameters than softplus and ReLu.\nProof. Consider an evidential loss L, which is formally defined in Eqns. (21), (22), and (23), is used to train the evidential\nmodel, let o, e \u2208 RK denote the neural network output vector before applying the activation A, and the evidence vector,\nrespectively, for a network with weight w. For a data sample x, if the network outputs ok < 0, \u2200k \u2208 [K], we have:\n1. ReLu:\n\u2202L1\n\u2202w =\nX\nk\n\u2202L1\n\u2202ek\n\u2202ek\n\u2202ok\n\u2202ok\n\u2202w = 0 (see Eqn. 8),\n2. SoftPlus:\n\u2202L2\n\u2202w =\nX\nk\n\u2202L2\n\u2202ek\n\u2202ek\n\u2202ok\n\u2202ok\n\u2202w =\nX\nk\n\u2202L2\n\u2202ek\n\u2202ok\n\u2202w Sigmoid(ok) ( see Eqn. 9),\n19", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2698, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "32e163df-c232-4caf-9175-bd3366f96b11": {"__data__": {"id_": "32e163df-c232-4caf-9175-bd3366f96b11", "embedding": null, "metadata": {"page_label": "20", "file_name": "Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_type": "application/pdf", "file_size": 2538334, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7caeaab5-80fd-4d57-b502-1d62c9fb45d3", "node_type": "4", "metadata": {"page_label": "20", "file_name": "Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_type": "application/pdf", "file_size": 2538334, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "6027cf2a7f7f4df25a01155118d62ecbd2217cf383fcd1c82d0b0f33acadb941", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "22040c12-6938-4add-997e-e8a4f7f5e731", "node_type": "1", "metadata": {}, "hash": "38d71b8a635ad9170252e66fa422d59a56f6115b940be170f9cbf0474b52dee1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Learn to Accumulate Evidence from All Training Samples: Theory and Practice\n3. Exponential:\n\u2202L3\n\u2202w =\nX\nk\n\u2202L3\n\u2202ek\n\u2202ek\n\u2202ok\n\u2202ok\n\u2202w =\nX\nk\n\u2202L3\n\u2202ek\n\u2202ok\n\u2202w exp(ok) =\nX\nk\n\u2202L3\n\u2202ek\n\u2202ok\n\u2202w {[1 + exp(ok)]Sigmoid(ok)} (see Eqn. 10)\nThus, we have \u2202L3\n\u2202w \u2265 \u2202L2\n\u2202w \u2265 \u2202L1\n\u2202w , which implies that A = exp leads to a larger update to the network than both Softplus\nand ReLu. This completes the proof. Now we carry out an analysis of the three activations.\nAnalysis:\nConsider a representative K\u2212class evidential classification model that trains using Type II Maximum Likelihood evidential\nloss. Consider an input x with one-hot label of y, PK\nk=1 yk = 1. For this evidential framework, the Type II Maximum\nLikelihood loss (LLog(x, y)) and its gradient with the logits o ( Eqn. 34) are given by\nLLog(x, y) = log S \u2212\nKX\nk=1\nyk log \u03b1k & gradk = \u2202LLog(x, y)\n\u2202ok\n=\n\u0010 1\nS \u2212 yk\n\u03b1k\n\u0011\u2202ek\n\u2202ok\n(56)\nCase I and II: ReLU(.) and SoftPlus(.) to transform logits to evidence.\n\u2022 Zero evidence region: For ReLU(.) based evidential models, if the logits value for class k i.e. ok is negative, then\nthe corresponding evidence for class k i.e. ek = 0, \u2202ek\n\u2202ok\n= 0 & gradk = \u2202LLog(x,y)\n\u2202ok\n= 0. So, there is no update to\nthe model through the nodes that output negative logits value. In the case of SoftPlus(.) based evidential models,\nthere is no update to the model when training samples lie in zero-evidence regions. This is possible in the condition of\nok \u2192 \u2212\u221e. In other cases, there will be some small finite small update in the accurate direction from the gradient.\n\u2022 Range of gradients: The range of gradients for both ReLU(.) and SoftPlus(.) based evidential models are\nidentical. Considering the gradient for the ground truth node i.e.yk = 1, the range of gradients is [ 1\nK \u2212 1, 0]. For\nall other nodes other than the ground truth node i.e. yk = 0, the range of gradients is [0, 1\nK ]. So, for classification\nproblems with a large number of classes, the gradient updates to the nodes that do not correspond to the ground truth\nclass will be bounded in a small range and is likely to be very small.\n\u2022 High incorrect evidence region: If the evidence for class k is very large i.e. ek \u2192 \u221e, then for ReLU(.), \u2202ek\nok\n= 1,\nand for SoftPlus(.), \u2202ek\nok\n= Sigmoid(ok) \u2192 1, 1\n\u03b1k\n= 1\nek+1 \u2192 0, 1\nS \u2192 0, & gradk = \u2202LLog(x,y)\n\u2202ok\n\u2192 0. For large\npositive model evidence, there is no update to the corresponding node of the neural network. The evidence can be\nfurther broken down into correct evidence (corresponding to the evidence for the ground truth class), and incorrect\nevidence (corresponding to the evidence for any other class other than the ground truth class). When the correct class\nevidence is large, the corresponding gradient is close to zero and there is no update to the model parameters which\nis desired. When the incorrect evidence is large, the model should be updated to minimize such incorrect evidence.\nHowever, the evidential models with ReLU and Softplus fail to minimize incorrect evidence when the incorrect\nevidence value is large. These necessities the need for incorrect evidence regularization terms.\nCase III: exp(.) to transform logits to evidence. Considering Eqn. Eqn.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3156, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "22040c12-6938-4add-997e-e8a4f7f5e731": {"__data__": {"id_": "22040c12-6938-4add-997e-e8a4f7f5e731", "embedding": null, "metadata": {"page_label": "20", "file_name": "Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_type": "application/pdf", "file_size": 2538334, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7caeaab5-80fd-4d57-b502-1d62c9fb45d3", "node_type": "4", "metadata": {"page_label": "20", "file_name": "Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_type": "application/pdf", "file_size": 2538334, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "6027cf2a7f7f4df25a01155118d62ecbd2217cf383fcd1c82d0b0f33acadb941", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "32e163df-c232-4caf-9175-bd3366f96b11", "node_type": "1", "metadata": {"page_label": "20", "file_name": "Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_type": "application/pdf", "file_size": 2538334, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "1dc2716a6a68fb0b13b746129e13cdcac6bb1c13f3c471947feca2d48db65963", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "For large\npositive model evidence, there is no update to the corresponding node of the neural network. The evidence can be\nfurther broken down into correct evidence (corresponding to the evidence for the ground truth class), and incorrect\nevidence (corresponding to the evidence for any other class other than the ground truth class). When the correct class\nevidence is large, the corresponding gradient is close to zero and there is no update to the model parameters which\nis desired. When the incorrect evidence is large, the model should be updated to minimize such incorrect evidence.\nHowever, the evidential models with ReLU and Softplus fail to minimize incorrect evidence when the incorrect\nevidence value is large. These necessities the need for incorrect evidence regularization terms.\nCase III: exp(.) to transform logits to evidence. Considering Eqn. Eqn. 34 and Eqn 26, the gradient of the loss with respect\nto the logits becomes\ngradk = \u2202LLog(x, y)\n\u2202ok\n=\n\u0010 1\nS \u2212 yk\n\u03b1k\n\u0011\n(ek) =\n\u0010 1\nS \u2212 yk\n\u03b1k\n\u0011\n(\u03b1k \u2212 1) (57)\n\u2022 Zero evidence region: In case of exp(.) based evidential models, except in the extreme cases of \u03b1k \u2192 \u221e, there will\nbe some signal to guide the model. In cases outside the zero-evidence region (i.e. outside \u03b1k \u2192 \u221e), there will be\nsome finite small update in the accurate direction from the gradient. Moreover, for same evidence values, the gradient\nof exp based model is larger than the SoftPlus based evidential model by a factor of 1 + exp(ok). Compared to\nSoftPlus models, the larger gradient is expected to help the model learn faster in low-evidence regions.\n\u2022 Range of gradients: For the ground truth node i.e.yk = 1, the range of gradients is [\u22121, 0]. For all nodes other than\nthe ground truth node i.e. yk = 0, the range of gradients is [0, 1]. Thus, the gradients are expected to be more expressive\nand accurate in guiding the evidential model compared to ReLU and SoftPlus based evidential models.\n20", "mimetype": "text/plain", "start_char_idx": 2290, "end_char_idx": 4222, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "6a8bc2a1-98b6-4717-8768-a52595020e6a": {"__data__": {"id_": "6a8bc2a1-98b6-4717-8768-a52595020e6a", "embedding": null, "metadata": {"page_label": "21", "file_name": "Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_type": "application/pdf", "file_size": 2538334, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d35c9166-27f9-406f-b1d6-a1ed2f8edd52", "node_type": "4", "metadata": {"page_label": "21", "file_name": "Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_type": "application/pdf", "file_size": 2538334, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "b1a1b4fe83f282a68f2a6b40b7d692fe5a0592789d768e793a36f446716f3c01", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Learn to Accumulate Evidence from All Training Samples: Theory and Practice\n\u2022 High evidence region: If the evidence for class k is very large i.e. ek \u2192 \u221e, then \u03b1k \u22121 \u2248 \u03b1k and gradk = smk \u2212yk.\nIn other words, the model\u2019s gradient updates become identical to the standard classification model (see Section A)\nwithout any learning issues.\nDue to smaller zero-evidence region, more expressive gradients, and no issue of learning in high incorrect evidence region,\nthe exponential-based evidential models are expected to be more effective compared to ReLU and SoftPlus based\nevidential models.\nE. Analysis of Evidential Losses\nHere, we analyze the three variants of evidential loss. As seen in Section D, exp function is expected to be superior to\nReLU and SoftPlus functions to transform the logits to evidence. Thus, in this section, we consider exp function to\ntransform the logits into evidence. However, the analysis holds true for all three functions.\n1. Bayes risk with the sum of squares loss (Eqn. 21)\nLMSE(x, y) =\nKX\nj=1\n(yj \u2212 \u03b1j\nS )2 + \u03b1j(S \u2212 \u03b1j)\nS2(S + 1) (58)\nThe loss can be simplified as\nLMSE(x, y) =\nKX\nj=1\n(yj \u2212 \u03b1j\nS )2 + \u03b1j(S \u2212 \u03b1j)\nS2(S + 1) (59)\n= 1 \u2212 2\u03b1gt\nS +\nP\nk \u03b12\nk\nS2 +\n2 P\ni\nP\nj \u03b1i\u03b1j\nS2(S + 1) (60)\n= 1 \u2212 2\u03b1gt\nS +\nP\nk \u03b12\nk + 2P\ni\nP\nj \u03b1i\u03b1j\nS2 +\n2 P\ni\nP\nj \u03b1i\u03b1j\nS2(S + 1) \u2212\n2 P\ni\nP\nj \u03b1i\u03b1j\nS2 (61)\n= 2 \u2212 2\u03b1gt\nS +\n2 P\ni\nP\nj \u03b1i\u03b1j\nS2\nh 1\n(S + 1) \u2212 1\ni\n(62)\n= 2 \u2212 2\u03b1gt\nS \u2212\n2 P\ni\nP\nj \u03b1i\u03b1j\nS(S + 1) (63)\nThe range of the two components in the loss is 0 \u2264 2\u03b1gt\nS +\n2 P\ni\nP\nj \u03b1i\u03b1j\nS(S+1) \u2264 2 and the loss is bounded in the range [0, 2].\nIn other words, the loss for any sample in the entire sample space is bounded in the range of [0, 2] no matter how severe\nthe mistake is. Such bounded loss is expected to restrict the model\u2019s learning capacity.\n2. Bayes risk with cross-entropy loss (Eqn. 22)\nLCE(x, y) =\nKX\nj=1\nyk\n\u0010\n\u03a8(S) \u2212 \u03a8(\u03b1k)\n\u0011\n= \u03a8(S) \u2212 \u03a8(\u03b1gt) (64)\nWhere \u03a8(.) is the Digamma function, and \u0393 is the gamma function. The functions and their gradients are defined as\n\u0393(z) = e\u2212\u03b3z\nz\n\u221eY\nn=1\n\u0010\n1 + z\nn\n\u0011\u22121\ne\nz\nn (65)\n\u03a8(z) = d\ndz log \u0393(z) = d\ndz\n\u0012\n\u2212 \u03b3z \u2212 log z +\n\u221eX\nn=1\n\u0010z\nn \u2212 log\n\u0000\n1 + z\nn\n\u0001\u0011\u0013\n(66)\n= \u2212\u03b3 \u2212 1\nz +\n\u221eX\nn=1\n1\nn \u2212 1\nn + z (67)\n\u2202\u03a8(z)\n\u2202z = \u2202\n\u2202z\n\u0012\n\u2212 \u03b3 \u2212 1\nz +\n\u221eX\nn=1\n1\nn \u2212 1\nn + z\n\u0013\n= 1\nz2 +\n\u221eX\nn=1\n1\n(n + z)2 (68)\n21", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2232, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1dbb7934-4c9e-4ee0-94d5-78591806796d": {"__data__": {"id_": "1dbb7934-4c9e-4ee0-94d5-78591806796d", "embedding": null, "metadata": {"page_label": "22", "file_name": "Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_type": "application/pdf", "file_size": 2538334, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4f91bdee-157e-4746-a894-9e2da47ef02c", "node_type": "4", "metadata": {"page_label": "22", "file_name": "Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_type": "application/pdf", "file_size": 2538334, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "0643e77bbae148dd5423bd3c334d4736d3c09629161ba4c298fcabe0149d9cfc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3c9467f1-00e2-452f-9473-75346f75bd61", "node_type": "1", "metadata": {}, "hash": "60b30441cfbdf39ad7aa9227862d1e761e269d191d75e651505922785902baaf", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Learn to Accumulate Evidence from All Training Samples: Theory and Practice\nNow, the Bayes risk with cross-entropy loss becomes\nLCE(x, y) = \u03a8(S) \u2212 \u03a8(\u03b1gt) (69)\n= 1\n\u03b1gt\n\u2212 1\nS + S\n\u221eX\nn=1\n1\nn(n + S) \u2212 \u03b1gt\n\u221eX\nn=1\n1\nn(n + \u03b1gt) (70)\nBoth the infinite sums (P\u221e\nn=1\n1\nn(n+S) and P\u221e\nn=1\n1\nn(n+\u03b1gt) ) converge and lie in the range of 0 to \u03c02\n6 . The minimum\npossible value of this loss is 0 when \u03b1gt \u2192 \u221e&S \u2248 \u03b1gt. The maximum possible value is \u221e when only S \u2192 \u221e. The\nloss lies in the range [0, \u221e] and is more expressive compared to MSE-based evidential loss.\nConsidering the gradient of the loss with respect to the ground truth node (i.e. \u03b1gt, ygt = 1),\n\u2202LCE(x, y)\n\u2202\u03b1gt\n= \u2202\n\u2202\u03b1gt\n\u03a8(S) \u2212 \u03a8(\u03b1gt) = 1\nS2 +\n\u221eX\nn=1\n1\n(n + S)2 \u2212 1\n\u03b12\ngt\n\u2212\n\u221eX\nn=1\n1\n(n + \u03b1gt)2 (71)\nAs \u03b1gt < S, the gradient is always negative. Thus, the model aims to maximize the correct evidence \u03b1gt. Considering\nthe gradient of the loss with respect to nodes not corresponding to the ground truth (i.e. \u03b1k, k\u0338= gt, yk = 0),\n\u2202LCEx, y)\n\u2202\u03b1k\n= \u2202\n\u2202\u03b1k\n\u03a8(S) \u2212 \u03a8(\u03b1gt) = \u2202\u03a8(S)\n\u2202S\n\u2202S\n\u2202\u03b1k\n= 1\nS2 +\n\u221eX\nn=1\n1\n(n + S)2 (72)\n\u2202LCEx, y)\n\u2202ok\n= \u2202LCEx, y)\n\u2202\u03b1k\n\u00d7 \u03b1k\nok\n=\n\u0010 1\nS2 +\n\u221eX\nn=1\n1\n(n + S)2\n\u0011\n(\u03b1k \u2212 1) (73)\nThe gradient at nodes that do not correspond to ground truth is always non-negative. However, this gradient is also\nminimum and 0 when S \u2192 \u221e& \u03b1k \u2192 \u221e. This is an undesired behavior as the model may be encouraged to always\nincrease the evidence for all the classes. Moreover, the gradient is zero and there is no update to the nodes when\nS \u2192 \u221e, & \u03b1k \u2192 \u221e. So, the incorrect evidence regularization to penalize the incorrect evidence is essential for the\nevidential model trained with this loss.\n3. Type II Maximum Likelihood loss (Eqn. 23)\nLLog(x, y) =\nKX\nk=1\nyk\n\u0010\nlog(S) \u2212 log(\u03b1k)\n\u0011\n= log(S) \u2212 log(\u03b1gt) (74)\nThe loss is bounded in the range of [0, \u221e] as the loss is minimum and 0 when \u03b1gt \u2192 S \u2192 \u221e, and maximum loss\nwhen \u03b1gt << S& S \u2192 \u221e. Thus, the loss is more expressive compared to MSE based evidential loss. Now, the\ngradient of the loss is given by\n\u2202LLog(x, y)\n\u2202ok\n= 1\nS\n\u2202S\n\u2202ok\n\u2212 yk\n1\n\u03b1k\n\u2202\u03b1k\n\u2202ok\n=\n\u0010 1\nS \u2212 yk\n\u03b1k\n\u0011\u2202ek\n\u2202ok\n=\n\u0010 1\nS \u2212 yk\n\u03b1k\n\u0011\n(\u03b1k \u2212 1) (75)\nHere, when S \u2192 \u221e& \u03b1k \u2192 \u221e, the gradient becomes \u2202LLog(x,y)\n\u2202ok\n\u2192 (1\u2212yk). This is highly desirable behavior for the\nmodel as it aims to minimize the evidence for the incorrect class and there will be no update to the node corresponding\nto the ground truth class if \u03b1k = \u03b1gt, ygt = 1.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2378, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3c9467f1-00e2-452f-9473-75346f75bd61": {"__data__": {"id_": "3c9467f1-00e2-452f-9473-75346f75bd61", "embedding": null, "metadata": {"page_label": "22", "file_name": "Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_type": "application/pdf", "file_size": 2538334, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4f91bdee-157e-4746-a894-9e2da47ef02c", "node_type": "4", "metadata": {"page_label": "22", "file_name": "Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_type": "application/pdf", "file_size": 2538334, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "0643e77bbae148dd5423bd3c334d4736d3c09629161ba4c298fcabe0149d9cfc", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1dbb7934-4c9e-4ee0-94d5-78591806796d", "node_type": "1", "metadata": {"page_label": "22", "file_name": "Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_type": "application/pdf", "file_size": 2538334, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "8ab6b3b7129f1ce417920c021ff8e17c909169bdd408e258cc2dd96800427896", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Now, the\ngradient of the loss is given by\n\u2202LLog(x, y)\n\u2202ok\n= 1\nS\n\u2202S\n\u2202ok\n\u2212 yk\n1\n\u03b1k\n\u2202\u03b1k\n\u2202ok\n=\n\u0010 1\nS \u2212 yk\n\u03b1k\n\u0011\u2202ek\n\u2202ok\n=\n\u0010 1\nS \u2212 yk\n\u03b1k\n\u0011\n(\u03b1k \u2212 1) (75)\nHere, when S \u2192 \u221e& \u03b1k \u2192 \u221e, the gradient becomes \u2202LLog(x,y)\n\u2202ok\n\u2192 (1\u2212yk). This is highly desirable behavior for the\nmodel as it aims to minimize the evidence for the incorrect class and there will be no update to the node corresponding\nto the ground truth class if \u03b1k = \u03b1gt, ygt = 1. Thus, the Type II based issue is expected to be superior to the other\ntwo losses as the range of loss is optimal (i.e. in the range [0, \u221e]), and no learning issue arises for samples with high\nincorrect evidence.\nF. Additional Experiments and Results\nWe first present the details of the models, hyperparameter settings, clarification regarding dead neuron issue, and experiments\nused in the work in Section F.1. We then present additional results and discussions, including Few-shot classification,\nand 200-class tiny-ImageNet Classification results, that show the effectiveness of the proposed model RED in Section F.3.\nFinally discuss some limitations and potential future works in Section F.4.\n22", "mimetype": "text/plain", "start_char_idx": 1951, "end_char_idx": 3077, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "0f6f1f46-be29-4317-b45e-0d29ff727ba9": {"__data__": {"id_": "0f6f1f46-be29-4317-b45e-0d29ff727ba9", "embedding": null, "metadata": {"page_label": "23", "file_name": "Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_type": "application/pdf", "file_size": 2538334, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9fced9de-97fc-4686-baef-00f0d9f8ca6c", "node_type": "4", "metadata": {"page_label": "23", "file_name": "Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_type": "application/pdf", "file_size": 2538334, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "24a90b02437e23e0fef8c205b8bb96bb76126c9d824a27403aa29243794a204a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Learn to Accumulate Evidence from All Training Samples: Theory and Practice\nF.1. Hyperparameter details\nFor Table 1 results, \u03bb1 = 1 .0 was used for MNIST experiments, \u03bb1 = 0 .1 was used for Cifar10 experiments, and\n\u03bb1 = 0.001 was used for Cifar100 experiments. Table 8, 9, and 10 present complete results across the hyperparameter\nvalues and experiment settings. MNIST model was trained on the LeNet model (Sensoy et al., 2018) for 50 epochs, and\nCifar10/Cifar100 models were trained on Resnet-18 based classifier (He et al., 2016) for 200 epochs. Few-shot classification\nexperiments were carried out with \u03bb1 = 0.1 using Resnet-12 based classifier (Chen et al., 2021). All results presented\nin this work are from local reproduction. MNIST models were trained with learning rate of 0.0001 and Adam optimizer\n(Kingma & Ba, 2014), and all remaining models were trained with learning rate of 0.1 and Stochastic Gradient Descent\noptimizer with momentum. Tabular results represent the mean and standard deviation from 3 independent runs of the model.\nIn the proposed model RED, correct evidence regularization is weighted by the parameter \u03bbcor whose value is given by the\npredicted vacuity \u03bd. \u03bbcor is treated as hyperparameter, i.e., constant weighting term in the loss during model update.\nF.2. Dead Neuron Issue Clarification\nInstead of using ReLU as an activation function in a standard deep neural network, evidential models introduce ReLU as\nnon-negative transformation function in the output layer to ensure that the predicted evidence is non-negative to satisfy\nthe requirement of evidential theory. This non-negative evidence vector parameterizes a Dirichlet prior for fine-grained\nuncertainty quantification that covers second-order uncertainty, including vacuity and dissonance. We theoretically and\nempirically show the learning deficiency of ReLU based evidential models and justify the advantage of using an exponential\nfunction to output (non-negative) evidence. We further introduce a correct evidence regularization term in the loss that\naddresses the learning deficiency from zero-evidence samples. The \u201cdead neuron\u201d issue in the activation functions has been\nstudied, and ReLU variations such as Exponential Linear Unit, Parametric ReLU, and Leaky ReLU have been developed to\naddress the issue. But, these activation functions will not be theoretically sound in the evidential framework as they are can\nlead to negative evidences. In this case, they can not serve as Dirichlet parameters that are interpreted as pseudo counts.\nF.3. Effectiveness of Regularized Evidential Model (RED)\nF.3.1. E VIDENTIAL ACTIVATION FUNCTION .\nIn this section, we present additional results (for section 5.2) with the MNIST classification problem using the LeNet\nmodel to empirically validate Theorem 2. We carry out experiments for evidential models trained using all three evidential\nlosses: Evidential MSE loss in (21), Evidential cross-entropy loss in (22), and Evidential Log loss in (23) with \u03bb1 =\n{0.0, 1.0, &10.0}. As can be seen in Figure 15, 16, and 17, using exp activation for transforming logits to evidence leads to\nsuperior performance in all settings compared to ReLU and Softplus based evidential models that empirically validates\nTheorem 2.\n(a) Trend for \u03bb1 = 0.0\n (b) Trend for \u03bb1 = 1.0\n (c) Trend for \u03bb1 = 10.0\nFigure 15.Impact of Evidential Activation to the test set accuracy of the model trained with MSE based evidential loss (Eqn. 21)\nF.3.2. C ORRECT EVIDENCE REGULARIZATION\nWe introduce the novel correct evidence regularization term to train the evidential model (Section 4.1). In this section, we\npresent additional results for the evidential model that uses exp activation. We trained the model using evidential losses\nwith different incorrect evidence regularization strengths ( \u03bb1 = 0, 1.0 & 10.0). As can be seen( Figure 18, and 19), the\nmodel with proposed correct-evidence regularization leads to improved generalization compared to the baseline model\n23", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3981, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "455a2576-c663-4b27-8838-caf8820141c4": {"__data__": {"id_": "455a2576-c663-4b27-8838-caf8820141c4", "embedding": null, "metadata": {"page_label": "24", "file_name": "Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_type": "application/pdf", "file_size": 2538334, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5cae1afd-9dee-456f-b93d-b3b04aea1e19", "node_type": "4", "metadata": {"page_label": "24", "file_name": "Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_type": "application/pdf", "file_size": 2538334, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "a3b56829df3d59bcef725416b0d181f1ba3004650148a93e9fee8a8dfe68718f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Learn to Accumulate Evidence from All Training Samples: Theory and Practice\n(a) Trend for \u03bb1 = 0.0\n (b) Trend for \u03bb1 = 1.0\n (c) Trend for \u03bb1 = 10.0\nFigure 16.Impact of Evidential Activation to test set accuracy of the model trained with cross-entropy based evidential loss (Eqn. 22)\n(a) Trend for \u03bb1 = 0.0\n (b) Trend for \u03bb1 = 1.0\n (c) Trend for \u03bb1 = 10.0\nFigure 17.Impact of Evidential Activation to the test set accuracy of the model trained with Type II based evidential loss (Eqn. 23)\nas the proposed correct-evidence regularization term enables the evidential model to learn from zero-evidence samples\ninstead of ignoring them. Moreover, even though strong incorrect evidence regularization hurts both model\u2019s generalization,\nthe proposed regularization leads to a more robust model that generalizes better. Finally, the MSE-based evidential model\nis hurt the most with strong incorrect evidence regularization as thee MSE based evidential loss is bounded in the range\n[0, 2], and the incorrect evidence-regularization term may easily dominate the overall loss compared to other evidential\nlosses. This can be seen in Figure 18(c) where the incorrect evidence regularization strength is large i.e. \u03bb1 = 10.0 and\nthe evidential model fails to train. Due to strong incorrect evidence regularization, the model may have learned to map all\ntraining samples to zero-evidence region. However, with the proposed regularization, the model continues to learn and\nachieves good generalization performance.\n(a) Trend for \u03bb1 = 0.0\n (b) Trend for \u03bb1 = 1.0\n (c) Trend for \u03bb1 = 10.0\nFigure 18.Impact of proposed Correct Evidence Regularization to the test set accuracy of the evidential model( Trained with Eqn. 21)\nF.3.3. F EW-SHOT CLASSIFICATION EXPERIMENTS\nIdeas presented in this work address the fundamental limitation of evidential classification framework that enables the\nevidential model to acquire knowledge from all the training samples. Using these ideas, evidential framework can be\nextended to challenging classification problems to the reasonable predictive performance. To this end, we experiment\nwith few-shot classification using 1-shot and 5-shot classification for the mini-ImageNet dataset (Vinyals et al., 2016). We\n24", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2229, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "9e11c56c-2de9-4505-92df-1de286bd95bb": {"__data__": {"id_": "9e11c56c-2de9-4505-92df-1de286bd95bb", "embedding": null, "metadata": {"page_label": "25", "file_name": "Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_type": "application/pdf", "file_size": 2538334, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6d64437e-60b9-4d40-9c27-bb4e2c24c4dd", "node_type": "4", "metadata": {"page_label": "25", "file_name": "Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_type": "application/pdf", "file_size": 2538334, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "4cef9d4b707e9b5277090e58ddb6bf6bbd538b84c569f60feb372aac42cd4364", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Learn to Accumulate Evidence from All Training Samples: Theory and Practice\n(a) Trend for \u03bb1 = 0.0\n (b) Trend for \u03bb1 = 1.0\n (c) Trend for \u03bb1 = 10.0\nFigure 19.Impact of proposed Correct Evidence Regularization to the test set accuracy of the evidnetial model (Trained with Eqn. 22)\nconsider the ResNet-12 backbone, classifier-baseline model (Chen et al., 2021), and its evidential extension. Table 7 shows\nthe results for 1-shot and 5-shot classification experiments. As can be seen, the ReLU and Softplus based evidential\nmodels have suboptimal performance as they avoid many training samples of the zero-evidence region. In contrast, the exp\nmodel has a better learning capacity that leads to superior performance. Finally, the proposed model RED can learn from all\ntraining samples, which leads to the best generalization performance among all the evidential models.\nTable 7.Few-Shot Classification Accuracy comparison: mini-ImageNet dataset\nStandard CE Model: 1 Shot: 57.9\u00b10.2%; 5-Shot: 76.9\u00b10.2%\n1-Shot Experiments\nRegularization ReLU SoftPlus exp RED (Ours)\n\u03bb1 = 0.000 38.78\u00b13.75 51.60\u00b10.40 57.11\u00b10.09 56.27\u00b10.15\n\u03bb1 = 0.100 31.15\u00b11.69 48.87\u00b10.21 56.43\u00b10.03 58.03\u00b10.39\n\u03bb1 = 1.000 20.00\u00b10.00 43.81\u00b10.56 27.43\u00b10.88 54.68\u00b10.45\n5-Shot Experiments\nRegularization ReLU SoftPlus exp Ours\n\u03bb1 = 0.000 52.66\u00b15.32 67.22\u00b10.17 75.87\u00b10.09 75.31\u00b10.13\n\u03bb1 = 0.100 43.95\u00b13.72 66.14\u00b10.05 74.08\u00b10.13 76.05\u00b10.17\n\u03bb1 = 1.000 20.00\u00b10.00 61.96\u00b10.61 34.01\u00b11.46 72.32\u00b10.20\nF.3.4. C OMPLEX DATASET /MODEL EXPERIMENTS\nWe also carry out experiment for a challenging 200-class classification problem over Tiny-ImageNet based on (Huynh,\n2022). We adapt the Swin Transformer to be evidential, and train all the models for 20 epochs with Evidential log loss\n(Eqn. 23). In this setting, ReLU based evidential model achieves 85.25% accuracy, softplus based model achieves 85.15 %\naccuracy, the exponential model improves over both to achieve 89.93 % accuracy, and our proposed model RED outperforms\nall the evidential models to achieve the greatest accuracy of 90.14%, empirically validating our theoretical analysis.\nF.4. Limitations and Future works\nWe carried out a theoretical investigation of the Evidential Classification models to identify their fundamental limitation:\ntheir inability to learn from zero evidence regions. The empirical study in this work is based on classification problems.\nWe next plan to extend the ideas to develop Evidential Segmentation and Evidential Object Detection models. Moreover,\nthis work identifies limitations of Evidential MSE loss in (21), and we plan to carry out a thorough theoretical analysis\nto analyze other evidential losses given in (23) and (22)). The proposed evidential model, similar to existing evidential\nclassification models, requires hyperparameter tuning for \u03bb1 i.e. the incorrect evidence regularization hyperparameter.\nIn addition, extending evidential models to noisy and incomplete data settings and investigating the benefits of leveraging\nuncertainty information could be interesting future work. Finally, It will be an interesting future work to extend the analysis\nand evidential models to tasks beyond classification, for instance to build effective evidential segmentation and object\ndetection models.\n25", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3243, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "87d5b2f7-52bc-4c27-b792-f5be2adef879": {"__data__": {"id_": "87d5b2f7-52bc-4c27-b792-f5be2adef879", "embedding": null, "metadata": {"page_label": "26", "file_name": "Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_type": "application/pdf", "file_size": 2538334, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "66760d97-fffb-49ff-8621-3a9deefaae1d", "node_type": "4", "metadata": {"page_label": "26", "file_name": "Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_type": "application/pdf", "file_size": 2538334, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "0529e2500f273d2ee70fdf0b0753abfd4a7227edf807a9e5f3c780842111f4c8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "23226c63-ec27-44bf-8223-0840f10fea8b", "node_type": "1", "metadata": {}, "hash": "401bbef13df656d142ea349e719aefa45ae8882fc604bff8fd2cecf5ca2ee97b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Learn to Accumulate Evidence from All Training Samples: Theory and Practice\nTable 8.Classification performance comparison: MNIST dataset\nStandard CE Model: 99.21\u00b10.03%\nLog loss\nRegularization ReLU SoftPlus exp RED (Ours)\n\u03bb1 = 0.000 97.06\u00b10.19 97.07\u00b10.24 98.85\u00b10.03 98.82\u00b10.04\n\u03bb1 = 1.000 98.19\u00b10.08 98.21\u00b10.05 98.79\u00b10.02 99.10\u00b10.02\n\u03bb1 = 10.000 83.17\u00b14.54 80.37\u00b118.70 98.14\u00b10.07 98.84\u00b10.03\nEvidential CE loss\n\u03bb1 = 0.000 97.03\u00b10.21 97.09\u00b10.21 98.84\u00b10.02 98.81\u00b10.01\n\u03bb1 = 1.000 98.27\u00b10.02 98.36\u00b10.02 98.87\u00b10.03 99.12\u00b10.02\n\u03bb1 = 10.000 97.46\u00b11.02 97.14\u00b11.42 98.31\u00b10.07 98.84\u00b10.04\nEvidential MSE loss\n\u03bb1 = 0.000 96.18\u00b10.02 96.20\u00b10.03 98.42\u00b10.03 98.41\u00b10.06\n\u03bb1 = 1.000 97.41\u00b10.22 97.45\u00b10.16 98.35\u00b10.05 99.02\u00b10.00\n\u03bb1 = 10.000 19.93\u00b16.98 27.14\u00b16.37 27.17\u00b13.72 98.76\u00b10.03\nTable 9.Classification performance comparison: Cifar10 Dataset\nStandard CE Model: 95.43\u00b10.02%\nLog loss\nRegularization ReLU SoftPlus exp RED (Ours)\n\u03bb1 = 0.000 43.83\u00b114.60 95.19\u00b10.10 95.35\u00b10.02 95.03\u00b10.14\n\u03bb1 = 0.100 41.43\u00b119.60 95.18\u00b10.11 95.11\u00b10.10 95.24\u00b10.06\n\u03bb1 = 1.000 38.42\u00b115.64 94.94\u00b10.22 93.95\u00b10.06 94.78\u00b10.17\n\u03bb1 = 10.000 10.00\u00b10.00 32.42\u00b16.99 23.29\u00b15.24 90.96\u00b10.35\n\u03bb1 = 50.000 10.00\u00b10.00 10.00\u00b10.00 12.47\u00b13.49 65.09\u00b10.74\nEvidential CE loss\n\u03bb1 = 0.000 79.19\u00b116.06 95.32\u00b10.17 95.38\u00b10.10 95.40\u00b10.14\n\u03bb1 = 0.100 75.97\u00b120.56 95.12\u00b10.05 95.33\u00b10.03 95.08\u00b10.07\n\u03bb1 = 1.000 75.83\u00b120.74 94.99\u00b10.08 94.65\u00b10.04 94.74\u00b10.11\n\u03bb1 = 10.000 10.00\u00b10.00 89.63\u00b10.38 56.54\u00b14.80 91.71\u00b10.23\n\u03bb1 = 50.000 10.00\u00b10.00 27.03\u00b12.62 25.33\u00b16.66 62.98\u00b10.84\nEvidential MSE loss\n\u03bb1 = 0.000 95.43\u00b10.05 95.35\u00b10.15 95.10\u00b10.04 94.92\u00b10.12\n\u03bb1 = 0.100 95.15\u00b10.10 95.04\u00b10.05 95.14\u00b10.03 95.03\u00b10.13\n\u03bb1 = 1.000 49.68\u00b129.48 93.51\u00b10.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1646, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "23226c63-ec27-44bf-8223-0840f10fea8b": {"__data__": {"id_": "23226c63-ec27-44bf-8223-0840f10fea8b", "embedding": null, "metadata": {"page_label": "26", "file_name": "Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_type": "application/pdf", "file_size": 2538334, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "66760d97-fffb-49ff-8621-3a9deefaae1d", "node_type": "4", "metadata": {"page_label": "26", "file_name": "Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_type": "application/pdf", "file_size": 2538334, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "0529e2500f273d2ee70fdf0b0753abfd4a7227edf807a9e5f3c780842111f4c8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "87d5b2f7-52bc-4c27-b792-f5be2adef879", "node_type": "1", "metadata": {"page_label": "26", "file_name": "Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_type": "application/pdf", "file_size": 2538334, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "639de4dd537f96a70811064b90699d5d2bda984eea12d25232d0d1399c938297", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "04 94.74\u00b10.11\n\u03bb1 = 10.000 10.00\u00b10.00 89.63\u00b10.38 56.54\u00b14.80 91.71\u00b10.23\n\u03bb1 = 50.000 10.00\u00b10.00 27.03\u00b12.62 25.33\u00b16.66 62.98\u00b10.84\nEvidential MSE loss\n\u03bb1 = 0.000 95.43\u00b10.05 95.35\u00b10.15 95.10\u00b10.04 94.92\u00b10.12\n\u03bb1 = 0.100 95.15\u00b10.10 95.04\u00b10.05 95.14\u00b10.03 95.03\u00b10.13\n\u03bb1 = 1.000 49.68\u00b129.48 93.51\u00b10.03 18.98\u00b11.82 94.90\u00b10.20\n\u03bb1 = 10.000 10.00\u00b10.00 10.00\u00b10.00 10.00\u00b10.00 90.15\u00b10.71\n\u03bb1 = 50.000 10.00\u00b10.00 10.00\u00b10.00 10.00\u00b10.00 27.11\u00b124.20\n26", "mimetype": "text/plain", "start_char_idx": 1359, "end_char_idx": 1786, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5b0a4418-152b-4a25-8f7d-a522223bd9c4": {"__data__": {"id_": "5b0a4418-152b-4a25-8f7d-a522223bd9c4", "embedding": null, "metadata": {"page_label": "27", "file_name": "Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_type": "application/pdf", "file_size": 2538334, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e11a913e-de48-41ec-9589-55b00054df69", "node_type": "4", "metadata": {"page_label": "27", "file_name": "Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Learn to Accumulate Evidence from All Training Samples Theory and Practice.pdf", "file_type": "application/pdf", "file_size": 2538334, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "7ee4732ccf4a979a975b5d5bdfce0ab5add5e2a6a7da88c4df670b64bcec03ca", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Learn to Accumulate Evidence from All Training Samples: Theory and Practice\nTable 10.Classification performance comparison: Cifar100 dataset\nStandard CE Model: 75.67 \u00b1 0.11\nLog loss\nRegularization ReLU SoftPlus exp RED (Ours)\n\u03bb1 = 0.000 56.69\u00b15.83 73.85\u00b10.20 76.25\u00b10.16 76.26\u00b10.27\n\u03bb1 = 0.001 61.27\u00b13.79 74.48\u00b10.17 76.12\u00b10.04 76.43\u00b10.21\n\u03bb1 = 0.010 54.20\u00b15.93 75.56\u00b10.43 76.02\u00b10.16 76.14\u00b10.09\n\u03bb1 = 0.100 20.29\u00b14.54 75.67\u00b10.22 72.72\u00b10.26 74.62\u00b10.21\n\u03bb1 = 1.000 1.00\u00b10.00 37.60\u00b10.82 2.59\u00b10.52 68.62\u00b10.03\n\u03bb1 = 2.000 1.00\u00b10.00 1.57\u00b10.35 0.97\u00b10.06 62.33\u00b10.52\nEvidential CE loss\n\u03bb1 = 0.000 66.37\u00b13.47 73.73\u00b10.38 75.91\u00b10.20 76.19\u00b10.22\n\u03bb1 = 0.001 68.62\u00b12.41 74.44\u00b10.08 76.23\u00b10.09 76.35\u00b10.06\n\u03bb1 = 0.010 71.94\u00b10.66 75.45\u00b10.12 75.95\u00b10.14 76.13\u00b10.24\n\u03bb1 = 0.100 67.25\u00b11.84 75.75\u00b10.21 74.02\u00b10.09 74.69\u00b10.13\n\u03bb1 = 1.000 1.00\u00b10.00 73.10\u00b10.20 37.36\u00b10.73 69.40\u00b10.16\n\u03bb1 = 2.000 1.00\u00b10.00 52.99\u00b10.56 12.94\u00b11.11 63.93\u00b10.34\nEvidential MSE loss\n\u03bb1 = 0.000 35.76\u00b12.81 20.45\u00b11.41 75.70\u00b10.47 75.55\u00b10.24\n\u03bb1 = 0.001 31.49\u00b10.31 15.74\u00b10.47 42.95\u00b10.76 75.73\u00b10.27\n\u03bb1 = 0.010 13.60\u00b12.44 1.00\u00b10.00 1.00\u00b10.00 75.35\u00b10.16\n\u03bb1 = 0.100 1.00\u00b10.00 1.00\u00b10.00 1.00\u00b10.00 74.00\u00b10.13\n\u03bb1 = 1.000 1.00\u00b10.00 1.00\u00b10.00 1.00\u00b10.00 66.61\u00b10.46\n\u03bb1 = 2.000 1.00\u00b10.00 1.00\u00b10.00 1.00\u00b10.00 63.01\u00b10.83\n27", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1239, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "2c555464-e4f6-448a-a485-9bbb1c3ac317": {"__data__": {"id_": "2c555464-e4f6-448a-a485-9bbb1c3ac317", "embedding": null, "metadata": {"file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\README.md", "file_name": "README.md", "file_type": "text/markdown", "file_size": 629, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "617fbb52-97af-4a49-bf31-5e77a2fcafab", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\README.md", "file_name": "README.md", "file_type": "text/markdown", "file_size": 629, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "352bd2e9c656c0de8e1edf7c84fad4bb61706fae0e504149b569399985d7bc8d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "# Raw Data Directory\r\n\r\nPlace your raw data files here for the RAG pipeline to process.\r\n\r\n## Supported File Types\r\nThe pipeline uses `SimpleDirectoryReader` and supports the following formats:\r\n- **PDF** (`.pdf`): Research papers, slides, documentation.\r\n- **Text** (`.txt`): Plain text notes, logs.\r\n- **Markdown** (`.md`): Documentation, notes.\r\n- **Word** (`.docx`): Documents.\r\n- **PowerPoint** (`.pptx`): Presentations.\r\n- **CSV/Excel**: Structured data (may require specific formatting).\r\n\r\n## Automated Downloads\r\nThe `gather` script will automatically populate this directory with top ArXiv papers on specified topics.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 627, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b5411b1a-2e62-4f1c-85e6-b4b8b282d59f": {"__data__": {"id_": "b5411b1a-2e62-4f1c-85e6-b4b8b282d59f", "embedding": null, "metadata": {"page_label": "1", "file_name": "Safe Untrusted Proof-Carrying AI Agents toward the agentic lakehouse.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Safe Untrusted Proof-Carrying AI Agents toward the agentic lakehouse.pdf", "file_type": "application/pdf", "file_size": 752899, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "48802810-28be-4173-b838-94521469f095", "node_type": "4", "metadata": {"page_label": "1", "file_name": "Safe Untrusted Proof-Carrying AI Agents toward the agentic lakehouse.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Safe Untrusted Proof-Carrying AI Agents toward the agentic lakehouse.pdf", "file_type": "application/pdf", "file_size": 752899, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "e67992486fff57bc0ea31cdd5699ba86fb393873a15ea146c0e20dc19e76aacb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2fb71b5f-0d95-440c-a8a4-6eb286cdc201", "node_type": "1", "metadata": {}, "hash": "56157ec0bf24d4b0ddf922db754c9ce4c08811416cfbd27312bba2dec3bbee52", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Safe, Untrusted, \u201cProof-Carrying\u201d AI Agents:\ntoward the agentic lakehouse\nJacopo Tagliabue\nBauplan Labs\nNYC, USA\njacopo.tagliabue@bauplanlabs.com\nCiro Greco\nBauplan Labs\nNYC, USA\nciro.greco@bauplanlabs.com\nAbstract\u2014Data lakehouses run sensitive workloads, where\nAI-driven automation raises concerns about trust, correctness,\nand governance. We argue that API-first, programmable lake-\nhouses provide the right abstractions forsafe-by-design, agentic\nworkflows. Using Bauplan as a case study, we show how data\nbranching and declarative environments extend naturally to\nagents, enabling reproducibility and observability while reducing\nthe attack surface. We present a proof-of-concept in which agents\nrepair data pipelines using correctness checks inspired by proof-\ncarrying code. Our prototype demonstrates that untrusted AI\nagents can operate safely on production data and outlines a path\ntoward a fully agentic lakehouse.\nIndex Terms\u2014AI agents, lakehouse, data pipelines, versioning\nI. INTRODUCTION\nThe data lakehouse is thede factocloud architecture for\nanalytics and Artificial Intelligence (AI) workloads [2], [3],\nthanks to storage-compute decoupling, multi-language sup-\nport, and unified table semantics. As reasoning and tool usage\nin Large Language Models (LLMs) improve [4], autonomous\ndecisions (\u201cAI agents\u201d) are both supported by, and targeted\nat, cloud infrastructure: to what extent can agents manage the\ndata lifecycle in a lakehouse?\nPrima facie, the question appears both too hard and too\nbroad. On one hand, lakehouses are distributed systems built\nfor the collaboration of human teams on sensitive production\ndata, not point-wise tasks immediately suitable for end-to-end\nautomation. On the other, it is unclear how to prioritize agentic\nuse cases across such heterogeneous platforms.Thispaper is\na preliminary answer to these challenges: we detail lakehouse\nabstractions suitable for automation, and operationalize a\nprototype for an important use case:repairing data pipelines.\nPipelines are a compelling case study for three reasons: first,\nthey cover a large portion of lakehouse workloads, measured\nboth by developer time [5] and overall compute [6]. Second,\ndata engineers spend a significant amount of their time fixing\nthem [7], [8]. Finally, repairing pipelines is a canary test for\nagent penetration in high-stakes non-trivial scenarios, which\nare often hard even for expert humans [9], [10]. We summarize\nour contributions as follows:\n1) we introduce abstractions to model the data life-cycle\nin a programmable lakehouse [11], i.e. building and\nThanks to [1] for coming up with a great title (a long time ago, for a\ndifferent type of agents).\nexecuting cloud pipelines entirely throughcode. We\nargue that traditional systems resist automation mostly\nbecause of heterogeneous interfaces and complex access\npatterns, while code is thelingua francasuitable for\nagents, cloud systems, and human supervisors;\n2) we review common objections to automating high-stakes\nworkloads in light of the proposed abstractions: in\nparticular, we argue that our model promotes trustwor-\nthiness and correctness both in data and code artifacts;\n3) we release working code 1, showing a proof of concept\nfor self-repairing pipelines usingBauplanas a lake-\nhouse and an agentic loop. Starting from this prototype,\nwe conclude by outlining practical next steps for a full\nagentic lakehouse.\nThe paper is organized as follows. After reviewing agent-\nfriendly abstractions (Section II), we address key safety ob-\njections for high-stakes scenarios (Section III). Once safety\nis established, we describe a ReAct [12] loop built on these\nabstractions (Section IV). We put forward our working pro-\ntotype as a feasibility demonstration of safe-by-design data\nagents, not as a full-fledged experimental benchmark.\nWe believe that sharing working code is of great value\nto the community, especially in times of quickly shifting\nmental models. However, it is important to remember that\nour fundamental insights \u2013 programmability and safety \u2013 can\nbe replicated independently of the chosen APIs.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4093, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "2fb71b5f-0d95-440c-a8a4-6eb286cdc201": {"__data__": {"id_": "2fb71b5f-0d95-440c-a8a4-6eb286cdc201", "embedding": null, "metadata": {"page_label": "1", "file_name": "Safe Untrusted Proof-Carrying AI Agents toward the agentic lakehouse.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Safe Untrusted Proof-Carrying AI Agents toward the agentic lakehouse.pdf", "file_type": "application/pdf", "file_size": 752899, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "48802810-28be-4173-b838-94521469f095", "node_type": "4", "metadata": {"page_label": "1", "file_name": "Safe Untrusted Proof-Carrying AI Agents toward the agentic lakehouse.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Safe Untrusted Proof-Carrying AI Agents toward the agentic lakehouse.pdf", "file_type": "application/pdf", "file_size": 752899, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "e67992486fff57bc0ea31cdd5699ba86fb393873a15ea146c0e20dc19e76aacb", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b5411b1a-2e62-4f1c-85e6-b4b8b282d59f", "node_type": "1", "metadata": {"page_label": "1", "file_name": "Safe Untrusted Proof-Carrying AI Agents toward the agentic lakehouse.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Safe Untrusted Proof-Carrying AI Agents toward the agentic lakehouse.pdf", "file_type": "application/pdf", "file_size": 752899, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "ce177b491d91fdd08a564ecd58040217d4599902f54b9c0efcf1646cd4fc2451", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Starting from this prototype,\nwe conclude by outlining practical next steps for a full\nagentic lakehouse.\nThe paper is organized as follows. After reviewing agent-\nfriendly abstractions (Section II), we address key safety ob-\njections for high-stakes scenarios (Section III). Once safety\nis established, we describe a ReAct [12] loop built on these\nabstractions (Section IV). We put forward our working pro-\ntotype as a feasibility demonstration of safe-by-design data\nagents, not as a full-fledged experimental benchmark.\nWe believe that sharing working code is of great value\nto the community, especially in times of quickly shifting\nmental models. However, it is important to remember that\nour fundamental insights \u2013 programmability and safety \u2013 can\nbe replicated independently of the chosen APIs. For these\nreasons, we believe our paper to be valuable to a wide range\nof practitioners: on one hand, those looking for a new mental\nmap of this uncharted territory; on the other, those looking\nto be inspired by tinkering with existing implementations and\ninspecting systems working at scale.\nII. APROGRAMMABLE LAKEHOUSE\nIn aprogrammablelakehouse, theentiredata life-cycle \u2013\ndata, user and infrastructure management, pipeline and query\nexecution, runtime observability \u2013 is exposed through code\nabstractions: server-side APIs, SDK methods, CLI shortcuts.\nIn the rest of the paper,Bauplansnippets will be used as\nsample implementation, but the platform\u2019s composable nature\nmakes it easy to replicate these functionalities in different\n1Open source code is available at https://github.com/BauplanLabs/the-\nagentic-lakehouse.\narXiv:2510.09567v1  [cs.AI]  10 Oct 2025", "mimetype": "text/plain", "start_char_idx": 3293, "end_char_idx": 4957, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "0f63aba0-4019-4921-8059-9a6d2ca20b05": {"__data__": {"id_": "0f63aba0-4019-4921-8059-9a6d2ca20b05", "embedding": null, "metadata": {"page_label": "2", "file_name": "Safe Untrusted Proof-Carrying AI Agents toward the agentic lakehouse.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Safe Untrusted Proof-Carrying AI Agents toward the agentic lakehouse.pdf", "file_type": "application/pdf", "file_size": 752899, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "71e24d16-7899-4464-a3d7-237b4f4e19af", "node_type": "4", "metadata": {"page_label": "2", "file_name": "Safe Untrusted Proof-Carrying AI Agents toward the agentic lakehouse.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Safe Untrusted Proof-Carrying AI Agents toward the agentic lakehouse.pdf", "file_type": "application/pdf", "file_size": 752899, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "3a2ee9a3fa9c7330f033beaa0b7a7c86acd8ef8af4cfb052f63841be2845e13a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "23092e61-fd1b-4aac-aac9-b521ceaafdef", "node_type": "1", "metadata": {}, "hash": "04e9626e488f53cf907cb89c2af47cf58ba19a558e7b5bc5d5e239e0c0647318", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "architectures.2 We break down the pipeline life-cycle into two\nmajor components: pipeline definition and pipeline execution.\nA. Pipeline definition\nA pipeline is a DAG of transformations. A DAG starts from\nsource tables, which are progressively cleaned, augmented,\naggregated through the transformation code (expressed in SQL\nor Python). A successful execution produces intermediate and\nfinal data assets, which are then consumed by downstream\nsystems [15]. Fig. 2 shows a pipeline (hence, P) used as a\nrecurring example throughout the paper. Taking two tables\nfrom the NYC taxi dataset [16], P defines two new tables (A\nand B), based on two transformations (1 and 2). The Bauplan\nimplementation for P is as follows: 3\nListing 1. P as the Python filep.py\n@bauplan.model(materialization=\"REPLACE\", name=\"A\")\n@bauplan.python(\"3.10\", pip={\"pandas\": \"2.0\"})\ndefjoin_and_filter(\ntrips=bauplan.Model(\"taxi_trips\"),\nzones=bauplan.Model(\"taxi_zones\")\n):\n# some transformation here...\nreturntrips.join(zones).do_something()\n@bauplan.model(materialization=\"REPLACE\", name=\"B\")\n@bauplan.python(\"3.11\", pip={\"pandas\": \"1.5.3\"})\ndefclean_and_transform(\ndata=bauplan.Model(\"join_and_filter\")\n):\nreturndata.do_something()\nTwo important design choices are worth highlighting in\nconnection to our safety discussion (Section III):\n\u2022Function-as-a-service (FaaS) abstractions: business\nlogic is expressed in the body of plain vanilla func-\ntions with the signatureTable(s)\u2192Table. DAGs\nare functions chained through naming convention. These\nabstractions naturally map to a serverless runtime, which\ncan execute the requested computation efficiently [11];\n\u2022declarative I/O and infrastructure: functions are fully\nisolated (e.g. two functions, two versions ofpandas)\nand their Python environment is specified declaratively\n[17]. Reading tables and writing artifacts back to the\nlake is also fully declarative: users specify the needed\ninputs and desired output, the platform performs the\ncorresponding physical operations.\nB. Pipeline execution\nA human (or an agent) with the proper access can execute\np.pyby simply installing thebauplanpackage, and running\n2We refer the interested reader to existing papers, in particular [11], [13],\n[14].\n3Snippets are simplified in the interest of space: full code is available in\nthe open source repository.\nFig. 1.Transactional pipelines: a successful execution of P, followed by\none that failed after A materialization. Runs happen on data branches: input\ndata is from sources inmain, but writes are sandboxed so that materialized\ntables hitmainatomicallyonly on success (no half-written pipeline).\nit from the terminal, without any additional steps \u2013 no Docker,\nno Terraform, no JDBC clients: 4\nListing 2. Bauplan CLI\n$ pip install bauplan\n$ bauplan run --project_dir P_folder\nWhile simplicity is a virtue, for our present concerns we\nfocus on thetransactionalnature of therunAPI, which is\nobtained by re-purposing Git concepts to table evolution, and\nby managing data and compute as a logical whole even if\nphysically decoupled. Consider now the two runs depicted in\nFig. 1:run 1(successful) andrun 2(failed). The branches\nand merges in the picture are a graphical representation of the\nunderlying \u201cGit-for-Data\u201d abstractions [18]: if every change\nto the lake corresponds to acommit, abranchis the HEAD\nof a sequence of commits, and amergeatomically combines\ncommits from two branches. If themainbranch represents\nproduction,mergeoperations between on-branch writes and\nmainmimic software best practices, and provides a natural\nhook for testing and reviews.\nWhenrun 1starts, the execution is automatically moved\nto a copy-on-write branch: source tables will have the same\ndata as production, but the tables materialized by the run will\nbe written to this branch first and then merged tomainwith\nan atomic operation, promoting A\u2019 and B\u2019 as the new tables\nfor downstream consumers.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3899, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "23092e61-fd1b-4aac-aac9-b521ceaafdef": {"__data__": {"id_": "23092e61-fd1b-4aac-aac9-b521ceaafdef", "embedding": null, "metadata": {"page_label": "2", "file_name": "Safe Untrusted Proof-Carrying AI Agents toward the agentic lakehouse.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Safe Untrusted Proof-Carrying AI Agents toward the agentic lakehouse.pdf", "file_type": "application/pdf", "file_size": 752899, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "71e24d16-7899-4464-a3d7-237b4f4e19af", "node_type": "4", "metadata": {"page_label": "2", "file_name": "Safe Untrusted Proof-Carrying AI Agents toward the agentic lakehouse.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Safe Untrusted Proof-Carrying AI Agents toward the agentic lakehouse.pdf", "file_type": "application/pdf", "file_size": 752899, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "3a2ee9a3fa9c7330f033beaa0b7a7c86acd8ef8af4cfb052f63841be2845e13a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0f63aba0-4019-4921-8059-9a6d2ca20b05", "node_type": "1", "metadata": {"page_label": "2", "file_name": "Safe Untrusted Proof-Carrying AI Agents toward the agentic lakehouse.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Safe Untrusted Proof-Carrying AI Agents toward the agentic lakehouse.pdf", "file_type": "application/pdf", "file_size": 752899, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "e50ce6e7b0390cc499a9446ed0a5c3b32405b33d851be71c03f5dea2231a1c2a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "1:run 1(successful) andrun 2(failed). The branches\nand merges in the picture are a graphical representation of the\nunderlying \u201cGit-for-Data\u201d abstractions [18]: if every change\nto the lake corresponds to acommit, abranchis the HEAD\nof a sequence of commits, and amergeatomically combines\ncommits from two branches. If themainbranch represents\nproduction,mergeoperations between on-branch writes and\nmainmimic software best practices, and provides a natural\nhook for testing and reviews.\nWhenrun 1starts, the execution is automatically moved\nto a copy-on-write branch: source tables will have the same\ndata as production, but the tables materialized by the run will\nbe written to this branch first and then merged tomainwith\nan atomic operation, promoting A\u2019 and B\u2019 as the new tables\nfor downstream consumers. The importance of coupling runs\nwith abranch-then-mergepattern becomes evident withrun\n2, which failed before a new version of B was materialized:\nno merge happens forrun 2, so no dirty read can occur in\nmain\u2013 downstream systems will still read a consistent pair,\nA\u2019 and B\u2019, not A\u201d and B\u2019. In other words,branchesallow sand-\nboxed writes starting from production reads, i.e. working with\nproduction data without the risk of destroying production.\nGit-for-Data abstractions at the execution level therefore\ncomplement the functional abstractions at the definition level,\nsolving three important use cases through simple APIs:\n\u2022reproducibility: runs are immutably, deterministically\nidentified through a pointer to the startingcommitand\na copy of the code;\n4To get a first-person perspective on the developer experience, the\nreader is invited to pause and watch a recorded run before continuing:\nhttps://www.loom.com/share/99ac0d5b5f944fc9aeef132bfaea0881", "mimetype": "text/plain", "start_char_idx": 3092, "end_char_idx": 4853, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "147ed8be-4e6e-45de-9c68-ba7f42ac940c": {"__data__": {"id_": "147ed8be-4e6e-45de-9c68-ba7f42ac940c", "embedding": null, "metadata": {"page_label": "3", "file_name": "Safe Untrusted Proof-Carrying AI Agents toward the agentic lakehouse.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Safe Untrusted Proof-Carrying AI Agents toward the agentic lakehouse.pdf", "file_type": "application/pdf", "file_size": 752899, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5cce9f87-4830-4ffd-95f8-8168b8e69f9d", "node_type": "4", "metadata": {"page_label": "3", "file_name": "Safe Untrusted Proof-Carrying AI Agents toward the agentic lakehouse.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Safe Untrusted Proof-Carrying AI Agents toward the agentic lakehouse.pdf", "file_type": "application/pdf", "file_size": 752899, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "6f2c123ad309e531467c732104c8afdaf0824a8c6563960c79ffc8a213579ba1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9fac8ce4-588d-4df0-aaf4-ef00824eda4a", "node_type": "1", "metadata": {}, "hash": "67cf538b3995bbea43cf5e8b20a8606382edd28a1919d9376d95bb77a00d575a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "\u2022transactionality: thebranch-then-mergepattern allows\ntransactional pipelines, i.e. doing MVCC in a \u201cdecon-\nstructed database\u201d [19], [20];\n\u2022reversibility: writes are immutable but never final, as we\ncan always revert to a previous state through the relevant\ncommit.\nC. Code as the universal interface\nAgents are a combination of reasoning (provided by LLMs)\nand tool usage. In a lakehouse, tools should allowobservabil-\nityof present and past runs andreproducibilityof existing\npipelines: if a lakehouse offers these capabilities in typed,\ndocumented methods, exposing them for agentic usage is as\ntrivial as wrapping those methods in MCP routes. In other\nwords, from the point of view of the relevant abstractions\n\u2013 functions, decorators, data branches \u2013, a programmable\nlakehouseis alreadyan agentic lakehouse.\nIn this sense, while serving overlapping use cases, the\ndesign distance between a programmable lakehouse and tradi-\ntional systems could not be greater. Practitioners today have\nto context-switch between one-off Spark clusters, development\nnotebooks, SQL editors, cloud warehouses and more [5];code,\non the other hand, is a universally understood interface \u2013 easy\nfor agents to master, immediate for the cloud to runandsafe\nfor humans to properly supervise.\nIII. THESAFETYCHECKLIST\nMaking agents able to \u201ccode the lakehouse\u201d through\npurpose-built abstractions is a necessary, but not sufficient\ncondition to repair pipelines in production. We also need to\nmake sure that the proposed abstractions addresstrust and\ncorrectnessconcerns over malicious or (simply wrong) usage\n(Table I).\nTrust in data: can agents access data they are not supposed\nto? No. I/O is always mediated by the platform: agents\nhave no access to the physical data layer (S3), so reads and\nwrites happen always in platform space, not user space. More\ngenerally, sincealloperations are API operations, RBAC over\nAPI keys provides fine-grained permissions with a minimal\nattack surface.\nTrust in code: can agents run untrusted packages or access\nmalicious web resources? No. Functions are run as indepen-\ndent processes isolated from their host and other functions,\nwithno internet access(since I/O is in platform space,\nusers do not even access S3!). The declarative syntax makes\n(dis)allowing packages as trivial as checking a decorator\nagainst a whitelist. Once again, a concise surface comes\nin handy, as there is only one entry point for dependency\nmanagement.\nCorrectness in data: can agents damage production data?\nNo. First, incomplete pipelines will not affect downstream sys-\ntems (Section II-B); second, removing merge-to-main permis-\nsions will make a human review necessary to reach production;\nthird, humans canalwaysrevert tables using past commits.\nCorrectness in code: can agents push to production silent\nbugs? No (as in, no more than humans can). The key insight\nTABLE I\nMAPPING SAFETY CONCERNS TO ABSTRACTIONS\nConcern Mode Abstraction\nTrust Data Declarative I/O\nTrust Code FaaS runtime\nCorrectness Data Transactional runs\nCorrectness Code Verify-then-merge\nis that even untrusted code can be useful provided a hard-to-\nfake correctness test. The aptly titled [1] defines an interesting\nprotocol: a \u201cconsumer\u201d specifies safety conditions upfront,\na \u201cproducer\u201d provides evidence that its work satisfies them.\nIf the consumer is satisfied, the work is now \u201ctrusted\u201d. We\ncan imagine a similar protocol, where the \u201cconsumer\u201d is the\nlakehouse owner (i.e. a data engineer), and the \u201cproducer\u201d is\nthe agent repairing the pipeline on her behalf. The evidence\nwill revolve around the pipeline output (A and B in P) meeting\ncertain criteria. In the prototype below, the owner came up\nwith a verifier, i.e. a functionBranch\u2192boolto allow an\nagent branch to be merged. Unlike the original protocol, data\nverifiers are less about formal properties and more about the\nbusiness context for the generated tables: the same principles\napply though, here strengthened by the \u201cpull-request\u201d flow that\nGit-for-Data enables. Importantly, verifiers use the same APIs\nas the agents, leveraging once again the unified access to data\nand compute, with no semantic or infrastructure drift between\nlakehouse clients.\nIV.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4191, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "9fac8ce4-588d-4df0-aaf4-ef00824eda4a": {"__data__": {"id_": "9fac8ce4-588d-4df0-aaf4-ef00824eda4a", "embedding": null, "metadata": {"page_label": "3", "file_name": "Safe Untrusted Proof-Carrying AI Agents toward the agentic lakehouse.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Safe Untrusted Proof-Carrying AI Agents toward the agentic lakehouse.pdf", "file_type": "application/pdf", "file_size": 752899, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5cce9f87-4830-4ffd-95f8-8168b8e69f9d", "node_type": "4", "metadata": {"page_label": "3", "file_name": "Safe Untrusted Proof-Carrying AI Agents toward the agentic lakehouse.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Safe Untrusted Proof-Carrying AI Agents toward the agentic lakehouse.pdf", "file_type": "application/pdf", "file_size": 752899, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "6f2c123ad309e531467c732104c8afdaf0824a8c6563960c79ffc8a213579ba1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "147ed8be-4e6e-45de-9c68-ba7f42ac940c", "node_type": "1", "metadata": {"page_label": "3", "file_name": "Safe Untrusted Proof-Carrying AI Agents toward the agentic lakehouse.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Safe Untrusted Proof-Carrying AI Agents toward the agentic lakehouse.pdf", "file_type": "application/pdf", "file_size": 752899, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "1646ae02724a59fb313b1194062e43a136df16d001bb79a9bed490d37f050a33", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We\ncan imagine a similar protocol, where the \u201cconsumer\u201d is the\nlakehouse owner (i.e. a data engineer), and the \u201cproducer\u201d is\nthe agent repairing the pipeline on her behalf. The evidence\nwill revolve around the pipeline output (A and B in P) meeting\ncertain criteria. In the prototype below, the owner came up\nwith a verifier, i.e. a functionBranch\u2192boolto allow an\nagent branch to be merged. Unlike the original protocol, data\nverifiers are less about formal properties and more about the\nbusiness context for the generated tables: the same principles\napply though, here strengthened by the \u201cpull-request\u201d flow that\nGit-for-Data enables. Importantly, verifiers use the same APIs\nas the agents, leveraging once again the unified access to data\nand compute, with no semantic or infrastructure drift between\nlakehouse clients.\nIV. A PROOF OFCONCEPT\nrun 2failed (Fig. 1): can an agent repair it? Now that we\nknow it is safe to do so, we combine what we learned so far\ninto an agentic loop and draw some preliminary conclusions\nfrom the prototype.\nA. The self-repairing setup\nOur setup is as follows:\n\u2022Bauplan as the programmable lakehouse;\n\u2022the Bauplan MCP 5, exposing as tools the lakehouse APIs:\ntools can be used forobservability(e.g. get failed jobs and\ntheir logs),data exploration(query tables, check types),\nexecution(create branches, start a run)\n\u2022smolagents 6 is the ReAct framework, handling the\nloop, tool calls and logs:smolagentsperforms reason-\ning in Python, making it effective at pipeline reasoning\nwhen compared to JSON-based tool calling [21], [22]\n\u2022LLM inference provided by OpenAI, Anthropic and To-\ngetherAI through a configurable LiteLLM 7 interface;\n\u2022a verifier functionBranch\u2192bool, which is the \u201cproof-\nchecking\u201d step before merging into main.\n5https://github.com/BauplanLabs/bauplan-mcp-server\n6https://huggingface.co/docs/smolagents/index\n7https://docs.litellm.ai/", "mimetype": "text/plain", "start_char_idx": 3365, "end_char_idx": 5251, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5e24c8ca-db47-47f3-b2f2-3c255a164ded": {"__data__": {"id_": "5e24c8ca-db47-47f3-b2f2-3c255a164ded", "embedding": null, "metadata": {"page_label": "4", "file_name": "Safe Untrusted Proof-Carrying AI Agents toward the agentic lakehouse.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Safe Untrusted Proof-Carrying AI Agents toward the agentic lakehouse.pdf", "file_type": "application/pdf", "file_size": 752899, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "531524fe-39e7-446c-bfd5-721e713f81cf", "node_type": "4", "metadata": {"page_label": "4", "file_name": "Safe Untrusted Proof-Carrying AI Agents toward the agentic lakehouse.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Safe Untrusted Proof-Carrying AI Agents toward the agentic lakehouse.pdf", "file_type": "application/pdf", "file_size": 752899, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "265151f8212b02c2ac8bfd8047beb2494798237f147f88918e8073f7d2ef2b86", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Fig. 2.Sample pipeline: upstream from the pipeline, two source tables containing taxi trips and location data, downstream, multiple consumers. The pipeline\nitself is a two-node DAG, with compute steps ingray\u2013 1 and 2 \u2013, and tables ingreen\u2013 A and B.\nFig. 3.Safe, untrusted lakehouse agents: the agent leverages LLMs and\ntools to repair a data pipeline. When an answer is produced, a deterministic\ncheck in the \u201couter loop\u201d verifies that it is safe to merge to production.\nB. Running an experiment\nFig. 3 illustrates the high-level flow of an experiment:\nthe agentic loop (reasoning and tool usage), the underlying\nprogrammable lakehouse (entirely accessible through APIs,\nwrapped by the MCP server), the proof-checking step at the\nend. In the example scenario, the script first launches a faulty\npipeline to create the conditions for agentic repair: following\nprior reports [10] and industry experience, we simulate a\npackage mismatch around the release ofNumPy 2.0that\ncaused crashes in containers usingpandas 2.0.\nA sample run8 using a model such asSonnet 4.5hits exactly\nall the abstractions discussed above: retrieving logs, querying\nthe state of the lake, using declarative code to specify in-\nfrastructure changes, creating debug branches from production\ndata, running code safely. Because this paper focuses on map-\n8https://www.loom.com/share/fc79b52601074a5ba06e2f0272be3c62\nping abstractions for the agentic lakehouse and demonstrating\nfeasibility, a thorough exploration of this experimental space\nis beyond scope. However, a few considerations are in order:\n\u2022as a testament to the complexity of the task, frontier\nmodel performance vary greatly in success rate, token\nusage and number of tool calls; as system builders\ndownstream from LLMs, the crucial takeaway is that even\nwhen models failed (e.g.,GPT-5-mini), the lakehouse\nexhibited no disruption or unsafe behavior;\n\u2022industry-leading traditional stacks \u2013 such as Snowflake\nwith dbt \u2013 do not support agentic repair, even if they\nboth have MCP servers and serve overlapping use cases.\nMCPs are a necessary but not sufficient condition for\nautomation;\n\u2022because switching models is a single configuration\nchange, we can easily imagine a budget-constrained sce-\nnario in which model selection is step-dependent, or a\ntime-constrained situation in which data branches support\nconcurrency control at scale for models in parallel.\nV. CONCLUSION AND FUTURE WORK\nIn a moment when most infrastructure agents are geared\ntoward specific tasks [23], programmable lakehouses have the\nambition to support agentic reasoning across the full life-\ncycle of data. To the best of our knowledge, we addressed\nfor the first time the open-ended challenge of repairing cloud\npipelines. We argued that a programmable lakehouse is already\nagentic, and that declarative DAGs and Git-like data man-\nagement are ideally suited to support safe-by-design agentic\nusage.\nTo move beyond the hype, it is necessary to build novel\nsystems and share working implementations. However, we\nalso recognize that the infrastructure underlying agentic ab-\nstractions may need to evolve accordingly. Although massive\nparallelism is outside the scope of our prototype, it is arguably\nthe primary challenge for OLAP systems in the age of agentic\ndata exploration [24]. We look forward to continuing to share\nwith the community our research journey toward a fully\nagentic lakehouse.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3399, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e65e59dd-775c-4c22-89f6-6c8a319ebf5b": {"__data__": {"id_": "e65e59dd-775c-4c22-89f6-6c8a319ebf5b", "embedding": null, "metadata": {"page_label": "5", "file_name": "Safe Untrusted Proof-Carrying AI Agents toward the agentic lakehouse.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Safe Untrusted Proof-Carrying AI Agents toward the agentic lakehouse.pdf", "file_type": "application/pdf", "file_size": 752899, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "66ad9bbf-b68e-4fb3-a2a0-1d0afa59cddc", "node_type": "4", "metadata": {"page_label": "5", "file_name": "Safe Untrusted Proof-Carrying AI Agents toward the agentic lakehouse.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Safe Untrusted Proof-Carrying AI Agents toward the agentic lakehouse.pdf", "file_type": "application/pdf", "file_size": 752899, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "2ee11d505b43e1d49dd01b8fb38c0629e16f9dbfcbec4921a131279fb34d1796", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "76ffa9d1-cf25-404f-af7e-a73cbebaaa61", "node_type": "1", "metadata": {}, "hash": "5a5bf7bf6dea988da9d863ddd16859171fe69a57f90af7520704dcd29cea20c6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "REFERENCES\n[1] G. C. Necula and P. Lee, \u201cSafe, untrusted agents using proof-carrying\ncode,\u201d inMobile Agents and Security. Berlin, Heidelberg: Springer-\nVerlag, 1998, p. 61\u201391.\n[2] D. Mazumdar, J. Hughes, and J. Onofre, \u201cThe data lakehouse:\nData warehousing and more,\u201d 2023. [Online]. Available:\nhttps://arxiv.org/abs/2310.08697\n[3] M. A. Zaharia, A. Ghodsi, R. Xin, and M. Armbrust, \u201cLakehouse: A new\ngeneration of open platforms that unify data warehousing and advanced\nanalytics,\u201d inConference on Innovative Data Systems Research, 2021.\n[4] Z. Shen, \u201cLlm with tools: A survey,\u201darXiv preprint arXiv:2409.18807,\n2024.\n[5] C. G. Tapan Srivastava, Jacopo Tagliabue, \u201cEudoxia: a faas scheduling\nsimulator for the composable lakehouse,\u201dProceedings of Workshops at\nthe 51th International Conference on Very Large Data Bases, 2025.\n[6] A. van Renen, D. Horn, P. Pfeil, K. E. Vaidya, W. Dong,\nM. Narayanaswamy, Z. Liu, G. Saxena, A. Kipf, and T. Kraska, \u201cWhy\ntpc is not enough: An analysis of the amazon redshift fleet,\u201d inVLDB\n2024, 2024.\n[7] Data World, \u201cBurned-out data engineers are\ncalling for dataops,\u201d 2021. [Online]. Avail-\nable: https://data.world/reports-and-tools/data-engineering-survey-2021-\nburned-out-data-engineers-are-calling-for-dataops/\n[8] Z. Wang, T.-H. P. Chen, H. Zhang, and S. Wang, \u201cAn empirical study on\nthe challenges that developers encounter when developing apache spark\napplications,\u201dJournal of Systems and Software, vol. 194, p. 111488,\n2022.\n[9] J. Yasmin, J. Wang, Y . Tian, and B. Adams, \u201cAn empirical study of\ndevelopers\u2019 challenges in implementing workflows as code: A case\nstudy on apache airflow,\u201dArXiv, vol. abs/2406.00180, 2024. [Online].\nAvailable: https://api.semanticscholar.org/CorpusID:270213226\n[10] H. Foidl, V . Golendukhina, R. Ramler, and M. Felderer, \u201cData\npipeline quality: Influencing factors, root causes of data-related\nissues, and processing problem areas for developers,\u201dJournal of\nSystems and Software, vol. 207, p. 111855, 2024. [Online]. Available:\nhttps://www.sciencedirect.com/science/article/pii/S0164121223002509\n[11] J. Tagliabue, T. Caraza-Harter, and C. Greco, \u201cBauplan: Zero-copy,\nscale-up faas for data pipelines,\u201d inProceedings of the 10th\nInternational Workshop on Serverless Computing, ser. WoSC10 \u201924.\nNew York, NY , USA: Association for Computing Machinery, 2024, p.\n31\u201336. [Online]. Available: https://doi.org/10.1145/3702634.3702955\n[12] S. Yao, J. Zhao, D. Yu, N. Du, I. Shafran, K. Narasimhan, and Y . Cao,\n\u201cReact: Synergizing reasoning and acting in language models,\u201d 2023.\n[Online]. Available: https://arxiv.org/abs/2210.03629\n[13] J. Tagliabue, R. Curtin, and C. Greco, \u201c FaaS and Furious:\nabstractions and differential caching for efficient data pre-\nprocessing ,\u201d in2024 IEEE International Conference on\nBig Data (BigData). Los Alamitos, CA, USA: IEEE\nComputer Society, Dec. 2024, pp. 3562\u20133567. [Online].", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2881, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "76ffa9d1-cf25-404f-af7e-a73cbebaaa61": {"__data__": {"id_": "76ffa9d1-cf25-404f-af7e-a73cbebaaa61", "embedding": null, "metadata": {"page_label": "5", "file_name": "Safe Untrusted Proof-Carrying AI Agents toward the agentic lakehouse.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Safe Untrusted Proof-Carrying AI Agents toward the agentic lakehouse.pdf", "file_type": "application/pdf", "file_size": 752899, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "66ad9bbf-b68e-4fb3-a2a0-1d0afa59cddc", "node_type": "4", "metadata": {"page_label": "5", "file_name": "Safe Untrusted Proof-Carrying AI Agents toward the agentic lakehouse.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Safe Untrusted Proof-Carrying AI Agents toward the agentic lakehouse.pdf", "file_type": "application/pdf", "file_size": 752899, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "2ee11d505b43e1d49dd01b8fb38c0629e16f9dbfcbec4921a131279fb34d1796", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e65e59dd-775c-4c22-89f6-6c8a319ebf5b", "node_type": "1", "metadata": {"page_label": "5", "file_name": "Safe Untrusted Proof-Carrying AI Agents toward the agentic lakehouse.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Safe Untrusted Proof-Carrying AI Agents toward the agentic lakehouse.pdf", "file_type": "application/pdf", "file_size": 752899, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "b1152e8ead450a0bcaffad2ac9a2db05a8c4c227b368cbfa5658e64438ff024c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "83808c69-31e8-42ae-b76e-515722df651f", "node_type": "1", "metadata": {}, "hash": "2712ea81ab437fe32e44335c9bad1d62676dc11bc6c8c50a11b5856c121f53d9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "31\u201336. [Online]. Available: https://doi.org/10.1145/3702634.3702955\n[12] S. Yao, J. Zhao, D. Yu, N. Du, I. Shafran, K. Narasimhan, and Y . Cao,\n\u201cReact: Synergizing reasoning and acting in language models,\u201d 2023.\n[Online]. Available: https://arxiv.org/abs/2210.03629\n[13] J. Tagliabue, R. Curtin, and C. Greco, \u201c FaaS and Furious:\nabstractions and differential caching for efficient data pre-\nprocessing ,\u201d in2024 IEEE International Conference on\nBig Data (BigData). Los Alamitos, CA, USA: IEEE\nComputer Society, Dec. 2024, pp. 3562\u20133567. [Online]. Available:\nhttps://doi.ieeecomputersociety.org/10.1109/BigData62323.2024.10825377\n[14] J. Tagliabue and C. Greco, \u201cReproducible data science over data lakes:\nreplayable data pipelines with bauplan and nessie,\u201d inProceedings of\nthe Eighth Workshop on Data Management for End-to-End Machine\nLearning, ser. DEEM \u201924. New York, NY , USA: Association\nfor Computing Machinery, 2024, p. 67\u201371. [Online]. Available:\nhttps://doi.org/10.1145/3650203.3663335\n[15] S. Salami, \u201cHub star modeling 2.0 for medallion architecture,\u201d 2025.\n[Online]. Available: https://arxiv.org/abs/2504.08788\n[16] NYC.Gov, \u201cTlc trip record data,\u201d 2025. [Online]. Available:\nhttps://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page\n[17] J. Tagliabue, H. Bowne-Anderson, V . Tuulos, S. Goyal, R. Cledat,\nand D. Berg, \u201cReasonable scale machine learning with open-source\nmetaflow,\u201d 2023. [Online]. Available: https://arxiv.org/abs/2303.11761\n[18] J. Tagliabue, C. Greco, and L. Bigon, \u201cBuilding a serverless data\nlakehouse from spare parts,\u201dArXiv, vol. abs/2308.05368, 2023. [Online].\nAvailable: https://api.semanticscholar.org/CorpusID:260775634\n[19] A. Cerone, G. Bernardi, and A. Gotsman, \u201cA Framework for Trans-\nactional Consistency Models with Atomic Visibility,\u201d in26th Inter-\nnational Conference on Concurrency Theory (CONCUR 2015), ser.\nLeibniz International Proceedings in Informatics (LIPIcs), L. Aceto\nand D. de Frutos Escrig, Eds., vol. 42. Dagstuhl, Germany: Schloss\nDagstuhl \u2013 Leibniz-Zentrum f \u00a8ur Informatik, 2015, pp. 58\u201371.\n[20] Jacopo Tagliabue et al., \u201cUnder Review,\u201d 2025.\n[21] K. Yang, J. Liu, J. Wu, C. Yang, Y . R. Fung, S. Li, Z. Huang,\nX. Cao, X. Wang, Y . Wang, H. Ji, and C. Zhai, \u201cIf llm is the\nwizard, then code is the wand: A survey on how code empowers\nlarge language models to serve as intelligent agents,\u201d 2024. [Online].\nAvailable: https://arxiv.org/abs/2401.00812\n[22] X. Wang, Y . Chen, L. Yuan, Y . Zhang, Y . Li, H. Peng, and H. Ji,\n\u201cExecutable code actions elicit better llm agents,\u201d 2024. [Online].\nAvailable: https://arxiv.org/abs/2402.01030\n[23] Y . Gu, Y . Xiong, J. Mace, Y . Jiang, Y .", "mimetype": "text/plain", "start_char_idx": 2334, "end_char_idx": 4980, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "83808c69-31e8-42ae-b76e-515722df651f": {"__data__": {"id_": "83808c69-31e8-42ae-b76e-515722df651f", "embedding": null, "metadata": {"page_label": "5", "file_name": "Safe Untrusted Proof-Carrying AI Agents toward the agentic lakehouse.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Safe Untrusted Proof-Carrying AI Agents toward the agentic lakehouse.pdf", "file_type": "application/pdf", "file_size": 752899, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "66ad9bbf-b68e-4fb3-a2a0-1d0afa59cddc", "node_type": "4", "metadata": {"page_label": "5", "file_name": "Safe Untrusted Proof-Carrying AI Agents toward the agentic lakehouse.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Safe Untrusted Proof-Carrying AI Agents toward the agentic lakehouse.pdf", "file_type": "application/pdf", "file_size": 752899, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "2ee11d505b43e1d49dd01b8fb38c0629e16f9dbfcbec4921a131279fb34d1796", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "76ffa9d1-cf25-404f-af7e-a73cbebaaa61", "node_type": "1", "metadata": {"page_label": "5", "file_name": "Safe Untrusted Proof-Carrying AI Agents toward the agentic lakehouse.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Safe Untrusted Proof-Carrying AI Agents toward the agentic lakehouse.pdf", "file_type": "application/pdf", "file_size": 752899, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "9079eabcc8c44c6b833cbed7370d089ea8fe1d1669aba58ff995cc4888df71ba", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "R. Fung, S. Li, Z. Huang,\nX. Cao, X. Wang, Y . Wang, H. Ji, and C. Zhai, \u201cIf llm is the\nwizard, then code is the wand: A survey on how code empowers\nlarge language models to serve as intelligent agents,\u201d 2024. [Online].\nAvailable: https://arxiv.org/abs/2401.00812\n[22] X. Wang, Y . Chen, L. Yuan, Y . Zhang, Y . Li, H. Peng, and H. Ji,\n\u201cExecutable code actions elicit better llm agents,\u201d 2024. [Online].\nAvailable: https://arxiv.org/abs/2402.01030\n[23] Y . Gu, Y . Xiong, J. Mace, Y . Jiang, Y . Hu, B. Kasikci, and P. Cheng,\n\u201cArgos: Agentic time-series anomaly detection with autonomous rule\ngeneration via large language models,\u201d 2025. [Online]. Available:\nhttps://arxiv.org/abs/2501.14170\n[24] S. Liu, S. Ponnapalli, S. Shankar, S. Zeighami, A. Zhu, S. Agarwal,\nR. Chen, S. Suwito, S. Yuan, I. Stoica, M. Zaharia, A. Cheung,\nN. Crooks, J. E. Gonzalez, and A. G. Parameswaran, \u201cSupporting\nour ai overlords: Redesigning data systems to be agent-first,\u201d 2025.\n[Online]. Available: https://arxiv.org/abs/2509.00997", "mimetype": "text/plain", "start_char_idx": 4485, "end_char_idx": 5498, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ab2920af-d9d7-4d26-9b9c-a5d6fd8b9f6d": {"__data__": {"id_": "ab2920af-d9d7-4d26-9b9c-a5d6fd8b9f6d", "embedding": null, "metadata": {"page_label": "1", "file_name": "Teaching NLP with Bracelets and Restaurant Menus An Interactive Workshop for Italian Students.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Teaching NLP with Bracelets and Restaurant Menus An Interactive Workshop for Italian Students.pdf", "file_type": "application/pdf", "file_size": 3286173, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8182cd31-5d48-4aa4-888a-538a8d6ce947", "node_type": "4", "metadata": {"page_label": "1", "file_name": "Teaching NLP with Bracelets and Restaurant Menus An Interactive Workshop for Italian Students.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Teaching NLP with Bracelets and Restaurant Menus An Interactive Workshop for Italian Students.pdf", "file_type": "application/pdf", "file_size": 3286173, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "374ac25a017cc334f10cf22053a12bced8dbdc816e95b1f8fb085e65bfea8c62", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Teaching NLP with Bracelets and Restaurant Menus:\nAn Interactive Workshop for Italian Students\nLudovica Pannitto\nUniversity of Trento\nludovica.pannitto@unitn.it\nLucia Busso\nAston University\nl.busso@aston.ac.uk\nClaudia Roberta Combei\nUniversity of Bologna\nclaudiaroberta.combei@unibo.it\nLucio Messina\nIndependent Researcher\nlucio.messina@autistici.org\nAlessio Miaschi\nUniversity of Pisa\nalessio.miaschi@phd.unipi.it\nGabriele Sarti\nUniversity of Trieste\ngsarti@sissa.it\nMalvina Nissim\nUniversity of Groningen\nm.nissim@rug.nl\nAbstract\nAlthough Natural Language Processing (NLP)\nis at the core of many tools young people\nuse in their everyday life, high school cur-\nricula (in Italy) do not include any computa-\ntional linguistics education. This lack of ex-\nposure makes the use of such tools less re-\nsponsible than it could be and makes choos-\ning computational linguistics as a university\ndegree unlikely. To raise awareness, curios-\nity, and longer-term interest in young people,\nwe have developed an interactive workshop de-\nsigned to illustrate the basic principles of NLP\nand computational linguistics to high school\nItalian students aged between 13 and 18 years.\nThe workshop takes the form of a game in\nwhich participants play the role of machines\nneeding to solve some of the most common\nproblems a computer faces in understanding\nlanguage: from voice recognition to Markov\nchains to syntactic parsing. Participants are\nguided through the workshop with the help\nof instructors, who present the activities and\nexplain core concepts from computational lin-\nguistics. The workshop was presented at nu-\nmerous outlets in Italy between 2019 and 2021,\nboth face-to-face and online.\n1 Introduction\nHave you used Google this week? This question\nwould kick off the activity that we describe in this\npaper every time we delivered it. And a number of\nfollow-up comments would generally appear.What\nfor? Translating, getting some help for homework,\nlooking for info, writing collaboratively - and get-\nting spelling correction!\nIn our workshops, we talk to groups of teenagers\n\u2013 even if someone has not personally used any of\nthose tools on a daily basis, it is utmost unlikely that\nthey have never interacted with a vocal assistant,\nwondered how their email spam \ufb01lter works, used\ntext predictions, or spoken to a chat-bot. Also,\napplications that do not require a proactive role\nof the user are growing: most of us, for example,\nare subject to targeted advertising, pro\ufb01led on the\ncontent we produce and share on social media.\nNatural Language Processing (NLP) has grown\nat an incredibly fast pace, and it is at the core of\nmany of the tools we use every day.1 At the same\ntime, though, awareness of its underlying mecha-\nnisms and the scienti\ufb01c discussion that has led to\nsuch innovations, and even NLP\u2019s very existence\nas a scienti\ufb01c discipline is generally much less\nwidespread and is basically unknown to the general\npublic (Grandi and Masini, 2018).\nA concurrent cause to this lack of awareness re-\nsides in the fact that in more traditional high-school\nformal education systems, such as the Italian one,\n\u201cyoung disciplines\" such as Linguistics and Com-\nputer Science tend to be overlooked. Grammar,\nthat in a high-school setting is the closest \ufb01eld to\nLinguistics, is rarely taught as a descriptive disci-\npline; oftentimes, it is presented as a set of norms\nthat one should follow in order to speak and write\ncorrectly in a given language. While this approach\nhas its bene\ufb01ts, it is particularly misleading when\nit comes to what actual linguistic research is about.\nSimilarly, Computer Science is often misread by\nthe general public as an activity that deals with\ncomputers, while aspects concerning information\ntechnology and language processing are often ne-\n1In this discussion, and throughout the paper, we con\ufb02ate\nthe terms Natural Language Processing and Computational\nLinguistics and use them interchangeably.\narXiv:2104.12422v2  [cs.CL]  14 May 2021", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3961, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "0843cce8-40e1-4bae-bddc-1f32f86051af": {"__data__": {"id_": "0843cce8-40e1-4bae-bddc-1f32f86051af", "embedding": null, "metadata": {"page_label": "2", "file_name": "Teaching NLP with Bracelets and Restaurant Menus An Interactive Workshop for Italian Students.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Teaching NLP with Bracelets and Restaurant Menus An Interactive Workshop for Italian Students.pdf", "file_type": "application/pdf", "file_size": 3286173, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "cd4215e9-931c-40f0-99c8-604f0ad0264b", "node_type": "4", "metadata": {"page_label": "2", "file_name": "Teaching NLP with Bracelets and Restaurant Menus An Interactive Workshop for Italian Students.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Teaching NLP with Bracelets and Restaurant Menus An Interactive Workshop for Italian Students.pdf", "file_type": "application/pdf", "file_size": 3286173, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "35a94709eefb1564bfda4cb1d0fafdf4defb8f9de3817e40b2a56aff1819e6f0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "22cf4a36-db05-457c-b52e-e3cb380b5ee0", "node_type": "1", "metadata": {}, "hash": "bb34cb91dd4a66332c8520f6fb2b228ed2b6559daea90113058de41f04c45a61", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "glected. This often leads to two important conse-\nquences. First, despite the overwhelming amount\nof NLP applications, students and citizens at large\nlack the basic notions that would allow them to\nfully understand technology and interact with it in\na responsible and critical way. Second, high-school\nstudents might not be aware of Computational Lin-\nguistics as an option for their university degree.\nOftentimes, students that enrol in Humanities de-\ngrees are mainly interested in literature and they\nonly get acquainted with linguistics as discipline\nat university. At the same time, most Computer\nScience curricula in Italian higher education rarely\nfocus on natural language-based applications. As a\nresult, Computational Linguistics as such is practi-\ncally never taught before graduate studies.\nAs members of the Italian Association for Com-\nputational Linguistics (AILC, www.ai-lc.it)\nwe have long felt the need to bridge this knowl-\nedge gap, and made dissemination a core goal of\nthe Association. As a step in this direction, we\nhave developed a dissemination activity that cov-\ners the basic aspects of what it means to process\nand analyze language computationally. This is the\n\ufb01rst activity of its kind developed and promoted by\nAILC, and to the best of our knowledge, among the\n\ufb01rst in Italy at large.\nThis contribution describes the activity itself, the\nway it was implemented as a workshop for high\nschool students in the context of several dissemina-\ntion events, and how it can serve as a blueprint to\ndevelop similar activities for yet new languages.\n2 Genesis and Goals\nWe set to develop an activity whose main aim\nwould be to provide a broad overview of language\nmodeling, and, most importantly, to highlight the\nopen challenges in language understanding and\ngeneration.\nWithout any ambition to present and explain the\nactual NLP techniques to students, we rather fo-\ncused on showing how language, which is usually\nconceptualized by the layperson as a simple and\nmonolithic object, is instead a complex strati\ufb01ca-\ntion of interconnected layers that need to be disen-\ntangled in order to provide a suitable formalization.\nIn developing our activity, we took inspiration\nfrom the word salad Linguistic Puzzle, as pub-\nlished in Radev and Pustejovsky (2013):\nCharlie and Jane had been passing notes in\nclass, when suddenly their teacher Mr. John-\nson saw what was going on. He rushed to\nthe back of the class, took the note Charlie\nhad just passed Jane, and ripped it up, drop-\nping the pieces on the \ufb02oor. Jane noticed\nthat he had managed to rip each word of\nthe message onto a separate piece of paper.\nThe pieces of paper were, in alphabetical\norder, as follows: dog, in, is, my, school,\nthe. Most likely, what did Charlie\u2019s note\noriginally say?\nThe problem includes a number of follow up\nquestions that encourage the student to re\ufb02ect upon\nthe boundaries of sentence structure. In particular,\nwe found that the word salad puzzle would give us\nthe opportunity to introduce some of the core as-\npects of Computational Linguistics\u2019 research. Ap-\nproaching the problem with no previous knowledge\nhelps raising some crucial questions, such as: what\nare the building blocks of our linguistic ability that\nallow us to perform such a task?, how much knowl-\nedge can we extract from text alone? , what does\nlinguistic knowledge look like?\nSince the workshop here presented is the \ufb01rst\nactivity of this kind in the Italian context, we took\ninspiration from games and problems such as those\noutlined in Radev and Pustejovsky (2013) and used\nfor the North American Computational Linguis-\ntics Olympiads, similar to the ones described in\nVan Halteren (2002) and Iomdin et al. (2013). Par-\nticularly, we were inspired by the institution of\n(Computational) Linguistic Olympiads in making\nour workshop a problem-solving game with dif-\nferent activities, each related to a different aspect\nof computational language processing. Linguistic\nOlympiads are now an established annual event in\nmany parts of the world since they \ufb01rst took place\nin Moscow in 1965. In these competitions students\n(generally of high-school age) are faced with lin-\nguistic problems of varying nature, that require\nparticipants to use problem-solving abilities to un-\ncover underlying patterns or rules in the data.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4294, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "22cf4a36-db05-457c-b52e-e3cb380b5ee0": {"__data__": {"id_": "22cf4a36-db05-457c-b52e-e3cb380b5ee0", "embedding": null, "metadata": {"page_label": "2", "file_name": "Teaching NLP with Bracelets and Restaurant Menus An Interactive Workshop for Italian Students.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Teaching NLP with Bracelets and Restaurant Menus An Interactive Workshop for Italian Students.pdf", "file_type": "application/pdf", "file_size": 3286173, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "cd4215e9-931c-40f0-99c8-604f0ad0264b", "node_type": "4", "metadata": {"page_label": "2", "file_name": "Teaching NLP with Bracelets and Restaurant Menus An Interactive Workshop for Italian Students.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Teaching NLP with Bracelets and Restaurant Menus An Interactive Workshop for Italian Students.pdf", "file_type": "application/pdf", "file_size": 3286173, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "35a94709eefb1564bfda4cb1d0fafdf4defb8f9de3817e40b2a56aff1819e6f0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0843cce8-40e1-4bae-bddc-1f32f86051af", "node_type": "1", "metadata": {"page_label": "2", "file_name": "Teaching NLP with Bracelets and Restaurant Menus An Interactive Workshop for Italian Students.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Teaching NLP with Bracelets and Restaurant Menus An Interactive Workshop for Italian Students.pdf", "file_type": "application/pdf", "file_size": 3286173, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "5cddde2c543ddc2ee3743c57680a4bbb9d877c3bc22149295651b17c9d06d8e3", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "(2013). Par-\nticularly, we were inspired by the institution of\n(Computational) Linguistic Olympiads in making\nour workshop a problem-solving game with dif-\nferent activities, each related to a different aspect\nof computational language processing. Linguistic\nOlympiads are now an established annual event in\nmany parts of the world since they \ufb01rst took place\nin Moscow in 1965. In these competitions students\n(generally of high-school age) are faced with lin-\nguistic problems of varying nature, that require\nparticipants to use problem-solving abilities to un-\ncover underlying patterns or rules in the data. For\nan in-depth discussion of the history and diffusion\nof Linguistic Olympiads in the world, see Derzhan-\nski and Payne (2010) and Littell et al. (2013).\nIn the choice of algorithms to include in our\ndissemination activity, we decided to leave aside\nneural networks and instead focus on traditional\nstatistical approaches, both for historical reasons\nand for the fact that these convey more clearly the\ndistinction between different layers of linguistic in-\nformation and their roles in language modeling. A", "mimetype": "text/plain", "start_char_idx": 3685, "end_char_idx": 4803, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3a1776cb-ca46-4ff5-9c05-03d2b3908de0": {"__data__": {"id_": "3a1776cb-ca46-4ff5-9c05-03d2b3908de0", "embedding": null, "metadata": {"page_label": "3", "file_name": "Teaching NLP with Bracelets and Restaurant Menus An Interactive Workshop for Italian Students.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Teaching NLP with Bracelets and Restaurant Menus An Interactive Workshop for Italian Students.pdf", "file_type": "application/pdf", "file_size": 3286173, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6d8f42cc-45e9-46b6-9cfc-9a8761acf7af", "node_type": "4", "metadata": {"page_label": "3", "file_name": "Teaching NLP with Bracelets and Restaurant Menus An Interactive Workshop for Italian Students.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Teaching NLP with Bracelets and Restaurant Menus An Interactive Workshop for Italian Students.pdf", "file_type": "application/pdf", "file_size": 3286173, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "e7d4f29a1bd5aca469af9949fe439d14e6b9e18897396fe3e5b31dfba19478a0", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Activity \u0097 Aim\n1. Get to know a (computational) linguist 10\u2019 collaboratively build a de\ufb01nition for linguistics as a\nstudy \ufb01eld\n2. Are computers able to hear? 15\u2019 familiarize with the concept of simulation of humans\u2019\nspeech perception abilities\n3. Are computers able to read? 30\u2019 introduce corpora as sources of linguistic knowledge\nand statistical patterns as structural aspects of lan-\nguage\n4. Do computers know grammar? 30\u2019 introduce human annotation and meta-linguistic\nknowledge as powerful research tools\n5. Becoming a computational linguist 5\u2019 evaluate pros and cons of the two presented ap-\nproaches, future directions and discuss about what\u2019s\nneeded to become a computational linguist\nTable 1: Sections of the activity, with their planned duration and a broad aim for each of them.\nbrief discussion of most recent NLP technologies,\nincluding the application of neural networks, is in-\ncluded in the \ufb01nal part of the workshop (Sec. 3.5).\nThe activity is targeted at students in their last\nyear of middle school (13 years of age) or older.\nWhile we believe 13 is a good entry point, there\nisn\u2019t an actual upper-bound, since the activity can\nbe enjoyed by people of any older age (though\nin practice participation was mainly offered to\nschools, with the oldest students being 18-19). We\nthought this would be the appropriate target audi-\nence of this workshop for two main reasons. On\nthe one hand, we believe that coming to the activity\nwith a richer metalinguistic background, typically\nacquired during the \ufb01rst Italian school cycle, would\nbe bene\ufb01cial for the attendees to better grasp the\ndifferences between the scienti\ufb01c approach to lan-\nguage and the more prescriptive approach they are\nexposed to in school. On the other hand we also\nconceived the activity as a way of helping students\nin their future study choices: we therefore included\nboth students attending their last year of middle\nschool and therefore about to choose a high school\ncurriculum as well as high school students, the lat-\nter in order to provide them with more options for\ntheir university choices.\n3 The activity\nWe planned our dissemination activity for a 90\nminutes time slot, divided into \ufb01ve parts, as detailed\nin Table 1.\n3.1 Get to know a (computational) linguist\nThe \ufb01rst 15 minutes of the workshop are organized\nboth as an ice-breaker activity for the attendees,\nand as a brief introduction to linguistics and com-\nputational linguistics more speci\ufb01cally.\nDuring the introduction we tried to demystify\nsome common misconceptions about linguistics,\n(i.e., a linguist knows many languages, linguists get\nsometimes confused with speech therapists, a lin-\nguist will correct my grammar, etc.): we presented\nparticipants with a list of possible de\ufb01nitions, and\nthey had to identify appropriate ones. We broadly\nde\ufb01ned linguistics as the study of language as a bio-\nlogical, psychological, and cultural object. Compu-\ntational Linguistics was then introduced both as a\ncommercial and engineering-oriented \ufb01eld, as well\nas a purely scienti\ufb01c research discipline.2\nWe also brie\ufb02y discussed linguistic questions\nwith participants as an example of the kind of prob-\nlems that a linguist tries to approach during their re-\nsearch activity. These included: \"How many ways\nof pronouncing n do we have in Italian?\", \"Do num-\nbers from one to ten exist in all languages?\".\nFor the following parts of the activity, the intro-\nduction to each sub-part was dedicated to a re\ufb02ec-\ntion upon what it means for us humans to hear,\nread and understand grammar, and whether there\nis a difference when the same tasks are performed\nby computers.\n3.2 Are computers able tohear?\nAs vocal assistants such as Alexa, Siri, Google\nHome etc. have become increasingly popular, we\nchose them as tangible examples of NLP technolo-\ngies to kick-off the games. The aim of this section\n2We relied on the de\ufb01nition reported in https:\n//www.thebritishacademy.ac.uk/blog/\nwhat-computational-linguistics/.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3955, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f83fb6d7-1087-43f2-aaec-55c1fe3a608c": {"__data__": {"id_": "f83fb6d7-1087-43f2-aaec-55c1fe3a608c", "embedding": null, "metadata": {"page_label": "4", "file_name": "Teaching NLP with Bracelets and Restaurant Menus An Interactive Workshop for Italian Students.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Teaching NLP with Bracelets and Restaurant Menus An Interactive Workshop for Italian Students.pdf", "file_type": "application/pdf", "file_size": 3286173, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f0161f95-883f-4fa1-8f51-6513d40aaef8", "node_type": "4", "metadata": {"page_label": "4", "file_name": "Teaching NLP with Bracelets and Restaurant Menus An Interactive Workshop for Italian Students.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Teaching NLP with Bracelets and Restaurant Menus An Interactive Workshop for Italian Students.pdf", "file_type": "application/pdf", "file_size": 3286173, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "21b993e8d55b175258fd10f6e84d1c7bb890463ef2795e8d6e03213f689f507c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "of the workshop was to demonstrate two points:\n\u2022 computers do not necessarily solve linguistic\ntasks the way we solve them; they are there-\nfore simulating our abilities without replicat-\ning them;\n\u2022 consequently, the concepts of easy and dif\ufb01-\ncult tasks have to be carefully revised when\napplied to language models.\nWe introduced the McGurk effect (McGurk and\nMacDonald, 1976), to clarify how hearing lan-\nguage is a complex task in itself, involving a broad\nset of aspects beyond simple sound perception,\nsuch as the visual system as well as the expec-\ntations regarding the upcoming input. Computers\non the other hand hear on the basis of an audio\nsignal that is processed (Figure 1), at least in the\nmost traditional and well-established architectures,\nwithout access to higher level linguistic knowledge\nor information from the communicative context.\nWe then brie\ufb02y presented speech recognition as a\ndirect optimization task (i.e., given an audio signal,\n\ufb01nd the word in a given database that maximises the\nprobability of being associated to that signal) and\nintroduced one of the major challenges that speech\nrecognition models are still facing, namely the abil-\nity to adapt to different speech styles (i.e., speakers\nof different language varieties and dialects, non-\nnative speakers, speakers with impairments etc.).\nIn order to further demonstrate this, we tested\nthe attendees\u2019 ability to adapt their hearing skills\nto different speech varieties by making them hear\nconversations in various Italian regional accents.3\nWhile we asked attendees to guess the name of\nthe region of the speakers, the actual aim was to\nshow how we easily adapt to understand speech,\ndifferently from speech recognition systems.\n3.3 Are computers able toread?\nFrom this moment on, the attendees worked on\nwritten text. The activity described in this section\nwas aimed at showing how salient aspects related\nto language structure can be derived from the sta-\ntistical properties of language.\nFollowing the \u201cWord Salad\u201d puzzle (Radev and\nPustejovsky, 2013), the ability of reading was pre-\nsented as follows: given a set of words, are we\nable to rearrange them in a plausible sentence-like\norder? We demonstrated how this is an easy task\n3Conversations were extracted from corpus CLIPS (Al-\nbano Leoni et al., 2007).\nFigure 1: Representation of the audio signal for the ital-\nian word destra (en. right). This was used to show how\ninformation coming from audio signals can be repre-\nsented in a way that easily allows the computer to per-\nform a pattern matching task, but would be impossible\nto process for humans.\nfor human beings, when one deals with a language\nthey are familiar with (Figure 2), while, generally\none may not be able to perform the same task in\ncase of unknown languages (Figure 3), where each\npossible ordering seems equally plausible.\nFigure 2: A set of Italian words (from the top left cor-\nner, en. is, garden, in, my, the, dog): when asked to re-\narrange them into a sentence, participants would \ufb01rst\ncome up with the most likely ordering (i.e., il mio cane\n\u00e8 nel giardino , en. my dog is in the garden ) and if\nprompted they would then produce more creative sen-\ntences (i.e., il cane nel giardino \u00e8 mio , en. the dog in\nthe garden is mine ). They would however never con-\nsider ungrammatical sequences as possible sentences.\nWe therefore gave the attendees a deck of un-\nknown, mysterious tokens (left card in Figure 4)\nand asked them to come up with the most plausi-\nble sentence that contained all of them. The cards\nrepresented either Italian or English words (par-\nticipants were divided into two teams, each one\ndealing with a different masked language) which\nhad however been transliterated into an unknown\nalphabet. While this was obviously an impossible\ntask to solve, it gave us the opportunity to introduce\na notion of probability in the linguistic realm. We\ncast it as the expectation that we humans bear on", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3932, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8ae4d42e-d28a-4e7d-88e6-6dfaacf53606": {"__data__": {"id_": "8ae4d42e-d28a-4e7d-88e6-6dfaacf53606", "embedding": null, "metadata": {"page_label": "5", "file_name": "Teaching NLP with Bracelets and Restaurant Menus An Interactive Workshop for Italian Students.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Teaching NLP with Bracelets and Restaurant Menus An Interactive Workshop for Italian Students.pdf", "file_type": "application/pdf", "file_size": 3286173, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "789cbb66-582a-443b-bccc-898e26215823", "node_type": "4", "metadata": {"page_label": "5", "file_name": "Teaching NLP with Bracelets and Restaurant Menus An Interactive Workshop for Italian Students.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Teaching NLP with Bracelets and Restaurant Menus An Interactive Workshop for Italian Students.pdf", "file_type": "application/pdf", "file_size": 3286173, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "7be383d4b390b714620350f8939809d75732510aafa4e51b382842890e897202", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Figure 3: The \ufb01gure depicts the same situation as Fig-\nure 2, this time with non-words.\nFigure 4: Example cards, both showing a word. Left:\na card for the \ufb01rst activity, with a button loop to thread\nit in a sentence. Right: a card for the second activity,\nwith part of speech (number) at the bottom.\nthe appearance of a speci\ufb01c linguistic sequence,\nand the subsequent need for a source of linguistic\nknowledge to implement the same notion.\nWhen asked to perform the same task on the\nwords of Figure 2, participants produced sentences\nin a quite consistent order, and the most prototyp-\nical sentences (e.g., il mio cane \u00e8 nel giardino ,\nen. my dog is in the garden) were usually elicited\nbefore some less typical ones (e.g., il cane nel gi-\nardino \u00e8 mio, en. the dog in the garden is mine ),\nwhile agrammatical sentences (i.e., random permu-\ntations) were never produced.\nWe justi\ufb01ed their responses by explaining that\nhumans accumulate a great amount of linguistic\nknowledge throughout their lifetime that helps\nthem re\ufb01ne these expectations, while machines are\ninstead in principle unbiased towards having any\nspeci\ufb01c preference. This observation allowed us to\nintroduce participants to the notion of corpus as a\nlarge collection of linguistic data that mimics the\namount of data we are exposed to as humans.\nEach team was then provided with a corpus writ-\nten in an unknown language (approximately 60 sen-\ntences hand-crafted by transliterating a portion of\nthe \u201cSnow White\u201d tale into a mysterious alphabet\nFigure 5). Concurrently, we introduced a simple\nalgorithm to tell apart sentence-like orderings of\nthe provided tokens from the random ones.\nThe algorithm, which we called The bracelet\nmethod (Figure 6), is based on the Markov Chain\nFigure 5: The \ufb01gure shows one of the corpora that was\ngiven to participants, 5 A3 tables containing approx.\n60 transliterated sentences from the \u201cSnow White\u201d\nfairy tale, and tokens with buttonholes that had to be\nsearched in the corpus and threaded into sentences.\nprocedure: in a scenario similar to that of lining\npearls up to form a bracelet, participants could\ndecompose the task of forming up a sentence into\nsmaller tasks. To make the operation more concrete,\nwe equipped each card with a button loop as shown\nin Figure 4: this way cards could be physically\nthreaded together to form a sentence.\nThe \ufb01rst step consisted in choosing the \ufb01rst word,\nfor the beginning of the sentence. Since partic-\nipants were facing a language they didn\u2019t know,\nthey were not aware of language structures nor of\nthe meaning of the tokens. In such a situation, they\ncould decide whether it was plausible to use a given\nword at the beginning of a sentence just by look-\ning up in the corpus sentences that began the same\nway. If they found at least one sentence that began\nwith the same word, it meant that that was a licit\nposition and they could use it to start the sentence.\nThe activity continued as follows: sticking to the\nbracelet metaphor, participants needed to select and\ninsert the following \"pearl\" into the thread: ideally\nthe pearl should pair well with the previous one, as\nwe might want to avoid colour mismatch (e.g., it\nis well known that blue and green do not go well\ntogether). The metaphor highlights therefore a core\naspect that holds true for language as well, namely\nthat we can condition our choice on a variable num-\nber of previous choices, and this will in\ufb02uence the\ncomplexity of the obtained pattern.\nThe activity resulted in a number of sentence-\nbracelets, as shown in Figure 7, that were then kept\naside to be translated at the end of the workshop.\n3.4 Do computers know grammar?\nWhile the previous game highlighted the impor-\ntance of statistical information in NLP, in accor-", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3739, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "64633193-e263-48fb-85b0-c124a391e00e": {"__data__": {"id_": "64633193-e263-48fb-85b0-c124a391e00e", "embedding": null, "metadata": {"page_label": "6", "file_name": "Teaching NLP with Bracelets and Restaurant Menus An Interactive Workshop for Italian Students.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Teaching NLP with Bracelets and Restaurant Menus An Interactive Workshop for Italian Students.pdf", "file_type": "application/pdf", "file_size": 3286173, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "eeeeda32-5bf5-4134-9c36-0c0782e9b23a", "node_type": "4", "metadata": {"page_label": "6", "file_name": "Teaching NLP with Bracelets and Restaurant Menus An Interactive Workshop for Italian Students.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Teaching NLP with Bracelets and Restaurant Menus An Interactive Workshop for Italian Students.pdf", "file_type": "application/pdf", "file_size": 3286173, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "1b6a07084ead13e38702ab3b9f44547057c158883effb6420f0a86d17763e83b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Figure 6: The bracelet method applied on the Italian\nwords \u00e8, mio, giardino, nel(en. is, my, garden, in). The\nalgorithm is based on bigram co-occurrences, so the\nchoice for each word is based solely on the previously\nchosen one. Colors indicate probabilities, which are\ncomputed based only on the previously chosen word\ncane (en. dog). The \ufb01rst token, il (en. the), appears\ngrayed as it is ignored for the choice.\nFigure 7: The \ufb01gure shows the result of a bracelet\nsequence: tokens are threaded together based on co-\noccurrences in the corpus.\ndance with the overall aim of the activity, we also\nwanted to introduce some of the algorithms that are\nmore deeply rooted in the linguistic tradition.\nIn order to do so, we introduced the notion of\ngrammar as a descriptive abstraction over a set of\nexamples. Out of the linguistic context, to exem-\nplify this notion of grammar metaphorically, we\npresented participants with a set of possible restau-\nrant menus (Figure 8), and encouraged them to\ncome up with the general rule that the restaurant\nowner must have had in mind when choosing those\ncombinations. All menus were built as a traditional\nItalian full meal, composed by two main dishes and\na dessert. We perpetuated the metaphor showing\nhow, once a set of rules is de\ufb01ned, these can be\nused both to decide if a new menu is likely to be\npart of the same set (Figure 9) and also to generate\nnew meals (Figure 10).\nThis metaphor, which was readily grasped by\nmost participants, was useful to show how different\ncomponents can be combined together in a mean-\nFigure 8: Each block in the image corresponds to a pos-\nsible Italian full meal, consisting of: \ufb01rst course (e.g.\nfusilli al pomodoro), second course (e.g. pollo agli as-\nparagi) and dessert (e.g. pannacotta).\nFigure 9: Step-by-step process to assess the validity of\na given menu. In the top-right corner the rules for cre-\nating a full meal are shown . Categories are de\ufb01ned re-\ncursively until each course that constitute the full menu\nis obtained and therefore reduced to the initial category\nof a pasto (en. meal).\nFigure 10: Process \ufb02ow for creating a new meal from\nthe initial category pasto (en. meal) up to the leaves\ncontaining terminal symbols such as penne, funghi,\npollo etc. (en., a type of pasta, mushrooms, chicken).\nThe rules used to generate are the same used for the\nreduction process, reported in Figure 9.\ningful way, as it happens in grammar. Before mov-\ning back to the corpus, we showed them what a\nformal grammar of the menus could look like.\nWe had previously annotated the corpus with", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2568, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e8101beb-8a1d-4dfb-9144-1c48bf6aada3": {"__data__": {"id_": "e8101beb-8a1d-4dfb-9144-1c48bf6aada3", "embedding": null, "metadata": {"page_label": "7", "file_name": "Teaching NLP with Bracelets and Restaurant Menus An Interactive Workshop for Italian Students.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Teaching NLP with Bracelets and Restaurant Menus An Interactive Workshop for Italian Students.pdf", "file_type": "application/pdf", "file_size": 3286173, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ab2030b5-fe75-450b-b26b-1a466cf0cf84", "node_type": "4", "metadata": {"page_label": "7", "file_name": "Teaching NLP with Bracelets and Restaurant Menus An Interactive Workshop for Italian Students.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Teaching NLP with Bracelets and Restaurant Menus An Interactive Workshop for Italian Students.pdf", "file_type": "application/pdf", "file_size": 3286173, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "13a70f270ff405eaea4eb2b11a7e0864162b895a04fe2e7e4a0628fd46f6a0c0", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "syntactic and morpho-syntactic information (i.e.,\npart of speech), as shown in Figures 5 and 12. We\ntherefore asked participants to extract from the cor-\npus a possible grammar, namely a set of attested\nrules and use them to generate a new sentence.\nIn order to write the grammar, participants were\ngiven several physical materials: felt strips repro-\nducing the colors of the annotation, a deck of cards\nwith numbers (identifying parts of speech) and a\ndeck of \u201c=\u201d symbols (Figure 11).\nFigure 11: A set of rules extracted during the activity\nfrom the corpus. Each rule is made of felt strips for\nphrases, cards with numbers indicating parts of speech,\nand \u201c=\u201d cards.\nWith a new deck of words (Figure 4, right panel),\nnot all of which present in the corpus, participants\nhad to generate a sentence using the previously\ncomposed rules.\n3.5 Becoming a computational linguist\nAt this point, participants had created a number\nof sentences by means of the two techniques de-\nscribed above. It is now time to discover that the\nmysterious languages they worked on were actually\nEnglish and Italian. This was achieved in practice\nby superimposing a Plexiglas frame on the A3 cor-\npus pages (Figure 13): the true nature of the cor-\npora was this way revealed as the participants could\nsee the original texts (in Italian and English) and\ntranslate the sentences they had created previously.\nThe outcome of the activity stimulated discus-\nsion amongst the participants, highlighting pros\nand cons of each approach and how could they be\nintegrated into real-life technologies. Our work-\nshop ended with a brief description of more recent\nNLP technologies and their commercial applica-\ntions, such as recommender systems, automatic\ntranslation, text completion, etc.\nSince the target audience consisted mostly of\nmiddle- and high-school students, we offered an\noverview of what it takes to become a computa-\nFigure 12: Example of categories (it. Categorie), e.g.\nphrases and POS tags, and rules (it. Regole) for the sen-\ntence \"lo specchio rispose a la regina\" (en. the mirror\nanswered to the queen).\nFigure 13: A trial session of the workshop (the picture\nshows some of the tutors explaining the game to AILC\nmembers): the original language of the corpus has just\nbeen revealed by superimposing Plexiglas supports on\nthe corpus tables.\ntional linguist and where one could study computa-\ntional linguistics in Italy.\n4 The workshop in action\nThe activity \u2013 here outlined in its complete and\noriginal form \u2013 was presented in various outlets\nduring the last year and a half.\nIt was initially designed for the 2019 edition of\nthe \u201cBergamo Scienza\u201d Science Festival4, where it\nwas presented live to over 450 participants in the\ncourse of two weeks. A simpli\ufb01ed \"print-and-play\"\nversion of the workshop was also presented at the\n2020 edition of the SISSA5 Student Day.\nDue to the Covid-19 pandemic, all other pre-\nsentations of the activity had to be moved online.\nTransposing the workshop crucially meant striving\nto maintaining the interactive nature of the activi-\nties without the possibility of meeting face to face.\n4https://festival.bergamoscienza.it/\n5https://www.sissa.it/", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3163, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "152f4ed6-59ee-424f-8a64-85ac3478cc15": {"__data__": {"id_": "152f4ed6-59ee-424f-8a64-85ac3478cc15", "embedding": null, "metadata": {"page_label": "8", "file_name": "Teaching NLP with Bracelets and Restaurant Menus An Interactive Workshop for Italian Students.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Teaching NLP with Bracelets and Restaurant Menus An Interactive Workshop for Italian Students.pdf", "file_type": "application/pdf", "file_size": 3286173, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "18c1b152-077b-4f0e-b1fd-cee76fc657aa", "node_type": "4", "metadata": {"page_label": "8", "file_name": "Teaching NLP with Bracelets and Restaurant Menus An Interactive Workshop for Italian Students.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Teaching NLP with Bracelets and Restaurant Menus An Interactive Workshop for Italian Students.pdf", "file_type": "application/pdf", "file_size": 3286173, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "ba011275d2251bfed897f98f6ff4c23a4f73324e1fdae52fd19f40b454acd3f8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "676f9d3e-cd3f-4de9-92b9-0d9afa417d36", "node_type": "1", "metadata": {}, "hash": "78b8fcba3034d1f0d13e000e73942bfea633a2ef911b4fb9b97804e89145491d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "To do so, we integrated our original presentation on\nGoogle slides with the interactive presentation soft-\nware Mentimeter6 - which we used for questions,\npolling and quizzes. The corpus and tokens were\npresented via a web interface created especially for\nthis purpose7.\nThe interface presented the masked corpus, com-\nplete with POS tags and syntactic annotations. For\nthe bracelet activity participants were automatically\nassigned a number of tokens which they could use\nto build a sentence by simply dragging and drop-\nping them. (Figures 14 and 15).\nThis online version was crafted in the \ufb01rst place\nto be presented at \u201cFestival della Scienza\u201d8 (Fig-\nure 16), a science festival primarily aimed at school\nstudents held each year in Genoa, where multi-\nple 45\u2019 sessions of the workshop were run over\nthe course of two days. The fourth activity (Sec-\ntion 3.4) involved \"bootstrapping\" syntactic rules\nbased only on our color-based annotation. To\nsimplify online interaction, we only used the un-\nmasked Italian version of the corpus and partic-\nipants played collectively, helping each other to\nbuild sentences and grammatical rules: rules were\ncollected through a Mentimeter poll, while a sen-\ntence was generated in a guided demonstration by\nthe presenter of the workshop.\nOverall, the workshop transposed really well\nonline, and was extremely well-received in this ver-\nsion as well. The online modality also allowed us\nto present it to a more vast and varied audience\nthan just students: a version for the general pub-\nlic was presented at European Researchers\u2019 Night\n(Bright Night9) at the University of Pisa, thanks to\na collaboration with the Computational Linguistics\nLaboratory10 and ILC-CNR11 in November 2020,\na dedicated session was run for the High school\nITS Buzzi12 (Prato, Italy) in December 2020, and\nduring the second edition of the Science Web Fes-\ntival13 in April 2021.\n5 Reusability: this activity as a blueprint\nAs mentioned in Section 1, our activity was in-\nspired by a collection of English-language prob-\n6https://www.menti.com/\n7A demo of the interface can be found at https://\ndonde.altervista.org/\n8http://www.festivalscienza.it\n9https://www.bright-night.it/\n10http://colinglab.humnet.unipi.it/\n11http://www.ilc.cnr.it/\n12https://www.tulliobuzzi.edu.it/\n13https://www.sciencewebfestival.it/\nlems created for students participating in the Com-\nputational Linguistics Olympiad (Radev and Puste-\njovsky, 2013).\nWe adapted the original activity to the Italian\nlanguage and context. While we tried to choose\nwidely shared linguistic principles to communi-\ncate, the operation of adapting the game to a dif-\nferent language obviously requires language spe-\nci\ufb01c choices and details, which would have to be\nre-evaluated when porting the activity to yet new\nlanguages. Particularly, since the materials have\nbeen developed for Italian, it might be the case\nthat transposition is not straightforward for some\nlanguages. However, we believe that the general\nstructure of our workshop can serve as a blueprint\nfor extension to new languages. For this reason\nall the relevant materials are made available in a\ndedicated repository. The repository includes both\nscripts to reproduce our activity as well as a gen-\neral set of insights/recommendations regarding the\nstructure and principles of the workshop.\nAll of the scripts necessary to produce the ma-\nterials used in the game\u2019s work\ufb02ow in a different\nlanguage are made available in our open-access\nrepository14. To get the activity into production\nusing the scripts provided, one only needs to create\nan annotated corpus in the target language.\nOur speci\ufb01c choice of \u201cSnow White\u201d as a cor-\npus is motivated by the fact that the story can be\nphrased in a fairly repetitive formulation, with two\nadvantages. One is that enough bigrams are present\nthat enable the generation of new sentences with\nthe bracelet method. The other one is that the story\ncontains recognizable characters (e.g., the dwarfs,\nthe evil queen etc.), so that, when the unmasked\ntext is revealed, the process results intuitively trans-\nparent.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4086, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "676f9d3e-cd3f-4de9-92b9-0d9afa417d36": {"__data__": {"id_": "676f9d3e-cd3f-4de9-92b9-0d9afa417d36", "embedding": null, "metadata": {"page_label": "8", "file_name": "Teaching NLP with Bracelets and Restaurant Menus An Interactive Workshop for Italian Students.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Teaching NLP with Bracelets and Restaurant Menus An Interactive Workshop for Italian Students.pdf", "file_type": "application/pdf", "file_size": 3286173, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "18c1b152-077b-4f0e-b1fd-cee76fc657aa", "node_type": "4", "metadata": {"page_label": "8", "file_name": "Teaching NLP with Bracelets and Restaurant Menus An Interactive Workshop for Italian Students.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Teaching NLP with Bracelets and Restaurant Menus An Interactive Workshop for Italian Students.pdf", "file_type": "application/pdf", "file_size": 3286173, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "ba011275d2251bfed897f98f6ff4c23a4f73324e1fdae52fd19f40b454acd3f8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "152f4ed6-59ee-424f-8a64-85ac3478cc15", "node_type": "1", "metadata": {"page_label": "8", "file_name": "Teaching NLP with Bracelets and Restaurant Menus An Interactive Workshop for Italian Students.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Teaching NLP with Bracelets and Restaurant Menus An Interactive Workshop for Italian Students.pdf", "file_type": "application/pdf", "file_size": 3286173, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "3a003429a5fbaa9d781391470bc942a6d6d5dec633189261655e7a07c9f6f4fa", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "All of the scripts necessary to produce the ma-\nterials used in the game\u2019s work\ufb02ow in a different\nlanguage are made available in our open-access\nrepository14. To get the activity into production\nusing the scripts provided, one only needs to create\nan annotated corpus in the target language.\nOur speci\ufb01c choice of \u201cSnow White\u201d as a cor-\npus is motivated by the fact that the story can be\nphrased in a fairly repetitive formulation, with two\nadvantages. One is that enough bigrams are present\nthat enable the generation of new sentences with\nthe bracelet method. The other one is that the story\ncontains recognizable characters (e.g., the dwarfs,\nthe evil queen etc.), so that, when the unmasked\ntext is revealed, the process results intuitively trans-\nparent. Such characteristics are desirable for the\nactivity, and should be kept in mind when choosing\na new text for a new language.\nIn addition to sharing scripts, we are sharing here\nthe core structure and principles the workshop re-\nlies on, which can be reproduced when replicating\nthe activity also for a different language.\nParts 1&2 At the beginning of the workshop (see\nSection 3.2) we show the limits of current tech-\nnologies, in particular in terms of adaptability. To\nthis end, we exploited diatopic variations, such\nas Italian regional accents, since this is an aspect\nreadily available to Italian students. In the con-\n14https://bitbucket.org/melfnt/\nmalvisindi", "mimetype": "text/plain", "start_char_idx": 3327, "end_char_idx": 4754, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "2ccc01b0-c9f3-401e-851a-6483c5ca1f6a": {"__data__": {"id_": "2ccc01b0-c9f3-401e-851a-6483c5ca1f6a", "embedding": null, "metadata": {"page_label": "9", "file_name": "Teaching NLP with Bracelets and Restaurant Menus An Interactive Workshop for Italian Students.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Teaching NLP with Bracelets and Restaurant Menus An Interactive Workshop for Italian Students.pdf", "file_type": "application/pdf", "file_size": 3286173, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "04e3ebd1-b8db-4062-a283-53bc6832b97d", "node_type": "4", "metadata": {"page_label": "9", "file_name": "Teaching NLP with Bracelets and Restaurant Menus An Interactive Workshop for Italian Students.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Teaching NLP with Bracelets and Restaurant Menus An Interactive Workshop for Italian Students.pdf", "file_type": "application/pdf", "file_size": 3286173, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "0ca256041fe2d73932e403d72c6caa99607ab07dbb17b248a5b2d6159dfe8885", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Figure 14: Interface of the online website used for the \"bracelet\" game (section 3.3) during the online workshops.\nPlayers can collaboratively drag and drop their card from the bottom left panel to form sentences in the top-left\npanel. The corpus is shown in the right panel.\nFigure 15: After the games the website shows the translation of the corpus (see for example line 1) and cards.\ntext of other languages, the same concept could be\nhowever shown by different means, such as the in-\n\ufb02uence of jargon or minority languages, or simply\ndifferences in accuracy depending on, age or other\nsocio-demographic and socio-cultural variables.\nPart 3 The next part of the activity (Section 3.3)\nis the most easily reproducible in a different lan-\nguage as it only exploits statistical co-occurrences\nas a cue for linguistic structure. The only pre-\nrequisite here is the availability of a suf\ufb01ciently\nstandardized tokenization for the language of inter-\nest. As described above, we masked the language\nthrough a transliteration system: this was achieved\neither substituting words with sequences of sym-\nbols, or with non-words. In the case of non-words,\nthese were generated by manually de\ufb01ning a series\nof phonotactic constraints for Italian, which should\nbe adapted to the target language.\nThe main message to be conveyed through this\nactivity is that language is a complex system; we\ndid this disentangling semantics from the purely\nsymbolic tier, which is the one computers most\ncommonly manipulate.\nPart 4 The following part of the activity (Sec-\ntion 3.4) focuses on the expressive power residing\nin the de\ufb01nition of auxiliary categories as descrip-\ntors of linguistic evidence. We achieved this by sim-\nplifying a constituent-based annotation for our cor-", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1755, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "665978c3-7758-423d-8713-292bcc7d611a": {"__data__": {"id_": "665978c3-7758-423d-8713-292bcc7d611a", "embedding": null, "metadata": {"page_label": "10", "file_name": "Teaching NLP with Bracelets and Restaurant Menus An Interactive Workshop for Italian Students.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Teaching NLP with Bracelets and Restaurant Menus An Interactive Workshop for Italian Students.pdf", "file_type": "application/pdf", "file_size": 3286173, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "cb8766eb-2995-4d1c-b4d4-9ca879952fd6", "node_type": "4", "metadata": {"page_label": "10", "file_name": "Teaching NLP with Bracelets and Restaurant Menus An Interactive Workshop for Italian Students.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Teaching NLP with Bracelets and Restaurant Menus An Interactive Workshop for Italian Students.pdf", "file_type": "application/pdf", "file_size": 3286173, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "8c448f0b325c45ad944a4183da6e372a91be166bf79de92275819759b1c3a4b7", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Figure 16: The picture was taken during a workshop session at \u201cFestival della Scienza\u201d in Genoa, in October 2020.\nDue to the COVID-19 pandemic, students were participating remotely. The left panel shows the tutor handling\ngrammatical categories (colored cards for syntactic phrases and letters for parts of speech); the right panel shows a\nscreenshot of the slides employed during the activity. Students could see both panels as slides were streamed while\na webcam was capturing the tutor\u2019s movements.\npus: having continuous constituents easily allowed\nfor the physical implementation of re-writing rules\n(i.e., participants had some physical constituents\nand tokens that could be used to simulate the gen-\neration process). We built categories in order to\nextract from the text a simple regular grammar,\nand we were especially careful about the fact that\nboth the Italian and the English corpus showed a\nsimilar structure and therefore complexity level.\nMore speci\ufb01cally, we restricted to \ufb01ve types of\nhigher-level categories that acted as phrases (i.e.,\nsentence, noun phrase, verb phrase, prepositional\nphrase, subordinate clause): this is of course a huge\nsimpli\ufb01cation and, for the sake of the activity, we\noverlooked some relevant linguistic phenomena.\nWe reckon that this approach might not be\nportable to languages that exhibit a \ufb02exible word\norder, so alternative solutions should be sought.\nPart 5 The last section of the activity was dedi-\ncated to a discussion on the presented methods for\nlanguage generation. After that and depending on\nthe audience, we presented some options to pursue\nstudies in Computational Linguistics in Italy, which\nwould of course have to be adapted to the target\nsocial context. Lay publications concerned with\nComputational Linguistics are also unfortunately\nnot very common in Italy, therefore we took the\nopportunity to provide participants with some sug-\ngestions for further readings (Masini and Grandi,\n2017).\n6 Looking back and ahead\nIn the previous sections we described an interactive\nworkshop designed to illustrate the basic princi-\nples of Computational Linguistics to young Italian\nstudents. It is the \ufb01rst activity of its kind to be de-\nsigned by the Italian Association for Computational\nLinguistics and among the \ufb01rst dissemination activ-\nities in Italy for Computational Linguistics directed\nto young students.\nThe activity had the broad aim of increasing\nawareness towards applications based on language\ntechnology, and introduce students to the study of\nlanguage as a scienti\ufb01c discipline.\nWe run the activity in both face-to-face and, due\nto COVID-19, online form: generally speaking, we\nreceived enthusiastic feedback both from younger\nparticipants and from the more general public. We\nadapted the activity to a variety of formats and time-\nslots, ranging from 30 to 90 minutes: the amount\nof time required to approach the game and get ac-\nquainted with the concepts is of course variable,\nand depends on the participants\u2019 background and\non the level of engagement that is expected of them.\nGenerally speaking 45 minutes are enough for a\npresentation including some interaction with the\naudience, especially in the online setting, but at\nleast 90 minutes are needed for the participants to\nfully experiment with the hands-on activity.\nWe want to speci\ufb01cally stress how time is a cen-\ntral ingredient in the activity. While the game-\nrelated aspects remained engaging and fun even\nin the shortened, online versions, in order to fully\ngrasp the mechanisms underlying the presented al-\ngorithms longer sessions would be needed. In fact,\nwe often felt that more time would be bene\ufb01cial for\na deeper discussion about language as an object of\nstudy itself, and about language as data on which\ntheories can be built. In particular, shorter time-\nslots or less guided activities enhance the risk of", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3851, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "351519a5-7135-4f82-b2a6-6485cd44c96f": {"__data__": {"id_": "351519a5-7135-4f82-b2a6-6485cd44c96f", "embedding": null, "metadata": {"page_label": "11", "file_name": "Teaching NLP with Bracelets and Restaurant Menus An Interactive Workshop for Italian Students.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Teaching NLP with Bracelets and Restaurant Menus An Interactive Workshop for Italian Students.pdf", "file_type": "application/pdf", "file_size": 3286173, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7e81fd11-d143-4fdf-8f67-17f94973dbea", "node_type": "4", "metadata": {"page_label": "11", "file_name": "Teaching NLP with Bracelets and Restaurant Menus An Interactive Workshop for Italian Students.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Teaching NLP with Bracelets and Restaurant Menus An Interactive Workshop for Italian Students.pdf", "file_type": "application/pdf", "file_size": 3286173, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "2dae01ee3f1a948ec18f03291ca40923a683fb283676ba8e4007ccc98a875452", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d34fda78-ab08-4f1a-a10f-21a3de4a98ad", "node_type": "1", "metadata": {}, "hash": "4fb11f18bf6e40cde05f949bd053dbc31dab7c636019cfbc1793234ac9c78698", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "participants approaching the challenge as a puzzle\nthat they can solve regardless of linguistic knowl-\nedge. This is because the approach to language we\nare presenting is entirely new to our audience, and\nnot only to the younger students.\nAlthough we did not have a formal system in\nplace to collect systematic feedback, the overall\nresponse across venues and conditions has been\nextremely positive.15 Curiosity and engagement\nof participants remained high both onsite and on-\nline, and we received many questions on several\naspects of Natural Language Processing and neural\nnetworks, as well as concerning its role in Arti\ufb01cial\nIntelligence at large.\nThe participants\u2019 enthusiastic questions gave us\nmany ideas for future dissemination activities. In\nfact, the technological world is advancing fast and\nwe \ufb01rmly believe that it is necessary to spread more\nawareness on the inner workings of AI-based tech-\nnologies, to develop a society-level conscience to\napproach them in a critical way.\nThe activity described in this paper was targeted\nat middle to high-school students as well as the gen-\neral public. It would be interesting to engage with\na younger audience as well, as communicating the\nstudy and (computational) modelling of language\nto them would raise awareness towards language\nstudies as a scienti\ufb01c discipline.\nFor our activity, we took inspiration from one of\nthe problems proposed in Radev and Pustejovsky\n(2013). Puzzles such as the ones presented at the\nComputational Linguistics Competitions are a great\nway to introduce both important challenges and the\nmethodology to solve them, as they stimulate stu-\ndents to investigate linguistic aspects in a bottom-\nup fashion. Organizing the competition in Italy\nwould represent a bigger project for our associa-\ntion, to be addressed in the coming years.\nAcknowledgements\nThe workshop has been developed with help and\nsupport from many parties. We would like to thank\nthem all here. First and foremost, all the partici-\npants that enthusiastically played along and made\nthe activity a success. Along with them, four tutors\nand science animators helped us greatly in putting\nour ideas into practice. We are also grateful to Berg-\namoScienza, Festival della Scienza, Science Web\n15Following one reviewer\u2019s suggestion, we have imple-\nmented such a system for our latest presentation at the Science\nWeb Festival 2021 as a Google form that we circulated among\nparticipants at the end of the presentation.\nFestival for hosting the activity during the festivals,\nto ILC-CNR \u201cAntonio Zampolli\" and ColingLab\n(University of Pisa) for hosting us during the Eu-\nropean Researchers\u2019 Night, and to the Scuola In-\nternazionale Superiore di Studi Avanzati (SISSA)\nand ITI Tullio Buzzi. We are also grateful to Dr.\nMirko Lai, who has collaborated on the develop-\nment of the web interface for the online versions of\nour activity. We would like to thank AILC (Italian\nAssociation of Computational Linguistics) for en-\ncouraging us to design the activity and supporting\nus throughout the process. Last but not least, we\nthank the true hero of our workshop: GingerCat16.\nReferences\nFederico Albano Leoni, Francesco Cutugno, and Re-\nnata Savy. 2007. Clips (corpora e lessici di ital-\niano parlato e scritto). Linguistica computazionale:\nricerche monolingui e multilingui.\nIvan A Derzhanski and Thomas Payne. 2010. The lin-\nguistics olympiads: Academic competitions in lin-\nguistics for secondary school students. Linguistics\nat school: language awareness in primary and sec-\nondary education, pages 213\u201326.\nNicola Grandi and Francesca Masini. 2018. Perch\u00e9 la\nlinguistica ha bisogno di divulgazione (e viceversa).\nIn N. Grandi and F. Masini, editors, La linguistica\ndella divulgazione, la divulgazione della linguistica.\nAtti del IV Convegno Interannuale SLI nuova serie ,\npages 5\u201312. SLI, Roma.\nBoris Iomdin, Alexander Piperski, and Anton Somin.\n2013. Linguistic problems based on text corpora.\nIn Proceedings of the Fourth Workshop on Teaching\nNLP and CL, pages 9\u201317.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4012, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d34fda78-ab08-4f1a-a10f-21a3de4a98ad": {"__data__": {"id_": "d34fda78-ab08-4f1a-a10f-21a3de4a98ad", "embedding": null, "metadata": {"page_label": "11", "file_name": "Teaching NLP with Bracelets and Restaurant Menus An Interactive Workshop for Italian Students.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Teaching NLP with Bracelets and Restaurant Menus An Interactive Workshop for Italian Students.pdf", "file_type": "application/pdf", "file_size": 3286173, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7e81fd11-d143-4fdf-8f67-17f94973dbea", "node_type": "4", "metadata": {"page_label": "11", "file_name": "Teaching NLP with Bracelets and Restaurant Menus An Interactive Workshop for Italian Students.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Teaching NLP with Bracelets and Restaurant Menus An Interactive Workshop for Italian Students.pdf", "file_type": "application/pdf", "file_size": 3286173, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "2dae01ee3f1a948ec18f03291ca40923a683fb283676ba8e4007ccc98a875452", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "351519a5-7135-4f82-b2a6-6485cd44c96f", "node_type": "1", "metadata": {"page_label": "11", "file_name": "Teaching NLP with Bracelets and Restaurant Menus An Interactive Workshop for Italian Students.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Teaching NLP with Bracelets and Restaurant Menus An Interactive Workshop for Italian Students.pdf", "file_type": "application/pdf", "file_size": 3286173, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "9dc1defb28cbc23c963d3105c82168141dd42fba415044fa121bbca9e45bbbee", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "2010. The lin-\nguistics olympiads: Academic competitions in lin-\nguistics for secondary school students. Linguistics\nat school: language awareness in primary and sec-\nondary education, pages 213\u201326.\nNicola Grandi and Francesca Masini. 2018. Perch\u00e9 la\nlinguistica ha bisogno di divulgazione (e viceversa).\nIn N. Grandi and F. Masini, editors, La linguistica\ndella divulgazione, la divulgazione della linguistica.\nAtti del IV Convegno Interannuale SLI nuova serie ,\npages 5\u201312. SLI, Roma.\nBoris Iomdin, Alexander Piperski, and Anton Somin.\n2013. Linguistic problems based on text corpora.\nIn Proceedings of the Fourth Workshop on Teaching\nNLP and CL, pages 9\u201317.\nPatrick Littell, Lori Levin, Jason Eisner, and Dragomir\nRadev. 2013. Introducing computational concepts in\na linguistics olympiad. In Proceedings of the Fourth\nWorkshop on Teaching NLP and CL, pages 18\u201326.\nFrancesca Masini and Nicola Grandi. 2017. Tutto ci\u00f2\nche hai sempre voluto sapere sul linguaggio e sulle\nlingue. Caissa Italia.\nHarry McGurk and John MacDonald. 1976. Hearing\nlips and seeing voices. Nature, 264(5588):746\u2013748.\nDragomir Radev and James Pustejovsky. 2013.Puzzles\nin logic, languages and computation: the red book .\nSpringer.\nHans Van Halteren. 2002. Teaching nlp/cl through\ngames: The case of parsing. In Proceedings of the\nACL-02 Workshop on Effective Tools and Methodolo-\ngies for Teaching Natural Language Processing and\nComputational Linguistics, pages 1\u20139.\n16https://icons8.com/illustrations/\nstyle--ginger-cat-1", "mimetype": "text/plain", "start_char_idx": 3352, "end_char_idx": 4849, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a77be512-9fa5-413c-9568-9306fd9c4154": {"__data__": {"id_": "a77be512-9fa5-413c-9568-9306fd9c4154", "embedding": null, "metadata": {"page_label": "1", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a6bb627e-c456-49bc-b3d5-448447e87f86", "node_type": "4", "metadata": {"page_label": "1", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "08fdf63e79f1c85be7e12eb4c3a03d0dce97004f944ce3331b9f4f73ca0f8c0e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "00cd395b-3217-496d-8ef3-ad7c5d06a1d1", "node_type": "1", "metadata": {}, "hash": "ca949bd1b401b4cd749d75c43c86aa9bd319dd02333a9327db32daf49ba2eac3", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The Modern Mathematics of Deep Learning\u2217\nJulius Berner\u2020 Philipp Grohs\u2021 Gitta Kutyniok\u00a7 Philipp Petersen\u2021\nAbstract\nWe describe the new \ufb01eld of mathematical analysis of deep learning. This \ufb01eld emerged around a list\nof research questions that were not answered within the classical framework of learning theory. These\nquestions concern: the outstanding generalization power of overparametrized neural networks, the role of\ndepth in deep architectures, the apparent absence of the curse of dimensionality, the surprisingly successful\noptimization performance despite the non-convexity of the problem, understanding what features are\nlearned, why deep architectures perform exceptionally well in physical problems, and which \ufb01ne aspects\nof an architecture a\ufb00ect the behavior of a learning task in which way. We present an overview of modern\napproaches that yield partial answers to these questions. For selected approaches, we describe the main\nideas in more detail.\nContents\n1 Introduction 2\n1.1 Notation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4\n1.2 Foundations of learning theory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4\n1.3 Do we need a new theory? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17\n2 Generalization of large neural networks 22\n2.1 Kernel regime . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23\n2.2 Norm-based bounds and margin theory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24\n2.3 Optimization and implicit regularization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25\n2.4 Limits of classical theory and double descent . . . . . . . . . . . . . . . . . . . . . . . . . . . 27\n3 The role of depth in the expressivity of neural networks 29\n3.1 Approximation of radial functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29\n3.2 Deep ReLU networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31\n3.3 Alternative notions of expressivity . .", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2109, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "00cd395b-3217-496d-8ef3-ad7c5d06a1d1": {"__data__": {"id_": "00cd395b-3217-496d-8ef3-ad7c5d06a1d1", "embedding": null, "metadata": {"page_label": "1", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a6bb627e-c456-49bc-b3d5-448447e87f86", "node_type": "4", "metadata": {"page_label": "1", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "08fdf63e79f1c85be7e12eb4c3a03d0dce97004f944ce3331b9f4f73ca0f8c0e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a77be512-9fa5-413c-9568-9306fd9c4154", "node_type": "1", "metadata": {"page_label": "1", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "9f320cccf759083a15ac5f714887936a06396fc10e165d5047e1593a82612e1f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": ". . 27\n3 The role of depth in the expressivity of neural networks 29\n3.1 Approximation of radial functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29\n3.2 Deep ReLU networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31\n3.3 Alternative notions of expressivity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32\n4 Deep neural networks overcome the curse of dimensionality 34\n4.1 Manifold assumption . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34\n4.2 Random sampling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35\n4.3 PDE assumption . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36\n5 Optimization of deep neural networks 39\n5.1 Loss landscape analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39\n5.2 Lazy training and provable convergence of stochastic gradient descent . . . . . . . . . . . . . 41\n\u2217A version of this review paper appears as a chapter in the book \u201cMathematical Aspects of Deep Learning\u201d by Cambridge\nUniversity Press.\n\u2020Faculty of Mathematics, University of Vienna.\n\u2021Faculty of Mathematics and Research Network DataScience@UniVienna, University of Vienna.\n\u00a7Department of Mathematics, Ludwig Maximilian University of Munich, and Department of Physics and Technology,\nUniversity of Troms\u00f8.\n1\narXiv:2105.04026v2  [cs.LG]  8 Feb 2023", "mimetype": "text/plain", "start_char_idx": 1782, "end_char_idx": 3262, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "65b1a335-4cf2-4d48-a9ea-0d1256d4ca67": {"__data__": {"id_": "65b1a335-4cf2-4d48-a9ea-0d1256d4ca67", "embedding": null, "metadata": {"page_label": "2", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ac0e6c14-6143-4286-b40f-b346270e5489", "node_type": "4", "metadata": {"page_label": "2", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "c481ff4d460df823a3b25ed32b4d97dae70bfa0542ed7fa48dac09d6055f6141", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9885ecfe-7182-4d5a-a1e3-d82f0a498a37", "node_type": "1", "metadata": {}, "hash": "9fcceb83f9180cca0526aa9aa22cbd2ece5abb1c0a7b5bf9122c3a3b044e4e63", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "6 Tangible e\ufb00ects of special architectures 44\n6.1 Convolutional neural networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45\n6.2 Residual neural networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46\n6.3 Framelets and U-Nets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47\n6.4 Batch normalization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49\n6.5 Sparse neural networks and pruning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 50\n6.6 Recurrent neural networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 52\n7 Describing the features a deep neural network learns 52\n7.1 Invariances and the scattering transform . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 52\n7.2 Hierarchical sparse representations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53\n8 E\ufb00ectiveness in natural sciences 55\n8.1 Deep neural networks meet inverse problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55\n8.2 PDE-based models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56\n1 Introduction\nDeep learning has undoubtedly established itself as the outstanding machine learning technique of recent\ntimes. This dominant position was claimed through a series of overwhelming successes in widely di\ufb00erent\napplication areas.\nPerhaps the most famous application of deep learning and certainly one of the \ufb01rst where these techniques\nbecame state-of-the-art is image classi\ufb01cation [ LBBH98, KSH12, SLJ+15, HZRS16]. In this area, deep\nlearning is nowadays the only method that is seriously considered.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1735, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "9885ecfe-7182-4d5a-a1e3-d82f0a498a37": {"__data__": {"id_": "9885ecfe-7182-4d5a-a1e3-d82f0a498a37", "embedding": null, "metadata": {"page_label": "2", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ac0e6c14-6143-4286-b40f-b346270e5489", "node_type": "4", "metadata": {"page_label": "2", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "c481ff4d460df823a3b25ed32b4d97dae70bfa0542ed7fa48dac09d6055f6141", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "65b1a335-4cf2-4d48-a9ea-0d1256d4ca67", "node_type": "1", "metadata": {"page_label": "2", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "c43c0c494cd1366556c25665d1a638b9700c35c70b0801aae21c0b7e6e02a120", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56\n1 Introduction\nDeep learning has undoubtedly established itself as the outstanding machine learning technique of recent\ntimes. This dominant position was claimed through a series of overwhelming successes in widely di\ufb00erent\napplication areas.\nPerhaps the most famous application of deep learning and certainly one of the \ufb01rst where these techniques\nbecame state-of-the-art is image classi\ufb01cation [ LBBH98, KSH12, SLJ+15, HZRS16]. In this area, deep\nlearning is nowadays the only method that is seriously considered. The prowess of deep learning classi\ufb01ers\ngoes so far that they often outperform humans in image labelling tasks [HZRS15].\nA second famous application area is the training of deep-learning-based agents to play board games or\ncomputer games, such as Atari games [ MKS+13]. In this context, probably the most prominent achievement\nyet is the development of an algorithm that beat the best human player in the game of Go [SHM+16, SSS+17]\u2014\na feat that was previously unthinkable owing to the extreme complexity of this game. Besides, even in\nmultiplayer, team-based games with incomplete information deep-learning-based agents nowadays outperform\nworld-class human teams [BBC+19, VBC+19].\nIn addition to playing games, deep learning has also led to impressive breakthroughs in the natural\nsciences. For example, it is used in the development of drugs [ MSL+15], molecular dynamics [ FHH+17], or in\nhigh-energy physics [BSW14]. One of the most astounding recent breakthroughs in scienti\ufb01c applications\nis the development of a deep-learning-based predictor for the folding behavior of proteins [ SEJ+20]. This\npredictor is the \ufb01rst method to match the accuracy of lab-based methods.\nFinally, in the vast \ufb01eld of natural language processing, which includes the subtasks of understanding,\nsummarizing, or generating text, impressive advances were made based on deep learning. Here, we refer\nto [YHPC18] for an overview. One technique that recently stood out is based on a so-called transformer neural\nnetwork [BCB15, VSP+17]. This network structure gave rise to the impressive GPT-3 model [ BMR+20]\nwhich not only creates coherent and compelling texts but can also produce code, such as, for the layout of a\nwebpage according to some instructions that a user inputs in plain English. Transformer neural networks\nhave also been successfully employed in the \ufb01eld of symbolic mathematics [SGHK18, LC19].\nIn this article, we present and discuss the mathematical foundations of the success story outlined above.\nMore precisely, our goal is to outline the newly emerging \ufb01eld of mathematical analysis of deep learning . To\naccurately describe this \ufb01eld, a necessary preparatory step is to sharpen our de\ufb01nition of the term deep\nlearning. For the purposes of this article, we will use the term in the following narrow sense: Deep learning\nrefers to techniques where deep neural networks 1 are trained with gradient-based methods . This narrow\n1We will de\ufb01ne the term neural network later but, for this de\ufb01nition, one can view it as a parametrized family of functions\nwith a di\ufb00erentiable parametrization.\n2", "mimetype": "text/plain", "start_char_idx": 1135, "end_char_idx": 4325, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "0554e0da-b43b-430a-8015-9dd91b97a5ea": {"__data__": {"id_": "0554e0da-b43b-430a-8015-9dd91b97a5ea", "embedding": null, "metadata": {"page_label": "3", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e97167ce-af33-40d1-bb9f-9481ff89bcae", "node_type": "4", "metadata": {"page_label": "3", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "cd954d7bfa9a2bf4c2936a62a30f2074a2238ea50fab0ddfaeb93f0bb50c5b3b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "78c6a24a-c20d-4e04-9024-8ff527e8895d", "node_type": "1", "metadata": {}, "hash": "242e0d091e7eae2382ded480837dbd0df4d9fdc9c68e4e557ce61d8448109842", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "de\ufb01nition is helpful to make this article more concise. We would like to stress, however, that we do not claim\nin any way that this is the best or the right de\ufb01nition of deep learning.\nHaving \ufb01xed a de\ufb01nition of deep learning, three questions arise concerning the aforementioned emerging\n\ufb01eld of mathematical analysis of deep learning: To what extent is a mathematical theory necessary? Is it\ntruly a new \ufb01eld? What are the questions studied in this area?\nLet us start by explaining the necessity of a theoretical analysis of the tools described above. From a\nscienti\ufb01c perspective, the primary reason why deep learning should be studied mathematically is simple\ncuriosity. As we will see throughout this article, many practically observed phenomena in this context are\nnot explained theoretically. Moreover, theoretical insights and the development of a comprehensive theory\nare often the driving force underlying the development of new and improved methods. Prominent examples\nof mathematical theories with such an e\ufb00ect are the theory of \ufb02uid mechanics which is an invaluable asset\nto the design of aircraft or cars and the theory of information that a\ufb00ects and shapes all modern digital\ncommunication. In the words of Vladimir Vapnik 2: \u201cNothing is more practical than a good theory\u201d, [ Vap13,\nPreface]. In addition to being interesting and practical, theoretical insight may also be necessary. Indeed, in\nmany applications of machine learning, such as medical diagnosis, self-driving cars, and robotics, a signi\ufb01cant\nlevel of control and predictability of deep learning methods is mandatory. Also, in services, such as banking\nor insurance, the technology should be controllable to guarantee fair and explainable decisions.\nLet us next address the claim that the \ufb01eld of mathematical analysis of deep learning is a newly emerging\narea. In fact, under the aforementioned de\ufb01nition of deep learning, there are two main ingredients of the\ntechnology: deep neural networks and gradient-based optimization. The \ufb01rst arti\ufb01cial neuron was already\nintroduced in 1943 in [ MP43]. This neuron was not trained but instead used to explain a biological neuron.\nThe \ufb01rst multi-layered network of such arti\ufb01cial neurons that was also trained can be found in [ Ros58].\nSince then, various neural network architectures have been developed. We will discuss these architectures in\ndetail in the following sections. The second ingredient, gradient-based optimization, is made possible by the\nobservation that due to the graph-based structure of neural networks the gradient of an objective function\nwith respect to the parameters of the neural network can be computed e\ufb03ciently. This has been observed in\nvarious ways, see [Kel60, Dre62, Lin70, RHW86]. Again, these techniques will be discussed in the upcoming\nsections. Since then, techniques have been improved and extended. As the rest of the manuscript is spent\nreviewing these methods, we will keep the discussion of literature at this point brief. Instead, we refer to\nsome overviews of the history of deep learning from various perspectives: [LBH15, Sch15, GBC16, HH19].\nGiven the fact that the two main ingredients of deep neural networks have been around for a long\ntime, one would expect that a comprehensive mathematical theory has been developed that describes\nwhy and when deep-learning-based methods will perform well or when they will fail. Statistical learning\ntheory [AB99, Vap99, CS02, BBL03, Vap13] describes multiple aspects of the performance of general learning\nmethods and in particular deep learning. We will review this theory in the context of deep learning in\nSubsection 1.2 below. Hereby, we focus on classical, deep learning-related results that we consider well-known\nin the machine learning community. Nonetheless, the choice of these results is guaranteed to be subjective.\nWe will \ufb01nd that the presented, classical theory is too general to explain the performance of deep learning\nadequately. In this context, we will identify the following questions that appear to be di\ufb03cult to answer\nwithin the classical framework of learning theory: Why do trained deep neural networks not over\ufb01t on the\ntraining data despite the enormous power of the architecture? What is the advantage of deep compared to\nshallow architectures? Why do these methods seemingly not su\ufb00er from the curse of dimensionality?", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4361, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "78c6a24a-c20d-4e04-9024-8ff527e8895d": {"__data__": {"id_": "78c6a24a-c20d-4e04-9024-8ff527e8895d", "embedding": null, "metadata": {"page_label": "3", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e97167ce-af33-40d1-bb9f-9481ff89bcae", "node_type": "4", "metadata": {"page_label": "3", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "cd954d7bfa9a2bf4c2936a62a30f2074a2238ea50fab0ddfaeb93f0bb50c5b3b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0554e0da-b43b-430a-8015-9dd91b97a5ea", "node_type": "1", "metadata": {"page_label": "3", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "bd739a3e4a728bb1488fed41944f707ff3727d04c527c74e6bec48ec71d35099", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We will review this theory in the context of deep learning in\nSubsection 1.2 below. Hereby, we focus on classical, deep learning-related results that we consider well-known\nin the machine learning community. Nonetheless, the choice of these results is guaranteed to be subjective.\nWe will \ufb01nd that the presented, classical theory is too general to explain the performance of deep learning\nadequately. In this context, we will identify the following questions that appear to be di\ufb03cult to answer\nwithin the classical framework of learning theory: Why do trained deep neural networks not over\ufb01t on the\ntraining data despite the enormous power of the architecture? What is the advantage of deep compared to\nshallow architectures? Why do these methods seemingly not su\ufb00er from the curse of dimensionality? Why\ndoes the optimization routine often succeed in \ufb01nding good solutions despite the non-convexity, non-linearity,\nand often non-smoothness of the problem? Which aspects of an architecture a\ufb00ect the performance of the\nassociated models and how? Which features of data are learned by deep architectures? Why do these methods\nperform as well as or better than specialized numerical tools in natural sciences?\nThe new \ufb01eld of mathematical analysis of deep learning has emerged around questions like the ones listed\nabove. In the remainder of this article, we will collect some of the main recent advances to answer these\nquestions. Because this \ufb01eld of mathematical analysis of deep learning is incredibly active and new material\nis added at breathtaking speeds, a brief survey on recent advances in this area is guaranteed to miss not only\n2This claim can be found earlier in a non-mathematical context in the works of Kurt Lewin [Lew43].\n3", "mimetype": "text/plain", "start_char_idx": 3560, "end_char_idx": 5300, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "4f325d4d-1645-4253-b138-e1c1c324450c": {"__data__": {"id_": "4f325d4d-1645-4253-b138-e1c1c324450c", "embedding": null, "metadata": {"page_label": "4", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5b4a4a81-0e34-4844-a957-88f42954e54b", "node_type": "4", "metadata": {"page_label": "4", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "2365876e02a86d542df73eff7d1207280ae97d7538f0cf58d1d2143b3f162789", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "308a579c-6a8b-47b9-ba18-4a4d0607c64a", "node_type": "1", "metadata": {}, "hash": "0f5c72a88b96ef5ed6c2551964b21c37dbd1fec122b29ae8ae090f14851565b1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "a couple of references but also many of the most essential ones. Therefore, we do not strive for a complete\noverview, but instead, showcase several fundamental ideas on a mostly intuitive level. In this way, we hope to\nallow the reader to familiarize themselves with some exciting concepts and provide a convenient entry-point\nfor further studies.\n1.1 Notation\nWe denote by N the set of natural numbers, by Z the set of integers and by R the \ufb01eld of real numbers.\nFor N \u2208N, we denote by [ N] the set {1,...,N }. For two functions f,g : X \u2192[0,\u221e), we write f \u2272 g, if\nthere exists a universal constant c such that f(x) \u2264cg(x) for all x\u2208X. In a pseudometric space ( X,dX),\nwe de\ufb01ne the ball of radius r \u2208(0,\u221e) around a point x\u2208X by BdX\nr (x) or Br(x) if the pseudometric dX\nis clear from the context. By \u2225\u00b7\u2225 p, p \u2208[1,\u221e], we denote the \u2113p-norm, and by \u27e8\u00b7,\u00b7\u27e9the Euclidean inner\nproduct of given vectors. By \u2225\u00b7\u2225op we denote the operator norm induced by the Euclidean norm and by\n\u2225\u00b7\u2225F the Frobenius norm of given matrices. For p\u2208[1,\u221e], s\u2208[0,\u221e), d\u2208N, and X\u2282 Rd, we denote by\nWs,p(X) the Sobolev-Slobodeckij space, which for s= 0 is just a Lebesgue space, i.e., W0,p(X) = Lp(X).\nFor measurable spaces X and Y, we de\ufb01ne M(X,Y) to be the set of measurable functions from X to Y.\nWe denote by \u02c6g the Fourier transform3 of a tempered distribution g. For probabilistic statements, we will\nassume a suitable underlying probability space with probability measure P . For an X-valued random variable\nX, we denote by E [X] and V [X] its expectation and variance and by P X the image measure of X on X,\ni.e., P X(A) = P (X \u2208A) for every measurable set A\u2282X. If possible, we use the corresponding lowercase\nletter to denote the realization x\u2208X of the random variable X for a given outcome. We write I d for the\nd-dimensional identity matrix and, for a set A, we write 1A for the indicator function of A, i.e., 1A(x) = 1 if\nx\u2208A and 1A(x) = 0 else.\n1.2 Foundations of learning theory\nBefore we continue to describe recent developments in the mathematical analysis of deep learning methods,\nwe start by providing a concise overview of the classical mathematical and statistical theory underlying\nmachine learning tasks and algorithms which, in their most general form, can be formulated as follows.\nDe\ufb01nition 1.1 (Learning - informal). Let X,Y, and Zbe measurable spaces. In a learning task, one is given\ndata in Zand a loss function L: M(X,Y) \u00d7Z\u2192 R. The goal is to choose a hypothesis set F\u2282M (X,Y)\nand construct a learning algorithm, i.e., a mapping\nA:\n\u22c3\nm\u2208N\nZm \u2192F,\nthat uses training data s= (z(i))m\ni=1 \u2208Zm to \ufb01nd a model fs = A(s) \u2208F that performs well on the training\ndata s and also generalizes to unseen data z \u2208Z. Here, performance is measured via the loss function L\nand the corresponding loss L(fs,z) and, informally speaking, generalization means that the out-of-sample\nperformance of fs at z behaves similar to the in-sample performance on s.\nDe\ufb01nition 1.1 is deliberately vague on how to measure generalization performance. Later, we will often\nstudy the expected out-of-sample performance. To talk about expected performance, a data distribution\nneeds to be speci\ufb01ed. We will revisit this point in Assumption 1.10 and De\ufb01nition 1.11.\nFor simplicity, we focus on one-dimensional, supervised prediction tasks with input features in Euclidean\nspace as de\ufb01ned in the following.\nDe\ufb01nition 1.2 (Prediction task).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3396, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "308a579c-6a8b-47b9-ba18-4a4d0607c64a": {"__data__": {"id_": "308a579c-6a8b-47b9-ba18-4a4d0607c64a", "embedding": null, "metadata": {"page_label": "4", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5b4a4a81-0e34-4844-a957-88f42954e54b", "node_type": "4", "metadata": {"page_label": "4", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "2365876e02a86d542df73eff7d1207280ae97d7538f0cf58d1d2143b3f162789", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4f325d4d-1645-4253-b138-e1c1c324450c", "node_type": "1", "metadata": {"page_label": "4", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "521d205613032a5c0ad834329a0001575e0aa71e000bbffaf8faab01846d0e47", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Here, performance is measured via the loss function L\nand the corresponding loss L(fs,z) and, informally speaking, generalization means that the out-of-sample\nperformance of fs at z behaves similar to the in-sample performance on s.\nDe\ufb01nition 1.1 is deliberately vague on how to measure generalization performance. Later, we will often\nstudy the expected out-of-sample performance. To talk about expected performance, a data distribution\nneeds to be speci\ufb01ed. We will revisit this point in Assumption 1.10 and De\ufb01nition 1.11.\nFor simplicity, we focus on one-dimensional, supervised prediction tasks with input features in Euclidean\nspace as de\ufb01ned in the following.\nDe\ufb01nition 1.2 (Prediction task). In a prediction task, we have that Z:= X\u00d7Y , i.e., we are given training\ndata s = (( x(i),y(i)))m\ni=1 that consist of input features x(i) \u2208X and corresponding labels y(i) \u2208Y . For\none-dimensional regression tasks with Y\u2282 R, we consider the quadratic loss L(f,(x,y)) = (f(x) \u2212y)2 and,\n3Respecting common notation, we will also use the hat symbol to denote the minimizer of the empirical risk \u02c6fs in De\ufb01nition 1.8\nbut this clash of notation does not cause any ambiguity.\n4", "mimetype": "text/plain", "start_char_idx": 2698, "end_char_idx": 3867, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "01addac1-c7d8-4f50-b927-f971ed791063": {"__data__": {"id_": "01addac1-c7d8-4f50-b927-f971ed791063", "embedding": null, "metadata": {"page_label": "5", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f3ecccd2-e8f9-4111-919c-6f53f8e48d41", "node_type": "4", "metadata": {"page_label": "5", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "900bcc577fc355215da51048205755032bc3a5119682bf205d9bfef933e6fc48", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c30eeaac-2854-44b6-8b37-f63350b6ade6", "node_type": "1", "metadata": {}, "hash": "54db8cb219aa9963b69258ef50996748bfc65265977c859df110f9fa65c60bc6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "for binary classi\ufb01cation tasks with Y= {\u22121,1}, we consider the 0-1 loss L(f,(x,y)) = 1(\u2212\u221e,0)(yf(x)). We\nassume that our input features are in Euclidean space, i.e., X\u2282 Rd with input dimension d\u2208N.\nIn a prediction task, we aim for a model fs: X\u2192Y , such that, for unseen pairs ( x,y) \u2208X\u00d7Y , fs(x) is\na good prediction of the true label y. However, note that large parts of the presented theory can be applied\nto more general settings.\nRemark 1.3 (Learning tasks). Apart from straightforward extensions to multi-dimensional prediction tasks\nand other loss functions, we want to mention that unsupervised and semi-supervised learning tasks are\noften treated as prediction tasks. More precisely, one transforms unlabeled training data z(i) into features\nx(i) = T1(z(i)) \u2208X and labels y(i) = T2(z(i)) \u2208Y using suitable transformations T1 : Z\u2192X , T2 : Z\u2192Y . In\ndoing so, one asks for a model fs approximating the transformation T2 \u25e6T\u22121\n1 : X\u2192Y which is, e.g., done in\norder to learn feature representations or invariances.\nFurthermore, one can consider density estimation tasks, where X= Z, Y:= [0,\u221e], and Fconsists of\nprobability densities with respect to some \u03c3-\ufb01nite reference measure \u00b5 on Z. One then aims for a probability\ndensity fs that approximates the density of the unseen data z with respect to \u00b5. One can perform L2(\u00b5)-\napproximation based on the discretization L(f,z) = \u22122f(z) + \u2225f\u22252\nL2(\u00b5) or maximum likelihood estimation\nbased on the surprisal L(f,z) = \u2212log(f(z)).\nIn deep learning the hypothesis set Fconsists of realizations of neural networks \u03a6a(\u00b7,\u03b8), \u03b8 \u2208P, with\na given architecture a and parameter set P. In practice, one uses the term neural network for a range of\nfunctions that can be represented by directed acyclic graphs, where the vertices correspond to elementary\nalmost everywhere di\ufb00erentiable functions parametrizable by \u03b8\u2208P and the edges symbolize compositions\nof these functions. In Section 6, we will review some frequently used architectures, in the other sections,\nhowever, we will mostly focus on fully connected feedforward (FC) neural networks as de\ufb01ned below.\nDe\ufb01nition 1.4 (FC neural network). A fully connected feedforward neural network is given by its architecture\na= (N,\u03f1), where L\u2208N, N \u2208NL+1, and \u03f1: R \u2192R. We refer to \u03f1 as the activation function, to L as the\nnumber of layers, and to N0, NL, and N\u2113, \u2113\u2208[L\u22121], as the number of neurons in the input, output, and\n\u2113-th hidden layer, respectively. We denote the number of parameters by\nP(N) :=\nL\u2211\n\u2113=1\nN\u2113N\u2113\u22121 + N\u2113\nand de\ufb01ne the corresponding realization function \u03a6a: RN0 \u00d7RP(N) \u2192RNL which satis\ufb01es for every input\nx\u2208RN0 and parameters\n\u03b8= (\u03b8(\u2113))L\n\u2113=1 = ((W(\u2113),b(\u2113)))L\n\u2113=1 \u2208\nL\n\u00d7\n\u2113=1\n(RN\u2113\u00d7N\u2113\u22121 \u00d7RN\u2113) \u223c= RP(N)\nthat \u03a6a(x,\u03b8) = \u03a6(L)(x,\u03b8), where\n\u03a6(1)(x,\u03b8) = W(1)x+ b(1),\n\u00af\u03a6(\u2113)(x,\u03b8) = \u03f1\n(\n\u03a6(\u2113)(x,\u03b8)\n)\n, \u2113 \u2208[L\u22121], and\n\u03a6(\u2113+1)(x,\u03b8) = W(\u2113+1) \u00af\u03a6(\u2113)(x,\u03b8) + b(\u2113+1), \u2113 \u2208[L\u22121],\n(1.1)\nand \u03f1 is applied componentwise.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2882, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c30eeaac-2854-44b6-8b37-f63350b6ade6": {"__data__": {"id_": "c30eeaac-2854-44b6-8b37-f63350b6ade6", "embedding": null, "metadata": {"page_label": "5", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f3ecccd2-e8f9-4111-919c-6f53f8e48d41", "node_type": "4", "metadata": {"page_label": "5", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "900bcc577fc355215da51048205755032bc3a5119682bf205d9bfef933e6fc48", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "01addac1-c7d8-4f50-b927-f971ed791063", "node_type": "1", "metadata": {"page_label": "5", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "4f5809b0e0c2fd41d649d92fd2dec0c3da5acf4725e2f1db97f501b5240de2a3", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We refer to W(\u2113) \u2208RN\u2113\u00d7N\u2113\u22121 and b(\u2113) \u2208RN\u2113 as the weight matrices and\nbias vectors, and to \u00af\u03a6(\u2113) and \u03a6(\u2113) as the activations and pre-activations of the N\u2113 neurons in the \u2113-th layer.\nThe width and depth of the architecture are given by \u2225N\u2225\u221e and L and we call the architecture deep if L> 2\nand shallow if L= 2.\nThe underlying directed acyclic graph of FC networks is given by compositions of the a\ufb03ne linear maps\nx \u21a6\u2192W(\u2113)x+ b(\u2113), \u2113 \u2208[L], with the activation function \u03f1 intertwined, see Figure 1.1. Typical activation\n5", "mimetype": "text/plain", "start_char_idx": 2883, "end_char_idx": 3397, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "2b17367c-7442-433d-b535-4ead9a5da1f7": {"__data__": {"id_": "2b17367c-7442-433d-b535-4ead9a5da1f7", "embedding": null, "metadata": {"page_label": "6", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "388eea62-d4e4-484d-a6f2-b4ec38768125", "node_type": "4", "metadata": {"page_label": "6", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "0220e62a6d0b355c645fb4adbd96d05e942257c82261a3270263dd95dc3f6adc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "99f6d8ce-56e3-4e26-8fa4-060f61923764", "node_type": "1", "metadata": {}, "hash": "ce6349cc88c9056db148d982e7f5892e0c4bf3b746f38a6a0c6e025d6235884e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "x1\nx2\nx3\n\u03a6(1)\n1\n\u03a6(1)\n2\n\u03a6(1)\n3\n\u03a6(1)\n4\nx\u21a6\u2192W(1)x+ b(1)\n\u00af\u03a6(1)\n1\n\u00af\u03a6(1)\n2\n\u00af\u03a6(1)\n3\n\u00af\u03a6(1)\n4\n\u03f1\n\u03a6(2)\n1\n\u03a6(2)\n2\n\u03a6(2)\n3\n\u03a6(2)\n4\n\u03a6(2)\n5\n\u03a6(2)\n6\nx\u21a6\u2192W(2)x+ b(2)\n\u00af\u03a6(2)\n1\n\u00af\u03a6(2)\n2\n\u00af\u03a6(2)\n3\n\u00af\u03a6(2)\n4\n\u00af\u03a6(2)\n5\n\u00af\u03a6(2)\n6\n\u03f1 \u03a6ax\u21a6\u2192W(3)x+ b(3)\nFigure 1.1: Graph (grey) and (pre-)activations of the neurons (white) of a deep fully connected feedforward\nneural network \u03a6a: R3 \u00d7R53 \u21a6\u2192R with architecture a= ((3,4,6,1),\u03f1) and parameters \u03b8= ((W(\u2113),b(\u2113))3\n\u2113=1.\nfunctions used in practice are variants of the recti\ufb01ed linear unit (ReLU) given by \u03f1R(x) := max{0,x}and\nsigmoidal functions \u03f1\u2208C(R) satisfying \u03f1(x) \u21921 for x\u2192\u221e and \u03f1(x) \u21920 for x\u2192\u2212\u221e, such as the logistic\nfunction \u03f1\u03c3(x) := 1/(1 +e\u2212x) (often referred to as the sigmoid function). See also Table 1 for a comprehensive\nlist of widely used activation functions.\nRemark 1.5 (Neural networks). If not further speci\ufb01ed, we will use the term (neural) network, or the\nabbreviation NN, to refer to FC neural networks. Note that many of the architectures used in practice (see\nSection 6) can be written as special cases of De\ufb01nition 1.4 where, e.g., speci\ufb01c parameters are prescribed by\nconstants or shared with other parameters. Furthermore, note that a\ufb03ne linear functions are NNs with depth\nL= 1. We will also consider biasless NNs given by linear mappings without bias vector, i.e., b(\u2113) = 0, \u2113\u2208[L].\nIn particular, any NN can always be written without bias vectors by rede\ufb01ning\nx\u2192\n[x\n1\n]\n, (W(\u2113),b(\u2113)) \u2192\n[\nW(\u2113) b(\u2113)\n0 1\n]\n, \u2113 \u2208[L\u22121], and (W(L),b(L)) \u2192\n[\nW(L) b(L)]\n.\nTo enhance readability we will often not specify the underlying architecture a= (N,\u03f1) or the parameters \u03b8\u2208\nRP(N) and use the term NN to refer to the architecture as well as the realization functions \u03a6a(\u00b7,\u03b8): RN0 \u2192RNL\nor \u03a6a: RN0 \u00d7RP(N) \u2192RNL. However, we want to emphasize that one cannot infer the underlying architecture\nor properties like magnitude of parameters solely from these functions as the mapping (a,\u03b8) \u21a6\u2192\u03a6a(\u00b7,\u03b8) is\nhighly non-injective. As an example, we can set W(L) = 0 which implies \u03a6a(\u00b7,\u03b8) = b(L) for all architectures\na= (N,\u03f1) and all values of (W(\u2113),b(\u2113))L\u22121\n\u2113=1 .\nIn view of our considered prediction tasks in De\ufb01nition 1.2, this naturally leads to the following hypothesis\nsets of neural networks.\nDe\ufb01nition 1.6 (Hypothesis sets of neural networks) .", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2247, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "99f6d8ce-56e3-4e26-8fa4-060f61923764": {"__data__": {"id_": "99f6d8ce-56e3-4e26-8fa4-060f61923764", "embedding": null, "metadata": {"page_label": "6", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "388eea62-d4e4-484d-a6f2-b4ec38768125", "node_type": "4", "metadata": {"page_label": "6", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "0220e62a6d0b355c645fb4adbd96d05e942257c82261a3270263dd95dc3f6adc", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2b17367c-7442-433d-b535-4ead9a5da1f7", "node_type": "1", "metadata": {"page_label": "6", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "d5bfd3e291e7b53d139d194bdddb05a4b3e202410dde5fe1ec5a26884fdbc0cd", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "However, we want to emphasize that one cannot infer the underlying architecture\nor properties like magnitude of parameters solely from these functions as the mapping (a,\u03b8) \u21a6\u2192\u03a6a(\u00b7,\u03b8) is\nhighly non-injective. As an example, we can set W(L) = 0 which implies \u03a6a(\u00b7,\u03b8) = b(L) for all architectures\na= (N,\u03f1) and all values of (W(\u2113),b(\u2113))L\u22121\n\u2113=1 .\nIn view of our considered prediction tasks in De\ufb01nition 1.2, this naturally leads to the following hypothesis\nsets of neural networks.\nDe\ufb01nition 1.6 (Hypothesis sets of neural networks) . Let a = ( N,\u03f1) be a NN architecture with input\ndimension N0 = d, output dimension NL = 1, and measurable activation function \u03f1. For regression tasks the\ncorresponding hypothesis set is given by\nFa =\n{\n\u03a6a(\u00b7,\u03b8): \u03b8\u2208RP(N)}\nand for classi\ufb01cation tasks by\nFa,sgn =\n{\nsgn(\u03a6a(\u00b7,\u03b8)): \u03b8\u2208RP(N)}\n, where sgn(x) :=\n{\n1, if x\u22650,\n\u22121, if x< 0.\n6", "mimetype": "text/plain", "start_char_idx": 1719, "end_char_idx": 2577, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e70446d2-ffda-46e4-80ec-cc49babc4579": {"__data__": {"id_": "e70446d2-ffda-46e4-80ec-cc49babc4579", "embedding": null, "metadata": {"page_label": "7", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7f2d16ed-f192-4be3-bea8-7a06fe3cc8d1", "node_type": "4", "metadata": {"page_label": "7", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "577ff413c3d55202c696876569daf0d920d5c7d2f4bc1352de3366755ed8dbc8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Name Given as a function of x\u2208R by Plot\nlinear x\nHeaviside / step function 1(0,\u221e)(x)\nlogistic / sigmoid 1\n1+e\u2212x\nrecti\ufb01ed linear unit (ReLU) max{0,x}\npower recti\ufb01ed linear unit max{0,x}k for k\u2208N\nparametric ReLU (PReLU) max{ax,x} for a\u22650, a\u0338= 1\nexponential linear unit (ELU) x\u00b71[0,\u221e)(x) + (ex \u22121) \u00b71(\u2212\u221e,0)(x)\nsoftsign x\n1+|x|\ninverse square root linear unit x\u00b71[0,\u221e)(x) + x\u221a\n1+ax2 \u00b71(\u2212\u221e,0)(x) for a> 0\ninverse square root unit x\u221a\n1+ax2 for a> 0\ntanh ex\u2212e\u2212x\nex+e\u2212x\narctan arctan(x)\nsoftplus ln(1 + ex)\nGaussian e\u2212x2/2\nTable 1: List of commonly used activation functions.\n7", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 569, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "82820804-bfdf-43bf-a481-a4e97a79b8cc": {"__data__": {"id_": "82820804-bfdf-43bf-a481-a4e97a79b8cc", "embedding": null, "metadata": {"page_label": "8", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4308e195-72ea-4bd0-add0-e28d8f12bcd9", "node_type": "4", "metadata": {"page_label": "8", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "86fdcb9347ec6b8e42e3402c3ad2d307a0096c74b1db8059dc8db1d961af8989", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Note that we compose the output of the NN with the sign function in order to obtain functions mapping\nto Y= {\u22121,1}. This can be generalized to multi-dimensional classi\ufb01cation tasks by replacing the sign by an\nargmax function. Given a hypothesis set, a popular learning algorithm is empirical risk minimization (ERM),\nwhich minimizes the average loss on the given training data, as described in the next de\ufb01nitions.\nDe\ufb01nition 1.7 (Empirical risk). For training data s = (z(i))m\ni=1 \u2208Zm and a function f \u2208M(X,Y), we\nde\ufb01ne the empirical risk by\n\u02c6Rs(f) := 1\nm\nm\u2211\ni=1\nL(f,z(i)).\nDe\ufb01nition 1.8 (ERM learning algorithm) . Given a hypothesis set F, an empirical risk minimization\nalgorithm Aerm chooses4 for training data s\u2208Zm a minimizer \u02c6fs \u2208F of the empirical risk in F, i.e.,\nAerm(s) \u2208arg min\nf\u2208F\n\u02c6Rs(f). (1.2)\nRemark 1.9 (Surrogate loss and regularization) . Note that, for classi\ufb01cation tasks, one needs to optimize\nover non-di\ufb00erentiable functions with discrete outputs in (1.2). For NN hypothesis sets Fa,sgn one typically\nuses the corresponding hypothesis set for regression tasks Fa to \ufb01nd an approximate minimizer \u02c6fsurr\ns \u2208Fa of\n1\nm\nm\u2211\ni=1\nLsurr(f,z(i)),\nwhere Lsurr : M(X,R) \u00d7Z\u2192 R is a surrogate loss guaranteeing that sgn( \u02c6fsurr\ns ) \u2208arg minf\u2208Fa,sgn\n\u02c6Rs(f). A\nfrequently used surrogate loss is the logistic loss 5 given by\nLsurr(f,z) = log\n(\n1 + e\u2212yf(x)\n)\n.\nIn various learning tasks one also adds regularization terms to the minimization problem in (1.2), such as\npenalties on the norm of the parameters of the NN, i.e.,\nmin\n\u03b8\u2208RP(N)\n\u02c6Rs(\u03a6a(\u00b7,\u03b8)) + \u03b1\u2225\u03b8\u22252\n2,\nwhere \u03b1\u2208(0,\u221e) is a regularization parameter. Note that in this case the minimizer depends on the chosen\nparameters \u03b8 and not only on the realization function \u03a6a(\u00b7,\u03b8), see also Remark 1.5.\nComing back to our initial, informal description of learning in De\ufb01nition 1.1, we have now outlined\npotential learning tasks in De\ufb01nition 1.2, NN hypothesis sets in De\ufb01nition 1.6, a metric for the in-sample\nperformance in De\ufb01nition 1.7, and a corresponding learning algorithm in De\ufb01nition 1.8. However, we are still\nlacking a mathematical concept to describe the out-of-sample (generalization) performance of our learning\nalgorithm. This question has been intensively studied in the \ufb01eld of statistical learning theory, see Section 1\nfor various references.\nIn this \ufb01eld one usually establishes a connection between unseen data z and the training data s= (z(i))m\ni=1\nby imposing that zand z(i), i\u2208[m], are realizations of independent samples drawn from the same distribution.\nAssumption 1.10 (Independent and identically distributed data) . We assume that z(1),...,z (m),z are\nrealizations of i.i.d. random variables Z(1),...,Z (m),Z.\n4For simplicity, we assume that the minimum is attained which, for instance, is the case if Fis a compact topological space\non which \u02c6Rs is continuous. Hypothesis sets of NNs F(N,\u03f1) constitute a compact space if, e.g., one chooses a compact parameter\nset P\u2282 RP(N) and a continuous activation function \u03f1. One could also work with approximate minimizers, see [AB99].\n5This can be viewed as cross-entropy between the label y and the output of f composed with a logistic function \u03f1\u03c3. In a\nmulti-dimensional setting one can replace the logistic function with a softmax function.\n8", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3263, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "65d2d176-5594-4b32-bc6f-30675be72b49": {"__data__": {"id_": "65d2d176-5594-4b32-bc6f-30675be72b49", "embedding": null, "metadata": {"page_label": "9", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1ab1fd64-9043-46f8-860e-493e2d3a8e9a", "node_type": "4", "metadata": {"page_label": "9", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "22935d96f46e9b69f9d5e9ba5b599999d92a81770b3fab59b573c8b3ea749111", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In this formal setting, we can compute the average out-of-sample performance of a model. Recall from\nour notation in Section 1.1 that we denote by P Z the image measure of Z on Z, which is the underlying\ndistribution of our training data S = (Z(i))m\ni=1 \u223cP m\nZ and unknown data Z \u223cP Z.\nDe\ufb01nition 1.11 (Risk). For a function f \u2208M(X,Y), we de\ufb01ne 6 the risk by\nR(f) := E\n[\nL(f,Z)\n]\n=\n\u222b\nZ\nL(f,z) dP Z(z).\nDe\ufb01ning S := (Z(i))m\ni=1, the risk of a model fS = A(S) is thus given by R(fS) = E\n[\nL(fS,Z)|S\n]\n.\nFor prediction tasks, we can write Z = (X,Y ), such that the input features and labels are given by an\nX-valued random variable X and a Y-valued random variable Y, respectively. Note that for classi\ufb01cation\ntasks the risk equals the probability of misclassi\ufb01cation\nR(f) = E [1(\u2212\u221e,0)(Yf (X))] = P [f(X) \u0338= Y].\nFor noisy data, there might be a positive, lower bound on the risk, i.e., an irreducible error. If the lower\nbound on the risk is attained, one can also de\ufb01ne the notion of an optimal solution to a learning task.\nDe\ufb01nition 1.12 (Bayes-optimal function). A function f\u2217 \u2208 M(X,Y) achieving the smallest risk, the\nso-called Bayes risk\nR\u2217:= inf\nf\u2208M(X,Y)\nR(f),\nis called a Bayes-optimal function.\nFor the prediction tasks in De\ufb01nition 1.2, we can represent the risk of a function with respect to the\nBayes risk and compute the Bayes-optimal function, see, e.g., [CZ07, Propositions 1.8 and 9.3].\nLemma 1.1 (Regression and classi\ufb01cation risk) . For a regression task with V [Y] < \u221e, the risk can be\ndecomposed into\nR(f) = E\n[\n(f(X) \u2212E [Y|X])2]\n+ R\u2217, f \u2208M(X,Y), (1.3)\nwhich is minimized by the regression function f\u2217(x) = E [Y|X = x]. For a classi\ufb01cation task, the risk can be\ndecomposed into\nR(f) = E\n[\n|E [Y|X]|1(\u2212\u221e,0)(E [Y|X]f(X))\n]\n+ R\u2217, f \u2208M(X,Y),\nwhich is minimized by the Bayes classi\ufb01er f\u2217(x) = sgn(E [Y|X = x]).\nAs our model fS is depending on the random training data S, the risk R(fS) is a random variable and\nwe might aim7 for R(fS) small with high probability or in expectation over the training data. The challenge\nfor the learning algorithm Ais to minimize the risk by only using training data but without knowing the\nunderlying distribution. One can even show that for every learning algorithm there exists a distribution\nwhere convergence of the expected risk of fS to the Bayes risk is arbitrarily slow with respect to the number\nof samples m [DGL96, Theorem 7.2].\nTheorem 1.13 (No free lunch) . Let am \u2208(0,\u221e), m \u2208N, be a monotonically decreasing sequence with\na1 \u22641/16. Then for every learning algorithm Aof a classi\ufb01cation task there exists a distribution P Z such\nthat for every m\u2208N and training data S \u223cP m\nZ it holds that\nE\n[\nR(A(S))\n]\n\u2265R\u2217+ am.\n6Note that this requires z\u21a6\u2192L(f,z) to be measurable for every f \u2208M(X,Y), which is the case for our considered prediction\ntasks.\n7In order to make probabilistic statements on R(fS) we assume that R(fS) is a random variable, i.e., measurable. This is,\ne.g., the case if Fconstitutes a measurable space and s\u21a6\u2192A(s) and f \u2192R|F are measurable.\n9", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3008, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d1a11195-06ae-4a82-9ebe-14e3d016c55d": {"__data__": {"id_": "d1a11195-06ae-4a82-9ebe-14e3d016c55d", "embedding": null, "metadata": {"page_label": "10", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "be85231e-714a-4143-b2d6-228d2a25c524", "node_type": "4", "metadata": {"page_label": "10", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "490885db27f1bfe31cb88c64e09be845ccc3089a6f63b55152be89d2483bfdc2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Figure 1.2: Illustration of the errors (A)\u2013(C) in the decomposition of (1.4). It shows an exemplary risk \u02c6R\n(blue) and empirical risk \u02c6Rs (red) with respect to the projected space of measurable functions M(X,Y).\nNote that the empirical risk and thus \u03b5gen and \u03b5opt depend on the realization s= (z(i))m\ni=1 of the training\ndata S \u223cP m\nZ.\nTheorem 1.13 shows the non-existence of a universal learning algorithm for every data distribution P Z and\nshows that useful bounds must necessarily be accompanied by a priori regularity conditions on the underlying\ndistribution P Z. Such prior knowledge can then be incorporated in the choice of the hypothesis set F. To\nillustrate this, let f\u2217\nF\u2208arg minf\u2208FR(f) be a best approximation in F, such that we can bound the error\nR(fS) \u2212R\u2217= R(fS) \u2212\u02c6RS(fS) + \u02c6RS(fS) \u2212\u02c6RS(f\u2217\nF) + \u02c6RS(f\u2217\nF) \u2212R(f\u2217\nF) + R(f\u2217\nF) \u2212R\u2217\n\u2264\u03b5opt + 2\u03b5gen + \u03b5approx (1.4)\nby\n(A) an optimization error \u03b5opt := \u02c6RS(fS) \u2212\u02c6RS( \u02c6fS) \u2265 \u02c6RS(fS) \u2212\u02c6RS(f\u2217\nF), with \u02c6fS as in De\ufb01nition 1.8,\n(B) a (uniform8) generalization error \u03b5gen := supf\u2208F|R(f) \u2212\u02c6RS(f)|\u2265 max{R(fS) \u2212\u02c6RS(fS), \u02c6RS(f\u2217\nF) \u2212\nR(f\u2217\nF)}, and\n(C) an approximation error \u03b5approx := R(f\u2217\nF) \u2212R\u2217,\nsee also Figure 1.2. The approximation error is decreasing when enlarging the hypothesis set, but taking\nF= M(X,Y) prevents controlling the generalization error, see also Theorem 1.13. This suggests a sweet-spot\nfor the complexity of our hypothesis set F and is usually referred to as the bias-variance trade-o\ufb00, see\nalso Figure 1.4 below. In the next sections, we will sketch mathematical ideas to tackle each of the errors\nin (A)\u2013(C) in the context of deep learning. Observe that we bound the generalization and optimization\nerror with respect to the empirical risk \u02c6RS and its minimizer \u02c6fS which is motivated by the fact that in\ndeep-learning-based applications one typically tries to minimize variants of \u02c6RS.\n1.2.1 Optimization\nThe \ufb01rst error in the decomposition of (1.4) is the optimization error: \u03b5opt. This error is primarily in\ufb02uenced\nby the numerical algorithm Athat is used to \ufb01nd the model fs in a hypothesis set of NNs for given training\ndata s\u2208Zm. We will focus on the typical setting where such an algorithm tries to approximately minimize\nthe empirical risk \u02c6Rs. While there are many conceivable methods to solve this minimization problem, by\nfar the most common are gradient-based methods. The main reason for the popularity of gradient-based\n8Although this uniform deviation can be a coarse estimate it is frequently considered to allow for the application of uniform\nlaws of large numbers from the theory of empirical processes.\n10", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2599, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "899b6875-bc0f-4d42-a660-7006a800d1df": {"__data__": {"id_": "899b6875-bc0f-4d42-a660-7006a800d1df", "embedding": null, "metadata": {"page_label": "11", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1903e3f4-bc07-4500-98fb-15f8177ff7c3", "node_type": "4", "metadata": {"page_label": "11", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "ae65fc4feedd40c12ff6149a866799c6c2c84e78c9c61e3b98e4176bd12e8894", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "258d38d6-2b64-416b-a289-18e0c834aa18", "node_type": "1", "metadata": {}, "hash": "fcde206b93eac5de0f7e5603eaa6bf50af6e437f8220acaeb5aa0e416391d64d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "methods is that for FC networks as in De\ufb01nition 1.4, the accurate and e\ufb03cient computation of pointwise\nderivatives \u2207\u03b8\u03a6a(x,\u03b8) is possible by means of automatic di\ufb00erentiation, a speci\ufb01c form of which is often\nreferred to as the backpropagation algorithm [Kel60, Dre62, Lin70, RHW86, GW08]. This numerical scheme\nis also applicable in general settings, such as, when the architecture of the NN is given by a general directed\nacyclic graph. Using these pointwise derivatives, one usually attempts to minimize the empirical risk \u02c6Rs by\nupdating the parameters \u03b8 according to a variant of stochastic gradient descent (SGD), which we shall review\nbelow in a general formulation:\nAlgorithm 1: Stochastic gradient descent\nInput : Di\ufb00erentiable function r: Rp \u2192R, sequence of step-sizes \u03b7k \u2208(0,\u221e), k\u2208[K],\nRp-valued random variable \u0398(0).\nOutput : Sequence of Rp-valued random variables (\u0398(k))K\nk=1.\nfor k= 1,...,K do\nLet D(k) be a random variable such that E [D(k)|\u0398(k\u22121)] = \u2207r(\u0398(k\u22121));\nSet \u0398(k) := \u0398(k\u22121) \u2212\u03b7kD(k);\nend\nIf D(k) is chosen deterministically in Algorithm 1, i.e., D(k) = \u2207r(\u0398(k\u22121)), then the algorithm is known as\ngradient descent. To minimize the empirical loss, we apply SGD with r: RP(N) \u2192R set to r(\u03b8) = \u02c6Rs(\u03a6a(\u00b7,\u03b8)).\nMore concretely, one might choose a batch-size m\u2032\u2208N with m\u2032\u2264m and consider the iteration\n\u0398(k) := \u0398(k\u22121) \u2212\u03b7k\nm\u2032\n\u2211\nz\u2208S\u2032\n\u2207\u03b8L(\u03a6a(\u00b7,\u0398(k\u22121)),z), (1.5)\nwhere S\u2032 is a so-called mini-batch of size |S\u2032|= m\u2032 chosen uniformly9 at random from the training data\ns. The sequence of step-sizes ( \u03b7k)k\u2208N is often called learning rate in this context. Stopping at step K, the\noutput of a deep learning algorithm Ais then given by\nfs = A(s) = \u03a6a(\u00b7,\u00af\u03b8),\nwhere \u00af\u03b8 can be chosen to be the realization of the last parameter \u0398 (K) of (1.5) or a convex combination of\n(\u0398(k))K\nk=1 such as the mean.\nAlgorithm 1 was originally introduced in [ RM51] in the context of \ufb01nding the root of a nondecreasing\nfunction from noisy measurements. Shortly afterwards this idea was applied to \ufb01nd a unique minimum of a\nLipschitz-regular function that has no \ufb02at regions away from the global minimum [KW52].\nIn some regimes, we can guarantee convergence of SGD at least in expectation, see [ NY83, NJLS09,\nSSSSS09], [SDR14, Section 5.9], [SSBD14, Chapter 14]. One prototypical convergence guarantee that is found\nin the aforementioned references in various forms is stated below.\nTheorem 1.14 (Convergence of SGD) . Let p,K \u2208N and let r: Rp \u2283B1(0) \u2192R be di\ufb00erentiable and\nconvex. Further let (\u0398(k))K\nk=1 be the output of Algorithm 1 with initialization \u0398(0) = 0, step-sizes \u03b7k = K\u22121/2,\nk\u2208[K], and random variables (D(k))K\nk=1 satisfying that \u2225D(k)\u22252 \u22641 almost surely for all k\u2208[K]. Then\nE [r(\u00af\u0398)] \u2212r(\u03b8\u2217) \u2264 1\u221a\nK\n,\nwhere \u00af\u0398 := 1\nK\n\u2211K\nk=1 \u0398(k) and \u03b8\u2217\u2208arg min\u03b8\u2208B1(0) r(\u03b8).\nTheorem 1.14 can be strengthened to yield a faster convergence rate if the convexity is replaced by strict\nconvexity.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2870, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "258d38d6-2b64-416b-a289-18e0c834aa18": {"__data__": {"id_": "258d38d6-2b64-416b-a289-18e0c834aa18", "embedding": null, "metadata": {"page_label": "11", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1903e3f4-bc07-4500-98fb-15f8177ff7c3", "node_type": "4", "metadata": {"page_label": "11", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "ae65fc4feedd40c12ff6149a866799c6c2c84e78c9c61e3b98e4176bd12e8894", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "899b6875-bc0f-4d42-a660-7006a800d1df", "node_type": "1", "metadata": {"page_label": "11", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "1f5435c4b56a1d15db70cecc50daa9dd197bb083eb4a28f2eebcaf7a4f57d18f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Further let (\u0398(k))K\nk=1 be the output of Algorithm 1 with initialization \u0398(0) = 0, step-sizes \u03b7k = K\u22121/2,\nk\u2208[K], and random variables (D(k))K\nk=1 satisfying that \u2225D(k)\u22252 \u22641 almost surely for all k\u2208[K]. Then\nE [r(\u00af\u0398)] \u2212r(\u03b8\u2217) \u2264 1\u221a\nK\n,\nwhere \u00af\u0398 := 1\nK\n\u2211K\nk=1 \u0398(k) and \u03b8\u2217\u2208arg min\u03b8\u2208B1(0) r(\u03b8).\nTheorem 1.14 can be strengthened to yield a faster convergence rate if the convexity is replaced by strict\nconvexity. If r is not convex, then convergence to a global minimum can in general not be guaranteed. In\nfact, in that case, stochastic gradient descent may converge to a local, non-global minimum, see Figure 1.3\nfor an example.\n9We remark that in practice one typically picks S\u2032by selecting a subset of training data in a way to cover the full training\ndata after one epoch of \u2308m/m\u2032\u2309many steps. This, however, does not necessarily yield an unbiased estimator D(k) of \u2207\u03b8r(\u0398(k\u22121))\ngiven \u0398(k\u22121).\n11", "mimetype": "text/plain", "start_char_idx": 2464, "end_char_idx": 3356, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "40e5bb45-b79f-4960-9f58-278f36214f12": {"__data__": {"id_": "40e5bb45-b79f-4960-9f58-278f36214f12", "embedding": null, "metadata": {"page_label": "12", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8d44095a-c65f-4834-8c8d-9bedc6da9743", "node_type": "4", "metadata": {"page_label": "12", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "50db44d0692542417e8722bcf67736d94f10e2986866ec1227fc1e3085c4071a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Figure 1.3: Examples of the dynamics of gradient descent (left) and stochastic gradient descent (right) for an\nobjective function with one non-global minimum next to the global minimum. We see that depending on the\ninitial condition and also on \ufb02uctuations in the stochastic part of SGD the algorithm can fail or succeed in\n\ufb01nding the global minimum.\nMoreover, gradient descent, i.e., the deterministic version of Algorithm 1, will stop progressing if at any\npoint the gradient of r vanishes. This is the case in every stationary point of r. A stationary point is either a\nlocal minimum, a local maximum, or a saddle point. One would expect that if the direction of the step D(k)\nin Algorithm 1 is not deterministic, then the random \ufb02uctuations may allow the iterates to escape saddle\npoints. Indeed, results guaranteeing convergence to local minima exist under various conditions on the type\nof saddle points that r admits, [NJLS09, GL13, GHJY15, LSJR16, JKNvW20].\nIn addition, many methods that improve the convergence by, for example, introducing more elaborate\nstep-size rules or a momentum term have been established. We shall not review these methods here, but\ninstead refer to [GBC16, Chapter 8] for an overview.\n1.2.2 Approximation\nGenerally speaking, NNs, even FC NNs (see De\ufb01nition 1.4) with onlyL= 2 layers, are universal approximators,\nmeaning that under weak conditions on the activation function \u03f1 they can approximate any continuous\nfunction on a compact set up to arbitrary precision [Cyb89, Fun89, HSW89, LLPS93].\nTheorem 1.15 (Universal approximation theorem). Let d\u2208N, let K \u2282Rd be compact, and let \u03f1\u2208L\u221e\nloc(R)\nbe an activation function such that the closure of the points of discontinuity of \u03f1 is a Lebesgue null set.\nFurther let\n\u02dcF:=\n\u22c3\nn\u2208N\nF((d,n,1),\u03f1)\nbe the corresponding set of two-layer NN realizations. Then it holds that C(K) \u2282cl( \u02dcF) (where the closure is\ntaken with respect to the topology induced by the L\u221e(K)-norm) if and only if there does not exist a polynomial\np: R \u2192R with p= \u03f1 almost everywhere.\nThe theorem can be proven by the theorem of Hahn\u2013Banach, which implies that \u02dcFbeing dense in some\n12", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2132, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ce08e953-1108-406f-93f3-ef4ee4d15500": {"__data__": {"id_": "ce08e953-1108-406f-93f3-ef4ee4d15500", "embedding": null, "metadata": {"page_label": "13", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "61083f84-5a31-421f-9306-68b76f191817", "node_type": "4", "metadata": {"page_label": "13", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "10698b826437bb4ac3e3fe230542be5fce81bca6d11379bbf7972785707f75bf", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7633e7b8-650a-498c-a744-90f513d43d7d", "node_type": "1", "metadata": {}, "hash": "f15e619fbcc8b3ff7a2e9274d4d8cefae55e0589cfa5044bb549e6e99daaabb2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "real normed vector space Sis equivalent to the following condition: For all non-trivial functionals F \u2208S\u2032\\{0}\nfrom the topological dual space of Sthere exist parameters w\u2208Rd and b\u2208R such that\nF(\u03f1(\u27e8w,\u00b7\u27e9+ b)) \u0338= 0.\nIn case of S= C(K) we have by the Riesz\u2013Markov\u2013Kakutani representation theorem that S\u2032is the space of\nsigned Borel measures on K, see [Rud06]. Therefore, Theorem 1.15 holds, if \u03f1is such that, for a signed Borel\nmeasure \u00b5, \u222b\nK\n\u03f1(\u27e8w,x\u27e9+ b) d\u00b5(x) = 0 (1.6)\nfor all w \u2208Rd and b \u2208R implies that \u00b5 = 0. An activation function \u03f1 satisfying this condition is called\ndiscriminatory. It is not hard to see that any sigmoidal \u03f1is discriminatory. Indeed, assume that \u03f1satis\ufb01es (1.6)\nfor all w\u2208Rd and b\u2208R. Since for every x\u2208Rd it holds that \u03f1(ax+ b) \u21921(0,\u221e)(x) +\u03f1(b)1{0}(x) for a\u2192\u221e,\nwe conclude by superposition and passing to the limit that for all c1,c2 \u2208R and w\u2208Rd, b\u2208R\n\u222b\nK\n1[c1,c2](\u27e8w,x\u27e9+ b) d\u00b5(x) = 0.\nRepresenting the exponential function x\u21a6\u2192e\u22122\u03c0ix as the limit of sums of elementary functions yields that\u222b\nK e\u22122\u03c0i(\u27e8w,x\u27e9+b) d\u00b5(x) = 0 for all w\u2208Rd, b\u2208R. Hence, the Fourier transform of \u00b5 vanishes which implies\nthat \u00b5= 0.\nTheorem 1.15 addresses a uniform approximation problem on a general compact set. If we are given a\n\ufb01nite number of points and only care about good approximation at these points, then one can ask if this\napproximation problem is potentially simpler. Below we see that, if the number of neurons is larger or equal\nto the number of data points, then one can always interpolate, i.e., exactly \ufb01t the data on a given \ufb01nite\nnumber of points.\nProposition 1.1 (Interpolation). Let d,m \u2208N, let x(i) \u2208Rd, i \u2208[m], with x(i) \u0338= x(j) for i \u0338= j, let\n\u03f1\u2208C(R), and assume that \u03f1 is not a polynomial. Then, there exist parameters \u03b8(1) \u2208Rm\u00d7d \u00d7Rm with the\nfollowing property: For every k\u2208N and every sequence of labels y(i) \u2208Rk, i\u2208[m], there exist parameters\n\u03b8(2) = (W(2),0) \u2208Rk\u00d7m \u00d7Rk for the second layer of the NN architecture a= ((d,m,k ),\u03f1) such that\n\u03a6a(x(i),(\u03b8(1),\u03b8(2))) = y(i), i \u2208[m].\nLet us sketch the proof in the following. First, note that Theorem 1.15 also holds for functionsg\u2208C(K,Rm)\nwith multi-dimensional output by approximating each one-dimensional component x\u21a6\u2192(g(x))i and stacking\nthe resulting networks. Second, one can add an additional row containing only zeros to the weight matrix\nW(1) of the approximating neural network as well as an additional entry to the vector b(1). The e\ufb00ect of\nthis is that we obtain an additional neuron with constant output. Since \u03f1\u0338= 0, we can choose b(1) such that\nthe output of this neuron is not zero. Therefore, we can include the bias vector b(2) of the second layer\ninto the weight matrix W(2), see also Remark 1.5. Now choose g\u2208C(Rm,Rm) to be a function satisfying\ng(x(i)) = e(i), i\u2208[m], where e(i) \u2208Rm denotes the i-th standard basis vector.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2809, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "7633e7b8-650a-498c-a744-90f513d43d7d": {"__data__": {"id_": "7633e7b8-650a-498c-a744-90f513d43d7d", "embedding": null, "metadata": {"page_label": "13", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "61083f84-5a31-421f-9306-68b76f191817", "node_type": "4", "metadata": {"page_label": "13", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "10698b826437bb4ac3e3fe230542be5fce81bca6d11379bbf7972785707f75bf", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ce08e953-1108-406f-93f3-ef4ee4d15500", "node_type": "1", "metadata": {"page_label": "13", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "8e0bc0f48b36cc230eac480fe64b55e8543af2eaaabfa088b90e312d1f75c6e6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Second, one can add an additional row containing only zeros to the weight matrix\nW(1) of the approximating neural network as well as an additional entry to the vector b(1). The e\ufb00ect of\nthis is that we obtain an additional neuron with constant output. Since \u03f1\u0338= 0, we can choose b(1) such that\nthe output of this neuron is not zero. Therefore, we can include the bias vector b(2) of the second layer\ninto the weight matrix W(2), see also Remark 1.5. Now choose g\u2208C(Rm,Rm) to be a function satisfying\ng(x(i)) = e(i), i\u2208[m], where e(i) \u2208Rm denotes the i-th standard basis vector. By the discussion before there\nexists a neural network architecture \u02dca= ((d,n,m ),\u03f1) and parameters \u02dc\u03b8= ((\u02dcW(1),\u02dcb(1)),(\u02dcW(2),0)) such that\n\u2225\u03a6\u02dca(\u00b7,\u02dc\u03b8) \u2212g\u2225L\u221e(K) < 1\nm, (1.7)\nwhere K is a compact set with x(i) \u2208K, i\u2208[m]. Let us abbreviate the output of the activations in the \ufb01rst\nlayer evaluated at the input features by\n\u02dcA:=\n[\n\u03f1(\u02dcW(1)(x(1)) +\u02dcb(1))) ...\u03f1 (\u02dcW(1)(x(m)) +\u02dcb(1)))\n]\n\u2208Rn\u00d7m.\nThe equivalence of the max and operator norm and (1.7) establish that\n\u2225\u02dcW(2) \u02dcA\u2212Im\u2225op \u2264m max\ni,j\u2208[m]\n\u23d0\u23d0(\u02dcW(2) \u02dcA\u2212Im)i,j\n\u23d0\u23d0= mmax\nj\u2208[m]\n\u2225\u03a6\u02dca(x(j),\u02dc\u03b8) \u2212g(x(j))\u2225\u221e<1,\n13", "mimetype": "text/plain", "start_char_idx": 2232, "end_char_idx": 3361, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "7b84ebcc-dadb-4d9f-b91c-e77d411a0a80": {"__data__": {"id_": "7b84ebcc-dadb-4d9f-b91c-e77d411a0a80", "embedding": null, "metadata": {"page_label": "14", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "79c5fdb3-d643-4015-9c66-b2e61aaab2b9", "node_type": "4", "metadata": {"page_label": "14", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "73b244e894c64047bf5d9f085fa2e4a2bfb4e489eae27d1ac2b3b15f6dcf26f7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "606ff04a-ec88-42b0-acf9-1049f88b5a39", "node_type": "1", "metadata": {}, "hash": "f742a05e2be73571823ce2cadd4498ceb838664d59d1db8e75d3030fa0407387", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "where Im denotes the m\u00d7m identity matrix. Thus, the matrix \u02dcW(2) \u02dcA\u2208Rm\u00d7m needs to have full rank and\nwe can extract m linearly independent rows from \u02dcA resulting in an invertible matrix A\u2208Rm\u00d7m. Now, we\nde\ufb01ne the desired parameters \u03b8(1) for the \ufb01rst layer by extracting the corresponding rows from \u02dcW(1) and \u02dcb(1)\nand the parameters \u03b8(2) of the second layer by\nW(2) :=\n[\ny(1) ...y (m)]\nA\u22121 \u2208Rk\u00d7m.\nThis proves that with any discriminatory activation function we can interpolate arbitrary training data\n(x(i),y(i)) \u2208Rd \u00d7Rk, i\u2208[m], using a two-layer NN with m hidden neurons, i.e., O(m(d+ k)) parameters.\nOne can also \ufb01rst project the input features to a one-dimensional line where they are separated and then\napply Proposition 1.1 with d= 1. For nearly all activation functions, this can be represented by a three-layer\nNN using only O(d+ mk) parameters10.\nBeyond interpolation results, one can obtain a quantitative version of Theorem 1.15 if one knows additional\nregularity properties of the Bayes optimal function f\u2217, such as smoothness, compositionality, and symmetries.\nFor surveys on such results, we refer the reader to [ DHP20, GRK20]. For instructive purposes, we review one\nsuch result, which can be found in [Mha96, Theorem 2.1], below:\nTheorem 1.16 (Approximation of smooth functions) . Let d,k \u2208N and p\u2208[1,\u221e]. Further let \u03f1\u2208C\u221e(R)\nand assume that \u03f1is not a polynomial. Then there exists a constant c\u2208(0,\u221e) with the following property: For\nevery n\u2208N there exist parameters \u03b8(1) \u2208Rn\u00d7d\u00d7Rn for the \ufb01rst layer of the NN architecture a= ((d,n, 1),\u03f1)\nsuch that for every g\u2208Wk,p((0,1)d) it holds that\ninf\n\u03b8(2)\u2208R1\u00d7n\u00d7R\n\u2225\u03a6a(\u00b7,(\u03b8(1),\u03b8(2))) \u2212g\u2225Lp((0,1)d) \u2264cn\u2212d\nk\u2225g\u2225Wk,p((0,1)d).\nTheorem 1.16 shows that NNs achieve the same optimal approximation rates that, for example, spline-\nbased approximation yields for smooth functions. The idea behind this theorem is based on a strategy that is\nemployed repeatedly throughout the literature. This is the idea of re-approximating classical approximation\nmethods by NNs and thereby transferring the approximation rates of these methods to NNs. In the example\nof Theorem 1.16, approximation by polynomials is used. The idea is that due to the non-vanishing derivatives\nof the activation function11, one can approximate every univariate polynomial via divided di\ufb00erences of the\nactivation function. Speci\ufb01cally, accepting unbounded parameter magnitudes, for any activation function\n\u03f1: R \u2192R which is p-times di\ufb00erentiable at some point \u03bb\u2208R with \u03f1(p)(\u03bb) \u0338= 0, one can approximate the\nmonomial x\u21a6\u2192xp on a compact set K \u2282R up to arbitrary precision by a \ufb01xed-size NN via rescaled p-th\norder di\ufb00erence quotients as\nlim\nh\u21920\nsup\nx\u2208K\n\u23d0\u23d0\u23d0\np\u2211\ni=0\n(\u22121)i(p\ni\n)\nhp\u03f1(p)(\u03bb)\u03f1\n(\n(p/2 \u2212i)hx+ \u03bb\n)\n\u2212xp\n\u23d0\u23d0\u23d0= 0. (1.8)\nLet us end this subsection by clarifying the connection of the approximation results above to the error\ndecomposition of (1.4). Consider, for simplicity, a regression task with quadratic loss.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2923, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "606ff04a-ec88-42b0-acf9-1049f88b5a39": {"__data__": {"id_": "606ff04a-ec88-42b0-acf9-1049f88b5a39", "embedding": null, "metadata": {"page_label": "14", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "79c5fdb3-d643-4015-9c66-b2e61aaab2b9", "node_type": "4", "metadata": {"page_label": "14", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "73b244e894c64047bf5d9f085fa2e4a2bfb4e489eae27d1ac2b3b15f6dcf26f7", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7b84ebcc-dadb-4d9f-b91c-e77d411a0a80", "node_type": "1", "metadata": {"page_label": "14", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "d35b94b48302efa27bc07e08b71fed5213422684216125ed2940a57886fb1bf1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "(1.8)\nLet us end this subsection by clarifying the connection of the approximation results above to the error\ndecomposition of (1.4). Consider, for simplicity, a regression task with quadratic loss. Then, the approximation\nerror \u03b5approx equals a common L2-error\n\u03b5approx = R(f\u2217\nF) \u2212R\u2217(\u2217)\n=\n\u222b\nX\n(f\u2217\nF(x) \u2212f\u2217(x))2 dP X(x)\n(\u2217)\n= min\nf\u2208F\n\u2225f \u2212f\u2217\u22252\nL2(P X)\n\u2264min\nf\u2208F\n\u2225f \u2212f\u2217\u22252\nL\u221e(X),\nwhere the identities marked by ( \u2217) follow from Lemma 1.1. Hence, Theorem 1.15 postulates that \u03b5approx \u21920\nfor increasing NN sizes, whereas Theorem 1.16 additionally explains how fast \u03b5approx converges to 0.\n10To avoid the m\u00d7d weight matrix (without using shared parameters as in [ ZBH+17]) one interjects an approximate one-\ndimensional identity [PV18, De\ufb01nition 2.5], which can be arbitrarily well approximated by a NN with architecture a= ((1,2,1),\u03f1)\ngiven that \u03f1\u2032(\u03bb) \u0338= 0 for some \u03bb\u2208R, see (1.8) below.\n11The Baire category theorem ensures that for a non-polynomial \u03f1\u2208C\u221e(R) there exists \u03bb\u2208R with \u03f1(p)(\u03bb) \u0338= 0 for all p\u2208N,\nsee, e.g., [Don69, Chapter 10].\n14", "mimetype": "text/plain", "start_char_idx": 2725, "end_char_idx": 3759, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "388dda35-36d1-470e-a6e2-e3c956808e9c": {"__data__": {"id_": "388dda35-36d1-470e-a6e2-e3c956808e9c", "embedding": null, "metadata": {"page_label": "15", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "945e5d8f-1672-4ab5-a945-d8c32d647d34", "node_type": "4", "metadata": {"page_label": "15", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "cda4e83071bb2612f2ef65590199651cb4f23f2ae3f4de0e1b24af6737c67295", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f42429c6-97b4-4348-abed-7a6349e36190", "node_type": "1", "metadata": {}, "hash": "4537b6cab35975370c8f6937643a2a2ec9e001be6285798eac6e69d957cb3618", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "1.2.3 Generalization\nTowards bounding the generalization error \u03b5gen = supf\u2208F|R(f) \u2212\u02c6RS(f)|, one observes that, for every f \u2208F,\nAssumption 1.10 ensures that L(f,Z(i)), i \u2208[m], are i.i.d. random variables. Thus, one can make use of\nconcentration inequalities to bound the deviation of the empirical risk \u02c6RS(f) = 1\nm\n\u2211m\ni=1 L(f,Z(i)) from its\nexpectation R(f). For instance, assuming boundedness 12 of the loss, Hoe\ufb00ding\u2019s inequality [ Hoe63] and a\nunion bound directly imply the following generalization guarantee for countable, weighted hypothesis sets F,\nsee, e.g., [BBL03].\nTheorem 1.17 (Generalization bound for countable, weighted hypothesis sets) . Let m\u2208N, \u03b4\u2208(0,1) and\nassume that Fis countable. Further let p be a probability distribution on Fand assume that L(f,Z) \u2208[0,1]\nalmost surely for every f \u2208F. Then with probability 1\u2212\u03b4 (with respect to repeated sampling of P m\nZ-distributed\ntraining data S) it holds for every f \u2208F that\n|R(f) \u2212\u02c6RS(f)|\u2264\n\u221a\nln(1/p(f)) + ln(2/\u03b4)\n2m .\nWhile the weighting p needs to be chosen before seeing the training data, one could incorporate prior\ninformation on the learning algorithm A. For \ufb01nite hypothesis sets without prior information, setting\np(f) = 1/|F|for every f \u2208F, Theorem 1.17 implies that, with high probability, it holds that\n\u03b5gen \u2272\n\u221a\nln(|F|)\nm . (1.9)\nAgain, one notices that, in line with the bias-variance trade-o\ufb00, the generalization bound is increasing with\nthe size of the hypothesis set |F|. Although in practice the parameters \u03b8\u2208RP(N) of a NN are discretized\naccording to \ufb02oating-point arithmetic, the corresponding quantities |Fa|or |Fa,sgn|would be huge and we\nneed to \ufb01nd a replacement for the \ufb01niteness condition.\nWe will focus on binary classi\ufb01cation tasks and present a main result of VC theory which is to a great\nextent derived from the work of Vladimir Vapnik and Alexey Chervonenkis [VC71]. While in (1.9) we counted\nthe number of functions in F, we now re\ufb01ne this analysis to the number of functions restricted to a \ufb01nite\nsubset of X, given by the growth function\ngrowth(m,F) := max\n(x(i))m\ni=1\u2208Xm\n|{f|(x(i))m\ni=1\n: f \u2208F}|.\nThe growth function can be interpreted as the maximal number of classi\ufb01cation patterns in {\u22121,1}m which\nfunctions in Fcan realize on mpoints and thus growth(m,F) \u22642m. The asymptotic behavior of the growth\nfunction is determined by a single intrinsic dimension of our hypothesis set F, the so-called VC-dimension\nVCdim(F) := sup\n{\nm\u2208N \u222a{0}: growth( m,F) = 2m}\n,\nwhich de\ufb01nes the largest number of points such that Fcan realize any classi\ufb01cation pattern, see, e.g., [ AB99,\nBBL03]. There exist various results on VC-dimensions of NNs with di\ufb00erent activation functions, see,\nfor instance, [ BH89, KM97, BMM98, Sak99]. We present the result of [ BMM98] for piecewise polynomial\nactivation functions \u03f1. It establishes a bound on the VC-dimension of hypothesis sets of NNs for classi\ufb01cation\ntasks F(N,\u03f1),sgn that scales, up to logarithmic factors, linear in the number of parameters P(N) and quadratic\nin the number of layers L.\nTheorem 1.18 (VC-dimension of neural network hypothesis sets). Let \u03f1be a piecewise polynomial activation\nfunction.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3133, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f42429c6-97b4-4348-abed-7a6349e36190": {"__data__": {"id_": "f42429c6-97b4-4348-abed-7a6349e36190", "embedding": null, "metadata": {"page_label": "15", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "945e5d8f-1672-4ab5-a945-d8c32d647d34", "node_type": "4", "metadata": {"page_label": "15", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "cda4e83071bb2612f2ef65590199651cb4f23f2ae3f4de0e1b24af6737c67295", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "388dda35-36d1-470e-a6e2-e3c956808e9c", "node_type": "1", "metadata": {"page_label": "15", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "ad36f3984a9bc1444804a11b82c95c0aa39ddbcf4063df24b316e166141c4be7", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "There exist various results on VC-dimensions of NNs with di\ufb00erent activation functions, see,\nfor instance, [ BH89, KM97, BMM98, Sak99]. We present the result of [ BMM98] for piecewise polynomial\nactivation functions \u03f1. It establishes a bound on the VC-dimension of hypothesis sets of NNs for classi\ufb01cation\ntasks F(N,\u03f1),sgn that scales, up to logarithmic factors, linear in the number of parameters P(N) and quadratic\nin the number of layers L.\nTheorem 1.18 (VC-dimension of neural network hypothesis sets). Let \u03f1be a piecewise polynomial activation\nfunction. Then there exists a constant c\u2208(0,\u221e) such that for every L\u2208N and N \u2208NL+1 it holds that\nVCdim(F(N,\u03f1),sgn) \u2264c\n(\nP(N)Llog(P(N)) + P(N)L2)\n.\n12Note that for our classi\ufb01cation tasks in De\ufb01nition 1.2 it holds that L(f,Z) \u2208{0,1}for every f \u2208F. For the regression\ntasks, one typically assumes boundedness conditions, such as |Y|\u2264 c and supf\u2208F|f(X)|\u2264 c almost surely for some c\u2208(0,\u221e),\nwhich yields that sup f\u2208F|L(f,Z)|\u2264 4c2.\n15", "mimetype": "text/plain", "start_char_idx": 2575, "end_char_idx": 3552, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "0485283e-4100-42e5-817d-3024c8c43e0c": {"__data__": {"id_": "0485283e-4100-42e5-817d-3024c8c43e0c", "embedding": null, "metadata": {"page_label": "16", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "54a51921-8b13-47cc-8eae-46d96b665531", "node_type": "4", "metadata": {"page_label": "16", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "fbcb7810bb46d54ed976daf4ffb9b29a96c3a8bc494f2e553f0007b1e60496f0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "85ef6169-8eeb-4f4f-adb9-42493246c37e", "node_type": "1", "metadata": {}, "hash": "afabbf88ba57aa67c272e801aee4f7583aab3a249d46d13b5f267482b809de4a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Given (x(i))m\ni=1 \u2208Xm, there exists a partition of RP(N) such that \u03a6(x(i),\u00b7), i\u2208[m], are polynomials on\neach region of the partition. The proof of Theorem 1.18 is based on bounding the number of such regions\nand the number of classi\ufb01cation patterns of a set of polynomials.\nA \ufb01nite VC-dimension ensures the following generalization bound [Tal94, AB99]:\nTheorem 1.19 (VC-dimension generalization bound). There exists a constant c\u2208(0,\u221e) with the following\nproperty: For every classi\ufb01cation task as in De\ufb01nition 1.2, every Z-valued random variable Z, and every\nm\u2208N, \u03b4\u2208(0,1) it holds with probability 1 \u2212\u03b4 (with respect to repeated sampling of P m\nZ-distributed training\ndata S) that\nsup\nf\u2208F\n|R(f) \u2212\u02c6RS(f)|\u2264 c\n\u221a\nVCdim(F) + log(1/\u03b4))\nm .\nIn summary, using NN hypothesis sets F(N,\u03f1),sgn with a \ufb01xed depth and piecewise polynomial activation\n\u03f1 for a classi\ufb01cation task, with high probability it holds that\n\u03b5gen \u2272\n\u221a\nP(N) log(P(N))\nm . (1.10)\nIn the remainder of this section we will sketch a proof of Theorem 1.19 and, in doing so, present\nfurther concepts and complexity measures connected to generalization bounds. We start by observing that\nMcDiarmid\u2019s inequality [McD89] ensures that \u03b5gen is sharply concentrated around its expectation, i.e., with\nprobability 1 \u2212\u03b4 it holds that13\n\u23d0\u23d0\u03b5gen \u2212E\n[\n\u03b5gen]\u23d0\u23d0\u2272\n\u221a\nlog(1/\u03b4)\nm . (1.11)\nTo estimate the expectation of the uniform generalization error we employ a symmetrization argu-\nment [GZ84]. De\ufb01ne G:= L\u25e6F := {L(f,\u00b7): f \u2208F}, let \u02dcS = ( \u02dcZ(i))m\ni=1 \u223cP m\nZ be a test data set independent\nof S, and note that R(f) = E [ \u02c6R\u02dcS(f)]. By properties of the conditional expectation and Jensen\u2019s inequality\nit holds that\nE\n[\n\u03b5gen]\n= E\n[\nsup\nf\u2208F\n|R(f) \u2212\u02c6RS(f)|\n]\n= E\n[\nsup\ng\u2208G\n1\nm\n\u23d0\u23d0\nm\u2211\ni=1\nE\n[\ng( \u02dcZ(i)) \u2212g(Z(i))|S\n]\u23d0\u23d0\n]\n\u2264E\n[\nsup\ng\u2208G\n1\nm\n\u23d0\u23d0\nm\u2211\ni=1\ng( \u02dcZ(i)) \u2212g(Z(i))\n\u23d0\u23d0\n]\n= E\n[\nsup\ng\u2208G\n1\nm\n\u23d0\u23d0\nm\u2211\ni=1\n\u03c4i\n(\ng( \u02dcZ(i)) \u2212g(Z(i))\n)\u23d0\u23d0\n]\n\u22642E\n[\nsup\ng\u2208G\n1\nm\n\u23d0\u23d0\nm\u2211\ni=1\n\u03c4ig(Z(i))\n\u23d0\u23d0\n]\n,\nwhere we used that multiplications with Rademacher variables ( \u03c41,...,\u03c4 m) \u223cU({\u22121,1}m) only amount\nto interchanging Z(i) with \u02dcZ(i) which has no e\ufb00ect on the expectation, since Z(i) and \u02dcZ(i) have the same\ndistribution. The quantity\nRm(G) := E\n[\nsup\ng\u2208G\n\u23d0\u23d01\nm\nm\u2211\ni=1\n\u03c4ig(Z(i))\n\u23d0\u23d0\n]\nis called the Rademacher complexity14 of G. One can also prove a corresponding lower bound [ vdVW97], i.e.,\nRm(G) \u2212 1\u221am \u2272 E\n[\n\u03b5gen]\n\u2272 Rm(G).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2324, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "85ef6169-8eeb-4f4f-adb9-42493246c37e": {"__data__": {"id_": "85ef6169-8eeb-4f4f-adb9-42493246c37e", "embedding": null, "metadata": {"page_label": "16", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "54a51921-8b13-47cc-8eae-46d96b665531", "node_type": "4", "metadata": {"page_label": "16", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "fbcb7810bb46d54ed976daf4ffb9b29a96c3a8bc494f2e553f0007b1e60496f0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0485283e-4100-42e5-817d-3024c8c43e0c", "node_type": "1", "metadata": {"page_label": "16", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "bcb2208168027393987d3f3b483b3ebae8cae242dee630771bb0c08719cf9491", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The quantity\nRm(G) := E\n[\nsup\ng\u2208G\n\u23d0\u23d01\nm\nm\u2211\ni=1\n\u03c4ig(Z(i))\n\u23d0\u23d0\n]\nis called the Rademacher complexity14 of G. One can also prove a corresponding lower bound [ vdVW97], i.e.,\nRm(G) \u2212 1\u221am \u2272 E\n[\n\u03b5gen]\n\u2272 Rm(G). (1.12)\n13For precise conditions to ensure that the expectation of \u03b5gen is well-de\ufb01ned, we refer the reader to [vdVW97, Dud14].\n14Due to our decomposition in (1.4), we want to uniformly bound the absolute value of the di\ufb00erence between the risk and\nthe empirical risk. It is also common to just bound supf\u2208FR(f) \u2212\u02c6RS(f) leading to a de\ufb01nition of the Rademacher complexity\nwithout the absolute values which can be easier to deal with.\n16", "mimetype": "text/plain", "start_char_idx": 2122, "end_char_idx": 2760, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "0ab1e6f5-5cf4-444f-aa42-0cc0e8197617": {"__data__": {"id_": "0ab1e6f5-5cf4-444f-aa42-0cc0e8197617", "embedding": null, "metadata": {"page_label": "17", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2d5dd2f9-9fa2-456d-b60a-2170cbe91df3", "node_type": "4", "metadata": {"page_label": "17", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "d42631a35a72437708a1667de5c12d0c477273e5580160aef25fd01255b19204", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6bc95d12-6caa-4147-b98c-d15237e4f197", "node_type": "1", "metadata": {}, "hash": "75c489f10344aa511dac2e1c87d19559383be6cfbcd89b9b0d8acec30ac3c9e2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Now we use a chaining method to bound the Rademacher complexity of Fby covering numbers on di\ufb00erent\nscales. Speci\ufb01cally, Dudley\u2019s entropy integral [Dud67, LT91] implies that\nRm(G) \u2272 E\n[\u222b \u221e\n0\n\u221a\nlog N\u03b1(G,dS)\nm d\u03b1\n]\n, (1.13)\nwhere\nN\u03b1(G,dS) := inf\n{\n|G|: G\u2282G, G\u2282\n\u22c3\ng\u2208G\nBdS\n\u03b1 (g)\n}\ndenotes the covering number with respect to the (random) pseudometric given by\ndS(f,g) = d(Z(i))m\ni=1\n(f,g) :=\n\ued6a\ued6b\ued6b\u221a1\nm\nm\u2211\ni=1\n(\nf(Z(i)) \u2212g(Z(i))\n)2\n.\nFor the 0-1 loss L(f,z) = 1(\u2212\u221e,0)(yf(x)) = (1 \u2212f(x)y)/2, we can get rid of the loss function by the fact that\nN\u03b1(G,dS) = N2\u03b1(F,d(X(i))m\ni=1\n). (1.14)\nThe proof is completed by combining the inequalities in (1.11), (1.12), (1.13) and (1.14) with a result of\nDavid Haussler [Hau95] which shows that for \u03b1\u2208(0,1) we have\nlog(N\u03b1(F,d(X(i))m\ni=1\n)) \u2272 VCdim(F) log(1/\u03b1). (1.15)\nWe remark that this resembles a typical behavior of covering numbers. For instance, the logarithm of the\ncovering number log(N\u03b1(M)) of a compact d-dimensional Riemannian manifold Messentially scales like\ndlog(1/\u03b1). Finally, note that there exists a similar bound to the one in (1.15) for bounded regression tasks\nmaking use of the so-called fat-shattering dimension [MV03, Theorem 1].\n1.3 Do we need a new theory?\nDespite the already substantial insight that the classical theories provide, a lot of open questions remain. We\nwill outline these questions below. The remainder of this article then collects modern approaches to explain\nthe following issues:\nWhy do large neural networks not over\ufb01t? In Subsection 1.2.2, we have observed that three-layer\nNNs with commonly used activation functions and only O(d+ m) parameters can interpolate any training\ndata (x(i),y(i)) \u2208Rd\u00d7R, i\u2208[m]. While this speci\ufb01c representation might not be found in practice, [ ZBH+17]\nindeed trained convolutional 15 NNs with ReLU activation function and about 1 .6 million parameters to\nachieve zero empirical risk on m= 50000 training images of the CIFAR10 dataset [ KH09] with 32 \u00d732 pixels\nper image, i.e., d= 1024. For such large NNs, generalization bounds scaling with the number of parameters\nP(N) as the VC-dimension bound in (1.10) are vacuous. However, they observed close to state-of-the-art\ngeneralization performance16.\nGenerally speaking, NNs in practice are observed to generalize well despite having more parameters than\ntraining samples (usually referred to as overparametrization) and approximately interpolating the training data\n(usually referred to as over\ufb01tting). As we cannot perform any better on the training data, there is no trade-o\ufb00\nbetween \ufb01t to training data and complexity of the hypothesis set Fhappening, seemingly contradicting\nthe classical bias-variance trade-o\ufb00 of statistical learning theory. This is quite surprising, especially given\nthe following additional empirical observations in this regime, see [ NTS14, ZBH+17, NBMS17, BHMM19,\nNKB+20]:\n15The basic de\ufb01nition of a convolutional NN will be given in Section 6. In [ ZBH+17] more elaborate versions such as an\nInception architecture [SLJ+15] are employed.\n16In practice one usually cannot measure the risk R(fs) and instead evaluates the performance of a trained model fs by\n\u02c6R\u02dcs(fs) using test data \u02dcs, i.e., realizations of i.i.d.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3199, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "6bc95d12-6caa-4147-b98c-d15237e4f197": {"__data__": {"id_": "6bc95d12-6caa-4147-b98c-d15237e4f197", "embedding": null, "metadata": {"page_label": "17", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2d5dd2f9-9fa2-456d-b60a-2170cbe91df3", "node_type": "4", "metadata": {"page_label": "17", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "d42631a35a72437708a1667de5c12d0c477273e5580160aef25fd01255b19204", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0ab1e6f5-5cf4-444f-aa42-0cc0e8197617", "node_type": "1", "metadata": {"page_label": "17", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "92b7cc67831b0ba93576419608c72d072e5d2a54812141db276a887a51380320", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "As we cannot perform any better on the training data, there is no trade-o\ufb00\nbetween \ufb01t to training data and complexity of the hypothesis set Fhappening, seemingly contradicting\nthe classical bias-variance trade-o\ufb00 of statistical learning theory. This is quite surprising, especially given\nthe following additional empirical observations in this regime, see [ NTS14, ZBH+17, NBMS17, BHMM19,\nNKB+20]:\n15The basic de\ufb01nition of a convolutional NN will be given in Section 6. In [ ZBH+17] more elaborate versions such as an\nInception architecture [SLJ+15] are employed.\n16In practice one usually cannot measure the risk R(fs) and instead evaluates the performance of a trained model fs by\n\u02c6R\u02dcs(fs) using test data \u02dcs, i.e., realizations of i.i.d. random variables distributed according to P Z and drawn independently of the\ntraining data. In this context one often calls Rs(fs) the training error and R\u02dcs(fs) the test error.\n17", "mimetype": "text/plain", "start_char_idx": 2459, "end_char_idx": 3380, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ffe850f2-bd72-4464-9c2c-86375f2a5613": {"__data__": {"id_": "ffe850f2-bd72-4464-9c2c-86375f2a5613", "embedding": null, "metadata": {"page_label": "18", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "46bf62b4-5558-43c6-8d65-4f3b62dae319", "node_type": "4", "metadata": {"page_label": "18", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "bef0a0cb779d9b34239cf606df268a8d5ea5847e27ffeef4e41cc6da37573908", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "373929c7-7adb-456f-a25d-de8b9ba0de15", "node_type": "1", "metadata": {}, "hash": "d313e9056d7f71aea3d107d01df27ab537b9cfb2ef023f45a7d913ab16275bc0", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "0 6 12 18\nd\n0.00\n0.05\n0.10\nlinear regression ( = 0)\ntest\ntrain\n0 20 40\nd\n10 10\n10 1\n108\n0 6 12 18\nd\n0.00\n0.05\n0.10\nridge regression ( = 0.001)\ntest\ntrain\n0 20 40\nd\n10 2\n10 1\n0.5\n 0.0 0.5\nx\n0\n1\n2\nd = 40\n= 0\n= 0.001\nf *\ntraining data\nFigure 1.4: The \ufb01rst plot (and its semi-log inset) shows median and interquartile range of the test and training\nerrors of ten independent linear regressions with m= 20 samples, polynomial input features X = (1,Z,...,Z d)\nof degree d\u2208[40], and labels Y = f\u2217(Z) + \u03bd, where Z \u223cU([\u22120.5,0.5]), f\u2217is a polynomial of degree three,\nand \u03bd \u223cN(0,0.01). This clearly re\ufb02ects the classical u-shaped bias-variance curve with a sweet-spot at d= 3\nand drastic over\ufb01tting beyond the interpolation threshold at d= 20. However, the second plot shows that\nwe can control the complexity of our hypothesis set of linear models by restricting the Euclidean norm of\ntheir parameters using ridge regression with a small regularization parameter \u03b1 = 10\u22123, i.e., minimizing\nthe regularized empirical risk 1\nm\n\u2211m\ni=1(\u03a6(X(i),\u03b8) \u2212Y(i))2 + \u03b1\u2225\u03b8\u22252\n2, where \u03a6( \u00b7,\u03b8) = \u27e8\u03b8,\u00b7\u27e9. Corresponding\nexamples of \u02c6fs are depicted in the last plot.\n1. Zero training error on random labels: Zero empirical risk can also be achieved for random labels using\nthe same architecture and training scheme with only slightly increased training time: This suggests\nthat the considered hypothesis set of NNs Fcan \ufb01t arbitrary binary labels, which would imply that\nVCdim(F) \u2248m or Rm(F) \u22481 rendering our uniform generalization bounds in Theorem 1.19 and\nin (1.12) vacuous.\n2. Lack of explicit regularization: The test error depends only mildly on explicit regularization like norm-\nbased penalty terms or dropout (see [ G\u00b4 er17] for an explanation of di\ufb00erent regularization methods): As\nsuch regularization methods are typically used to decrease the complexity of F, one might ask if there\nis any implicit regularization (see Figure 1.4), constraining the range of our learning algorithm Ato\nsome smaller, potentially data-dependent subset, i.e., A(s) \u2208 \u02dcFs \u228a F.\n3. Dependence on the optimization: The same NN trained to zero empirical risk using di\ufb00erent variants of\nSGD or starting from di\ufb00erent initializations can exhibit di\ufb00erent test errors: This indicates that the\ndynamics of gradient descent and properties of the local neighborhood around the model fs = A(s)\nmight be correlated with generalization performance.\n4. Interpolation of noisy training data: One still observes low test error when training up to approximately\nzero empirical risk using a regression (or surrogate) loss on noisy training data. This is particularly\ninteresting, as the noise is captured by the model but seems not to hurt generalization performance.\n5. Further overparametrization improves generalization performance: Further increasing the NN size can\nlead to even lower test error: Together with the previous item, this might ask for a di\ufb00erent treatment\nof models complex enough to \ufb01t the training data. According to the traditional lore \u201cThe training error\ntends to decrease whenever we increase the model complexity, that is, whenever we \ufb01t the data harder.\nHowever with too much \ufb01tting, the model adapts itself too closely to the training data, and will not\ngeneralize well (i.e., have large test error)\u201d, [ HTF01]. While this \ufb02awlessly describes the situation for\ncertain machine learning tasks (see Figure 1.4), it seems not to be directly applicable here.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3424, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "373929c7-7adb-456f-a25d-de8b9ba0de15": {"__data__": {"id_": "373929c7-7adb-456f-a25d-de8b9ba0de15", "embedding": null, "metadata": {"page_label": "18", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "46bf62b4-5558-43c6-8d65-4f3b62dae319", "node_type": "4", "metadata": {"page_label": "18", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "bef0a0cb779d9b34239cf606df268a8d5ea5847e27ffeef4e41cc6da37573908", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ffe850f2-bd72-4464-9c2c-86375f2a5613", "node_type": "1", "metadata": {"page_label": "18", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "ab4796155b62aa4c898a4d4be6cd52695722b3b990e65d515db8def83330561f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This is particularly\ninteresting, as the noise is captured by the model but seems not to hurt generalization performance.\n5. Further overparametrization improves generalization performance: Further increasing the NN size can\nlead to even lower test error: Together with the previous item, this might ask for a di\ufb00erent treatment\nof models complex enough to \ufb01t the training data. According to the traditional lore \u201cThe training error\ntends to decrease whenever we increase the model complexity, that is, whenever we \ufb01t the data harder.\nHowever with too much \ufb01tting, the model adapts itself too closely to the training data, and will not\ngeneralize well (i.e., have large test error)\u201d, [ HTF01]. While this \ufb02awlessly describes the situation for\ncertain machine learning tasks (see Figure 1.4), it seems not to be directly applicable here.\nIn summary, this suggests that the generalization performance of NNs depends on an interplay of the data\ndistribution P Z combined with properties of the learning algorithm A, such as the optimization procedure\nand its range. In particular, classical uniform bounds as in Item (B) of our error decomposition might only\n18", "mimetype": "text/plain", "start_char_idx": 2588, "end_char_idx": 3746, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b538336a-590a-467c-994e-2bf4294cf002": {"__data__": {"id_": "b538336a-590a-467c-994e-2bf4294cf002", "embedding": null, "metadata": {"page_label": "19", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6810f935-b315-4872-9ab1-276a40e46e44", "node_type": "4", "metadata": {"page_label": "19", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "44cae6ed45ce29086100682ad10de908919c2fa4fc7fa4400c8976fe17cc529d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4ce76e13-7b10-4e8f-91dc-76419c43ba24", "node_type": "1", "metadata": {}, "hash": "6a26341fffa77a9d4729e72aab2d40d952a3a53b57954c951265711fb2a64ed0", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "deliver insu\ufb03cient explanation, see also [ NK19]. The mismatch between predictions of classical theory and\nthe practical generalization performance of deep NNs is often referred to as generalization puzzle. In Section 2\nwe will present possible explanations for this phenomenon.\nWhat is the role of depth? We have seen in Subsection 1.2.2 that NNs can closely approximate every\nfunction if they are su\ufb03ciently wide [ Cyb89, Fun89, HSW89]. There are additional classical results that even\nprovide a trade-o\ufb00 between the width and the approximation accuracy [ CLM94, Mha96, MP99]. In these\nresults, the central concept is the width of a NN. In modern applications, however at least as much focus if\nnot more lies on the depth of the underlying architectures, which can have more than 1000 layers [ HZRS16].\nAfter all, the depth of NNs is responsible for the name of deep learning.\nThis consideration begs the question of whether there is a concrete mathematically quanti\ufb01able bene\ufb01t of\ndeep architectures over shallow NNs. Indeed, we will see e\ufb00ects of depth at many places throughout this\nmanuscript. However, one of the aspects of deep learning that is most clearly a\ufb00ected by deep architectures\nis the approximation theoretical aspect. In this framework, we will discuss in Section 3 multiple approaches\nthat describe the e\ufb00ect of depth.\nWhy do neural networks perform well in very high-dimensional environments? We have seen\nin Subsection 1.2.2 and will see in Section 3 that from the perspective of approximation theory deep NNs\nmatch the performance of the best classical approximation tool in virtually every task. In practice, we\nobserve something that is even more astounding. In fact, NNs seem to perform incredibly well on tasks that\nno classical, non-specialized approximation method can even remotely handle. The approximation problem\nthat we are talking about here is that of approximation of high-dimensional functions. Indeed, the classical\ncurse of dimensionality [Bel52, NW09] postulates that essentially every approximation method deteriorates\nexponentially fast with increasing dimension.\nFor example, for the uniform approximation error of 1-Lipschitz continuous functions on a d-dimensional\nunit cube in the uniform norm, we have a lower bound of \u2126( p\u22121/d), for p\u2192\u221e, when approximating with a\ncontinuous scheme17 of p free parameters [DeV98].\nOn the other hand, in most applications, the input dimensions are massive. For example, the following\ndatasets are typically used as benchmarks in image classi\ufb01cation problems: MNIST [ LBBH98] with 28 \u00d728\npixels per image, CIFAR-10/CIFAR-100 [KH09] with 32\u00d732 pixels per image and ImageNet [DDS+09, KSH12]\nwhich contains high-resolution images that are typically down-sampled to 256 \u00d7256 pixels. Naturally, in\nreal-world applications, the input dimensions may well exceed those of these test problems. However, already\nfor the simplest of the test cases above, the input dimension isd= 784. If we use d= 784 in the aforementioned\nlower bound for the approximation of 1-Lipschitz functions, then we require O(\u03b5\u2212784) parameters to achieve\na uniform error of \u03b5 \u2208(0,1). Already for moderate \u03b5 this value will quickly exceed the storage capacity\nof any conceivable machine in this universe. Considering the aforementioned curse of dimensionality, it is\npuzzling to see that NNs perform adequately in this regime. In Section 4, we describe three approaches that\no\ufb00er explanations as to why deep NN-based approximation is not rendered meaningless in the context of\nhigh-dimensional input dimensions.\nWhy does stochastic gradient descent converge to good local minima despite the non-convexity\nof the problem? As mentioned in Subsection 1.2.1, a convergence guarantee of stochastic gradient descent\nto a global minimum is typically only given if the underlying objective function admits some form of convexity.\nHowever, the empirical risk of a NN, i.e., \u02c6Rs(\u03a6(\u00b7,\u03b8)), is typically not a convex function with respect to the\nparameters \u03b8. For a simple intuitive reason why this function fails to be convex, it is instructive to consider\nthe following example.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4111, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "4ce76e13-7b10-4e8f-91dc-76419c43ba24": {"__data__": {"id_": "4ce76e13-7b10-4e8f-91dc-76419c43ba24", "embedding": null, "metadata": {"page_label": "19", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6810f935-b315-4872-9ab1-276a40e46e44", "node_type": "4", "metadata": {"page_label": "19", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "44cae6ed45ce29086100682ad10de908919c2fa4fc7fa4400c8976fe17cc529d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b538336a-590a-467c-994e-2bf4294cf002", "node_type": "1", "metadata": {"page_label": "19", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "7b7476beaa488a58d74d1fa1088005d13c1a42d6b657a9d9ad7be204b300de5f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Considering the aforementioned curse of dimensionality, it is\npuzzling to see that NNs perform adequately in this regime. In Section 4, we describe three approaches that\no\ufb00er explanations as to why deep NN-based approximation is not rendered meaningless in the context of\nhigh-dimensional input dimensions.\nWhy does stochastic gradient descent converge to good local minima despite the non-convexity\nof the problem? As mentioned in Subsection 1.2.1, a convergence guarantee of stochastic gradient descent\nto a global minimum is typically only given if the underlying objective function admits some form of convexity.\nHowever, the empirical risk of a NN, i.e., \u02c6Rs(\u03a6(\u00b7,\u03b8)), is typically not a convex function with respect to the\nparameters \u03b8. For a simple intuitive reason why this function fails to be convex, it is instructive to consider\nthe following example.\n17One can achieve better rates at the cost of discontinuous (with respect to the function to be approximated) parameter\nassignment. This can be motivated by the use of space-\ufb01lling curves. In the context of NNs with piecewise polynomial activation\nfunctions, a rate of p\u22122/d can be achieved by very deep architectures [Yar18a, YZ20].\n19", "mimetype": "text/plain", "start_char_idx": 3249, "end_char_idx": 4448, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "6a937a97-dd88-45f8-b4b9-4033dff65df9": {"__data__": {"id_": "6a937a97-dd88-45f8-b4b9-4033dff65df9", "embedding": null, "metadata": {"page_label": "20", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ff3f6703-3601-4572-b67c-23ebbd05d5dd", "node_type": "4", "metadata": {"page_label": "20", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "126594a105e827d511d8b83bcdae60767ec177130c6004880ddbedd639d0c1a2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Figure 1.5: Two-dimensional projection of the loss landscape of a neural network with four layers and ReLU\nactivation function on four di\ufb00erent scales. From top-left to bottom-right, we zoom into the global minimum\nof the landscape.\nExample 1.20. Consider the NN\n\u03a6(x,\u03b8) = \u03b81\u03f1R(\u03b83x+ \u03b85) + \u03b82\u03f1R(\u03b84x+ \u03b86), \u03b8 \u2208R6, x \u2208R,\nwith the ReLU activation function \u03f1R(x) = max{0,x}. It is not hard to see that the two parameter values\n\u03b8= (1,\u22121,1,1,1,0) and \u00af\u03b8= (\u22121,1,1,1,0,1) produce the same realization function 18, i.e., \u03a6(\u00b7,\u03b8) = \u03a6(\u00b7,\u00af\u03b8).\nHowever, since (\u03b8+ \u00af\u03b8)/2 = (0,0,1,1,1/2,1/2), we conclude that \u03a6(\u00b7,(\u03b8+ \u00af\u03b8)/2) = 0. Clearly, for the data\ns= ((\u22121,0),(1,1)), we now have that\n\u02c6Rs(\u03a6(\u00b7,\u03b8)) = \u02c6Rs(\u03a6(\u00b7,\u00af\u03b8)) = 0 and \u02c6Rs\n(\n\u03a6(\u00b7,(\u03b8+ \u00af\u03b8)/2)\n)\n= 1\n2,\nshowing the non-convexity of \u02c6Rs.\n18This corresponds to interchanging the two neurons in the hidden layer. In general it holds that the realization function of a\nFC NN is invariant under permutations of the neurons in a given hidden layer.\n20", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 975, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "08039658-2335-43fc-a53b-6ba055c7c852": {"__data__": {"id_": "08039658-2335-43fc-a53b-6ba055c7c852", "embedding": null, "metadata": {"page_label": "21", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3bff6b00-6fcb-4b8d-b1aa-6a615fdaec51", "node_type": "4", "metadata": {"page_label": "21", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "08bfba1ba9562ca7ee3a54a37f79fd1ccf2104c389e3b2ff65b6614d8dbe29fd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "22bc9140-bfc2-4e55-bf78-e9467e99ae82", "node_type": "1", "metadata": {}, "hash": "48b8677c77482bc81c7d818ed103fe601f9ce21ecc6c3d4291ede852bee4b560", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Given this non-convexity, Algorithm 1 faces serious challenges. Firstly, there may exist multiple suboptimal\nlocal minima. Secondly, the objective may exhibit saddle points, some of which may be of higher order, i.e.,\nthe Hessian vanishes. Finally, even if no suboptimal local minima exist, there may be extensive areas of the\nparameter space where the gradient is very small, so that escaping these regions can take a very long time.\nThese issues are not mere theoretical possibilities, but will almost certainly arise. For example, [ AHW96,\nSS18] show the existence of many suboptimal local minima in typical learning tasks. Moreover, for \ufb01xed-sized\nNNs, it has been shown in [ BEG19, PRV20], that with respect to Lp-norms the set of NNs is generally a\nvery non-convex and non-closed set. Also, the map \u03b8\u21a6\u2192\u03a6a(\u00b7,\u03b8) is not a quotient map, i.e., not continuously\ninvertible when accounting for its non-injectivity. In addition, in various situations \ufb01nding the global optimum\nof the minimization problem is shown to be NP-hard in general [ BR89, Jud90, \u02c7S\u00b4 \u0131m02]. In Figure 1.5 we\nshow the two-dimensional projection of a loss landscape, i.e., the projection of the graph of the function\n\u03b8\u21a6\u2192 \u02c6Rs(\u03a6(\u00b7,\u03b8)). It is apparent from the visualization that the problem exhibits more than one minimum.\nWe also want to add that in practice one neglects that the loss is only almost everywhere di\ufb00erentiable in\ncase of piecewise smooth activation functions, such as the ReLU, although one could resort to subgradient\nmethods [KL18].\nIn view of these considerations, the classical framework presented in Subsection 1.2.1 o\ufb00ers no explanation\nas to why deep learning works in practice. Indeed, in the survey [ OM98, Section 1.4] the state of the art in\n1998 was summarized by the following assessment: \u201cThere is no formula to guarantee that (1) the NN will\nconverge to a good solution, (2) convergence is swift, or (3) convergence even occurs at all.\u201d\nNonetheless, in applications, not only would an explanation of when and why SGD converges be extremely\ndesirable, convergence is also quite often observed even though there is little theoretical explanation for it in\nthe classical set-up. In Section 5, we collect modern approaches explaining why and when convergence occurs\nand can be guaranteed.\nWhich aspects of a neural network architecture a\ufb00ect the performance of deep learning? In\nthe introduction to classical approaches to deep learning above, we have seen that in classical results, such as\nin Theorem 1.16, only the e\ufb00ect of few aspects of the NN architectures are considered. In Theorem 1.16 only\nthe impact of the width of the NN was studied. In further approximation theorems below, e.g., in Theorems 2.1\nand 3.2, we will additionally have a variable depth of NNs. However, for deeper architectures, there are\nmany additional aspects of the architecture that could potentially a\ufb00ect the performance of the model for\nthe associated learning task. For example, even for a standard FC NN with L layers as in De\ufb01nition 1.4,\nthere is a lot of \ufb02exibility in choosing the number of neurons ( N1,...,N L\u22121) \u2208NL\u22121 in the hidden layers.\nOne would expect that certain choices a\ufb00ect the capabilities of the NNs considerably and some choices are\npreferable over others. Note that, one aspect of the neural network architecture that can have a profound\ne\ufb00ect on the performance, especially regarding approximation theoretical aspects of the performance, is the\nchoice of the activation function. For example, in [ MP99, Yar21] activation functions were found that allow\nuniform approximation of continuous functions to arbitrary accuracy with \ufb01xed-size neural networks. In the\nsequel we will, however, focus on architectural aspects other than the activation function.\nIn addition, practitioners have invented an immense variety of NN architectures for speci\ufb01c problems.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3857, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "22bc9140-bfc2-4e55-bf78-e9467e99ae82": {"__data__": {"id_": "22bc9140-bfc2-4e55-bf78-e9467e99ae82", "embedding": null, "metadata": {"page_label": "21", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3bff6b00-6fcb-4b8d-b1aa-6a615fdaec51", "node_type": "4", "metadata": {"page_label": "21", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "08bfba1ba9562ca7ee3a54a37f79fd1ccf2104c389e3b2ff65b6614d8dbe29fd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "08039658-2335-43fc-a53b-6ba055c7c852", "node_type": "1", "metadata": {"page_label": "21", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "0d43516ef7e6682d3e0072c125398d767882803eafed31b88a30bd92b6fbc473", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "One would expect that certain choices a\ufb00ect the capabilities of the NNs considerably and some choices are\npreferable over others. Note that, one aspect of the neural network architecture that can have a profound\ne\ufb00ect on the performance, especially regarding approximation theoretical aspects of the performance, is the\nchoice of the activation function. For example, in [ MP99, Yar21] activation functions were found that allow\nuniform approximation of continuous functions to arbitrary accuracy with \ufb01xed-size neural networks. In the\nsequel we will, however, focus on architectural aspects other than the activation function.\nIn addition, practitioners have invented an immense variety of NN architectures for speci\ufb01c problems. These\ninclude NNs with convolutional blocks [LBBH98], with skip connections [HZRS16], sparse connections [ZAP16,\nBBC17], batch normalization blocks [ IS15], and many more. In addition, for sequential data, recurrent\nconnections are used [ RHW86] and these often have forget mechanisms [ HS97] or other gates [ CvMG+14]\nincluded in their architectures.\nThe choice of an appropriate NN architecture is essential to the success of many deep learning tasks. This\ngoes so far, that frequently an architecture search is applied to \ufb01nd the most suitable one [ ZL17, PGZ+18].\nIn most cases, though, the design and choice of the architecture is based on the intuition of the practitioner.\nNaturally, from a theoretical point of view, this situation is not satisfactory. Instead, it would be highly\ndesirable to have a mathematical theory guiding the choice of NN architectures. More concretely, one\nwould wish for mathematical theorems that identify those architectures that work for a speci\ufb01c problem and\nthose that will yield suboptimal results. In Section 6, we discuss various results that explain theoretically\nquanti\ufb01able e\ufb00ects of certain aspects or building blocks of NN architectures.\n21", "mimetype": "text/plain", "start_char_idx": 3128, "end_char_idx": 5045, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "4081def5-86ef-463e-839a-9ea0a8912667": {"__data__": {"id_": "4081def5-86ef-463e-839a-9ea0a8912667", "embedding": null, "metadata": {"page_label": "22", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d1e8ccb1-1fb3-4536-ba94-30657d180b19", "node_type": "4", "metadata": {"page_label": "22", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "d17128c69b9abc887f96c9812941cfb0d16c746739c833c330b34ef3c627b986", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "78c3d25a-919a-49a9-8cbc-f7a9e0073335", "node_type": "1", "metadata": {}, "hash": "3bdeae04ab4c4030239730dc0e8d45349736a35e24872aba31549600d54703cb", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Which features of data are learned by deep architectures? It is commonly believed that the neurons\nof NNs constitute feature extractors in di\ufb00erent levels of abstraction that correspond to the layers. This\nbelief is partially grounded in experimental evidence as well as in drawing connections to the human visual\ncortex, see [GBC16, Chapter 9.10].\nUnderstanding the features that are learned can, in a way, be linked to understanding the reasoning\nwith which a NN-based model ended up with its result. Therefore, analyzing the features that a NN learns\nconstitutes a data-aware approach to understanding deep learning. Naturally, this falls outside of the scope of\nthe classical theory, which is formulated in terms of optimization, generalization, and approximation errors.\nOne central obstacle towards understanding these features theoretically is that, at least for practical\nproblems, the data distribution is unknown. However, one often has partial knowledge. One example is that\nin image classi\ufb01cation it appears reasonable to assume that any classi\ufb01er is translation and rotation invariant\nas well as invariant under small deformations. In this context, it is interesting to understand under which\nconditions trained NNs admit the same invariances.\nBiological NNs such as the visual cortex are believed to be evolved in a way that is based on sparse\nmultiscale representations of visual information [ OF96]. Again, a fascinating question is whether NNs trained\nin practice can be shown to favor such multiscale representations based on sparsity or if the architecture\nis theoretically linked to sparse representations. We will discuss various approaches studying the features\nlearned by neural networks in Section 7.\nAre neural networks capable of replacing highly specialized numerical algorithms in natural\nsciences? Shortly after their successes in various data-driven tasks in data science and AI applications,\nNNs have been used also as a numerical ansatz for solving highly complex models from the natural sciences\nwhich may be combined with data driven methods. This per se is not very surprising as many such models\ncan be formulated as optimization problems where the common deep learning paradigm can be directly\napplied. What might be considered surprising is that this approach seems to be applicable to a wide range of\nproblems which have previously been tackled by highly specialized numerical methods.\nParticular successes include the data-driven solution of ill-posed inverse problems [AM\u00a8OS19] which have,\nfor example, led to a fourfold speedup in MRI scantimes [ ZKS+18] igniting the research project fastmri.org.\nDeep-learning-based approaches have also been very successful in solving a vast array of di\ufb00erent partial\ndi\ufb00erential equation (PDE) models, especially in the high-dimensional regime [ EY18, RPK19, HSN20,\nPSMF20] where most other methods would su\ufb00er from the curse of dimensionality.\nDespite these encouraging applications, the foundational mechanisms governing their workings and\nlimitations are still not well understood. In Subsection 4.3 and Section 8 we discuss some theoretical and\npractical aspects of deep learning methods applied to the solution of inverse problems and PDEs.\n2 Generalization of large neural networks\nIn the following, we will shed light on the generalization puzzle of NNs as described in Subsection 1.3. We\nfocus on four di\ufb00erent lines of research which, of course, do not cover the wide range of available results.\nIn fact, we had to omit a discussion of a multitude of important works, some of which we reference in the\nfollowing paragraph.\nFirst, let us mention extensions of the generalization bounds presented in Subsection 1.2.3 making use\nof local Rademacher complexities [BBM05] or dropping assumptions on boundedness or rapidly decaying\ntails [Men14]. Furthermore, there are approaches to generalization which do not focus on the hypothesis set F,\ni.e., the range of the learning algorithm A, but the way Achooses its model fs. For instance, one can assume\nthat fs does not depend too strongly on each individual sample ( algorithmic stability [BE02, PRMN04]),\nonly on a subset of the samples ( compression bounds [AGNZ18]), or satis\ufb01es local properties ( algorithmic\nrobustness [XM12]). Finally, we refer the reader to [ JNM+20] and the references mentioned therein for an\nempirical study of various measures related to generalization.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4413, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "78c3d25a-919a-49a9-8cbc-f7a9e0073335": {"__data__": {"id_": "78c3d25a-919a-49a9-8cbc-f7a9e0073335", "embedding": null, "metadata": {"page_label": "22", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d1e8ccb1-1fb3-4536-ba94-30657d180b19", "node_type": "4", "metadata": {"page_label": "22", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "d17128c69b9abc887f96c9812941cfb0d16c746739c833c330b34ef3c627b986", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4081def5-86ef-463e-839a-9ea0a8912667", "node_type": "1", "metadata": {"page_label": "22", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "865d2ad2cb47fc4e75ea2be2818c1e166fa9f1731f557c6512f2a714cc433e8c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "First, let us mention extensions of the generalization bounds presented in Subsection 1.2.3 making use\nof local Rademacher complexities [BBM05] or dropping assumptions on boundedness or rapidly decaying\ntails [Men14]. Furthermore, there are approaches to generalization which do not focus on the hypothesis set F,\ni.e., the range of the learning algorithm A, but the way Achooses its model fs. For instance, one can assume\nthat fs does not depend too strongly on each individual sample ( algorithmic stability [BE02, PRMN04]),\nonly on a subset of the samples ( compression bounds [AGNZ18]), or satis\ufb01es local properties ( algorithmic\nrobustness [XM12]). Finally, we refer the reader to [ JNM+20] and the references mentioned therein for an\nempirical study of various measures related to generalization.\nNote that many results on generalization capabilities of NNs can still only be proven in simpli\ufb01ed settings,\ne.g., for deep linear NNs, i.e., \u03f1(x) = x, or basic linear models, i.e., one-layer NNs. Thus, we start by\n22", "mimetype": "text/plain", "start_char_idx": 3611, "end_char_idx": 4631, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "4d5f0e60-2f52-4a96-b2c6-c6dbe84ab12e": {"__data__": {"id_": "4d5f0e60-2f52-4a96-b2c6-c6dbe84ab12e", "embedding": null, "metadata": {"page_label": "23", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ef883407-1e48-4ec1-b2cf-37f0b283ce69", "node_type": "4", "metadata": {"page_label": "23", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "9ac8559ec361cd0cca5396cf12a874435be21725c5d0e55053a6683926559a41", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c0d67455-c6c6-4fdc-821a-4a6517ef8e6d", "node_type": "1", "metadata": {}, "hash": "b4c565339b41594b0c612e8766e3ba7ae7d1327fe776cc0ff1e7603ed9b9c8d2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "emphasizing the connection of deep, nonlinear NNs to linear models (operating on features given by a suitable\nkernel) in the in\ufb01nite width limit .\n2.1 Kernel regime\nWe consider a one-dimensional prediction setting where the loss L(f,(x,y)) depends on x\u2208X only through\nf(x) \u2208Y, i.e., there exists a function \u2113: Y\u00d7Y\u2192 R such that\nL(f,(x,y)) = \u2113(f(x),y).\nFor instance, in case of the quadratic loss we have that \u2113(\u02c6y,y) = ( \u02c6y\u2212y)2. Further, let \u03a6 be a NN with\narchitecture (N,\u03f1) = ((d,N1,...,N L\u22121,1),\u03f1) and let \u0398 0 be a RP(N)-valued random variable. For simplicity,\nwe evolve the parameters of \u03a6 according to the continuous version of gradient descent, so-called gradient \ufb02ow,\ngiven by\nd\u0398(t)\ndt = \u2212\u2207\u03b8 \u02c6Rs(\u03a6(\u00b7,\u0398(t))) = \u22121\nm\nm\u2211\ni=1\n\u2207\u03b8\u03a6(x(i),\u0398(t))Di(t), \u0398(0) = \u03980, (2.1)\nwhere Di(t) := \u2202\u2113(\u02c6y,y(i))\n\u2202\u02c6y |\u02c6y=\u03a6(x(i),\u0398(t)) is the derivative of the loss with respect to the prediction at input\nfeature x(i) at time t\u2208[0,\u221e). The chain rule implies the following dynamics of the NN realization\nd\u03a6(\u00b7,\u0398(t))\ndt = \u22121\nm\nm\u2211\ni=1\nK\u0398(t)(\u00b7,x(i))Di(t) (2.2)\nand its empirical risk\nd \u02c6Rs(\u03a6(\u00b7,\u0398(t))\ndt = \u2212 1\nm2\nm\u2211\ni=1\nm\u2211\nj=1\nDi(t)K\u0398(t)(x(i),x(j))Dj(t), (2.3)\nwhere K\u03b8, \u03b8\u2208RP(N), is the so-called neural tangent kernel (NTK)\nK\u03b8: Rd \u00d7Rd \u2192R, K \u03b8(x1,x2) =\n(\n\u2207\u03b8\u03a6(x1,\u03b8)\n)T\n\u2207\u03b8\u03a6(x2,\u03b8). (2.4)\nNow let \u03c3w,\u03c3b \u2208(0,\u221e) and assume that the initialization \u0398 0 consists of independent entries, where entries\ncorresponding to the weight matrix and bias vector in the \u2113-th layer follow a normal distribution with\nzero mean and variances \u03c32\nw/N\u2113 and \u03c32\nb, respectively. Under weak assumptions on the activation function,\nthe central limit theorem implies that the pre-activations converge to i.i.d. centered Gaussian processes in\nthe in\ufb01nite width limit N1,...,N L\u22121 \u2192\u221e, see [ LBN+18, MHR+18]. Similarly, also K\u03980 converges to a\ndeterministic kernel K\u221ewhich stays constant in time and only depends on the activation function \u03f1, the\ndepth L, and the initialization parameters \u03c3w and \u03c3b [JGH18, ADH+19, Yan19, LXS+20]. Thus, within the\nin\ufb01nite width limit, gradient \ufb02ow on the NN parameters as in (2.1) is equivalent to functional gradient \ufb02ow\nin the reproducing kernel Hilbert space (HK\u221e,\u2225\u00b7\u2225K\u221e) corresponding to K\u221e, see (2.2).\nBy (2.3), the empirical risk converges to a global minimum as long as the kernel evaluated at the input\nfeatures, \u00afK\u221e:= (K\u221e(x(i),x(j)))m\ni,j=1 \u2208Rm\u00d7m, is positive de\ufb01nite (see, e.g., [ JGH18, DLL+19] for suitable\nconditions) and the \u2113(\u00b7,y(i)) are convex and lower bounded.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2450, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c0d67455-c6c6-4fdc-821a-4a6517ef8e6d": {"__data__": {"id_": "c0d67455-c6c6-4fdc-821a-4a6517ef8e6d", "embedding": null, "metadata": {"page_label": "23", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ef883407-1e48-4ec1-b2cf-37f0b283ce69", "node_type": "4", "metadata": {"page_label": "23", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "9ac8559ec361cd0cca5396cf12a874435be21725c5d0e55053a6683926559a41", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4d5f0e60-2f52-4a96-b2c6-c6dbe84ab12e", "node_type": "1", "metadata": {"page_label": "23", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "665aec01c128ae207d388973dbba78e63ebef99c49b5e1b8d3ea9f3067d3aad1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Thus, within the\nin\ufb01nite width limit, gradient \ufb02ow on the NN parameters as in (2.1) is equivalent to functional gradient \ufb02ow\nin the reproducing kernel Hilbert space (HK\u221e,\u2225\u00b7\u2225K\u221e) corresponding to K\u221e, see (2.2).\nBy (2.3), the empirical risk converges to a global minimum as long as the kernel evaluated at the input\nfeatures, \u00afK\u221e:= (K\u221e(x(i),x(j)))m\ni,j=1 \u2208Rm\u00d7m, is positive de\ufb01nite (see, e.g., [ JGH18, DLL+19] for suitable\nconditions) and the \u2113(\u00b7,y(i)) are convex and lower bounded. For instance, in case of the quadratic loss the\nsolution of (2.2) is then given by\n\u03a6(\u00b7,\u0398(t)) = C(t)(y(i))m\ni=1 +\n(\n\u03a6(\u00b7,\u03980) \u2212C(t)(\u03a6(x(i),\u03980))m\ni=1\n)\n, (2.5)\nwhere C(t) :=\n(\n(K\u221e(\u00b7,x(i)))m\ni=1\n)T\n( \u00afK\u221e)\u22121(Im \u2212e\u22122 \u00afK\u221et\nm ). As the initial realization \u03a6( \u00b7,\u03980) constitutes a\ncentered Gaussian process, the second term in (2.5) follows a normal distribution with zero mean at each\ninput. In the limit t \u2192\u221e, its variance vanishes on the input features x(i), i \u2208[m], and the \ufb01rst term\nconvergences to the minimum kernel-norm interpolator, i.e., to the solution of\nmin\nf\u2208HK\u221e\n\u2225f\u2225K\u221e s.t. f(x(i)) = y(i).\n23", "mimetype": "text/plain", "start_char_idx": 1970, "end_char_idx": 3046, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "285c86b4-90ce-4894-8756-c9412336a579": {"__data__": {"id_": "285c86b4-90ce-4894-8756-c9412336a579", "embedding": null, "metadata": {"page_label": "24", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7f20761a-e0eb-4a36-a29f-3d481c8bb256", "node_type": "4", "metadata": {"page_label": "24", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "b8ea3e601425e96d097e28ca2cb75eec031e0748f1ec3177a3ed38dfa8e56f3f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e598aaf2-00b1-4a0e-a252-b4e974b39cb9", "node_type": "1", "metadata": {}, "hash": "aff9969ff0bfc31608e95dad90261a6f66a06fdbfc0d69a63ec54be80da98c74", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Therefore, within the in\ufb01nite width limit, the generalization properties of the NN could be described by the\ngeneralization properties of the minimizer in the reproducing kernel Hilbert space corresponding to the kernel\nK\u221e[BMM18, LR20, LRZ20, GMMM21, Li21].\nThis so-called lazy training, where a NN essentially behaves like a linear model with respect to the nonlinear\nfeatures x\u21a6\u2192\u2207\u03b8\u03a6(x,\u03b8), can already be observed in the non-asymptotic regime, see also Subsection 5.2. For\nsu\ufb03ciently overparametrized (P(N) \u226bm) and suitably initialized models, one can show that K\u03b8(0) is close\nto K\u221eat initialization and K\u03b8(t) stays close to K\u03b8(0) throughout training, see [ DZPS18, ADH+19, COB19,\nDLL+19]. The dynamics of the NN under gradient \ufb02ow in (2.2) and (2.3) can thus be approximated by the\ndynamics of the linearization of \u03a6 at initialization \u0398 0, given by\n\u03a6lin(\u00b7,\u03b8) := \u03a6(\u00b7,\u03980) + \u27e8\u2207\u03b8\u03a6(\u00b7,\u03980),\u03b8 \u2212\u03980\u27e9,\nwhich motivates to study the behavior of linear models in the overparametrized regime.\n2.2 Norm-based bounds and margin theory\nFor piecewise linear activation functions, one can improve upon the VC-dimension bounds in Theorem 1.18\nand show that, up to logarithmic factors, the VC-dimension is asymptotically bounded both above and below\nby P(N)L, see [BHLM19]. The lower bound shows that the generalization bound in Theorem 1.19 can only\nbe non-vacuous if the number of samples m scales at least linearly with the number of NN parameters P(N).\nHowever, heavily overparametrized NNs used in practice seem to generalize well outside of this regime.\nOne solution is to bound other complexity measures of NNs taking into account various norms on the\nparameters and avoid the direct dependence on the number of parameters [ Bar98]. For instance, we can\ncompute bounds on the Rademacher complexity of NNs with positively homogeneous activation function,\nwhere the Frobenius norm of the weight matrices is bounded, see also [ NTS15]. Note that, for instance,\nthe ReLU activation is positively homogeneous, i.e., it satis\ufb01es that \u03f1R(\u03bbx) = \u03bb\u03f1R(x) for all x \u2208R and\n\u03bb\u2208(0,\u221e).\nTheorem 2.1 (Rademacher complexity of neural networks). Let d\u2208N, assume that X= B1(0) \u2282Rd, and\nlet \u03f1 be a positively homogeneous activation function with Lipschitz constant 1. We de\ufb01ne the set of all\nbiasless NN realizations with depth L\u2208N, output dimension 1, and Frobenius norm of the weight matrices\nbounded by C \u2208(0,\u221e) as\n\u02dcFL,C :=\n{\n\u03a6(N,\u03f1)(\u00b7,\u03b8): N \u2208NL+1, N0 = d, NL = 1, \u03b8= ((W(\u2113),0))L\n\u2113=1 \u2208RP(N), \u2225W(\u2113)\u2225F \u2264C\n}\n.\nThen for every m\u2208N it holds that\nRm( \u02dcFL,C) \u2264C(2C)L\u22121\n\u221am .\nThe term 2 L\u22121 depending exponentially on the depth can be reduced to\n\u221a\nL or completely omitted\nby invoking also the spectral norm of the weight matrices [ GRS18]. Further, observe that for L = 1,\ni.e., linear classi\ufb01ers with bounded Euclidean norm, this bound is independent of the input dimension d.\nTogether with (1.12), this motivates why the regularized linear model in Figure 1.4 did perform well in the\noverparametrized regime.\nThe proof of Theorem 2.1 is based on the contraction property of the Rademacher complexity [ LT91]\nwhich establishes that\nRm(\u03f1\u25e6\u02dcF\u2113,C) \u22642Rm( \u02dcF\u2113,C), \u2113 \u2208N.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3121, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e598aaf2-00b1-4a0e-a252-b4e974b39cb9": {"__data__": {"id_": "e598aaf2-00b1-4a0e-a252-b4e974b39cb9", "embedding": null, "metadata": {"page_label": "24", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7f20761a-e0eb-4a36-a29f-3d481c8bb256", "node_type": "4", "metadata": {"page_label": "24", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "b8ea3e601425e96d097e28ca2cb75eec031e0748f1ec3177a3ed38dfa8e56f3f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "285c86b4-90ce-4894-8756-c9412336a579", "node_type": "1", "metadata": {"page_label": "24", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "d16eaae33383fecf85b155a86df6f919ad399c5dc99c5e7bdbfc944861149ce2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The term 2 L\u22121 depending exponentially on the depth can be reduced to\n\u221a\nL or completely omitted\nby invoking also the spectral norm of the weight matrices [ GRS18]. Further, observe that for L = 1,\ni.e., linear classi\ufb01ers with bounded Euclidean norm, this bound is independent of the input dimension d.\nTogether with (1.12), this motivates why the regularized linear model in Figure 1.4 did perform well in the\noverparametrized regime.\nThe proof of Theorem 2.1 is based on the contraction property of the Rademacher complexity [ LT91]\nwhich establishes that\nRm(\u03f1\u25e6\u02dcF\u2113,C) \u22642Rm( \u02dcF\u2113,C), \u2113 \u2208N.\nWe can iterate this together with the fact that for every \u03c4 \u2208{\u22121,1}m, and x\u2208RN\u2113\u22121 it holds that\nsup\n\u2225W(\u2113)\u2225F\u2264C\n\ued79\ued79\nm\u2211\ni=1\n\u03c4i\u03f1(W(\u2113)x)\n\ued79\ued79\n2 = C sup\n\u2225w\u22252\u22641\n\u23d0\u23d0\nm\u2211\ni=1\n\u03c4i\u03f1(\u27e8w,x\u27e9)\n\u23d0\u23d0.\n24", "mimetype": "text/plain", "start_char_idx": 2533, "end_char_idx": 3300, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c68a2840-4799-4755-8180-75f6c99dc3b6": {"__data__": {"id_": "c68a2840-4799-4755-8180-75f6c99dc3b6", "embedding": null, "metadata": {"page_label": "25", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2d50e2be-5c8a-41a2-b2c4-7ae1942dc685", "node_type": "4", "metadata": {"page_label": "25", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "51a6fc2b82e8556af24d8b9aa35b288e357fb10de59dc1602a7004ae8a6be684", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "24877f06-16ec-4d78-9d87-2dec4f5827ea", "node_type": "1", "metadata": {}, "hash": "5ec775df45942753e6b95b0eda0b7b1765da3ec99b0e3d24287260c79943de37", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In summary, one establishes that\nRm( \u02dcFL,C) = C\nmE\n[\nsup\nf\u2208\u02dcFL\u22121,C\n\ued79\ued79\nm\u2211\ni=1\n\u03c4i\u03f1(f(X(i)))\n\ued79\ued79\n2\n]\n\u2264C(2C)L\u22121\nm E\n[\ued79\ued79\nm\u2211\ni=1\n\u03c4iX(i)\ued79\ued79\n2\n]\n,\nwhich by Jensen\u2019s inequality yields the claim.\nRecall that for classi\ufb01cation problems one typically minimizes a surrogate loss Lsurr, see Remark 1.9.\nThis suggests that there could be a trade-o\ufb00 happening between complexity of the hypothesis class Fa and\nthe corresponding regression \ufb01t underneath, i.e., the margin M(f,z) := yf(x) by which a training example\nz= (x,y) has been classi\ufb01ed correctly by f \u2208Fa, see [BFT17, NBS18, JKMB19]. For simplicity, let us focus\non the ramp surrogate loss with con\ufb01dence \u03b3 >0, i.e., Lsurr\n\u03b3 (f,z) := \u2113\u03b3(M(f,z)), where\n\u2113\u03b3(t) := 1(\u2212\u221e,\u03b3](t) \u2212t\n\u03b31[0,\u03b3](t), t \u2208R.\nNote that the ramp function \u2113\u03b3 is 1/\u03b3-Lipschitz continuous. Using McDiarmid\u2019s inequality and a sym-\nmetrization argument similar to the proof of Theorem 1.19, combined with the contraction property of the\nRademacher complexity, yields the following bound on the probability of misclassi\ufb01cation: With probability\n1 \u2212\u03b4 for every f \u2208Fa it holds that\nP [sgn(f(X)) \u0338= Y] \u2264E\n[\nLsurr\n\u03b3 (f,Z)\n]\n\u2272 1\nm\nm\u2211\ni=1\nLsurr\n\u03b3 (f,Z(i)) + Rm(Lsurr\n\u03b3 \u25e6Fa) +\n\u221a\nln(1/\u03b4)\nm\n\u2272 1\nm\nm\u2211\ni=1\n1(\u2212\u221e,\u03b3)(Y(i)f(X(i))) + Rm(M \u25e6Fa)\n\u03b3 +\n\u221a\nln(1/\u03b4)\nm\n= 1\nm\nm\u2211\ni=1\n1(\u2212\u221e,\u03b3)(Y(i)f(X(i))) + Rm(Fa)\n\u03b3 +\n\u221a\nln(1/\u03b4)\nm .\nThis shows the trade-o\ufb00 between the complexity of Fa measured by Rm(Fa) and the fraction of training\ndata that has been classi\ufb01ed correctly with a margin of at least \u03b3. In particular this suggests, that (even if\nwe classify the training data correctly with respect to the 0-1 loss) it might be bene\ufb01cial to further increase\nthe complexity of Fa to simultaneously increase the margins by which the training data has been classi\ufb01ed\ncorrectly and thus obtain a better generalization bound.\n2.3 Optimization and implicit regularization\nThe optimization algorithm, which is usually a variant of SGD, seems to play an important role for the\ngeneralization performance. Potential indicators for good generalization performance are high speed of\nconvergence [HRS16] or \ufb02atness of the local minimum to which SGD converged, which can be characterized\nby the magnitude of the eigenvalues of the Hessian (or approximately as the robustness of the minimizer\nto adversarial perturbations on the parameter space), see [ KMN+17]. In [ DR17, NBMS17] generalization\nbounds depending on a concept of \ufb02atness are established by employing a PAC-Bayesian framework, which\ncan be viewed as a generalization of Theorem 1.17, see [ McA99]. Further, one can also unite \ufb02atness and\nnorm-based bounds by the Fisher\u2013Rao metric of information geometry [LPRS19].", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2635, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "24877f06-16ec-4d78-9d87-2dec4f5827ea": {"__data__": {"id_": "24877f06-16ec-4d78-9d87-2dec4f5827ea", "embedding": null, "metadata": {"page_label": "25", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2d50e2be-5c8a-41a2-b2c4-7ae1942dc685", "node_type": "4", "metadata": {"page_label": "25", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "51a6fc2b82e8556af24d8b9aa35b288e357fb10de59dc1602a7004ae8a6be684", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c68a2840-4799-4755-8180-75f6c99dc3b6", "node_type": "1", "metadata": {"page_label": "25", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "4a45664175e30e93eb2f5f3aae600cc3305f8c03848143abda8336af8cc58e71", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Potential indicators for good generalization performance are high speed of\nconvergence [HRS16] or \ufb02atness of the local minimum to which SGD converged, which can be characterized\nby the magnitude of the eigenvalues of the Hessian (or approximately as the robustness of the minimizer\nto adversarial perturbations on the parameter space), see [ KMN+17]. In [ DR17, NBMS17] generalization\nbounds depending on a concept of \ufb02atness are established by employing a PAC-Bayesian framework, which\ncan be viewed as a generalization of Theorem 1.17, see [ McA99]. Further, one can also unite \ufb02atness and\nnorm-based bounds by the Fisher\u2013Rao metric of information geometry [LPRS19].\nLet us motivate the link between generalization and \ufb02atness in the case of simple linear models: We\nassume that our model takes the form \u27e8\u03b8,\u00b7\u27e9, \u03b8\u2208Rd, and we will use the abbreviations\nr(\u03b8) := \u02c6Rs(\u27e8\u03b8,\u00b7\u27e9) and \u03b3(\u03b8) := min\ni\u2208[m]\nM(\u27e8\u03b8,\u00b7\u27e9,z(i)) = min\ni\u2208[m]\ny(i)\u27e8\u03b8,x(i)\u27e9\nthroughout this subsection to denote the empirical risk and the margin for given training data s =\n((x(i),y(i)))m\ni=1. We assume that we are solving a classi\ufb01cation task with the 0-1 loss and that our training\n25", "mimetype": "text/plain", "start_char_idx": 1967, "end_char_idx": 3112, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f1d9b863-2fa5-4e9e-8aa1-f90217396e9c": {"__data__": {"id_": "f1d9b863-2fa5-4e9e-8aa1-f90217396e9c", "embedding": null, "metadata": {"page_label": "26", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c99a54fc-b8fb-4b9e-b121-2beeb24440f4", "node_type": "4", "metadata": {"page_label": "26", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "bd23fce0ed933309b04ac74bee8ff18818a79c1f2138623a7cd122e1ff8e64f4", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "data is linearly separable. This means that there exists a minimizer \u02c6\u03b8\u2208Rd such that r(\u02c6\u03b8) = 0. We observe\nthat \u03b4-robustness in the sense that\nmax\n\u03b8\u2208B\u03b4(0)\nr(\u02c6\u03b8+ \u03b8) = r(\u02c6\u03b8) = 0\nimplies that\n0 < min\ni\u2208[m]\ny(i)\u27e8\u02c6\u03b8\u2212\u03b4y(i) x(i)\n\u2225x(i)\u22252\n,x(i)\u27e9\u2264 \u03b3(\u02c6\u03b8) \u2212\u03b4 min\ni\u2208[m]\n\u2225x(i)\u22252,\nsee also [PKL+17]. This lower bound on the margin \u03b3(\u02c6\u03b8) then ensures generalization guarantees as described\nin Subsection 2.2.\nEven without explicit19 control on the complexity of Fa, there do exist results showing that SGD acts as\nimplicit regularization [NTS14]. This is motivated by linear models where SGD converges to the minimal\nEuclidean norm solution for the quadratic loss and in the direction of the hard margin support vector machine\nsolution for the logistic loss on linearly separable data [ SHN+18]. Note that convergence to minimum norm\nor maximum margin solutions in particular decreases the complexity of our hypothesis set and thus improves\ngeneralization bounds, see Subsection 2.2.\nWhile we have seen this behavior of gradient descent for linear regression already in the more general\ncontext of kernel regression in Subsection 2.1, we want to motivate the corresponding result for classi\ufb01cation\ntasks in the following. We focus on the exponential surrogate loss Lsurr(f,z) = \u2113(M(f,z)) = e\u2212yf(x) with\n\u2113(z) = e\u2212z, but similar observations can be made for the logistic loss de\ufb01ned in Remark 1.9. We assume that\nthe training data is linearly separable, which guarantees the existence of \u02c6\u03b8\u0338= 0 with \u03b3(\u02c6\u03b8) >0. Then for every\nlinear model \u27e8\u03b8,\u00b7\u27e9, \u03b8\u2208Rd, it holds that\n\u27e8\u02c6\u03b8,\u2207\u03b8r(\u03b8)\u27e9= 1\nm\nm\u2211\ni=1\n\u2113\u2032(y(i)\u27e8\u03b8,x(i)\u27e9)\ued19 \ued18\ued17 \ued1a\n<0\ny(i)\u27e8\u02c6\u03b8,x(i)\u27e9\ued19 \ued18\ued17 \ued1a\n>0\n.\nA critical point \u2207\u03b8r(\u03b8) = 0 can therefore be approached if and only if for every i\u2208[m] we have\n\u2113\u2032(y(i)\u27e8\u03b8,x(i)\u27e9) = \u2212e\u2212y(i)\u27e8\u03b8,x(i)\u27e9\u21920,\nwhich is equivalent to \u2225\u03b8\u22252 \u2192\u221e and \u03b3(\u03b8) >0. Let us now de\ufb01ne\nr\u03b2(\u03b8) := \u2113\u22121(r(\u03b2\u03b8))\n\u03b2 , \u03b8 \u2208Rd, \u03b2\u2208(0,\u221e),\nand observe that\nr\u03b2(\u03b8) = \u2212log(r(\u03b2\u03b8))\n\u03b2 \u2192\u03b3(\u03b8), \u03b2 \u2192\u221e. (2.6)\nDue to this property, r\u03b2 is often referred to as the smoothed margin [LL19, JT19b]. We evolve \u03b8 according to\ngradient \ufb02ow with respect to the smoothed margin r1, i.e.,\nd\u03b8(t)\ndt = \u2207\u03b8r1(\u03b8(t)) = \u2212 1\nr(\u03b8(t))\u2207\u03b8r(\u03b8(t)),\nwhich produces the same trajectory as gradient \ufb02ow with respect to the empirical risk r under a rescaling\nof the time t. Looking at the evolution of the normalized parameters \u02dc\u03b8(t) = \u03b8(t)/\u2225\u03b8(t)\u22252, the chain rule\nestablishes that\nd\u02dc\u03b8(t)\ndt = P\u02dc\u03b8(t)\n\u2207\u03b8r\u03b2(t)(\u02dc\u03b8(t))\n\u03b2(t) with \u03b2(t) := \u2225\u03b8(t)\u22252 and P\u03b8 := Id \u2212\u03b8\u03b8T, \u03b8 \u2208Rd.\n19Note that also di\ufb00erent architectures can exhibit vastly di\ufb00erent inductive biases [ ZBH+20] and also within the architecture\ndi\ufb00erent parameters have di\ufb00erent importance, see [FC18, ZBS19] and Proposition 6.2.\n26", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2658, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f8117bf1-30ce-4ce1-a324-df24aeab0f9a": {"__data__": {"id_": "f8117bf1-30ce-4ce1-a324-df24aeab0f9a", "embedding": null, "metadata": {"page_label": "27", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "179937f3-5518-4faf-b3e9-7f03a18a6a30", "node_type": "4", "metadata": {"page_label": "27", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "abff43c84ab0e8c563f6eb0c7405aa5660fb215eac2f3c1ae4ffdf3e9dbe5b87", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "dc671a17-8adf-4653-93a8-c28169f4dea2", "node_type": "1", "metadata": {}, "hash": "f01e91b63eef091065284c8a052babb50ba1b005fa135bb1774b7309ef0a1fbf", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This shows that the normalized parameters perform projected gradient ascent with respect to the function\nr\u03b2(t), which converges to the margin due to (2.6) and the fact that \u03b2(t) = \u2225\u03b8(t)\u22252 \u2192\u221e when approaching a\ncritical point. This motivates that during gradient \ufb02ow the normalized parameters implicitly maximize the\nmargin. See [ GLSS18a, GLSS18b, LL19, NLG+19, CB20, JT20] for a precise analysis and various extensions,\ne.g., to homogeneous or two-layer NNs and other optimization geometries.\nTo illustrate one research direction, we present an exemplary result in the following. Let \u03a6 = \u03a6 (N,\u03f1) be\na biasless NN with parameters \u03b8= ((W(\u2113),0))L\n\u2113=0 and output dimension NL = 1. For given input features\nx \u2208RN0, the gradient \u2207W(\u2113)\u03a6 = \u2207W(\u2113)\u03a6(x,\u03b8) \u2208RN\u2113\u22121\u00d7N\u2113 with respect to the weight matrix in the \u2113-th\nlayer satis\ufb01es that\n\u2207W(\u2113)\u03a6 = \u03f1(\u03a6(\u2113\u22121)) \u2202\u03a6\n\u2202\u03a6(\u2113+1)\n\u2202\u03a6(\u2113+1)\n\u2202\u03a6(\u2113) = \u03f1(\u03a6(\u2113\u22121)) \u2202\u03a6\n\u2202\u03a6(\u2113+1) W(\u2113+1) diag\n(\n\u03f1\u2032(\u03a6(\u2113))\n)\n,\nwhere the pre-activations (\u03a6(\u2113))L\n\u2113=1 are given as in (1.1). Evolving the parameters according to gradient \ufb02ow\nas in (2.1) and using an activation function \u03f1 with \u03f1(x) = \u03f1\u2032(x)x, such as the ReLU, this implies that\ndiag\n(\n\u03f1\u2032(\u03a6(\u2113))\n)\nW(\u2113)(t)\n(dW(\u2113)(t)\ndt\n)T\n=\n(dW(\u2113+1)(t)\ndt\n)T\nW(\u2113+1)(t) diag\n(\n\u03f1\u2032(\u03a6(\u2113))\n)\n. (2.7)\nNote that this ensures the conservation of balancedness between weight matrices of adjacent layers, i.e.,\nd\ndt\n(\n\u2225W(\u2113+1)(t)\u22252\nF \u2212\u2225W(\u2113)(t)\u22252\nF\n)\n= 0,\nsee [DHL18]. Furthermore, for deep linear NNs, i.e., \u03f1(x) = x, the property in (2.7) implies conservation of\nalignment of left and right singular spaces of W(\u2113) and W(\u2113+1). This can then be used to show implicit pre-\nconditioning and convergence of gradient descent [ ACH18, ACGH19] and that, under additional assumptions,\ngradient descent converges to a linear predictor that is aligned with the maximum margin solution [JT19a].\n2.4 Limits of classical theory and double descent\nThere is ample evidence that classical tools from statistical learning theory alone, such as Rademacher\naverages, uniform convergence, or algorithmic stability may be unable to explain the full generalization\ncapabilities of NNs [ZBH+17, NK19]. It is especially hard to reconcile the classical bias-variance trade-o\ufb00 with\nthe observation of good generalization performance when achieving zero empirical risk on noisy data using a\nregression loss. On top of that, this behavior of overparametrized models in the interpolation regime turns out\nnot to be unique to NNs. Empirically, one observes for various methods (decision trees, random features, linear\nmodels) that the test error decreases even below the sweet-spot in the u-shaped bias-variance curve when\nfurther increasing the number of parameters [ BHMM19, GJS+20, NKB+20]. This is often referred to as the\ndouble descent curve or benign over\ufb01tting, see Figure 2.1. For special cases, e.g., linear regression or random\nfeature regression, such behavior can even be proven, see [HMRT19, MM19, BLLT20, BHX20, MVSS20].\nIn the following we analyze this phenomenon in the context of linear regression.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3008, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "dc671a17-8adf-4653-93a8-c28169f4dea2": {"__data__": {"id_": "dc671a17-8adf-4653-93a8-c28169f4dea2", "embedding": null, "metadata": {"page_label": "27", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "179937f3-5518-4faf-b3e9-7f03a18a6a30", "node_type": "4", "metadata": {"page_label": "27", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "abff43c84ab0e8c563f6eb0c7405aa5660fb215eac2f3c1ae4ffdf3e9dbe5b87", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f8117bf1-30ce-4ce1-a324-df24aeab0f9a", "node_type": "1", "metadata": {"page_label": "27", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "5a4aa5a55f8a5634597836c999a2dda24ee1c555d8525fb36360a8277490cead", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "On top of that, this behavior of overparametrized models in the interpolation regime turns out\nnot to be unique to NNs. Empirically, one observes for various methods (decision trees, random features, linear\nmodels) that the test error decreases even below the sweet-spot in the u-shaped bias-variance curve when\nfurther increasing the number of parameters [ BHMM19, GJS+20, NKB+20]. This is often referred to as the\ndouble descent curve or benign over\ufb01tting, see Figure 2.1. For special cases, e.g., linear regression or random\nfeature regression, such behavior can even be proven, see [HMRT19, MM19, BLLT20, BHX20, MVSS20].\nIn the following we analyze this phenomenon in the context of linear regression. Speci\ufb01cally, we focus\non a prediction task with quadratic loss, input features given by a centered Rd-valued random variable\nX, and labels given by Y = \u27e8\u03b8\u2217,X\u27e9+ \u03bd, where \u03b8\u2217\u2208Rd and \u03bd is a centered random variable independent\nof X. For training data S = ((X(i),Y (i)))m\ni=1, we consider the empirical risk minimizer \u02c6fS = \u27e8\u02c6\u03b8,\u00b7\u27e9with\nminimum Euclidean norm of its parameters \u02c6\u03b8or, equivalently, the limit of gradient \ufb02ow with zero initialization.\nUsing (1.3) and a bias-variance decomposition we can write\nE [R( \u02c6fS)|(X(i))m\ni=1] \u2212R\u2217= E [\u2225\u02c6fS \u2212f\u2217\u2225L2(P X)|(X(i))m\ni=1]\n= (\u03b8\u2217)TPE [XXT]P\u03b8\u2217+ E [\u03bd2]Tr\n(\n\u03a3+E [XXT]\n)\n,\nwhere \u03a3 := \u2211m\ni=1 X(i)(X(i))T, \u03a3 + denotes the Moore\u2013Penrose inverse of \u03a3, and P := I d \u2212\u03a3+\u03a3 is the\northogonal projector onto the kernel of \u03a3. For simplicity, we focus on the variance Tr\n(\n\u03a3+E [XXT]\n)\n, which can\n27", "mimetype": "text/plain", "start_char_idx": 2303, "end_char_idx": 3833, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "6e323a8c-568d-48c5-9578-b9be33ebcc35": {"__data__": {"id_": "6e323a8c-568d-48c5-9578-b9be33ebcc35", "embedding": null, "metadata": {"page_label": "28", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "264887f3-d4b6-4c45-82e5-479e1ee7ef04", "node_type": "4", "metadata": {"page_label": "28", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "4ac03be12c9362dd11b3c4eb6d1a65e9fa1d2043891c3eee7d1078b90bfa3e60", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Figure 2.1: This illustration shows the classical, underparametrized regime in green, where the u-shaped curve\ndepicts the bias-variance trade-o\ufb00 as explained in Section 1.2. Starting with complexity of our algorithm\nAlarger than the interpolation threshold we can achieve zero empirical risk \u02c6Rs(fs) (training error), where\nfs = A(s). Within this modern interpolation regime, the risk R(fs) (test error) might be even lower than\nat the classical sweet spot. Whereas complexity(A) traditionally refers to the complexity of the hypothesis\nset F, there is evidence that also the optimization scheme and the data is in\ufb02uencing the complexity\nleading to de\ufb01nitions like complexity(A) := max\n{\nm \u2208N: E\n[\u02c6RS(A(S))\n]\n\u2264\u03b5 with S \u223cP m\nZ\n}\n, for suitable\n\u03b5> 0 [NKB+20]. This illustration is based on [BHMM19].\nbe viewed as setting \u03b8\u2217= 0 and E [\u03bd2] = 1. Assuming that X has i.i.d. entries with unit variance and bounded\n\ufb01fth moment, the distribution of the eigenvalues of 1\nm\u03a3+ in the limit d,m \u2192\u221e with d\nm \u2192\u03ba\u2208(0,\u221e) can be\ndescribed via the Marchenko\u2013Pastur law. Therefore, the asymptotic variance can be computed explicitly as\nTr\n(\n\u03a3+E [XXT]\n)\n\u21921 \u2212max{1 \u2212\u03ba,0}\n|1 \u2212\u03ba| for d,m \u2192\u221e with d\nm \u2192\u03ba,\nalmost surely, see [ HMRT19]. This shows that despite interpolating the data we can decrease the risk in\nthe overparametrized regime \u03ba> 1. In the limit d,m \u2192\u221e, such benign over\ufb01tting can also be shown for\nmore general settings (including lazy training of NNs), some of which even achieve their optimal risk in the\noverparametrized regime [MM19, MZ20, LD21].\nFor normally distributed input features X such that E [XXT] has rank larger than m, one can also\ncompute the behavior of the variance in the non-asymptomatic regime [BLLT20]. De\ufb01ne\nk\u2217:= min{k\u22650:\n\u2211\ni>k\u03bbi\n\u03bbk+1\n\u2265cm},\nwhere \u03bb1 \u2265\u03bb2 \u2265\u00b7\u00b7\u00b7\u2265 \u03bbd \u22650 are the eigenvalues of E [XXT] in decreasing order and c\u2208(0,\u221e) is a universal\nconstant. Assuming that k\u2217/m is su\ufb03ciently small, with high probability it holds that\nTr\n(\n\u03a3+E [XXT]\n)\n\u2248k\u2217\nm + m\u2211\ni>k\u2217\u03bb2\ni\n(\u2211\ni>k\u2217\u03bbi)2 .\n28", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1994, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ea6610ca-6d7b-412f-8e9a-78670ff703ac": {"__data__": {"id_": "ea6610ca-6d7b-412f-8e9a-78670ff703ac", "embedding": null, "metadata": {"page_label": "29", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "38234d53-0ca9-4146-9ec4-2ade5b7c8b67", "node_type": "4", "metadata": {"page_label": "29", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "5f44a303d4282cfa0b6482607dead985cb1b8a437ee15542b4fe9dbb277ffd81", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fa142759-8335-47be-84a3-0b137ebeeed0", "node_type": "1", "metadata": {}, "hash": "7d55b82628a30052bfbcf44522287daf10fb3e716bd0aeae83683f9577bb6edb", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "0 50 100 150\nd\n0.0\n0.5\n1.0 variance\nFigure 2.2: The expected variance\nof linear regression in (2.8) with d\u2208\n[150] and Xi \u223cU({\u22121,1}), i\u2208[150],\nwhere Xi = X1 for i\u2208{10,..., 20}\u222a\n{30,..., 50}and all other coordinates\nare independent.\nThis precisely characterizes the regimes for benign over\ufb01tting in terms\nof the eigenvalues of the covariance matrix E [XXT]. Furthermore, it\nshows that adding new input feature coordinates and thus increasing\nthe number of parameters dcan lead to either an increase or decrease\nof the risk.\nTo motivate this phenomenon, which is considered in much more\ndepth in [ CMBK20], let us focus on a single sample m = 1 and\nfeatures X that take values in X = {\u22121,1}d. Then it holds that\n\u03a3+ = X(1)(X(1))T\n\u2225X(1)\u22254 = X(1)(X(1))T\nd2 and thus\nE\n[\nTr\n(\n\u03a3+E [XXT]\n)]\n= 1\nd2\n\ued79\ued79E\n[\nXXT]\ued79\ued792\nF. (2.8)\nIn particular, this shows that incrementing the input feature dimen-\nsions d\u21a6\u2192d+ 1 one can increase or decrease the risk depending on\nthe correlation of the coordinate Xd+1 with respect to the previous\ncoordinates (Xi)d\ni=1, see also Figure 2.2.\nGenerally speaking, overparametrization and perfectly \ufb01tting\nnoisy data does not exclude good generalization performance, see\nalso [ BRT19]. However, the risk crucially depends on the data\ndistribution and the chosen algorithm.\n3 The role of depth in the expressivity of neural networks\nThe approximation theoretical aspect of a NN architecture, responsible for the approximation component\n\u03b5approx := R(f\u2217\nF) \u2212R\u2217of the error R(fS) \u2212R\u2217in (1.4), is probably one of the most well-studied parts of the\ndeep learning pipe-line. The achievable approximation error of an architecture most directly describes the\npower of the architecture.\nAs mentioned in Subsection 1.3, many classical approaches only study the approximation theory of NNs\nwith few layers, whereas modern architectures are typically very deep. A \ufb01rst observation into the e\ufb00ect of\ndepth is that it can often compensate for insu\ufb03cient width. For example, in the context of the universal\napproximation theorem, it was shown that very narrow NNs are still universal if instead of increasing the\nwidth, the number of layers can be chosen arbitrarily [ HS17, Han19, KL20]. However, if the width of a NN\nfalls below a critical number, then the universality will not hold any longer.\nBelow, we discuss three additional observations that shed light on the e\ufb00ect of depth on the approximation\ncapacities or alternative notions of expressivity of NNs.\n3.1 Approximation of radial functions\nOne technique to study the impact of depth relies on the construction of speci\ufb01c functions which can be\nwell approximated by NNs of a certain depth, but require signi\ufb01cantly more parameters when approximated\nto the same accuracy by NNs of smaller depth. In the following we present one example for this type of\napproach, which can be found in [ES16].\nTheorem 3.1 (Power of depth). Let \u03f1\u2208{\u03f1R,\u03f1\u03c3,1(0,\u221e)}be the ReLU, the logistic, or the Heaviside function.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2952, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "fa142759-8335-47be-84a3-0b137ebeeed0": {"__data__": {"id_": "fa142759-8335-47be-84a3-0b137ebeeed0", "embedding": null, "metadata": {"page_label": "29", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "38234d53-0ca9-4146-9ec4-2ade5b7c8b67", "node_type": "4", "metadata": {"page_label": "29", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "5f44a303d4282cfa0b6482607dead985cb1b8a437ee15542b4fe9dbb277ffd81", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ea6610ca-6d7b-412f-8e9a-78670ff703ac", "node_type": "1", "metadata": {"page_label": "29", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "2dc30c4ca3a206bdd2cc6e2710fb3a085dcb49cb575ccecab3938036ef05a487", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Below, we discuss three additional observations that shed light on the e\ufb00ect of depth on the approximation\ncapacities or alternative notions of expressivity of NNs.\n3.1 Approximation of radial functions\nOne technique to study the impact of depth relies on the construction of speci\ufb01c functions which can be\nwell approximated by NNs of a certain depth, but require signi\ufb01cantly more parameters when approximated\nto the same accuracy by NNs of smaller depth. In the following we present one example for this type of\napproach, which can be found in [ES16].\nTheorem 3.1 (Power of depth). Let \u03f1\u2208{\u03f1R,\u03f1\u03c3,1(0,\u221e)}be the ReLU, the logistic, or the Heaviside function.\nThen there exist constants c,C \u2208(0,\u221e) with the following property: For every d\u2208N with d\u2265C there exist a\nprobability measure \u00b5 on Rd, a three-layer NN architecture a= (N,\u03f1) = ((d,N1,N2,1),\u03f1) with \u2225N\u2225\u221e\u2264Cd5,\nand corresponding parameters \u03b8\u2217\u2208RP(N) with \u2225\u03b8\u2217\u2225\u221e\u2264CdC and \u2225\u03a6a(\u00b7,\u03b8\u2217)\u2225L\u221e(Rd) \u22642 such that for every\nn\u2264cecd it holds that\ninf\n\u03b8\u2208RP((d,n,1))\n\u2225\u03a6((d,n,1),\u03f1)(\u00b7,\u03b8) \u2212\u03a6a(\u00b7,\u03b8\u2217)\u2225L2(\u00b5) \u2265c.\n29", "mimetype": "text/plain", "start_char_idx": 2295, "end_char_idx": 3335, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "92fc61de-7d9b-4c1a-b466-5f1a88e9e56b": {"__data__": {"id_": "92fc61de-7d9b-4c1a-b466-5f1a88e9e56b", "embedding": null, "metadata": {"page_label": "30", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "afe91798-85ed-4db9-89bd-1d14beba03b8", "node_type": "4", "metadata": {"page_label": "30", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "e45048117b5d60ba344f3860878cd42bfaeb807d67049ff6cc31b5e52c75e047", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fe8354f0-2492-439e-baac-b334181bb272", "node_type": "1", "metadata": {}, "hash": "933d5d4e37887f3be9374fb1e2c1ca7a2e81173368bc038cc91329cdab419dd7", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In fact, the activation function in Theorem 3.1 is only required to satisfy mild conditions and the result\nholds, for instance, also for more general sigmoidal functions. The proof of Theorem 3.1 is based on the\nconstruction of a suitable radial function g: Rd \u2192R, i.e., g(x) = \u02dcg(\u2225x\u22252\n2) for some \u02dcg: [0,\u221e) \u2192R, which\ncan be e\ufb03ciently approximated by three-layer NNs but approximation by only a two-layer NN requires\nexponentially large complexity, i.e., the width being exponential in d.\nThe \ufb01rst observation of [ ES16] is that g can typically be well approximated on a bounded domain by\na three-layer NN, if \u02dcg is Lipschitz continuous. Indeed, for the ReLU activation function it is not di\ufb03cult\nto show that, emulating a linear interpolation, one can approximate a univariate C-Lipschitz function\nuniformly on [0,1] up to precision \u03b5by a two-layer architecture of width O(C/\u03b5). The same holds for smooth,\nnon-polynomial activation functions due to Theorem 1.16. This implies that the squared Euclidean norm,\nas a sum of d univariate functions, i.e., [0,1]d \u220bx\u21a6\u2192\u2211d\ni=1 x2\ni, can be approximated up to precision \u03b5 by a\ntwo-layer architecture of width O(d2/\u03b5). Moreover, this shows that the third layer can e\ufb03ciently approximate\n\u02dcg, establishing approximation of g on a bounded domain up to precision \u03b5 using a three-layer architecture\nwith number of parameters polynomial in d/\u03b5.\nThe second step in [ES16] is to choose g in such a way that the realization of any two-layer neural network\n\u03a6 = \u03a6((d,n,1),\u03f1)(\u00b7,\u03b8) with width n not being exponential in d is on average (with respect to the probability\nmeasure \u00b5) a constant distance away from g. Their argument is heavily based on ideas from Fourier analysis\nand will be outlined below. In this context, let us recall that we denote by \u02c6f the Fourier transform of a\nsuitable function or, more generally, tempered distribution f.\nAssuming that the square-root \u03d5 of the density function associated with the probability measure \u00b5 as\nwell as \u03a6 and g are well-behaved, the Plancherel theorem yields that\n\u2225\u03a6 \u2212g\u22252\nL2(\u00b5) = \u2225\u03a6\u03d5\u2212g\u03d5\u22252\nL2(Rd) =\n\ued79\ued79\u02c6\u03a6\u03d5\u2212\u02c6g\u03d5\n\ued79\ued792\nL2(Rd). (3.1)\nNext, the speci\ufb01c structure of two-layer NNs is used, which implies that for every j \u2208[n] there exists\nwj \u2208Rd with \u2225wj\u22252 = 1 and \u03f1j: R \u2192R (subsuming the activation function \u03f1, the norm of wj, and the\nremaining parameters corresponding to the j-th neuron in the hidden layer) such that \u03a6 is of the form\nFigure 3.1: This illustration shows\nthe largest possible support (blue) of\n\u02c6\u03a6\u03d5, where \u02c6\u03d5 = 1Br(0) and \u03a6 is a\nshallow neural network with architec-\nture N = (2,4,1) and weight matrix\nW(1) = [w1 ...w 4]T in the \ufb01rst layer.\nAny radial function with enough of its\nL2-mass located at high frequencies\n(indicated by the red area) cannot be\nwell approximated by \u03a6\u03d5.\n\u03a6 =\nn\u2211\nj=1\n\u03f1j(\u27e8wj,\u00b7\u27e9) =\nn\u2211\nj=1\n(\u03f1j \u22971Rd\u22121) \u25e6Rwj.\nThe second equality follows by viewing the action of the j-th neuron\nas a tensor product of \u03f1j and the indicator function 1Rd\u22121(x) = 1,\nx \u2208Rd\u22121, composed with a d-dimensional rotation Rwj \u2208SO(d)\nwhich maps wj to the \ufb01rst standard basis vector e(1) \u2208Rd.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3068, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "fe8354f0-2492-439e-baac-b334181bb272": {"__data__": {"id_": "fe8354f0-2492-439e-baac-b334181bb272", "embedding": null, "metadata": {"page_label": "30", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "afe91798-85ed-4db9-89bd-1d14beba03b8", "node_type": "4", "metadata": {"page_label": "30", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "e45048117b5d60ba344f3860878cd42bfaeb807d67049ff6cc31b5e52c75e047", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "92fc61de-7d9b-4c1a-b466-5f1a88e9e56b", "node_type": "1", "metadata": {"page_label": "30", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "95f44c7f5b564afca01aaeccae55614258a36e1c93fd6ebb4d31f4a60bd8221f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Any radial function with enough of its\nL2-mass located at high frequencies\n(indicated by the red area) cannot be\nwell approximated by \u03a6\u03d5.\n\u03a6 =\nn\u2211\nj=1\n\u03f1j(\u27e8wj,\u00b7\u27e9) =\nn\u2211\nj=1\n(\u03f1j \u22971Rd\u22121) \u25e6Rwj.\nThe second equality follows by viewing the action of the j-th neuron\nas a tensor product of \u03f1j and the indicator function 1Rd\u22121(x) = 1,\nx \u2208Rd\u22121, composed with a d-dimensional rotation Rwj \u2208SO(d)\nwhich maps wj to the \ufb01rst standard basis vector e(1) \u2208Rd. Noting\nthat the Fourier transform respects linearity, rotations, and tensor\nproducts, we can compute\n\u02c6\u03a6 =\nn\u2211\nj=1\n(\u02c6\u03f1j \u2297\u03b4Rd\u22121) \u25e6Rwj,\nwhere \u03b4Rd\u22121 denotes the Dirac distribution onRd\u22121. In particular, the\nsupport of \u02c6\u03a6 has a particular star-like shape, namely \u22c3n\nj=1 span{wj},\nwhich are in fact lines passing through the origin.\nNow we choose \u03d5 to be the inverse Fourier transform of the\nindicator function of a ball Br(0) \u2282Rd with vol(Br(0)) = 1, ensuring\nthat \u03d52 is a valid probability density for \u00b5 as\n\u00b5(Rd) = \u2225\u03d52\u2225L1(Rd) = \u2225\u03d5\u22252\nL2(Rd) = \u2225\u02c6\u03d5\u22252\nL2(Rd) = \u22251Br(0)\u22252\nL2(Rd) = 1.\nUsing the convolution theorem, this choice of \u03d5 yields that\nsupp( \u02c6\u03a6\u03d5) = supp(\u02c6\u03a6 \u2217\u02c6\u03d5) \u2282\nn\u22c3\nj=1\n(span{wj}+ Br(0)) .\n30", "mimetype": "text/plain", "start_char_idx": 2629, "end_char_idx": 3760, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5a51e625-0d10-499b-8ef8-c9dfe3bd4d4b": {"__data__": {"id_": "5a51e625-0d10-499b-8ef8-c9dfe3bd4d4b", "embedding": null, "metadata": {"page_label": "31", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7fdcc2cd-5535-4b78-9fdc-fa6a5c90efce", "node_type": "4", "metadata": {"page_label": "31", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "ab482e9f1d017867c93bda3935a1cf72ef59158bbc5699e92c34a811c7148b34", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8f378dfb-fa87-48d0-a699-a4f8f3855126", "node_type": "1", "metadata": {}, "hash": "46ffc50fa849d17be88277f7286789f00b34b6b286596a25241b969e1e69ed6a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Thus the lines passing through the origin are enlarged to tubes. It is this particular shape which allows the\nconstruction of some gso that \u2225\u02c6\u03a6\u03d5\u2212\u02c6g\u03d5\u22252\nL2(Rd) can be suitably lower bounded, see also Figure 3.1. Intriguingly,\nthe peculiar behavior of high-dimensional sets now comes into play. Due to the well known concentration of\nmeasure principle, the variable n needs to be exponentially large for the set \u22c3n\nj=1 (span{wj}+ Br(0)) to be\nnot sparse. If it is smaller, one can construct a function g so that the main energy content of \u02c6g\u03d5 has a certain\ndistance from the origin, yielding a lower bound for \u2225\u02c6\u03a6\u03d5\u2212\u02c6g\u03d5\u22252 and hence \u2225\u03a6 \u2212g\u22252\nL2(\u00b5), see (3.1). One key\ntechnical problem is the fact that such a behavior for \u02c6g does not immediately imply a similar behavior of \u02c6g\u03d5,\nrequiring a quite delicate construction of g.\n3.2 Deep ReLU networks\n0 1\n8\n2\n8\n3\n8\n4\n8\n5\n8\n6\n8\n7\n8\n1\n0\n1\n32\n2\n32\n3\n32\n4\n32\n5\n32\n6\n32\n7\n32\n8\n32 g\nI1\ng I1\nI2 I1\ng I2\nI3 I2\nFigure 3.2: Interpolation In of [0,1] \u220b\nx\u21a6\u2192g(x) := x\u2212x2 on 2n+ 1 equidis-\ntant points, which can be represented\nas a sum In = \u2211n\nk=1 Ik \u2212Ik\u22121 =\u2211n\nk=1\nhk\n22k of n sawtooth functions.\nEach sawtooth function hk = hk\u22121 \u25e6h\nin turn can be written as a k-fold\ncomposition of a hat function h. This\nillustration is based on [EPGB19].\nMaybe for no activation function is the e\ufb00ect of depth clearer than\nfor the ReLU activation function \u03f1R(x) = max{0,x}. We refer to\ncorresponding NN architectures ( N,\u03f1R) as ReLU (neural) networks\n(ReLU NNs). A two-layer ReLU NN with one-dimensional input and\noutput is a function of the form\n\u03a6(x) =\nn\u2211\ni=1\nw(2)\ni \u03f1R(w(1)\ni x+ b(1)\ni ) + b(2), x \u2208R,\nwhere w(1)\ni ,w(2)\ni ,b(1)\ni ,b(2) \u2208R for i\u2208[n]. It is not hard to see that \u03a6\nis a continuous piecewise a\ufb03ne linear function. Moreover, \u03a6 has at\nmost n+ 1 a\ufb03ne linear pieces. On the other hand, notice that the\nhat function\nh: [0,1] \u2192[0,1],\nx\u21a6\u21922\u03f1R(x) \u22124\u03f1R(x\u22121\n2 ) =\n{\n2x, if 0 \u2264x< 1\n2 ,\n2(1 \u2212x), if 1\n2 \u2264x\u22641,\nis a NN with two layers and two neurons. Telgarsky observed that the\nn-fold convolution hn(x) := h\u25e6\u00b7\u00b7\u00b7\u25e6 h produces a sawtooth function\nwith 2n spikes [Tel15]. In particular, hn admits 2n a\ufb03ne linear pieces\nwith only 2nmany neurons. In this case, we see that deep ReLU NNs\nare in some sense exponentially more e\ufb03cient in generating a\ufb03ne\nlinear pieces.\nMoreover, it was noted in [ Yar17] that the di\ufb00erence of interpolations of [0 ,1] \u220bx\u21a6\u2192x\u2212x2 at 2n + 1\nand 2n\u22121 + 1 equidistant points equals the scaled sawtooth function hn\n22n, see Figure 3.2. This allows to\ne\ufb03ciently implement approximative squaring and, by polarization, also approximative multiplication using\nReLU NNs.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2600, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8f378dfb-fa87-48d0-a699-a4f8f3855126": {"__data__": {"id_": "8f378dfb-fa87-48d0-a699-a4f8f3855126", "embedding": null, "metadata": {"page_label": "31", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7fdcc2cd-5535-4b78-9fdc-fa6a5c90efce", "node_type": "4", "metadata": {"page_label": "31", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "ab482e9f1d017867c93bda3935a1cf72ef59158bbc5699e92c34a811c7148b34", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5a51e625-0d10-499b-8ef8-c9dfe3bd4d4b", "node_type": "1", "metadata": {"page_label": "31", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "d31c06390768635d87c12518e198f3e4074e525216279aaee3243e97f34c763f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In particular, hn admits 2n a\ufb03ne linear pieces\nwith only 2nmany neurons. In this case, we see that deep ReLU NNs\nare in some sense exponentially more e\ufb03cient in generating a\ufb03ne\nlinear pieces.\nMoreover, it was noted in [ Yar17] that the di\ufb00erence of interpolations of [0 ,1] \u220bx\u21a6\u2192x\u2212x2 at 2n + 1\nand 2n\u22121 + 1 equidistant points equals the scaled sawtooth function hn\n22n, see Figure 3.2. This allows to\ne\ufb03ciently implement approximative squaring and, by polarization, also approximative multiplication using\nReLU NNs. Composing these simple functions one can approximate localized Taylor polynomials and thus\nsmooth functions, see [Yar17]. We state below a generalization [GKP20] of the result of [Yar17] which includes\nmore general norms, but for p= \u221eand s= 0 coincides with the original result of Dmitry Yarotsky.\nTheorem 3.2 (Approximation of Sobolev-regular functions). Let d,k \u2208N with k\u22652, let p\u2208[1,\u221e], s\u2208[0,1],\nB \u2208(0,\u221e), and let \u03f1 be a piecewise linear activation function with at least one break-point. Then there\nexists a constant c\u2208(0,\u221e) with the following property: For every \u03b5\u2208(0,1/2) there exists a NN architecture\na= (N,\u03f1) with\nP(N) \u2264c\u03b5\u2212d/(k\u2212s) log(1/\u03b5)\nsuch that for every function g\u2208Wk,p((0,1)d) with \u2225g\u2225Wk,p((0,1)d) \u2264B it holds that\ninf\n\u03b8\u2208RP(N)\n\u2225\u03a6a(\u03b8,\u00b7) \u2212g\u2225Ws,p((0,1)d) \u2264\u03b5.\n31", "mimetype": "text/plain", "start_char_idx": 2086, "end_char_idx": 3375, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "33a24653-6f19-4a2b-94a3-8c6f20b75f22": {"__data__": {"id_": "33a24653-6f19-4a2b-94a3-8c6f20b75f22", "embedding": null, "metadata": {"page_label": "32", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a13ee79e-1f7c-4c64-b3ac-fe5ba9b1b7dc", "node_type": "4", "metadata": {"page_label": "32", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "93659235244012e31648f944420dc94f4c2237a650ce2f154c5eadfb748f93a2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "24833518-abeb-4715-961b-a36fd0f89d82", "node_type": "1", "metadata": {}, "hash": "9b8ce5b6b1125cfb78bd4597e1fda97b83b22d8a9475bcb6462134dd03ab497f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The ability of deep ReLU neural networks to emulate multiplication has also been employed to reap-\nproximate wide ranges of high-order \ufb01nite element spaces. In [ OPS20] and [ MOPS20] it was shown that\ndeep ReLU neural networks are capable of achieving the approximation rates of hp-\ufb01nite element methods.\nConcretely, this means that for piecewise analytic functions, which appear, for example, as solutions of elliptic\nboundary and eigenvalue problems with analytic data, exponential approximation rates can be achieved. In\nother words, the number of parameters of neural networks to approximate such a function in the W1,2-norm\nup to an error of \u03b5 is logarithmic in \u03b5.\nTheorem 3.2 requires the depth of the NN to grow. In fact, it can be shown that the same approximation\nrate cannot be achieved with shallow NNs. Indeed, there exists a certain optimal number of layers and, if\nthe architecture has fewer layers than optimal, then the NNs need to have signi\ufb01cantly more parameters, to\nachieve the same approximation \ufb01delity. This has been observed in many di\ufb00erent settings in [ LS17, SS17,\nYar17, PV18, EPGB19]. We state here the result of [Yar17]:\nTheorem 3.3 (Depth-width approximation trade-o\ufb00) . Let d,L \u2208N with L\u22652 and let g\u2208C2([0,1]d) be a\nfunction which is not a\ufb03ne linear. Then there exists a constant c\u2208(0,\u221e) with the following property: For\nevery \u03b5\u2208(0,1) and every ReLU NN architecture a= (N,\u03f1R) = ((d,N1,...,N L\u22121,1),\u03f1R) with L layers and\n\u2225N\u22251 \u2264c\u03b5\u22121/(2(L\u22121)) neurons it holds that\ninf\n\u03b8\u2208RP(N)\n\u2225\u03a6a(\u00b7,\u03b8) \u2212g\u2225L\u221e([0,1]d) \u2265\u03b5.\ndepth\nwidth\nFigure 3.3: Standard feed-forward neural network. For\ncertain approximation results, depth and width need\nto be in a \ufb01xed relationship to achieve optimal results.\nThis results is based on the observation that\nReLU NNs are piecewise a\ufb03ne linear. The number\nof pieces they admit is linked to their capacity of\napproximating functions that have non-vanishing\ncurvature. Using a construction similar to the ex-\nample at the beginning of this subsection, it can be\nshown that the number of pieces that can be gener-\nated using an architecture ((1,N1,...,N L\u22121,1),\u03f1R)\nscales roughly like \u220fL\u22121\n\u2113=1 N\u2113.\nIn the framework of the aforementioned results,\nwe can speak of a depth-width trade-o\ufb00, see also Fig-\nure 3.3. A \ufb01ne-grained estimate of achievable rates\nfor freely varying depths has also been established\nin [She20].\n3.3 Alternative notions of expressivity\nConceptual approaches to study the approximation power of deep NNs besides the classical approximation\nframework usually aim to relate structural properties of the NN to the \u201crichness\u201d of the set of possibly\nexpressed functions. One early result in this direction is [ MPCB14] which describes bounds on the number of\na\ufb03ne linear regions of a ReLU NN \u03a6(N,\u03f1R)(\u00b7,\u03b8). In a simpli\ufb01ed setting, we have seen estimates on the number\nof a\ufb03ne linear pieces already at the beginning of Subsection 3.2. A\ufb03ne linear regions can be de\ufb01ned as the\nconnected components of RN0 \\H, where H is the set of non-di\ufb00erentiability of the realization 20 \u03a6(N,\u03f1R)(\u00b7,\u03b8).\nA re\ufb01ned analysis on the number of such regions was, for example, conducted by [ HvdG19]. It is found that\ndeep ReLU neural networks can exhibit signi\ufb01cantly more regions than their shallow counterparts.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3244, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "24833518-abeb-4715-961b-a36fd0f89d82": {"__data__": {"id_": "24833518-abeb-4715-961b-a36fd0f89d82", "embedding": null, "metadata": {"page_label": "32", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a13ee79e-1f7c-4c64-b3ac-fe5ba9b1b7dc", "node_type": "4", "metadata": {"page_label": "32", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "93659235244012e31648f944420dc94f4c2237a650ce2f154c5eadfb748f93a2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "33a24653-6f19-4a2b-94a3-8c6f20b75f22", "node_type": "1", "metadata": {"page_label": "32", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "183f81ef05b15d09a14b354f456d8d65a386bdff3df9053a332fed3378b462e9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "One early result in this direction is [ MPCB14] which describes bounds on the number of\na\ufb03ne linear regions of a ReLU NN \u03a6(N,\u03f1R)(\u00b7,\u03b8). In a simpli\ufb01ed setting, we have seen estimates on the number\nof a\ufb03ne linear pieces already at the beginning of Subsection 3.2. A\ufb03ne linear regions can be de\ufb01ned as the\nconnected components of RN0 \\H, where H is the set of non-di\ufb00erentiability of the realization 20 \u03a6(N,\u03f1R)(\u00b7,\u03b8).\nA re\ufb01ned analysis on the number of such regions was, for example, conducted by [ HvdG19]. It is found that\ndeep ReLU neural networks can exhibit signi\ufb01cantly more regions than their shallow counterparts.\n20One can also study the potentially larger set of activation regionsgiven by the connected components of RN0 \\\n(\n\u222aL\u22121\n\u2113=1\n\u222aN\u2113\ni=1Hi,\u2113\n)\n, where\nHi,\u2113 := {x\u2208RN0 : \u03a6(\u2113)\ni (x,\u03b8) = 0},\nwith \u03a6(\u2113)\ni as in (1.1), is the set of non-di\ufb00erentiability of the activation of the i-th neuron in the \u2113-th layer. In contrast to the\nlinear regions, the activation regions are necessarily convex [RPK +17, HR19].\n32", "mimetype": "text/plain", "start_char_idx": 2627, "end_char_idx": 3642, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "23f1f6d1-c23d-4dbb-8384-29c71b880ae9": {"__data__": {"id_": "23f1f6d1-c23d-4dbb-8384-29c71b880ae9", "embedding": null, "metadata": {"page_label": "33", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c9e28ad3-7513-4b4e-a179-18849ad39c35", "node_type": "4", "metadata": {"page_label": "33", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "17507228a2fed61aaf071c570984091bc918b0f502f75407046c213ce26a7789", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Figure 3.4: Shape of the trajectory t \u21a6\u2192\u03a6((2,n,...,n,2),\u03f1R)(\u03b3(t),\u03b8) of the output of a randomly initialized\nnetwork with 0,3,10 hidden layers. The input curve \u03b3 is the circle given in the leftmost image. The hidden\nlayers have n= 20 neurons and the variance of the initialization is taken as 4 /n.\nThe reason for this e\ufb00ectiveness of depth is described by the following analogy: Through the ReLU each\nneuron Rd \u220bx\u21a6\u2192\u03f1R(\u27e8x,w\u27e9+ b), w\u2208Rd, b\u2208R, splits the space into two a\ufb03ne linear regions separated by\nthe hyperplane\n{x\u2208Rd: \u27e8x,w\u27e9+ b= 0}.\nA shallow ReLU NN \u03a6 ((d,n,1),\u03f1R)(\u00b7,\u03b8) with n neurons in the hidden layer therefore produces a number of\nregions de\ufb01ned through n hyperplanes. Using classical bounds on the number of regions de\ufb01ned through\nhyperplane arrangements [Zas75], one can bound the number of a\ufb03ne linear regions by \u2211d\nj=0\n(n\nj\n)\n. Deepening\nneural networks then corresponds to a certain folding of the input space. Through this interpretation it can\nbe seen that composing NNs can lead to a multiplication of the number of regions of the individual NNs\nresulting in an exponential e\ufb03ciency of deep neural networks in generating a\ufb03ne linear regions 21.\nThis approach was further developed in [RPK+17] to a framework to study expressivity that to some extent\nallows to include the training phase. One central object studied in [ RPK+17] are so-called trajectory lengths.\nIn this context, one analyzes how the length of a non-constant curve in the input space changes in expectation\nthrough the layers of a NN. The authors \ufb01nd an exponential dependence of the expected curve length on the\ndepth. Let us motivate this in the special case of a ReLU NN with architecture a= ((N0,n,...,n,N L),\u03f1R)\nand depth L\u2208N.\nGiven a non-constant continuous curve \u03b3: [0,1] \u2192RN0 in the input space, the length of the trajectory in\nthe \u2113-th layer of the NN \u03a6 a(\u00b7,\u03b8) is then given by\nLength(\u00af\u03a6(\u2113)(\u03b3(\u00b7),\u03b8)), \u2113 \u2208[L\u22121],\nwhere \u00af\u03a6(\u2113)(\u00b7,\u03b8) is the activation in the \u2113-th layer, see (1.1). Here the length of the curve is well-de\ufb01ned since\n\u00af\u03a6(\u2113)(\u00b7,\u03b8)) is continuous and therefore \u00af\u03a6(\u2113)(\u03b3(\u00b7),\u03b8) is continuous. Now, let the parameters \u0398 1 of the NN \u03a6 a\nbe initialized independently so that the entries corresponding to the weight matrices and bias vectors follow\na normal distribution with zero mean and variances 1 /n and 1, respectively. It is not hard to see, e.g., by\nProposition 1.1, that the probability that \u00af\u03a6(\u2113)(\u00b7,\u03981) will map \u03b3 to a non-constant curve is positive and hence,\nfor \ufb01xed \u2113\u2208[L\u22121],\nE\n[\nLength(\u00af\u03a6(\u2113)(\u03b3(\u00b7),\u03981))\n]\n= c> 0.\nLet \u03c3 \u2208 (0,\u221e) and consider a second initialization \u0398 \u03c3, where we change the variances of the entries\ncorresponding to the weight matrices and bias vectors to \u03c32/n and \u03c32, respectively. Recall that the ReLU is\npositively homogeneous, i.e., we have that \u03f1R(\u03bbx) = \u03bb\u03f1R(x) for all \u03bb\u2208(0,\u221e). Then it is clear that\n\u00af\u03a6(\u2113)(\u00b7,\u0398\u03c3) \u223c\u03c3\u2113\u00af\u03a6(\u2113)(\u00b7,\u03981),\n21However, to exploit this e\ufb03ciency with respect to the depth, one requires highly oscillating pre-activations which in turn can\nonly be achieved with a delicate selection of parameters. In fact, it can be shown that through random initialization the expected\nnumber of activation regions per unit cube depends mainly on the number of neurons in the NN, rather than its depth [ HR19].\n33", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3237, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1578b625-b2f4-4741-844f-0084d2afd9c5": {"__data__": {"id_": "1578b625-b2f4-4741-844f-0084d2afd9c5", "embedding": null, "metadata": {"page_label": "34", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "03c6cd1f-be62-42b7-b509-9bbdaabc3213", "node_type": "4", "metadata": {"page_label": "34", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "2ffb910af0961c1263b4399bd4035595daf514a119fda0eb3d7a23f79ebce39e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "i.e., the activations corresponding to the two initialization strategies are identically distributed up to the\nfactor \u03c3\u2113. Therefore, we immediately conclude that\nE\n[\nLength(\u00af\u03a6(\u2113)(\u03b3(\u00b7),\u0398\u03c3))\n]\n= \u03c3\u2113c.\nThis shows that the expected trajectory length depends exponentially on the depth of the NN, which is in\nline with the behavior of other notions of expressivity [ PLR+16]. In [ RPK+17] this result is also extended to\nthe tanh activation function and the constant cis more carefully resolved. Empirically one also \ufb01nds that the\nshapes of the trajectories become more complex in addition to becoming longer on average, see Figure 3.4.\n4 Deep neural networks overcome the curse of dimensionality\nM\nFigure 4.1: Illustration of a one-\ndimensional manifold Membedded\nin R3. For every point x\u2208M there\nexists a neighborhood in which the\nmanifold can be linearly projected\nonto its tangent space at x such that\nthe corresponding inverse function is\ndi\ufb00erentiable.\nIn Subsection 1.3, one of the main puzzles of deep learning that we\nidenti\ufb01ed was the surprising performance of deep architectures on\nproblems where the input dimensions are very high. This perfor-\nmance cannot be explained in the framework of classical approx-\nimation theory, since such results always su\ufb00er from the curse of\ndimensionality [Bel52, DeV98, NW09].\nIn this section, we present three approaches that o\ufb00er explana-\ntions of this phenomenon. As before, we had to omit certain ideas\nwhich have been very in\ufb02uential in the literature to keep the length\nof this section under control. In particular, an important line of\nreasoning is that functions to be approximated often have composi-\ntional structures which NNs may approximate very well as reviewed\nin [PMR+17]. Note that also a suitable feature descriptor, factor-\ning out invariances, might lead to a signi\ufb01cantly reduced e\ufb00ective\ndimension, see Subsection 7.1.\n4.1 Manifold assumption\nA \ufb01rst remedy to the high-dimensional curse of dimensionality is\nwhat we call the manifold assumption. Here it is assumed that we\nare trying to approximate a function\ng: Rd \u2283X\u2192 R,\nwhere d is very large. However, we are not seeking to optimize with respect to the uniform norm or a regular\nLp space, but instead consider a measure \u00b5 which is supported on a d\u2032-dimensional manifold M\u2282X . Then\nthe error is measured in the Lp(\u00b5)-norm. Here we consider the case where d\u2032\u226ad. This setting is appropriate\nif the data z= (x,y) of a prediction task is generated from a measure supported on M\u00d7R.\nThis set-up or generalizations thereof have been fundamental in [ CM18, SCC18, CJLZ19, SH19, CK20,\nNI20]. Let us describe an exemplary approach, where we consider locally Ck-regular functions and NNs with\nReLU activation functions below:\n1. Describe the regularity of g on the manifold: Naturally, we need to quantify the regularity of the\nfunction g restricted to Min an adequate way. The typical approach would be to make a de\ufb01nition\nvia local coordinate charts. If we assume that Mis an embedded submanifold of X, then locally,\ni.e., in a neighborhood of a point x \u2208M, the orthogonal projection of Monto the d\u2032-dimensional\ntangent space TxMis a di\ufb00eomorphism. The situation is depicted in Figure 4.1. Assuming Mto\nbe compact, we can choose a \ufb01nite set of open balls ( Ui)p\ni=1 that cover Mand on which the local\nprojections \u03b3i onto the respective tangent spaces as described above exists and are di\ufb00eomorphisms.\nNow we can de\ufb01ne the regularity of g via classical regularity. In this example, we say that g\u2208Ck(M)\nif g\u25e6\u03b3\u22121\ni \u2208Ck(\u03b3i(M\u2229Ui)) for all i\u2208[p].\n34", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3549, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "10b8718a-0adb-493c-8cc0-fde9567f811b": {"__data__": {"id_": "10b8718a-0adb-493c-8cc0-fde9567f811b", "embedding": null, "metadata": {"page_label": "35", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7d6a84cb-5208-424b-a403-25580b000ae7", "node_type": "4", "metadata": {"page_label": "35", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "9a85a00898217f441aff82af0a418e74403a1db1375508bf0371d4e5e53c4454", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d95f851d-6c05-4a99-95d7-a1a3ac1c369b", "node_type": "1", "metadata": {}, "hash": "98d2a0a33a976a9d30e2aa1d765d2db5d2bea236deae4ab905526456d41113c2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "2. Construct localization and charts via neural networks: According to the construction of local coordinate\ncharts in Step 1, we can write g as follows:\ng(x) =\np\u2211\ni=1\n\u03c6i(x)\n(\ng\u25e6\u03b3\u22121\ni (\u03b3i(x))\n)\n=:\np\u2211\ni=1\n\u02dcgi(\u03b3i(x),\u03c6i(x)), x \u2208M, (4.1)\nwhere \u03c6i is a partition of unity such that supp(\u03c6i) \u2282 Ui. Note that \u03b3i is a linear map, hence\nrepresentable by a one-layer NN. Since multiplication is a smooth operation, we have that if g\u2208Ck(M)\nthen \u02dcgi \u2208Ck(\u03b3i(M\u2229Ui) \u00d7[0,1]).\nThe partition of unity \u03c6i needs to be emulated by NNs. For example, if the activation function is the\nReLU, then such a partition can be e\ufb03ciently constructed. Indeed, in [ HLXZ20] it was shown that\nsuch NNs can represent linear \ufb01nite elements exactly with \ufb01xed-size NNs and hence a partition of unity\nsubordinate to any given covering of Mcan be constructed.\n3. Use a classical approximation result on the localized functions: By some form of Whitney\u2019s extension\ntheorem [Whi34], we can extend each \u02dcgi to a function \u00afgi \u2208Ck(X\u00d7[0,1]) which by classical results can be\napproximated up to an error of \u03b5> 0 by NNs of size O(\u03b5\u2212(d\u2032+1)/k) for \u03b5\u21920, see [Mha96, Yar17, SCC18].\n4. Use the compositionality of neural networks to build the \ufb01nal network: We have seen that every\ncomponent in the representation (4.1), i.e., \u02dcgi, \u03b3i, and \u03c6i can be e\ufb03ciently represented by NNs. In\naddition, composition and summation are operations which can directly be implemented by NNs through\nincreasing their depth and widening their layers. Hence (4.1) is e\ufb03ciently\u2014i.e., with a rate depending\nonly on d\u2032instead of the potentially much larger d\u2014approximated by a NN.\nOverall, we see that NNs are capable of learning local coordinate transformations and therefore reduce\nthe complexity of a high-dimensional problem to the underlying low-dimensional problem given by the data\ndistribution.\n4.2 Random sampling\nAlready in 1992, Andrew Barron showed that under certain seemingly very natural assumptions on the\nfunction to approximate, a dimension-independent approximation rate by NNs can be achieved [Bar92, Bar93].\nSpeci\ufb01cally, the assumption is formulated as a condition on the Fourier transform of a function and the result\nis as follows.\nTheorem 4.1 (Approximation of Barron-regular functions) . Let \u03f1: R \u2192R be the ReLU or a sigmoidal\nfunction. Then there exists a constant c \u2208(0,\u221e) with the following property: For every d,n \u2208N, every\nprobability measure \u00b5 supported on B1(0) \u2282Rd, and every g\u2208L1(Rd) with Cg :=\n\u222b\nRd \u2225\u03be\u22252|\u02c6g(\u03be)|d\u03be <\u221eit\nholds that\ninf\n\u03b8\u2208RP((d,n,1))\n\u2225\u03a6((d,n,1),\u03f1)(\u00b7,\u03b8) \u2212g\u2225L2(\u00b5) \u2264 c\u221anCg,\nNote that the L2-approximation error can be replaced by an L\u221e-estimate over the unit ball at the expense\nof a factor of the order of\n\u221a\nd on the right-hand side.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2702, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d95f851d-6c05-4a99-95d7-a1a3ac1c369b": {"__data__": {"id_": "d95f851d-6c05-4a99-95d7-a1a3ac1c369b", "embedding": null, "metadata": {"page_label": "35", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7d6a84cb-5208-424b-a403-25580b000ae7", "node_type": "4", "metadata": {"page_label": "35", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "9a85a00898217f441aff82af0a418e74403a1db1375508bf0371d4e5e53c4454", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "10b8718a-0adb-493c-8cc0-fde9567f811b", "node_type": "1", "metadata": {"page_label": "35", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "c9649d9323b84357ea830e0b815c382e32c1b5f71ffa3d565ffcc94a6f906781", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Let \u03f1: R \u2192R be the ReLU or a sigmoidal\nfunction. Then there exists a constant c \u2208(0,\u221e) with the following property: For every d,n \u2208N, every\nprobability measure \u00b5 supported on B1(0) \u2282Rd, and every g\u2208L1(Rd) with Cg :=\n\u222b\nRd \u2225\u03be\u22252|\u02c6g(\u03be)|d\u03be <\u221eit\nholds that\ninf\n\u03b8\u2208RP((d,n,1))\n\u2225\u03a6((d,n,1),\u03f1)(\u00b7,\u03b8) \u2212g\u2225L2(\u00b5) \u2264 c\u221anCg,\nNote that the L2-approximation error can be replaced by an L\u221e-estimate over the unit ball at the expense\nof a factor of the order of\n\u221a\nd on the right-hand side.\nThe key idea behind Theorem 4.1 is the following application of the law of large numbers: First, we\nobserve that, per assumption, g can be represented via the inverse Fourier transform, as\ng\u2212g(0) =\n\u222b\nRd\n\u02c6g(\u03be)(e2\u03c0i\u27e8\u00b7,\u03be\u27e9\u22121) d\u03be\n= Cg\n\u222b\nRd\n1\n\u2225\u03be\u22252\n(e2\u03c0i\u27e8\u00b7,\u03be\u27e9\u22121) 1\nCg\n\u2225\u03be\u22252\u02c6g(\u03be) d\u03be\n= Cg\n\u222b\nRd\n1\n\u2225\u03be\u22252\n(e2\u03c0i\u27e8\u00b7,\u03be\u27e9\u22121) d\u00b5g(\u03be),\n(4.2)\n35", "mimetype": "text/plain", "start_char_idx": 2236, "end_char_idx": 3024, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c30ca5b7-c03a-420b-a587-04e1f1329118": {"__data__": {"id_": "c30ca5b7-c03a-420b-a587-04e1f1329118", "embedding": null, "metadata": {"page_label": "36", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c90c7d5a-067a-4c5d-9cdc-9f8129fd09cc", "node_type": "4", "metadata": {"page_label": "36", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "e6eb5abf7bff466cdad3c4e796ad9bc9c8defd8520a9dfee5db825be0ab6dc27", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "533721ed-8bee-42be-aac0-68b084173b91", "node_type": "1", "metadata": {}, "hash": "605ef3d366aecbecc302009acd794754957f6304ba7cbdda7226ee2c9c5d79ae", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "where \u00b5g is a probability measure. Then it is further shown in [ Bar92] that there exist ( Rd \u00d7R)-valued\nrandom variables (\u039e,\u02dc\u039e) such that (4.2) can be written as\ng(x) \u2212g(0) = Cg\n\u222b\nRd\n1\n\u2225\u03be\u22252\n(e2\u03c0i\u27e8x,\u03be\u27e9\u22121) d\u00b5g(\u03be) = CgE\n[\n\u0393(\u039e,\u02dc\u039e)(x)\n]\n, x \u2208Rd, (4.3)\nwhere for every \u03be\u2208Rd, \u02dc\u03be\u2208R the function \u0393(\u03be,\u02dc\u03be): Rd \u2192R is given by\n\u0393(\u03be,\u02dc\u03be) := s(\u03be,\u02dc\u03be)(1(0,\u221e)(\u2212\u27e8\u03be/\u2225\u03be\u22252,\u00b7\u27e9\u2212 \u02dc\u03be) \u22121(0,\u221e)(\u27e8\u03be/\u2225\u03be\u22252,\u00b7\u27e9\u2212 \u02dc\u03be)) with s(\u03be,\u02dc\u03be) \u2208{\u22121,1}.\nNow, let ((\u039e(i),\u02dc\u039e(i)))i\u2208N be i.i.d. random variables with (\u039e (1),\u02dc\u039e(1)) \u223c(\u039e,\u02dc\u039e). Then, Bienaym\u00b4 e\u2019s identity and\nFubini\u2019s theorem establish that\nE\n[\ued79\ued79\ued79g\u2212g(0) \u2212Cg\nn\nn\u2211\ni=1\n\u0393(\u039e(i),\u02dc\u039e(i))\n\ued79\ued79\ued79\n2\nL2(\u00b5)\n]\n=\n\u222b\nB1(0)\nV\n[\nCg\nn\nn\u2211\ni=1\n\u0393(\u039e(i),\u02dc\u039e(i))(x)\n]\nd\u00b5(x)\n=\nC2\ng\n\u222b\nB1(0) V\n[\n\u0393(\u039e,\u02dc\u039e)(x)\n]\nd\u00b5(x)\nn \u2264(2\u03c0Cg)2\nn ,\n(4.4)\nwhere the last inequality follows from combining (4.3) with the fact that |e2\u03c0i\u27e8x,\u03be\u27e9\u22121|/\u2225\u03be\u22252 \u22642\u03c0, x\u2208B1(0).\nThis implies that there exists a realization (( \u03be(i),\u02dc\u03be(i)))i\u2208N of the random variables ((\u039e (i),\u02dc\u039e(i)))i\u2208N that\nachieves L2-approximation error of n\u22121/2. Therefore, it remains to show that NNs can well approximate\nthe functions ((\u0393(\u03be(i),\u02dc\u03be(i)))i\u2208N. Now it is not hard to see that the function 1(0,\u221e) and hence functions of\nthe form \u0393(\u03be,\u02dc\u03be), \u03be\u2208Rd, \u02dc\u03be\u2208R, can be arbitrarily well approximated with a \ufb01xed-size, two-layer NN with a\nsigmoidal or ReLU activation function. Thus, we obtain an approximation rate of n\u22121/2 when approximating\nfunctions with one \ufb01nite Fourier moment by two-layer NNs with n hidden neurons.\nIt was pointed out already in the dissertation of Emmanuel Cand` es [Can98] that the approximation rate\nof NNs for Barron-regular functions is also achievable by n-term approximation with complex exponentials, as\nis apparent by considering (4.2). However, for deeper NNs, the results also extend to high-dimensional non-\nsmooth functions, where Fourier-based methods are certain to su\ufb00er from the curse of dimensionality [CPV20].\nIn addition, the random sampling idea above was extended in [ EMW19b, EMWW20, EW20b, EW20c]\nto facilitate dimension-independent approximation of vastly more general function spaces. Basically, the\nidea is to use (4.3) as an inspiration and de\ufb01ne the generalized Barron space as all functions that may be\nrepresented as\nE\n[\n1(0,\u221e)(\u27e8\u039e,\u00b7\u27e9\u2212\u02dc\u039e)\n]\nfor any random variable (\u039e ,\u02dc\u039e).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2239, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "533721ed-8bee-42be-aac0-68b084173b91": {"__data__": {"id_": "533721ed-8bee-42be-aac0-68b084173b91", "embedding": null, "metadata": {"page_label": "36", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c90c7d5a-067a-4c5d-9cdc-9f8129fd09cc", "node_type": "4", "metadata": {"page_label": "36", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "e6eb5abf7bff466cdad3c4e796ad9bc9c8defd8520a9dfee5db825be0ab6dc27", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c30ca5b7-c03a-420b-a587-04e1f1329118", "node_type": "1", "metadata": {"page_label": "36", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "cc5eb9d8f388caa6eb2d18a439214fec0819cc983043e9ca9267016708bb14a8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "However, for deeper NNs, the results also extend to high-dimensional non-\nsmooth functions, where Fourier-based methods are certain to su\ufb00er from the curse of dimensionality [CPV20].\nIn addition, the random sampling idea above was extended in [ EMW19b, EMWW20, EW20b, EW20c]\nto facilitate dimension-independent approximation of vastly more general function spaces. Basically, the\nidea is to use (4.3) as an inspiration and de\ufb01ne the generalized Barron space as all functions that may be\nrepresented as\nE\n[\n1(0,\u221e)(\u27e8\u039e,\u00b7\u27e9\u2212\u02dc\u039e)\n]\nfor any random variable (\u039e ,\u02dc\u039e). In this context, deep and compositional versions of Barron spaces were\nintroduced and studied in [BK18, EMW19a, EW20a], which considerably extend the original theory.\n4.3 PDE assumption\nAnother structural assumption that leads to the absence of the curse of dimensionality in some cases is that\nthe function we are trying to approximate is given as the solution to a partial di\ufb00erential equation. It is by\nno means clear that this assumption leads to approximation without the curse of dimensionality, since most\nstandard methods, such as \ufb01nite elements, sparse grids, or spectral methods typically su\ufb00er from the curse of\ndimensionality.\nThis is not merely an abstract theoretical problem: Very recently, in [ AHNB+20] it was shown that two\ndi\ufb00erent gold standard methods for solving the multi-electron Schr\u00a8 odinger equation produce completely\ndi\ufb00erent interaction energy predictions when applied to large delocalized molecules. Classical numerical\nrepresentations are simply not expressive enough to accurately represent complicated high-dimensional\nstructures such as wave functions with long-range interactions.\nInterestingly, there exists an emerging body of work that shows that NNs do not su\ufb00er from these\nshortcomings and enjoy superior expressivity properties as compared to standard numerical representations.\n36", "mimetype": "text/plain", "start_char_idx": 1682, "end_char_idx": 3563, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e64259bf-9974-4ad0-bcbd-de8b5ddcbf5a": {"__data__": {"id_": "e64259bf-9974-4ad0-bcbd-de8b5ddcbf5a", "embedding": null, "metadata": {"page_label": "37", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "82e61a9d-2eef-49e6-9f5d-41562c995589", "node_type": "4", "metadata": {"page_label": "37", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "3dbd5fd12d7a4a87dfaa15bf87bc02738a1a1666ce46490331fccd01732198b7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "282ccc27-83b2-4f25-9139-03d36952d0b6", "node_type": "1", "metadata": {}, "hash": "ecf6cb1f3f10a44a08219a880c26d62159560450d4c24bd0e671256bd15ca62c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Such results include, for example, [GHJVW20, GS20, HJKN20] for (linear and semilinear) parabolic evolution\nequations, [GH22] for stationary elliptic PDEs, [ GH21] for nonlinear Hamilton\u2013Jacobi\u2013Bellman equations,\nor [KPRS19] for parametric PDEs. In all these cases, the absence of the curse of dimensionality in terms of\nthe theoretical approximation power of NNs could be rigorously established.\nOne way to prove such results is via stochastic representations of the PDE solutions, as well as associated\nsampling methods. We illustrate the idea for the simple case of linear Kolmogorov PDEs, that is the problem\nof representing the function g: Rd \u00d7[0,\u221e) \u2192R satisfying22\n\u2202g\n\u2202t(x,t) = 1\n2Tr\n(\n\u03c3(x,t)[\u03c3(x,t)]\u2217\u22072\nxg(x,t)\n)\n+ \u27e8\u00b5(x,t),\u2207xg(x,t)\u27e9, g (x,0) = \u03d5(x), (4.5)\nwhere the functions\n\u03d5: Rd \u2192R (initial condition) and \u03c3: Rd \u2192Rd\u00d7d, \u00b5 : Rd \u2192Rd (coe\ufb03cient functions)\nare continuous and satisfy suitable growth conditions. A stochastic representation of g is given via the Ito\nprocesses (Sx,t)t\u22650 satisfying\ndSx,t = \u00b5(Sx,t)dt+ \u03c3(Sx,t)dBt, Sx,0 = x, (4.6)\nwhere (Bt)t\u22650 is a d-dimensional Brownian motion. Then g is described via the Feynman\u2013Kac formula which\nstates that\ng(x,t) = E [\u03d5(Sx,t)], x \u2208Rd, t\u2208[0,\u221e). (4.7)\nRoughly speaking, a NN approximation result can be proven by \ufb01rst approximating, via the law of large\nnumbers,\ng(x,t) = E [\u03d5(Sx,t)] \u22481\nn\nn\u2211\ni=1\n\u03d5(S(i)\nx,t), (4.8)\nwhere ( S(i)\nx,t)n\ni=1 are i.i.d. random variables with S(1)\nx,t \u223c Sx,t. Care has to be taken to establish such\nan approximation uniformly in the computational domain , for example, for every ( x,t) in the unit cube\n[0,1]d \u00d7[0,1], see (4.4) for a similar estimate and [ GHJVW20, GS20] for two general approaches to ensure\nthis property. Aside from this issue, (4.8) represents a standard Monte Carlo estimator which can be shown\nto be free of the curse of dimensionality.\nAs a next step, one needs to establish that realizations of the processes ( x,t) \u21a6\u2192Sx,t can be e\ufb03ciently\napproximated by NNs. This can be achieved by emulating a suitable time-stepping scheme for the SDE (4.6)\nby NNs which, roughly speaking, can be done without incurring the curse of dimensionality whenever the\ncoe\ufb03cient functions \u00b5,\u03c3 can be approximated by NNs without incurring the curse of dimensionality and some\ngrowth conditions hold true. In a last step one assumes that the initial condition \u03d5 can be approximated by\nNNs without incurring the curse of dimensionality which, by the compositionality of NNs and the previous\nstep, directly implies that realizations of the processes ( x,t) \u21a6\u2192\u03d5(Sx,t) can be approximated by NNs without\nincurring the curse of dimensionality. By (4.8) this implies a corresponding approximation result for the\nsolution of the Kolmogorov PDE g in (4.5).\nInformally, we have discovered a regularity result for linear Kolmogorov equations, namely that (modulo\nsome technical conditions on \u00b5,\u03c3), the solution g of (4.5) can be approximated by NNs without incurring the\ncurse of dimensionality whenever the same holds true for the initial condition \u03d5, as well as the coe\ufb03cient\nfunctions \u00b5,\u03c3. In other words, the property of being approximable by NNs without curse of dimensionality is\npreserved under the \ufb02ow induced by the PDE (4.5).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3210, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "282ccc27-83b2-4f25-9139-03d36952d0b6": {"__data__": {"id_": "282ccc27-83b2-4f25-9139-03d36952d0b6", "embedding": null, "metadata": {"page_label": "37", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "82e61a9d-2eef-49e6-9f5d-41562c995589", "node_type": "4", "metadata": {"page_label": "37", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "3dbd5fd12d7a4a87dfaa15bf87bc02738a1a1666ce46490331fccd01732198b7", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e64259bf-9974-4ad0-bcbd-de8b5ddcbf5a", "node_type": "1", "metadata": {"page_label": "37", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "b35f5a4a69207bb3e6108115443afc1a6b115ae1005a9368517b49594f0d10df", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "By (4.8) this implies a corresponding approximation result for the\nsolution of the Kolmogorov PDE g in (4.5).\nInformally, we have discovered a regularity result for linear Kolmogorov equations, namely that (modulo\nsome technical conditions on \u00b5,\u03c3), the solution g of (4.5) can be approximated by NNs without incurring the\ncurse of dimensionality whenever the same holds true for the initial condition \u03d5, as well as the coe\ufb03cient\nfunctions \u00b5,\u03c3. In other words, the property of being approximable by NNs without curse of dimensionality is\npreserved under the \ufb02ow induced by the PDE (4.5). Some comments are in order:\n22The natural solution concept to this type of PDEs is the viscosity solution concept, a thorough study of which can be found\nin [HHJ15].\n37", "mimetype": "text/plain", "start_char_idx": 2624, "end_char_idx": 3379, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "6b86910e-1423-470a-92f7-fae69e9edc93": {"__data__": {"id_": "6b86910e-1423-470a-92f7-fae69e9edc93", "embedding": null, "metadata": {"page_label": "38", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6391cb5d-b101-4644-908b-40bdfd0e913f", "node_type": "4", "metadata": {"page_label": "38", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "4080fb71f0dc5c0100a5005df48873247678c7cf4dd495af9b6dd14325e07fa4", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Assumption on the initial condition: One may wonder if the assumption that the initial condition\n\u03d5 can be approximated by NNs without incurring the curse of dimensionality is justi\ufb01ed. This is at least\nthe case in many applications in computational \ufb01nance where the function \u03d5 typically represents an option\npricing formula and (4.5) represents the famous Black\u2013Scholes model. It turns out that nearly all common\noption pricing formulas are constructed from iterative applications of linear maps and maximum/minimum\nfunctions\u2014in other words, in many applications in computational \ufb01nance, the initial condition \u03d5 can be\nexactly represented by a small ReLU NN.\n101 102\ninput dimension\n107\n108\n109\n1010\n1011\nx cx2.36\n#parameters  avg. #steps\n\u00b1 2 std.\nFigure 4.2: Computational complexity as number of\nneural network parameters times number of SGD steps\nto solve heat equations of varying dimensions up to\na speci\ufb01ed precision. According to the \ufb01t above, the\nscaling is polynomial in the dimension [BDG20].\nGeneralization and optimization error: The\nFeynman\u2013Kac representation (4.7) directly implies\nthat g(\u00b7,t) can be computed as the Bayes opti-\nmal function of a regression task with input fea-\ntures X \u223cU([0,1]d) and labels Y = \u03d5(SX,t), which\nallows for an analysis of the generalization error\nas well as implementations based on ERM algo-\nrithms [BGJ20, BBG+21].\nWhile it is in principle possible to analyze the\napproximation and generalization error, the analysis\nof the computational cost and/or convergence of\ncorresponding SGD algorithms is completely open.\nSome promising numerical results exist, see, for\ninstance, Figure 4.2, but the stable training of NNs\napproximating PDEs to very high accuracy (that\nis needed in several applications such as quantum\nchemistry) remains very challenging. The recent\nwork [GV21] has even proven several impossibility\nresults in that direction.\nExtensions and abstract idea: Similar techniques may be used to prove expressivity results for nonlinear\nPDEs, for example, using nonlinear Feynman\u2013Kac-type representations of [ PP92] in place of (4.7) and\nmultilevel Picard sampling algorithms of [EHJK19] in place of (4.8).\nWe can also formulate the underlying idea in an abstract setting (a version of which has also been used in\nSubsection 4.2). Assume that a high-dimensional function g: Rd \u2192R admits a probabilistic representation of\nthe form\ng(x) = E [Yx], x \u2208Rd, (4.9)\nfor some random variable Yx which can be approximated by an iterative scheme\nY(L)\nx \u2248Yx and Y(\u2113)\nx = T\u2113(Y(\u2113\u22121)\nx ), \u2113 = 1,...,L,\nwith dimension-independent convergence rate. If we can approximate realizations of the initial mapping\nx \u21a6\u2192Y0\nx and the maps T\u2113, \u2113 \u2208[L], by NNs and the numerical scheme is stable enough, then we can also\napproximate Y(L)\nx using compositionality. Emulating a uniform Monte-Carlo approximator of (4.9) then\nleads to approximation results for g without curse of dimensionality. In addition, one can choose a Rd-valued\nrandom variable X as input features and de\ufb01ne the corresponding labels by YX to obtain a prediction task,\nwhich can be solved by means of ERM.\nOther methods: There exist a number of additional works related to the approximation capacities of\nNNs for high-dimensional PDEs, for example, [ EGJS18, LTY19, SZ19]. In most of these works, the proof\ntechnique consists of emulating an existing method that does not su\ufb00er from the curse of dimensionality. For\ninstance, in the case of \ufb01rst-order transport equations, one can show in some cases that NNs are capable of\nemulating the method of characteristics, which then also yields approximation results that are free of the\ncurse of dimensionality [LP21].\n38", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3659, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "edd7f8aa-bb1f-40db-90ca-263652935f4e": {"__data__": {"id_": "edd7f8aa-bb1f-40db-90ca-263652935f4e", "embedding": null, "metadata": {"page_label": "39", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2f22b994-8dbb-4d88-8352-5a9fdb868e06", "node_type": "4", "metadata": {"page_label": "39", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "178bcb255ba4d31d983fa5a9caf6e6b4cbf3eb7154ebfa3b7ee53e09f3e5d891", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "5 Optimization of deep neural networks\nWe recall from Subsections 1.3 and 1.2.1 that the standard algorithm to solve the empirical risk minimization\nproblem over the hypothesis set of NNs is stochastic gradient descent. This method would be guaranteed\nto converge to a global minimum of the objective if the empirical risk were convex, viewed as a function of\nthe NN parameters. However, this function is severely nonconvex, may exhibit (higher-order) saddle points,\nseriously suboptimal local minima, and wide \ufb02at areas where the gradient is very small.\nOn the other hand, in applications, excellent performance of SGD is observed. This indicates that the\ntrajectory of the optimization routine somehow misses suboptimal critical points and other areas that may\nlead to slow convergence. Clearly, the classical theory does not explain this performance. Below we describe\nsome exemplary novel approaches that give partial explanations of this success.\nIn the \ufb02avor of this article, the aim of this section is to present some selected ideas rather than giving an\noverview of the literature. To give at least some detail about the underlying ideas and to keep the length of\nthis section reasonable, a selection of results had to be made and some ground-breaking results had to be\nomitted.\n5.1 Loss landscape analysis\nGiven a NN \u03a6( \u00b7,\u03b8) and training data s\u2208Zm the function \u03b8 \u21a6\u2192r(\u03b8) := \u02c6Rs(\u03a6(\u00b7,\u03b8)) describes, in a natural\nway, through its graph, a high-dimensional surface. This surface may have regions associated with lower\nvalues of \u02c6Rs which resemble valleys of a landscape if they are surrounded by regions of higher values. The\nanalysis of the topography of this surface is called loss landscape analysis. Below we shall discuss a couple of\napproaches that yield deep insights into the shape of this landscape.\nIndex\nLoss\nNo negative curvature\nat globally minimal\nrisk.\nCritical points\nwith high risk\nare unstable.\n0 0 .25 0 .5\nFigure 5.1: Sketch of the distribution\nof critical points of the Hamiltonian\nof a spin glass model.\nSpin glass interpretation: One of the \ufb01rst discoveries about the\nshape of the loss landscape comes from deep results in statistical\nphysics. The Hamiltonian of the spin glass model is a random function\non the ( n\u22121)-dimensional sphere of radius \u221an. Making certain\nsimplifying assumptions, it was shown in [ CHM+15] that the loss of\na NN with random inputs can be considered as the Hamiltonian of a\nspin glass model, where the inputs of the model are the parameters\nof the NN.\nThis connection has far-reaching implications for the loss land-\nscape of NNs because of the following surprising property of the\nHamiltonian of spin glass models: Consider the set of critical points\nof this set, and associate to each point an index that denotes the\npercentage of the eigenvalues of the Hessian at that point which are\nnegative. This index corresponds to the relative number of directions\nin which the loss landscape has negative curvature. Then with high\nprobability, a picture like we see in Figure 5.1 emerges [ AA\u02c7C13].\nMore precisely, the further away from the optimal loss we are, the more unstable the critical points become.\nConversely, if one \ufb01nds oneself in a local minimum, it is reasonable to assume that the loss is close to the\nglobal minimum.\nWhile some of the assumptions establishing the connection between the spin glass model and NNs are\nunrealistic in practice [ CLA15], the theoretical distribution of critical points as in Figure 5.1 is visible in\nmany practical applications [DPG+14].\nPaths and level sets: Another line of research is to understand the loss landscape by analyzing paths\nthrough the parameter space. In particular, the existence of paths in parameter space, such that the associated\nempirical risks are monotone along the path. Surely, should there exist a path of nonincreasing empirical risk\nfrom every point to the global minimum, then we can be certain that no non-global minima exist, since no\n39", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3963, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5bee5618-8f26-487c-9665-d71cfad9648a": {"__data__": {"id_": "5bee5618-8f26-487c-9665-d71cfad9648a", "embedding": null, "metadata": {"page_label": "40", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "fbaf96c8-70f8-4731-bbdb-85d321f0991b", "node_type": "4", "metadata": {"page_label": "40", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "e24d2bd38fd1496b975f5fa4237f458121e1228c39b37768e7a6b7a5bfefda9f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b1ba3042-b143-4869-9fb1-68dc374be716", "node_type": "1", "metadata": {}, "hash": "d8460a99bfeef1d343a93e034f916849c70ca2466174f33048c0e53a5411afb8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "such path can escape a minimum. An even stronger result holds. In fact, the existence of such paths shows\nthat the loss landscape has connected level sets [FB17, VBB19].\nA crucial ingredient of the analysis of such paths are linear substructures. Consider a biasless two-layer\nNN \u03a6 of the form\nRd \u220bx\u21a6\u2192\u03a6(x,\u03b8) :=\nn\u2211\nj=1\n\u03b8(2)\nj \u03f1\n(\n\u27e8\u03b8(1)\nj ,\n[x\n1\n]\n\u27e9\n)\n, (5.1)\nwhere \u03b8(1)\nj \u2208Rd+1 for j \u2208[n], \u03b8(2) \u2208Rn, \u03f1 is a Lipschitz continuous activation function, and we augment the\nvector x by a constant 1 in the last coordinate as outlined in Remark 1.5. If we consider \u03b8(1) to be \ufb01xed,\nthen it is clear that the space\n\u02dcF\u03b8(1) := {\u03a6(\u00b7,\u03b8): \u03b8= (\u03b8(1),\u03b8(2)), \u03b8(2) \u2208Rn}\nis a linear space. If the risk 23 is convex, as is the case for the widely used quadratic or logistic loss, then\nthis implies that \u03b8(2) \u21a6\u2192r\n(\n(\u03b8(1),\u03b8(2))\n)\nis a convex map and hence, for every parameter set P\u2282 Rn this map\nassumes its maximum on \u2202P. Therefore, within the vast parameter space, there are many paths traveling\nalong which does not increase the risk above the risk of the start and end points.\nThis idea was, for example, used in [ FB17] in a way similar to the following simple sketch: Assume\nthat, for two parameters \u03b8 and \u03b8min there exists a linear subspace of NNs \u02dcF\u02c6\u03b8(1) such that there are paths \u03b31\nand \u03b32 connecting \u03a6(\u00b7,\u03b8) and \u03a6(\u00b7,\u03b8min) to \u02dcF\u02c6\u03b8(1) respectively. Further assume that the paths are such that\nalong \u03b31 and \u03b32 the risk does not signi\ufb01cantly exceed max{r(\u03b8),r(\u03b8min)}. Figure 5.2 shows a visualization of\nthese paths. In this case, a path from \u03b8 to \u03b8min not signi\ufb01cantly exceeding r(\u03b8) along the way is found by\nconcatenating the paths \u03b31, a path along \u02dcF\u02c6\u03b8(1), and \u03b32. By the previous discussion, we know that only \u03b31\nand \u03b32 determine the extent to which the combined path exceeds r(\u03b8) along its way. Hence, we need to ask\nabout the existence of \u02dcF\u02c6\u03b8(1) that facilitates the construction of appropriate \u03b31 and \u03b32.\n\u03a6(\u00b7,\u03b8min)\n\u03a6(\u00b7,\u03b8)\n\u02dcF\u02c6\u03b8(1)\n\u03a6(\u00b7,\u03b8\u2217)\u03b31\nFigure 5.2: Construction of a path\nfrom an initial point \u03b8 to the global\nminimum \u03b8min that does not have sig-\nni\ufb01cantly higher risk than the initial\npoint along the way. We depict here\nthe landscape as a function of the\nneural network realizations instead\nof their parametrizations so that this\nlandscape is convex.\nTo understand why a good choice of \u02dcF\u02c6\u03b8(1), so that the risk along\n\u03b31 and \u03b32 will not rise much higher than r(\u03b8), is likely possible, we\nset24\n\u02c6\u03b8(1)\nj :=\n{\n\u03b8(1)\nj for j \u2208[n/2],\n(\u03b8(1)\nmin)j for j \u2208[n] \\[n/2].\nIn other words, the \ufb01rst half of \u02c6\u03b8(1) is made from \u03b8(1) and the\nsecond from \u03b8(1)\nmin.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2541, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b1ba3042-b143-4869-9fb1-68dc374be716": {"__data__": {"id_": "b1ba3042-b143-4869-9fb1-68dc374be716", "embedding": null, "metadata": {"page_label": "40", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "fbaf96c8-70f8-4731-bbdb-85d321f0991b", "node_type": "4", "metadata": {"page_label": "40", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "e24d2bd38fd1496b975f5fa4237f458121e1228c39b37768e7a6b7a5bfefda9f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5bee5618-8f26-487c-9665-d71cfad9648a", "node_type": "1", "metadata": {"page_label": "40", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "cce96964cea98c9921c83ca78609d743afb85cc69b02513c12684b1f00be3d8b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We depict here\nthe landscape as a function of the\nneural network realizations instead\nof their parametrizations so that this\nlandscape is convex.\nTo understand why a good choice of \u02dcF\u02c6\u03b8(1), so that the risk along\n\u03b31 and \u03b32 will not rise much higher than r(\u03b8), is likely possible, we\nset24\n\u02c6\u03b8(1)\nj :=\n{\n\u03b8(1)\nj for j \u2208[n/2],\n(\u03b8(1)\nmin)j for j \u2208[n] \\[n/2].\nIn other words, the \ufb01rst half of \u02c6\u03b8(1) is made from \u03b8(1) and the\nsecond from \u03b8(1)\nmin. If \u03b8(1)\nj , j \u2208[N], are realizations of random variables\ndistributed uniformly on the d-dimensional unit sphere, then by\ninvoking standard covering bounds of spheres (e.g., [ Ver18, Corollary\n4.2.13]), we expect that, for \u03b5 >0 and a su\ufb03ciently large number\nof neurons n, the vectors ( \u03b8(1)\nj )n/2\nj=1 already \u03b5-approximate all vectors\n(\u03b8(1)\nj )n\nj=1. Replacing all vectors ( \u03b8(1)\nj )n\nj=1 by their nearest neighbor in\n(\u03b8(1)\nj )n/2\nj=1 can be done with a linear path in the parameter space, and,\ngiven that r is locally Lipschitz continuous and \u2225\u03b8(2)\u22251 is bounded,\nthis operation will not increase the risk by more than O(\u03b5). We\ndenote the vector resulting from this replacement procedure by \u03b8(1)\n\u2217 .\nSince for all j \u2208[n] \\[n/2] we now have that\n\u03f1\n(\n\u27e8(\u03b8(1)\n\u2217 )j,\n[\n\u00b7\n1\n]\n\u27e9\n)\n\u2208\n{\n\u03f1\n(\n\u27e8(\u03b8(1)\n\u2217 )k,\n[\n\u00b7\n1\n]\n\u27e9\n)\n: k\u2208[n/2]\n}\n,\n23As most statements in this subsection are valid for the empirical risk r(\u03b8) = \u02c6Rs(\u03a6(\u00b7,\u03b8)) as well as the risk r(\u03b8) = R(\u03a6(\u00b7,\u03b8)),\ngiven a suitable distribution of Z, we will just call r the risk.\n24We assume w.l.o.g. that n is a multiple of 2.\n40", "mimetype": "text/plain", "start_char_idx": 2101, "end_char_idx": 3609, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b5b1f751-916b-4796-bfd3-3abf90bab973": {"__data__": {"id_": "b5b1f751-916b-4796-bfd3-3abf90bab973", "embedding": null, "metadata": {"page_label": "41", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e7a885ea-3f65-4915-83cf-564f18e50bc9", "node_type": "4", "metadata": {"page_label": "41", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "006bb082cc124a1352a6ce257fda2648d35cf4d94dc8c318a310fd52e9585e8b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "79a75624-b355-4998-8258-d20e87aa68be", "node_type": "1", "metadata": {}, "hash": "56bb3636fe22c6eb70e02126f4bd32b39835372bcd51dc328c4733ad07106ab0", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "there exists a vector \u03b8(2)\n\u2217 with (\u03b8(2)\n\u2217 )j = 0, j \u2208[n] \\[n/2], so that\n\u03a6(\u00b7,(\u03b8(1)\n\u2217 ,\u03b8(2))) = \u03a6(\u00b7,(\u03b8(1)\n\u2217 ,\u03bb\u03b8(2)\n\u2217 + (1 \u2212\u03bb)\u03b8(2))), \u03bb \u2208[0,1].\nIn particular, this path does not change the risk between ( \u03b8(1)\n\u2217 ,\u03b8(2)) and (\u03b8(1)\n\u2217 ,\u03b8(2)\n\u2217 ). Now, since ( \u03b8(2)\n\u2217 )j = 0\nfor j \u2208[n] \\[n/2], the realization \u03a6( \u00b7,(\u03b8(1)\n\u2217 ,\u03b8(2)\n\u2217 )) is computed by a sub-network consisting of the \ufb01rst n/2\nhidden neurons and we can replace the parameters corresponding to the other neurons without any e\ufb00ect on\nthe realization function. Speci\ufb01cally, it holds that\n\u03a6(\u00b7,(\u03b8(1)\n\u2217 ,\u03b8(2)\n\u2217 )) = \u03a6(\u00b7,(\u03bb\u02c6\u03b8(1) + (1 \u2212\u03bb)\u03b8(1)\n\u2217 ,\u03b8(2)\n\u2217 )), \u03bb \u2208[0,1],\nyielding a path of constant risk between ( \u03b8(1)\n\u2217 ,\u03b8(2)\n\u2217 ) and ( \u02c6\u03b8(1),\u03b8(2)\n\u2217 ). Connecting these paths completes the\nconstruction of \u03b31 and shows that the risk along \u03b31 does not exceed that at \u03b8 by more than O(\u03b5). Of course,\n\u03b32 can be constructed in the same way. The entire construction is depicted in Figure 5.2.\nOverall, this derivation shows that for su\ufb03ciently wide NNs (appropriately randomly initialized) it is very\nlikely possible to almost connect a random parameter value to the global minimum with a path which along\nthe way does not need to climb much higher than the initial risk.\nIn [VBB19], a similar approach is taken and the convexity in the last layer is used. However, the authors\ninvoke the concept of intrinsic dimension to elegantly solve the non-linearity of r((\u03b8(1),\u03b8(2))) with respect to\n\u03b8(1). Additionally, [SS16] constructs a path of decreasing risk from random initializations. The idea here is\nthat if one starts at a point of su\ufb03ciently high risk, one can always \ufb01nd a path to the global optimum with\nstrictly decreasing risk. The intriguing insight behind this result is that if the initialization is su\ufb03ciently\nbad, i.e., worse than that of a NN outputting only zero, then there exist two operations that in\ufb02uence the\nrisk directly. Multiplying the last layer with a number smaller than one will decrease the risk, whereas the\nopposite will increase it. Using this tuning mechanism, any given potentially non-monotone path from the\ninitialization to the global minimum can be modi\ufb01ed so that it is strictly monotonically decreasing. In a\nsimilar spirit, [NH17] shows that if a deep NN has a layer with more neurons than training data points, then\nunder certain assumptions the training data will typically be mapped to linearly independent points in that\nlayer. Of course, this layer could then be composed with a linear map that maps the linearly independent\npoints to any desirable output, in particular one that achieves vanishing empirical risk, see also Proposition 1.1.\nAs for two-layer NNs, the previous discussion on linear paths immediately shows that in this situation a\nmonotone path to the global minimum exists.\n5.2 Lazy training and provable convergence of stochastic gradient descent\nWhen training highly overparametrized NNs, one often observes that the parameters of the NNs barely\nchange during training. In Figure 5.3, we show the relative distance that the parameters travel through the\nparameter space during the training of NNs of varying numbers of neurons per layer.\nThe e\ufb00ect described above has been observed repeatedly and theoretically explained, see, e.g., [ DZPS18,\nLL18, AZLS19, DLL+19, ZCZG20]. In Subsection 2.1, we have already seen a high-level overview and, in\nparticular, the function space perspective of this phenomenon in the in\ufb01nite width limit.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3433, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "79a75624-b355-4998-8258-d20e87aa68be": {"__data__": {"id_": "79a75624-b355-4998-8258-d20e87aa68be", "embedding": null, "metadata": {"page_label": "41", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e7a885ea-3f65-4915-83cf-564f18e50bc9", "node_type": "4", "metadata": {"page_label": "41", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "006bb082cc124a1352a6ce257fda2648d35cf4d94dc8c318a310fd52e9585e8b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b5b1f751-916b-4796-bfd3-3abf90bab973", "node_type": "1", "metadata": {"page_label": "41", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "32efa4ea5306fac18c0572513800fb5fcd3971456e4f50cccf7d54a9d65c5388", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "As for two-layer NNs, the previous discussion on linear paths immediately shows that in this situation a\nmonotone path to the global minimum exists.\n5.2 Lazy training and provable convergence of stochastic gradient descent\nWhen training highly overparametrized NNs, one often observes that the parameters of the NNs barely\nchange during training. In Figure 5.3, we show the relative distance that the parameters travel through the\nparameter space during the training of NNs of varying numbers of neurons per layer.\nThe e\ufb00ect described above has been observed repeatedly and theoretically explained, see, e.g., [ DZPS18,\nLL18, AZLS19, DLL+19, ZCZG20]. In Subsection 2.1, we have already seen a high-level overview and, in\nparticular, the function space perspective of this phenomenon in the in\ufb01nite width limit. Below we present a\nshort and highly simpli\ufb01ed derivation of this e\ufb00ect and show how it leads to provable convergence of gradient\ndescent for su\ufb03ciently overparametrized deep NNs.\nA simple learning model: We consider again the simple NN model of (5.1) with a smooth activation\nfunction \u03f1which is not a\ufb03ne linear. For the quadratic loss and training data s= ((x(i),y(i)))m\ni=1 \u2208(Rd\u00d7R)m,\nwhere xi \u0338= xj for all i\u0338= j, the empirical risk is given by\nr(\u03b8) = \u02c6Rs(\u03b8) = 1\nm\nm\u2211\ni=1\n(\u03a6(x(i),\u03b8) \u2212y(i))2.\nLet us further assume that \u0398 (1)\nj \u223cN(0,1/n)d+1, j \u2208[n], and \u0398 (2)\nj \u223cN(0,1/n), j \u2208[n], are independent\nrandom variables.\n41", "mimetype": "text/plain", "start_char_idx": 2623, "end_char_idx": 4051, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "bad82dc9-395e-4c17-a51f-076ccb4582b2": {"__data__": {"id_": "bad82dc9-395e-4c17-a51f-076ccb4582b2", "embedding": null, "metadata": {"page_label": "42", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "733191a2-af86-45fa-b525-856ed65adfe5", "node_type": "4", "metadata": {"page_label": "42", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "0d9656bca86313a29a58e96aec961734a4d0f69626651a7db27c0df625a0c8dc", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Figure 5.3: Four networks with architecture ((1 ,n,n, 1),\u03f1R) and n\u2208{20,100,500,2500}neurons per hidden\nlayer were trained by gradient descent to \ufb01t four points that are shown in the middle \ufb01gure as black dots.\nWe depict on the left the relative Euclidean distance of the parameters from the initialization through the\ntraining process. In the middle, we show the \ufb01nal trained NNs. On the right we show the behavior of the\ntraining error.\nA peculiar kernel: Next, we would like to understand how the gradient \u2207\u03b8r(\u0398) looks like with high\nprobability over the initialization \u0398 = (\u0398(1),\u0398(2)). Similar to (2.3), we have by restricting the gradient to\n\u03b8(2) and applying the chain rule that\n\u2225\u2207\u03b8r(\u0398)\u22252\n2 \u2265 4\nm2\n\ued79\ued79\ued79\nm\u2211\ni=1\n\u2207\u03b8(2)\u03a6(x(i),\u0398)(\u03a6(x(i),\u0398) \u2212y(i))\n\ued79\ued79\ued79\n2\n2\n= 4\nm2\n(\n(\u03a6(x(i),\u0398) \u2212y(i))m\ni=1\n)T \u00afK\u0398(\u03a6(x(j),\u0398) \u2212y(j))m\nj=1,\n(5.2)\nwhere \u00afK\u0398 is a random Rm\u00d7m-valued kernel given by\n( \u00afK\u0398)i,j :=\n(\n\u2207\u03b8(2)\u03a6(x(i),\u0398)\n)T\n\u2207\u03b8(2)\u03a6(x(j),\u0398), i,j \u2208[m].\nThis kernel is closely related to the neural tangent kernel in (2.4) evaluated at the features ( x(i))m\ni=1 and the\nrandom initialization \u0398. It is a slightly simpli\ufb01ed version thereof, as in (2.4) the gradient is taken with respect\nto the full vector \u03b8. This can also be regarded as the kernel associated with a random features model [RR+07].\nNote that for our two-layer NN we have that\n(\n\u2207\u03b8(2)\u03a6(x,\u0398)\n)\nk = \u03f1\n(\u27e8\n\u0398(1)\nk ,\n[x\n1\n]\u27e9)\n, x \u2208Rd, k\u2208[n].\nThus, we can write \u00afK\u0398 as the following sum of (random) rank one matrices:\n\u00afK\u0398 =\nn\u2211\nk=1\nvkvT\nk with vk =\n(\n\u03f1\n(\u27e8\n\u0398(1)\nk ,\n[\nx(i)\n1\n]\u27e9))m\ni=1\n\u2208Rm, k \u2208[n]. (5.3)\nThe kernel \u00afK\u0398 are symmetric and positive semi-de\ufb01nite by construction. It is positive de\ufb01nite if it is\nnon-singular, i.e., if at least m of the n vectors vk, k\u2208[n], are linearly independent. Proposition 1.1 shows\nthat for n= m the probability of that event is not zero, say \u03b4, and is therefore at least 1 \u2212(1 \u2212\u03b4)\u230an/m\u230bfor\narbitrary n. In other words, the probability increases rapidly with n. It is also clear from (5.3) that E [ \u00afK\u0398]\nscales linearly with n.\nFrom this intuitive derivation, we conclude that for su\ufb03ciently large n, with high probability \u00afK\u0398 is\na positive de\ufb01nite kernel with smallest eigenvalue \u03bbmin( \u00afK\u0398) scaling linearly with n. The properties of\n\u00afK\u0398, in particular its positive de\ufb01niteness, have been studied much more rigorously as already described in\nSubsection 2.1.\n42", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2328, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "0130df9e-d4c0-47ad-9115-86c7fc1406c7": {"__data__": {"id_": "0130df9e-d4c0-47ad-9115-86c7fc1406c7", "embedding": null, "metadata": {"page_label": "43", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "dc0ebc3a-0a26-400d-bc38-e3f109a5fe0a", "node_type": "4", "metadata": {"page_label": "43", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "5c663a2998ce76c62d597fd5df8cb447449ad0ba8a768fcde63b006c0edff98f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "78aa6706-abfb-43a8-a4e1-d310f77cd127", "node_type": "1", "metadata": {}, "hash": "071db0b9b58bd888192ab024f944a189efa1f9d2170df76642d5d6eb41ee780a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Control of the gradient: Applying the expected behavior of the smallest eigenvalue \u03bbmin( \u00afK\u0398) of \u00afK\u0398\nto (5.2), we conclude that with high probability\n\u2225\u2207\u03b8r(\u0398)\u22252\n2 \u2265 4\nm2 \u03bbmin( \u00afK\u0398)\u2225(\u03a6(x(i),\u0398) \u2212y(i))m\ni=1\u22252\n2 \u2273 n\nmr(\u0398). (5.4)\nTo understand what will happen when applying gradient descent, we \ufb01rst need to understand how the\nsituation changes in a neighborhood of \u0398. We \ufb01x x\u2208Rd and observe that by the mean value theorem for all\n\u00af\u03b8\u2208B1(0) we have \ued79\ued79\u2207\u03b8\u03a6(x,\u0398) \u2212\u2207\u03b8\u03a6(x,\u0398 + \u00af\u03b8)\n\ued79\ued792\n2 \u2272 sup\n\u02c6\u03b8\u2208B1(0)\n\ued79\ued79\u22072\n\u03b8\u03a6(x,\u0398 + \u02c6\u03b8)\n\ued79\ued792\nop, (5.5)\nwhere \u2225\u22072\n\u03b8\u03a6(x,\u0398 + \u02c6\u03b8)\u2225op denotes the operator norm of the Hessian of \u03a6( x,\u00b7) at \u0398 + \u02c6\u03b8. From inspection\nof (5.1), it is not hard to see that for all i,j \u2208[n] and k,\u2113 \u2208[d+ 1]\nE\n[(\u22022\u03a6(x,\u0398)\n\u2202\u03b8(2)\ni \u2202\u03b8(2)\nj\n)2\n]\n= 0, E\n[( \u22022\u03a6(x,\u0398)\n\u2202\u03b8(2)\ni \u2202(\u03b8(1)\nj )k\n)2\n]\n\u2272 \u03b4i,j, and E\n[( \u22022\u03a6(x,\u0398)\n\u2202(\u03b8(1)\ni )k\u2202(\u03b8(1)\nj )\u2113\n)2\n]\n\u2272 \u03b4i,j\nn ,\nwhere \u03b4i,j = 0 if i \u0338= j and \u03b4i,i = 1 for all i,j \u2208[n]. For su\ufb03ciently large n, we have that \u22072\n\u03b8\u03a6(x,\u0398) is\nin expectation approximately a block band matrix with band-width d+ 1. Therefore, we conclude that\nE\n[\n\u2225\u22072\n\u03b8\u03a6(x,\u0398)\u22252\nop\n]\n\u2272 1. Hence, we obtain by concentration of Gaussian random variables that with high\nprobability \u2225\u22072\n\u03b8\u03a6(x,\u0398)\u22252\nop \u2272 1. By the block-banded form of \u22072\n\u03b8\u03a6(x,\u0398) we have that, even after perturbation\nof \u0398 by a vector \u02c6\u03b8 with norm bounded by 1, the term \u2225\u22072\n\u03b8\u03a6(x,\u0398 + \u02c6\u03b8)\u22252\nop is bounded, which yields that the\nright-hand side of (5.5) is bounded with high probability.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1430, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "78aa6706-abfb-43a8-a4e1-d310f77cd127": {"__data__": {"id_": "78aa6706-abfb-43a8-a4e1-d310f77cd127", "embedding": null, "metadata": {"page_label": "43", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "dc0ebc3a-0a26-400d-bc38-e3f109a5fe0a", "node_type": "4", "metadata": {"page_label": "43", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "5c663a2998ce76c62d597fd5df8cb447449ad0ba8a768fcde63b006c0edff98f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0130df9e-d4c0-47ad-9115-86c7fc1406c7", "node_type": "1", "metadata": {"page_label": "43", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "14449b1d2fa1cd455b6c3821d685f382eb3e6a69561b8fc8bcae6d8453c550e6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Therefore, we conclude that\nE\n[\n\u2225\u22072\n\u03b8\u03a6(x,\u0398)\u22252\nop\n]\n\u2272 1. Hence, we obtain by concentration of Gaussian random variables that with high\nprobability \u2225\u22072\n\u03b8\u03a6(x,\u0398)\u22252\nop \u2272 1. By the block-banded form of \u22072\n\u03b8\u03a6(x,\u0398) we have that, even after perturbation\nof \u0398 by a vector \u02c6\u03b8 with norm bounded by 1, the term \u2225\u22072\n\u03b8\u03a6(x,\u0398 + \u02c6\u03b8)\u22252\nop is bounded, which yields that the\nright-hand side of (5.5) is bounded with high probability.\nUsing (5.5), we can extend (5.4), which holds with high probability, to a neighborhood of \u0398 by the\nfollowing argument: Let \u00af\u03b8\u2208B1(0), then\n\u2225\u2207\u03b8r(\u0398 + \u00af\u03b8)\u22252\n2 \u2265 4\nm2\n\ued79\ued79\ued79\nm\u2211\ni=1\n\u2207\u03b8(2)\u03a6(x(i),\u0398 + \u00af\u03b8)(\u03a6(x(i),\u0398 + \u00af\u03b8) \u2212y(i))\n\ued79\ued79\ued79\n2\n2\n=\n(5.5)\n4\nm2\n\ued79\ued79\ued79\nm\u2211\ni=1\n(\u2207\u03b8(2)\u03a6(x(i),\u0398) + O(1))(\u03a6(x(i),\u0398 + \u00af\u03b8) \u2212y(i))\n\ued79\ued79\ued79\n2\n2\n\u2273\n(\u2217)\n1\nm2 (\u03bbmin( \u00afK\u0398) + O(1))\u2225(\u03a6(x(i),\u0398 + \u00af\u03b8) \u2212y(i))m\ni=1\u22252\n2\n\u2273 n\nmr(\u0398 + \u00af\u03b8),\n(5.6)\nwhere the estimate marked by ( \u2217) uses the positive de\ufb01niteness of \u00afK\u0398 again and only holds for su\ufb03ciently\nlarge n, so that the O(1) term is negligible.\nWe conclude that, with high probability over the initialization \u0398, on a ball of \ufb01xed radius around \u0398 the\nsquared Euclidean norm of the gradient of the empirical risk is lower bounded by n\nm times the empirical risk.\nExponential convergence of gradient descent: For su\ufb03ciently small step sizes \u03b7, the observation\nin the previous paragraph yields the following convergence rate for gradient descent as in Algorithm 1,\nspeci\ufb01cally (1.5), with m\u2032= m and \u0398(0) = \u0398: If \u2225\u0398(k) \u2212\u0398\u2225\u2264 1 for all k\u2208[K+ 1], then25\nr(\u0398(K+1)) \u2248r(\u0398(K)) \u2212\u03b7\u2225\u2207\u03b8r(\u0398(K))\u22252\n2 \u2264\n(\n1 \u2212c\u03b7n\nm\n)\nr(\u0398(K)) \u2272\n(\n1 \u2212c\u03b7n\nm\n)K\n, (5.7)\nfor c\u2208(0,\u221e) so that \u2225\u2207\u03b8r(\u0398(k))\u22252\n2 \u2265cn\nmr(\u0398(k)) for all k\u2208[K].\n25Note that the step-size \u03b7 needs to be small enough to facilitate the approximation step in (5.7). Hence, we cannot simply\nput \u03b7= m/(cn) in (5.7) and have convergence after one step.\n43", "mimetype": "text/plain", "start_char_idx": 1018, "end_char_idx": 2803, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5e64c2bd-4e5b-4f41-a281-1c8ceb4a1d82": {"__data__": {"id_": "5e64c2bd-4e5b-4f41-a281-1c8ceb4a1d82", "embedding": null, "metadata": {"page_label": "44", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7fc6007a-767c-4390-9164-62179c873c77", "node_type": "4", "metadata": {"page_label": "44", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "8d185582124c53e3f7ce6565ce5ad2797a33cfb5f493ff74d71556109b67051d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "30934b34-7e35-49b4-93da-175808ded43b", "node_type": "1", "metadata": {}, "hash": "e62e8395e9a2b83937ac45e9b2a329e14447460d94658ffc491b53939a3b1fd2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Let us assume without proof that the estimate (5.6) could be extended to an equivalence. In other words,\nwe assume that we additionally have that \u2225\u2207\u03b8r(\u0398 + \u00af\u03b8)\u22252\n2 \u2272 n\nmr(\u0398 + \u00af\u03b8). This, of course, could be shown with\nsimilar tools as were used for the lower bound. Then we have that \u2225\u0398(k) \u2212\u0398\u22252 \u22641 for all k\u2272\n\u221a\nm/(\u03b72n).\nSetting t=\n\u221a\nm/(\u03b72n) and using the limit de\ufb01nition of the exponential function, i.e., limt\u2192\u221e(1\u2212x/t)t = e\u2212x,\nyields for su\ufb03ciently small \u03b7 that (5.7) is bounded by e\u2212c\n\u221a\nn/m.\nWe conclude that, with high probability over the initialization, gradient descent converges with an\nexponential rate to an arbitrary small empirical risk if the width nis su\ufb03ciently large. In addition, the iterates\nof the descent algorithm even stay in a small \ufb01xed neighborhood of the initialization during training . Because\nthe parameters only move very little, this type of training has also been coined lazy training [COB19].\nSimilar ideas as above, have led to groundbreaking convergence results of SGD for overparametrized NNs\nin much more complex and general settings, see, e.g., [DZPS18, LL18, AZLS19].\nIn the in\ufb01nite width limit, NN training is practically equivalent to kernel regression, see Subsection 2.1. If\nwe look at Figure 5.3 we see that the most overparametrized NN interpolates the data like a kernel-based\ninterpolator would. In a sense, which was also highlighted in [COB19], this shows that, while overparametrized\nNNs in the lazy training regime have very nice properties, they essentially act like linear methods.\n6 Tangible e\ufb00ects of special architectures\nIn this section, we describe results that isolate the e\ufb00ects of certain aspects of NN architectures. As we have\ndiscussed in Subsection 1.3, typically only either the depth or the number of parameters are used to study\ntheoretical aspects of NNs. We have seen instances of this throughout Sections 3 and 4. Moreover, also in\nSection 5, we saw that wider NNs enjoy certain very favorable properties from an optimization point of view.\nBelow, we introduce certain specialized NN architectures. We start with one of the most widely used\ntypes of NNs, the convolutional neural network (CNN). In Subsection 6.2 we introduce skip connections and\nin Subsection 6.3 we discuss a speci\ufb01c class of CNNs equipped with an encoder-decoder structure that are\nfrequently used in image processing techniques. We introduce the batch normalization block in Subsection 6.4.\nThen, we discuss sparsely connected NNs that typically result as an extraction from fully connected NNs in\nSubsection 6.5. Finally, we brie\ufb02y comment on recurrent neural networks in Subsection 6.6.\nAs we have noted repeatedly throughout this manuscript, it is impossible to give a full account of the\nliterature in a short introductory article. In this section, this issue is especially severe since the number of\nspecial architectures studied in practice is enormous. Therefore, we had to omit many very in\ufb02uential and\nwidely used neural network architectures. Among those are graph neural networks, which handle data from non-\nEuclidean input spaces. We refer to the survey articles [ BBL+17, WPC+21] for a discussion. Another highly\nsuccessful type of architectures are (variational) autoencoders [AHS85, HZ94]. These are neural networks\nwith a bottleneck that enforce a more e\ufb03cient representation of the data. Similarly, generative adversarial\nnetworks [GPAM+14] which are composed of two neural networks, one generator and one discriminator,\ncould not be discussed here. Another widely used component of architectures used in practice is the so-called\ndropout layer. This layer functions through removing some neurons randomly during training. This procedure\nempirically prevents over\ufb01tting. An in-detail discussion of the mathematical analysis behind this e\ufb00ect is\nbeyond the scope of this manuscript.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3838, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "30934b34-7e35-49b4-93da-175808ded43b": {"__data__": {"id_": "30934b34-7e35-49b4-93da-175808ded43b", "embedding": null, "metadata": {"page_label": "44", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7fc6007a-767c-4390-9164-62179c873c77", "node_type": "4", "metadata": {"page_label": "44", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "8d185582124c53e3f7ce6565ce5ad2797a33cfb5f493ff74d71556109b67051d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5e64c2bd-4e5b-4f41-a281-1c8ceb4a1d82", "node_type": "1", "metadata": {"page_label": "44", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "f91e9111d3d35764f2d08ac6962762ed13de04758edd4d7c0466ff66da9e13b7", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Among those are graph neural networks, which handle data from non-\nEuclidean input spaces. We refer to the survey articles [ BBL+17, WPC+21] for a discussion. Another highly\nsuccessful type of architectures are (variational) autoencoders [AHS85, HZ94]. These are neural networks\nwith a bottleneck that enforce a more e\ufb03cient representation of the data. Similarly, generative adversarial\nnetworks [GPAM+14] which are composed of two neural networks, one generator and one discriminator,\ncould not be discussed here. Another widely used component of architectures used in practice is the so-called\ndropout layer. This layer functions through removing some neurons randomly during training. This procedure\nempirically prevents over\ufb01tting. An in-detail discussion of the mathematical analysis behind this e\ufb00ect is\nbeyond the scope of this manuscript. We refer to [ WZZ+13, SHK+14, HV17, MAV18] instead. Finally, the\nvery successful attention mechanism [BCB15, VSP+17], that is the basis of transformer neural networks, had\nto be omitted.\nBefore we start describing certain e\ufb00ects of special NN architectures, a word of warning is required. The\nspecial building blocks, which will be presented below, have been developed based on a speci\ufb01c need in\napplications and are used and combined in a very \ufb02exible way. To describe these tools theoretically without\ncompletely in\ufb02ating the notational load, some simplifying assumptions need to be made. It is very likely that\nthe simpli\ufb01ed building blocks do not accurately re\ufb02ect the practical applications of these tools in all use cases.\n44", "mimetype": "text/plain", "start_char_idx": 2992, "end_char_idx": 4570, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "41feeed9-be6d-4556-a1b2-f8a307fa74fd": {"__data__": {"id_": "41feeed9-be6d-4556-a1b2-f8a307fa74fd", "embedding": null, "metadata": {"page_label": "45", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c185ac8c-06be-416b-9fdc-19087684730e", "node_type": "4", "metadata": {"page_label": "45", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "97ec145b276410ee4946fffd15bcd9827ab9530faa41cb472ca03c1e8fe69b4b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5edf799d-1457-484b-956e-d93d663b066e", "node_type": "1", "metadata": {}, "hash": "498d14bf61cf9ef67b05c23deb5bb2d01b5e8cbe420cd4a8a6c4e7884907c18d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "6.1 Convolutional neural networks\nEspecially for very high-dimensional inputs where the input dimensions are spatially related, fully connected\nNNs seem to require unnecessarily many parameters. For example, in image classi\ufb01cation problems, neigh-\nboring pixels very often share information and the spatial proximity should be re\ufb02ected in the architecture.\nBased on this observation, it appears reasonable to have NNs that have local receptive \ufb01elds in the sense\nthat they collect information jointly from spatially close inputs. In addition, in image processing, we are not\nnecessarily interested in a universal hypothesis set. A good classi\ufb01er is invariant under many operations, such\nas translation or rotation of images. It seems reasonable to hard-code such invariances into the architecture.\nThese two principles suggest that the receptive \ufb01eld of a NN should be the same on di\ufb00erent translated\npatches of the input. In this sense, parameters of the architecture can be reused. Together, these arguments\nmake up the three fundamental principles of convolutional NNs: local receptive \ufb01elds, parameter sharing,\nand equivariant representations, as introduced in [ LBD+89]. We will provide a mathematical formulation of\nconvolutional NNs below and then revisit these concepts.\nA convolutional NN corresponds to multiple convolutional blocks, which are special types of layers. For\na group G, which typically is either [ d] \u223c= Z/(dZ) or [d]2 \u223c= (Z/(dZ))2 for d\u2208N, depending on whether we\nare performing one-dimensional or two-dimensional convolutions, the convolution of two vectors a,b \u2208RG is\nde\ufb01ned as\n(a\u2217b)i =\n\u2211\nj\u2208G\najbj\u22121i, i \u2208G.\nNow we can de\ufb01ne a convolutional block as follows: Let \u02dcG be a subgroup of G, let p: G\u2192\u02dcG be a so-called\npooling-operator, and let C \u2208N denote the number of channels. Then, for a series of kernels \u03bai \u2208RG, i\u2208[C],\nthe output of a convolutional block is given by\nRG \u220bx\u21a6\u2192x\u2032:= (p(x\u2217\u03bai))C\ni=1 \u2208(R\n\u02dcG)C. (6.1)\nA typical example of a pooling operator is for G= (Z/(2dZ))2 and \u02dcG= (Z/(dZ))2 the 2 \u00d72 subsampling\noperator\np: RG \u2192R\n\u02dcG, x \u21a6\u2192(x2i\u22121,2j\u22121)d\ni,j=1.\nPopular alternatives are average pooling or max pooling. These operations then either pass the average or the\nmaximum over patches of similar size. The convolutional kernels correspond to the aforementioned receptive\n\ufb01elds. They can be thought of as local if they have small supports, i.e., few nonzero entries.\nAs explained earlier, a convolutional NN is built by stacking multiple convolutional blocks after another 26.\nAt some point, the output can be \ufb02attened, i.e., mapped to a vector and is then fed into a FC NN (see\nDe\ufb01nition 1.4). We depict this setup in Figure 6.1.\nOwing to the fact that convolution is a linear operation, depending on the pooling operation, one may\nwrite a convolutional block (6.1) as a FC NN. For example, if G= (Z/(2dZ))2 and the 2 \u00d72 subsampling\npooling operator is used, then the convolutional block could be written as x \u21a6\u2192Wx for a block circulant\nmatrix W \u2208R(Cd2)\u00d7(2d)2\n. Since we require W to have a special structure, we can interpret a convolutional\nblock as a special, restricted feed-forward architecture.\nAfter these considerations, it is natural to ask how the restriction of a NN to a pure convolutional\nstructure, i.e., consisting only of convolutional blocks, will a\ufb00ect the resulting hypothesis set. The \ufb01rst\nnatural question is whether the set of such NNs is still universal in the sense of Theorem 1.15. The answer to\nthis question depends strongly on the type of pooling and convolution that is allowed. If the convolution is\nperformed with padding, then the answer is yes [ OS19, Zho20b].", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3624, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5edf799d-1457-484b-956e-d93d663b066e": {"__data__": {"id_": "5edf799d-1457-484b-956e-d93d663b066e", "embedding": null, "metadata": {"page_label": "45", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c185ac8c-06be-416b-9fdc-19087684730e", "node_type": "4", "metadata": {"page_label": "45", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "97ec145b276410ee4946fffd15bcd9827ab9530faa41cb472ca03c1e8fe69b4b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "41feeed9-be6d-4556-a1b2-f8a307fa74fd", "node_type": "1", "metadata": {"page_label": "45", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "6599a8bd674e7e6235cd906095bac9960a3b024c8072e6f5fd086ce8562efb87", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Since we require W to have a special structure, we can interpret a convolutional\nblock as a special, restricted feed-forward architecture.\nAfter these considerations, it is natural to ask how the restriction of a NN to a pure convolutional\nstructure, i.e., consisting only of convolutional blocks, will a\ufb00ect the resulting hypothesis set. The \ufb01rst\nnatural question is whether the set of such NNs is still universal in the sense of Theorem 1.15. The answer to\nthis question depends strongly on the type of pooling and convolution that is allowed. If the convolution is\nperformed with padding, then the answer is yes [ OS19, Zho20b]. On the other hand, for circular convolutions\nand without pooling, universality does not hold, but the set of translation equivariant functions can be\nuniversally approximated [Yar18b, PV20]. Furthermore, [ Yar18b] illuminates the e\ufb00ect of subsample pooling\nby showing that, if no pooling is applied, then universality cannot be achieved, whereas if pooling is applied\n26We assume that the de\ufb01nition of a convolutional block is suitably extended to input data in the Cartesian product ( RG)C.\nFor instance, one can take an a\ufb03ne linear combination of C mappings as in (6.1) acting on each coordinate. Moreover, one may\nalso interject an activation function between the blocks.\n45", "mimetype": "text/plain", "start_char_idx": 2993, "end_char_idx": 4302, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "15a937e0-2550-4b73-8e0c-1b64a2499e0c": {"__data__": {"id_": "15a937e0-2550-4b73-8e0c-1b64a2499e0c", "embedding": null, "metadata": {"page_label": "46", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "76ea6bf7-ce7f-42d0-b254-73e1c37fea3d", "node_type": "4", "metadata": {"page_label": "46", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "dd0c9d2e435e13b54ccad455ae40bb1baa988555d5449e24f2eb8636eb127c59", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Convolution Pooling Convolution Pooling Fully connected NN\nFigure 6.1: Illustration of a convolutional neural network with two-dimensional convolutional blocks and\n2 \u00d72 subsampling as pooling operation.\nthen universality is possible. The e\ufb00ect of subsampling in CNNs from the viewpoint of approximation theory\nis further discussed in [ Zho20a]. The role of other types of pooling in enhancing invariances of the hypothesis\nset will be discussed in Subsection 7.1 below.\n6.2 Residual neural networks\nLet us \ufb01rst illustrate a potential obstacle when training deep NNs. Consider for L\u2208N the product operation\nRL \u220bx\u21a6\u2192\u03c0(x) =\nL\u220f\n\u2113=1\nx\u2113.\nIt is clear that\n\u2202\n\u2202xk\n\u03c0(x) =\nL\u220f\n\u2113\u0338=k\nx\u2113, x \u2208RL.\nTherefore, for su\ufb03ciently large L, we expect that\n\u23d0\u23d0\u2202\u03c0\n\u2202xk\n\u23d0\u23d0will be exponentially small, if |x\u2113|< \u03bb <1 for all\n\u2113\u2208[L] or exponentially large, if |x\u2113|>\u03bb> 1 for all \u2113\u2208[L]. The output of a general NN, considered as a\ndirected graph, is found by repeatedly multiplying the input with parameters in every layer along the paths\nthat lead from the input to the output neuron. Due to the aforementioned phenomenon, it is often observed\nthat training NNs su\ufb00ers from either the exploding or the vanishing gradient problem, which may prevent\nlower layers from training at all. The presence of an activation function is likely to exacerbate this e\ufb00ect. The\nexploding or vanishing gradient problem seems to be a serious obstacle towards e\ufb03cient training of deep NNs.\nIn addition to the vanishing and exploding gradient problems, there is an empirically observed degradation\nproblem [HZRS16]. This phrase describes the phenomenon that FC NNs seem to achieve lower accuracy on\nboth the training and test data when increasing their depth.\nFrom an approximation theoretic perspective, deep NNs should always be superior to shallow NNs. The\nreason for this is that NNs with two layers can either exactly represent the identity map or approximate it\narbitrarily well. Concretely, for the ReLU activation function \u03f1R we have that x= \u03f1R(x+ b) \u2212b for x\u2208Rd\nwith xi >\u2212bi, where b\u2208Rd. In addition, for any activation function \u03f1which is continuously di\ufb00erentiable on\na neighborhood of some point \u03bb\u2208R with \u03f1\u2032(\u03bb) \u0338= 0 one can approximate the identity arbitrary well, see (1.8).\nBecause of this, extending a NN architecture by one layer can only enlarge the associated hypothesis set.\nTherefore, one may expect that the degradation problem is more associated with the optimization aspect\nof learning. This problem is addressed by a small change to the architecture of a feed-forward NN in [HZRS16].\n46", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2553, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1560830c-1c3c-4fa2-bc50-f6ff4ed67562": {"__data__": {"id_": "1560830c-1c3c-4fa2-bc50-f6ff4ed67562", "embedding": null, "metadata": {"page_label": "47", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3ad6f4b3-100a-44df-b1f8-d40424f52a0f", "node_type": "4", "metadata": {"page_label": "47", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "5f0c3c3e7467d4073b1aa9f29f5a4686d15c0202b14d96865b860438a6ca0a26", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "803ddf7f-7017-41ce-a794-3b3ea592321b", "node_type": "1", "metadata": {}, "hash": "947606ce0f75545a5e9fb895cd70a6b7ebd7d1936f4e5b68b2fea41fdab4ca06", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "idR3 idR3 idR3 idR3\nFigure 6.2: Illustration of a neural network with residual blocks.\nInstead of de\ufb01ning a FC NN \u03a6 as in (1.1), one can insert a residual block in the \u2113-th layer by rede\ufb01ning27\n\u00af\u03a6(\u2113)(x,\u03b8) = \u03f1(\u03a6(\u2113)(x,\u03b8)) + \u00af\u03a6(\u2113\u22121)(x,\u03b8), (6.2)\nwhere we assume that N\u2113 = N\u2113\u22121. Such a block can be viewed as the sum of a regular FC NN and the\nidentity which is referred to as skip connection or residual connection. A sketch of a NN with residual blocks\nis shown in Figure 6.2. Inserting a residual block in all layers leads to a so-called residual NN.\nA prominent approach to analyze residual NNs is by establishing a connection with optimal control\nproblems and dynamical systems [ E17, TvG18, EHL19, LLS19, RH19, LML+20]. Concretely, if each layer of\na NN \u03a6 is of the form (6.2), then we have that\n\u00af\u03a6(\u2113) \u2212\u00af\u03a6(\u2113\u22121) = \u03f1(\u03a6(\u2113)) =: h(\u2113,\u03a6(\u2113)),\nwhere we abbreviate \u00af\u03a6(\u2113) = \u00af\u03a6(\u2113)(x,\u03b8) and set \u00af\u03a6(0) = x. Hence, ( \u00af\u03a6(\u2113))L\u22121\n\u2113=0 corresponds to an Euler discretization\nof the ODE\n\u02d9\u03c6(t) = h(t,\u03c6(t)), \u03c6 (0) = x,\nwhere t\u2208[0,L \u22121] and h is an appropriate function.\nUsing this relationship, deep residual NNs can be studied in the framework of the well-established theory\nof dynamical systems, where strong mathematical guarantees can be derived.\n6.3 Framelets and U-Nets\nOne of the most prominent application areas of deep NNs are inverse problems, particularly those in the \ufb01eld\nof imaging science, see also Subsection 8.1. A speci\ufb01c architectural design of CNNs, namely so-called U-nets\nintroduced in [RFB15], seems to perform best for this range of problems. We depict a sketch of a U-net in\nFigure 6.3. However, a theoretical understanding of the success of this architecture was lacking.\nRecently, an innovative approach called deep convolutional framelets was suggested in [ YHC18], which we\nnow brie\ufb02y explain. The core idea is to take a frame-theoretic viewpoint, see, e.g., [ CKP12], and regard the\nforward pass of a CNN as a decomposition in terms of a frame (in the sense of a generalized basis). A similar\napproach will be taken in Subsection 7.2 for understanding the learned kernels using sparse coding. However,\nbased on the analysis and synthesis operators of the corresponding frame, the usage of deep convolutional\nframelets naturally leads to a theoretical understanding of encoder-decoder architectures, such as U-nets.\nLet us describe this approach for one-dimensional convolutions on the group G:= Z/(dZ) with kernels\nde\ufb01ned on the subgroup H := Z/(nZ), where d,n \u2208N with n < d, see also Subsection 6.1. We de\ufb01ne\nthe convolution between u \u2208RG and v \u2208RH by zero-padding v, i.e., g\u2217\u25e6v := g\u2217\u00afv, where \u00afv \u2208RG is\nde\ufb01ned by \u00afvi = vi for i \u2208H and \u00afvi = 0 else. As an important tool, we consider the Hankel matrix\nHn(x) = (xi+j)i\u2208G,j\u2208H \u2208Rd\u00d7n associated with x\u2208RG. As one key property, matrix-vector multiplications\nwith Hankel matrices are translated to convolutions via 28\n\u27e8e(i),Hn(x)v\u27e9=\n\u2211\nj\u2208H\nxi+jvj = \u27e8x,e(i) \u2217\u25e6v\u27e9, i \u2208G, (6.3)\n27One can also skip multiple layers, e.g., in [ HZRS16] two or three layers skipped, use a simple transformation instead of the\nidentity [SGS15], or randomly drop layers [HSL +16].", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3107, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "803ddf7f-7017-41ce-a794-3b3ea592321b": {"__data__": {"id_": "803ddf7f-7017-41ce-a794-3b3ea592321b", "embedding": null, "metadata": {"page_label": "47", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3ad6f4b3-100a-44df-b1f8-d40424f52a0f", "node_type": "4", "metadata": {"page_label": "47", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "5f0c3c3e7467d4073b1aa9f29f5a4686d15c0202b14d96865b860438a6ca0a26", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1560830c-1c3c-4fa2-bc50-f6ff4ed67562", "node_type": "1", "metadata": {"page_label": "47", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "e902e5fa90ddc9dbb61cf97ecd93036c8096265b7cf91468f305c3a742d9fc04", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "As an important tool, we consider the Hankel matrix\nHn(x) = (xi+j)i\u2208G,j\u2208H \u2208Rd\u00d7n associated with x\u2208RG. As one key property, matrix-vector multiplications\nwith Hankel matrices are translated to convolutions via 28\n\u27e8e(i),Hn(x)v\u27e9=\n\u2211\nj\u2208H\nxi+jvj = \u27e8x,e(i) \u2217\u25e6v\u27e9, i \u2208G, (6.3)\n27One can also skip multiple layers, e.g., in [ HZRS16] two or three layers skipped, use a simple transformation instead of the\nidentity [SGS15], or randomly drop layers [HSL +16].\n28Here and in the following we naturally identify elements in RG and RH with the corresponding vectors in Rd and Rn.\n47", "mimetype": "text/plain", "start_char_idx": 2659, "end_char_idx": 3227, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "0f924c9f-34ee-44c2-bb38-507c517a6978": {"__data__": {"id_": "0f924c9f-34ee-44c2-bb38-507c517a6978", "embedding": null, "metadata": {"page_label": "48", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a23afbef-faa8-42eb-bb0b-d4af01c2b1af", "node_type": "4", "metadata": {"page_label": "48", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "399b834db0edb80b219ee4dadb7b5b8c29e1664f95aa05f6992d8cec067d2045", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Figure 6.3: Illustration of a simpli\ufb01ed U-net neural network. Down-arrows stand for pooling, up arrows for\ndeconvolution or upsampling, right arrows for convolution or fully connected steps. Dashed lines are skip\nconnections.\nwhere e(i) := 1{i}\u2208RG and v\u2208RH, see [YGLD17]. Further, we can recover the k-th coordinate of x by the\nFrobenius inner product between Hn(x) and the Hankel matrix associated with e(k), i.e.,\n1\nnTr\n(\nHn(e(k))THn(x)\n)\n= 1\nn\n\u2211\nj\u2208H\n\u2211\ni\u2208G\ne(k)\ni+jxi+j = 1\nn|H|xk = xk. (6.4)\nThis allows us to construct global and local bases as follows: Let p,q \u2208N, let U =\n[u1 \u00b7\u00b7\u00b7up\n]\n\u2208Rd\u00d7p,\nV =\n[v1 \u00b7\u00b7\u00b7vq\n]\n\u2208Rn\u00d7q, \u02dcU =\n[\u02dcu1 \u00b7\u00b7\u00b7 \u02dcup\n]\n\u2208Rd\u00d7p, and \u02dcV =\n[\u02dcv1 \u00b7\u00b7\u00b7 \u02dcvq\n]\n\u2208Rn\u00d7q, and assume that\nHn(x) = \u02dcUUTHn(x)V \u02dcVT. (6.5)\nFor p\u2265d and q\u2265n, this is, for instance, satis\ufb01ed if U and V constitute frames with \u02dcU and \u02dcV being their\nrespective dual frames, i.e., \u02dcUUT = Id and V \u02dcVT = In. As a special case, one can consider orthonormal bases\nU = \u02dcU and V = \u02dcV with p= d and q= n. In the case p= q= r\u2264n, where r is the rank of Hn(x), one can\nestablish (6.5) by choosing the left and right singular vectors of Hn(x) as U = \u02dcU and V = \u02dcV, respectively.\nThe identity in (6.5), in turn, ensures the following decomposition:\nx= 1\nn\np\u2211\ni=1\nq\u2211\nj=1\n\u27e8x,ui \u2217\u25e6vj\u27e9\u02dcui \u2217\u25e6\u02dcvj. (6.6)\nObserving that the vector vj \u2208RH interacts locally with x \u2208RG due to the fact that H \u2282G, whereas\nui \u2208RG acts on the entire vector x, we refer to ( vj)q\nj=1 as local and ( ui)p\ni=1 as global bases. In the context of\nCNNs, vi can be interpreted as local convolutional kernel and ui as pooling operation 29. The proof of (6.6)\n29Note that \u27e8x,ui\u2217\u25e6vj\u27e9can also be interpreted as \u27e8ui,vj \u22c6x\u27e9, where \u22c6denotes the cross-correlation between the zero-padded\nvj and x. This is in line with software implementations for deep learning applications, e.g., TensorFlow and PyTorch, where\ntypically cross-correlations are used instead of convolutions.\n48", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1900, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "6055b970-22d1-4fa6-a508-7022ddf32591": {"__data__": {"id_": "6055b970-22d1-4fa6-a508-7022ddf32591", "embedding": null, "metadata": {"page_label": "49", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0cb8eb46-a71d-4228-be2c-3f20cb31025a", "node_type": "4", "metadata": {"page_label": "49", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "5c48f15f02421040488bcb86329335f2006a3d20812c084e6cdf514b2fb4757c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "follows directly from properties (6.3), (6.4), and (6.5) as\nxk = 1\nnTr\n(\nHn(e(k))THn(x)\n)\n= 1\nnTr\n(\nHn(e(k))T \u02dcUUTHn(x)V \u02dcVT)\n= 1\nn\np\u2211\ni=1\nq\u2211\nj=1\n\u27e8ui,Hn(x)vj\u27e9\u27e8\u02dcui,Hn(e(k))\u02dcvj\u27e9.\nThe decomposition in (6.6) can now be interpreted as a composition of an encoder and a decoder,\nx\u21a6\u2192C = (\u27e8x,ui \u2217\u25e6vj\u27e9)i\u2208[p],j\u2208[q] and C \u21a6\u21921\nn\np\u2211\ni=1\nq\u2211\nj=1\nCi,j\u02dcui \u2217\u25e6\u02dcvj, (6.7)\nwhich relates it to CNNs equipped with an encoder-decoder structure such as U-nets, see Figure 6.3.\nGeneralizing this approach to multiple channels, it is possible to stack such encoders and decoders which\nleads to a layered version of (6.6). In [ YHC18] it is shown that one can make an informed decision on the\nnumber of layers based on the rank of Hn(x), i.e., the complexity of the input features x. Moreover, also an\nactivation function such as the ReLU or bias vectors can be included. The key question one can then ask is\nhow the kernels can be chosen to obtain sparse coe\ufb03cients C in (6.7) and a decomposition such as (6.6), i.e.,\nperfect reconstruction. If U and V are chosen as the left and right singular vectors of Hn(x), one obtains a\nvery sparse, however input-dependent, representation in (6.6) due to the fact that\nCi,j = \u27e8x,ui \u2217\u25e6vj\u27e9= \u27e8ui,Hn(x)vj\u27e9= 0, i \u0338= j.\nFinally, using the framework of deep convolutional framelets, theoretical reasons for including skip connections\ncan be derived, since they aid to obtain a perfect reconstruction.\n6.4 Batch normalization\nBatch normalization is a building block of NNs that was invented in [ IS15] with the goal to reduce so-called\ninternal covariance shift. In essence, this phrase describes the (undesirable) situation where during training\neach layer receives inputs with di\ufb00erent distribution. A batch normalization block is de\ufb01ned as follows: For\npoints b= (y(i))m\ni=1 \u2208(Rn)m and \u03b2,\u03b3 \u2208R, we de\ufb01ne\nBN(\u03b2,\u03b3)\nb (y) := \u03b3 y\u2212\u00b5b\n\u03c3b\n+ \u03b2, y \u2208Rn, with \u00b5b = 1\nm\nm\u2211\ni=1\ny(i) and \u03c32\nb = 1\nm\nm\u2211\ni=1\n(y(i) \u2212\u00b5b)2, (6.8)\nwhere all operations are to be understood componentwise, see Figure 6.4.\nSuch a batch normalization block can be added into a NN architecture. Then b is the output of the\nprevious layer over a batch or the whole training data 30. Furthermore, the parameters \u03b2,\u03b3 are variable and\ncan be learned during training. Note that, if one sets \u03b2 = \u00b5b and \u03b3 = \u03c3b, then BN(\u03b2,\u03b3)\nb (y) = y for all y\u2208Rn.\nTherefore, a batch normalization block does not negatively a\ufb00ect the expressivity of the architecture. On the\nother hand, batch normalization does have a tangible e\ufb00ect on the optimization aspects of deep learning.\nIndeed, in [STIM18, Theorem 4.1], the following observation was made:\n30In practice, one typically uses a moving average to estimate the mean \u00b5 and the standard deviation \u03c3 of the output of the\nprevious layer over the whole training data by only using batches.\n49", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2786, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b1aff551-2170-47ff-8017-b18cf6cf58a9": {"__data__": {"id_": "b1aff551-2170-47ff-8017-b18cf6cf58a9", "embedding": null, "metadata": {"page_label": "50", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "92238105-858e-4f90-b6ed-c7c7dbfa0e08", "node_type": "4", "metadata": {"page_label": "50", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "e29b191bd7fad3979962b7d5d26b303164d236c3a53e837d9b93913fb5ff15dc", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "\u00b5b \u03c3b\n\u03b2 \u03b3\n\u02c6y= y\u2212\u00b5b\n\u03c3b\nz= \u03b3\u02c6y+ \u03b2\nFigure 6.4: A batch normalization block after a fully connected neural network. The parameters \u00b5b,\u03c3b are\nthe mean and the standard deviation of the output of the fully connected network computed over a batch s,\ni.e., a set of inputs. The parameters \u03b2,\u03b3 are learnable parts of the batch normalization block.\nProposition 6.1 (Smoothening e\ufb00ect of batch normalization). Let m\u2208N with m\u22652 and for every \u03b2,\u03b3 \u2208R\nde\ufb01ne B(\u03b2,\u03b3) : Rm \u2192Rm by\nB(\u03b2,\u03b3)(b) = (BN(\u03b2,\u03b3)\nb (y(1)),..., BN(\u03b2,\u03b3)\nb (y(m))), b = (y(i))m\ni=1 \u2208Rm,\nwhere BN(\u03b2,\u03b3)\nb is given as in (6.8). Let \u03b2,\u03b3 \u2208R and let r: Rm \u2192R be a di\ufb00erentiable function. Then it\nholds for every b\u2208Rm that\n\u2225\u2207(r\u25e6B(\u03b2,\u03b3))(b)\u22252\n2 = \u03b32\n\u03c32\nb\n(\n\u2225\u2207r(b)\u22252 \u2212 1\nm\u27e81,\u2207r(b)\u27e92 \u2212 1\nm\u27e8B(0,1)(b),\u2207r(b)\u27e92)\n,\nwhere 1 = (1,..., 1) \u2208Rm and \u03c32\nb is given as in (6.8).\nFor multi-dimensional y(i) \u2208Rn, i\u2208[m], the same statement holds for all components as, by de\ufb01nition,\nthe batch normalization block acts componentwise. Proposition 6.1 follows from a convenient representation\nof the Jacobian of the mapping B(\u03b2,\u03b3), given by\n\u2202B(\u03b2,\u03b3)(b)\n\u2202b = \u03b3\n\u03c3b\n(\nIm \u2212 1\nm11T \u2212 1\nmB(0,1)(b)(B(0,1)(b))T\n)\n, b \u2208Rm,\nand the fact that { 1\u221am, 1\u221amB(0,1)(b)}constitutes an orthonormal set.\nChoosing rto mimic the empirical risk of a learning task, Proposition 6.1 shows that, in certain situations\u2014\nfor instance, if \u03b3 is smaller than \u03c3b or if m is not too large\u2014a batch normalization block can considerably\nreduce the magnitude of the derivative of the empirical risk with respect to the input of the batch normalization\nblock. By the chain rule, this implies that also the derivative of the empirical risk with respect to NN\nparameters in\ufb02uencing the input of the batch normalization block is reduced.\nInterestingly, a similar result holds for second derivatives [STIM18, Theorem 4.2] if ris twice di\ufb00erentiable.\nOne can conclude that adding a batch normalization block increases the smoothness of the optimization\nproblem. Since the parameters \u03b2 and \u03b3 were introduced, including a batch normalization block also increases\nthe dimension of the optimization problem by two.\n6.5 Sparse neural networks and pruning\nFor deep FC NNs, the number of trainable parameters usually scales like the square of the number of neurons.\nFor reasons of computational complexity and memory e\ufb03ciency, it appears sensible to seek for techniques to\nreduce the number of parameters or extract sparse subnetworks (see Figure 6.5) without a\ufb00ecting the output\n50", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2453, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d57c4995-c52a-41d2-a029-c28578759712": {"__data__": {"id_": "d57c4995-c52a-41d2-a029-c28578759712", "embedding": null, "metadata": {"page_label": "51", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3f830bbd-7bf1-4acc-9193-84dd47a2e212", "node_type": "4", "metadata": {"page_label": "51", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "997ca88bf9fe707ae463a72842b88ed7190149b17d2517fb6151da5ea5c088cc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9708271e-0459-4801-8068-9e5e6f14ef88", "node_type": "1", "metadata": {}, "hash": "b182a668760ecdb90120d6b3b4ff8e22152c8f2a0326a5bb25397260a981658c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Figure 6.5: A neural network with\nsparse connections.\nof a NN much. One way to do this is by pruning [LDS89, HMD16].\nHere, certain parameters of a NN are removed after training. This\nis done, for example, by setting these parameters to zero.\nIn this context, the lottery ticket hypothesis was formulated\nin [FC18]. It states: \u201cA randomly-initialized, dense NN contains a\nsubnetwork that is initialized such that\u2014when trained in isolation\u2014it\ncan match the test accuracy of the original NN after training for at\nmost the same number of iterations\u201d. In [ RWK+20] a similar hy-\npothesis was made and empirically studied. There, it is claimed that,\nfor a su\ufb03ciently overparametrized NN, there exists a subnetwork\nthat matches the performance of the large NN after training without\nbeing trained itself, i.e., already at initialization.\nUnder certain simplifying assumptions, the existence of favorable subnetworks is quite easy to prove. We\ncan use a technique that was previously indirectly used in Subsection 4.2\u2014the Carath\u00b4 eodory Lemma. This\nresult states the following: Let n \u2208N, C \u2208(0,\u221e), and let ( H,\u2225\u00b7\u2225) be a Hilbert space. Let F \u2282H with\nsupf\u2208F \u2225f\u2225\u2264 C and let g\u2208H be in the convex hull of F. Then there exist fi \u2208F, i\u2208[n], and c\u2208[0,1]n\nwith \u2225c\u22251 = 1 such that\n\ued79\ued79\ued79\ued79\ued79g\u2212\nn\u2211\ni=1\ncifi\n\ued79\ued79\ued79\ued79\ued79\u2264 C\u221an,\nsee, e.g., [Ver18, Theorem 0.0.2].\nProposition 6.2 (Carath\u00b4 eodory pruning). Let d,n \u2208N, with n\u2265100 and let \u00b5 be a probability measure\non the unit ball B1(0) \u2282Rd. Let a= ((d,n, 1),\u03f1R) be the architecture of a two-layer ReLU network and let\n\u03b8\u2208RP((d,n,1)) be corresponding parameters such that\n\u03a6a(\u00b7,\u03b8) =\nn\u2211\ni=1\nw(2)\ni \u03f1R(\u27e8(w(1)\ni ,\u00b7\u27e9+ b(1)\ni )),\nwhere (w(1)\ni ,b(1)\ni ) \u2208Rd\u00d7R, i\u2208[n], and w(2) \u2208Rn. Assume that for every i\u2208[n] it holds that \u2225w(1)\ni \u22252 \u22641/2\nand b(1)\ni \u22641/2. Then there exists a parameter \u02dc\u03b8\u2208RP((d,n,1)) with at least 99% of its entries being zero such\nthat\n\u2225\u03a6a(\u00b7,\u03b8) \u2212\u03a6a(\u00b7,\u02dc\u03b8)\u2225L2(\u00b5) \u226415\u2225w(2)\u22251\u221an .\nSpeci\ufb01cally, there exists an index set I \u2282[n] with |I|\u2264 n/100 such that \u02dc\u03b8 satis\ufb01es that\n\u02dcw(2)\ni = 0, if i /\u2208I, and ( \u02dcw(1)\ni ,\u02dcb(1)\ni ) =\n{\n(w(1)\ni ,b(1)\ni ), if i\u2208I,\n(0,0), if i /\u2208I.\nThe result is clear if w(2) = 0. Otherwise, de\ufb01ne\nfi := \u2225w(2)\u22251\u03f1R(\u27e8w(1)\ni ,\u00b7\u27e9+ b(1)\ni ), i \u2208[n],\nand observe that \u03a6 a(\u00b7,\u03b8) is in the convex hull of {fi}n\ni=1 \u222a{\u2212fi}n\ni=1. Moreover, by the Cauchy\u2013Schwarz\ninequality, it holds that\n\u2225fi\u2225L2(\u00b5) \u2264\u2225w(2)\u22251\u2225fi\u2225L\u221e(B1(0)) \u2264\u2225w(2)\u22251.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2360, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "9708271e-0459-4801-8068-9e5e6f14ef88": {"__data__": {"id_": "9708271e-0459-4801-8068-9e5e6f14ef88", "embedding": null, "metadata": {"page_label": "51", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3f830bbd-7bf1-4acc-9193-84dd47a2e212", "node_type": "4", "metadata": {"page_label": "51", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "997ca88bf9fe707ae463a72842b88ed7190149b17d2517fb6151da5ea5c088cc", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d57c4995-c52a-41d2-a029-c28578759712", "node_type": "1", "metadata": {"page_label": "51", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "5a3f2fc4b0beea8f419ce912d0673077b201dc93082f7721de163bec19d34787", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The result is clear if w(2) = 0. Otherwise, de\ufb01ne\nfi := \u2225w(2)\u22251\u03f1R(\u27e8w(1)\ni ,\u00b7\u27e9+ b(1)\ni ), i \u2208[n],\nand observe that \u03a6 a(\u00b7,\u03b8) is in the convex hull of {fi}n\ni=1 \u222a{\u2212fi}n\ni=1. Moreover, by the Cauchy\u2013Schwarz\ninequality, it holds that\n\u2225fi\u2225L2(\u00b5) \u2264\u2225w(2)\u22251\u2225fi\u2225L\u221e(B1(0)) \u2264\u2225w(2)\u22251.\nWe conclude with the Carath\u00b4 eodory Lemma that there exists I \u2282[n] with |I|= \u230an/100\u230b\u2265 n/200 and\nci \u2208[\u22121,1], i\u2208I, such that\n\ued79\ued79\ued79\ued79\ued79\u03a6a(\u00b7,\u03b8) \u2212\n\u2211\ni\u2208I\ncifi\n\ued79\ued79\ued79\ued79\ued79\nL2(\u00b5)\n\u2264\u2225w(2)\u22251\u221a\n|I|\n\u2264\n\u221a\n200\u2225w(2)\u22251\u221an ,\n51", "mimetype": "text/plain", "start_char_idx": 2090, "end_char_idx": 2557, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8b6214a9-eda6-48b7-a92e-534c1aaa3235": {"__data__": {"id_": "8b6214a9-eda6-48b7-a92e-534c1aaa3235", "embedding": null, "metadata": {"page_label": "52", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3ecb5304-c784-4118-922a-f7b6eeac23f1", "node_type": "4", "metadata": {"page_label": "52", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "b43dc61f8fa1eea320fd287966d9c35fc85cfa4189f61b8cd50655c084634f5b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "which yields the result.\nProposition 6.2 shows that certain, very wide NNs can be approximated very well by sparse subnetworks\nwhere only the output weight matrix needs to be changed. The argument of Proposition 6.2 is inspired\nby [BK18], where a much more re\ufb01ned result is shown for deep NNs.\n6.6 Recurrent neural networks\nFigure 6.6: Sketch of a recurrent neu-\nral network. Cycles in the computa-\ntional graph incorporate the sequen-\ntial structure of the input and output.\nRecurrent NNs are NNs where the underlying graph is allowed to\nexhibit cycles as in Figure 6.6, see [ Hop82, RHW86, Elm90, Jor90].\nPreviously, we had excluded cyclic computational graphs. For a feed-\nforward NN, the computation of internal states is naturally performed\nstep by step through the layers. Since the output of a layer does\nnot a\ufb00ect previous layers, the order in which the computations of\nthe NN are performed corresponds to the order of the layers. For\nrecurrent NNs, the concept of layers does not exist, and the order\nof operations is much more delicate. Therefore, one considers time\nsteps. In each time step, all possible computations of the graph are\napplied to the current state of the NN. This yields a new internal\nstate. Given that time steps arise naturally from the de\ufb01nition of\nrecurrent NNs, this NN type is typically used for sequential data.\nIf the input to a recurrent NN is a sequence, then every input determines the internal state of the recurrent\nNN for the following inputs. Therefore, one can claim that these NNs exhibit a memory. This fact is extremely\ndesirable in natural language processing, which is why recurrent NNs are widely used in this application.\nRecurrent NNs can be trained similarly to regular feed-forward NNs by an algorithm calledbackpropagation\nthrough time [MP69, Wer88, WZ95]. This procedure essentially unfolds the recurrent structure yielding a\nclassical NN structure. However, the algorithm may lead to very deep structures. Due to the vanishing\nand exploding gradient problem discussed earlier, very deep NNs are often hard to train. Because of this,\nspecial recurrent structures were introduced that include gates which prohibit too many recurrent steps; these\ninclude the widely used LSTMs [HS97].\nThe application area of recurrent NNs is typically quite di\ufb00erent from that of regular NNs since they\nare specialized on sequential data. Therefore, it is hard to quantify the e\ufb00ect of a recurrent connection on\na fully connected NN. However, it is certainly true that with recurrent connections certain computations\ncan be performed much more e\ufb03ciently than with feed-forward NN structures. A particularly interesting\nconstruction can be found in [ BF19, Theorem 4.4], where it is shown that a \ufb01xed size, recurrent NN with\nReLU activation function, can approximate the function x\u21a6\u2192x2 to any desired accuracy. The reason for\nthis e\ufb03cient representation can be seen when considering the self-referential de\ufb01nition of the approximant to\nx\u2212x2 shown in Figure 3.2.\nOn the other hand, with feed-forward NNs, it transpires from Theorem 3.3 that the approximation error\nof \ufb01xed-sized ReLU NNs for any non-a\ufb03ne function is greater than a positive lower bound.\n7 Describing the features a deep neural network learns\nThis section presents two viewpoints which help in understanding the nature of features that can be described\nby NNs. Section 7.1 summarizes aspects of the so-called scattering transform which constitutes a speci\ufb01c NN\narchitecture that can be shown to satisfy desirable properties, such as translation and deformation invariance.\nSection 7.2 relates NN features to the current paradigm of sparse coding.\n7.1 Invariances and the scattering transform\nOne of the \ufb01rst theoretical contributions to the understanding of the mathematical properties of CNNs\nis [Mal12]. The approach taken in that work is to consider speci\ufb01c CNN architectures with \ufb01xed parameters\n52", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3904, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "629f4da1-2129-4201-ad23-3f75ed5d6f27": {"__data__": {"id_": "629f4da1-2129-4201-ad23-3f75ed5d6f27", "embedding": null, "metadata": {"page_label": "53", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "bcb391ad-c487-4189-bad3-e742a75aa340", "node_type": "4", "metadata": {"page_label": "53", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "a5c28e43d8e9bca4ac33013b48161172b5b28ce8e0e650bc119e2d26f5266253", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1e4a7246-3d4a-4eda-98f7-70b2cc54eb7f", "node_type": "1", "metadata": {}, "hash": "b1133c9a8cf1e4bef412bd69c304acf5241aff7b9cfea1aebd5b3e89a51f5e62", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "that result in a stand-alone feature descriptor whose output may be fed into a subsequent classi\ufb01er (for example,\na kernel support vector machine or a trainable FC NN). From an abstract point of view, a feature descriptor\nis a function \u03a8 mapping from a signal space, such as L2(Rd) or the space of piecewise smooth functions, to\na feature space. In an ideal world, such a classi\ufb01er should \u201cfactor\u201d out invariances that are irrelevant to a\nsubsequent classi\ufb01cation problem while preserving all other information of the signal. A very simple example\nof a classi\ufb01er which is invariant under translations is the Fourier modulus \u03a8 : L2(Rd) \u2192L2(Rd), u\u21a6\u2192|\u02c6u|.\nThis follows from the fact that a translation of a signal uresults in a modulation of its Fourier transform, i.e.,\n\u02c6u(\u00b7\u2212\u03c4)(\u03c9) = e\u22122\u03c0i\u27e8\u03c4,\u03c9\u27e9\u02c6u(\u03c9), \u03c4,\u03c9 \u2208Rd. Furthermore, in most cases (for example, if u is a generic compactly\nsupported function [ GKR20]), u can be reconstructed up to a translation from its Fourier modulus [ GKR20]\nand an energy conservation property of the form \u2225\u03a8(u)\u2225L2 = \u2225u\u2225L2 holds true. Translation invariance is,\nfor example, typically exhibited by image classi\ufb01ers, where the label of an image does not change if it is\ntranslated.\nIn practical problems many more invariances arise. Providing an analogous representation that factors\nout general invariances would lead to a signi\ufb01cant reduction in the problem dimensionality and constitutes an\nextremely promising route towards dealing with the very high dimensionality that is commonly encountered\nin practical problems [ Mal16]. This program is carried out in [ Mal12] for additional invariances with respect\nto deformations u\u21a6\u2192u\u03c4 := u(\u00b7\u2212\u03c4(\u00b7)), where \u03c4: Rd \u2192Rd is a smooth mapping. Such transformations may\noccur in practice, for instance, as image warpings. In particular, a feature descriptor \u03a8 is designed that, with\na suitable norm \u2225\u00b7\u2225 on the image of \u03a8,\n(a) is Lipschitz continuous with respect to deformations in the sense that \u2225\u03a8(u) \u2212\u03a8(u\u03c4)\u2225\u2272 K(\u03c4,\u2207\u03c4,\u22072\u03c4)\nholds for some K that only mildly depends on \u03c4 and essentially grows linearly in \u2207\u03c4 and \u22072\u03c4,\n(b) is almost (i.e., up to a small and controllable error) invariant under translations of the input data, and\n(c) contains all relevant information on the input data in the sense that an energy conservation property\n\u2225\u03a8(u)\u2225\u2248\u2225 u\u2225L2\nholds true.\nObserve that, while the action of translations only represents a d-parameter group, the action of deforma-\ntions/warpings represents an in\ufb01nite-dimensional group. Hence, a deformation invariant feature descriptor\nrepresents a big potential for dimensionality reduction. Roughly speaking, the feature descriptor \u03a8 of [ Mal12]\n(also coined the scattering transform ) is de\ufb01ned by collecting features that are computed by iteratively\napplying a wavelet transform followed by a pointwise modulus non-linearity and a subsequent low-pass\n\ufb01ltering step, i.e.,\n|||u\u2217\u03c8j1|\u2217\u03c8j2 \u2217... |\u2217\u03c8j\u2113|\u2217\u03d5J,\nwhere \u03c8j refers to a wavelet at scale j and \u03d5J refers to a scaling function at scale J. The collection of all\nthese so-called scattering coe\ufb03cients can then be shown to satisfy the properties in (a)\u2013(c) above in a suitable\n(asymptotic) sense. The proof of this result relies on a subtle interplay between a \u201cdeformation covariance\u201d\nproperty of the wavelet transform and a \u201cregularizing\u201d property of the operation of convolution with the\nmodulus of a wavelet. We remark that similar results can be shown also for di\ufb00erent systems, such as Gabor\nframes [WGB17, CL19].\n7.2 Hierarchical sparse representations\nThe previous approach modeled the learned features by a speci\ufb01c dictionary, namely wavelets.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3604, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1e4a7246-3d4a-4eda-98f7-70b2cc54eb7f": {"__data__": {"id_": "1e4a7246-3d4a-4eda-98f7-70b2cc54eb7f", "embedding": null, "metadata": {"page_label": "53", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "bcb391ad-c487-4189-bad3-e742a75aa340", "node_type": "4", "metadata": {"page_label": "53", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "a5c28e43d8e9bca4ac33013b48161172b5b28ce8e0e650bc119e2d26f5266253", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "629f4da1-2129-4201-ad23-3f75ed5d6f27", "node_type": "1", "metadata": {"page_label": "53", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "f770496fd61705c10d130b8dc9a9e926e6f9b3d442fe8fdc24998f346e3bf3bc", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The proof of this result relies on a subtle interplay between a \u201cdeformation covariance\u201d\nproperty of the wavelet transform and a \u201cregularizing\u201d property of the operation of convolution with the\nmodulus of a wavelet. We remark that similar results can be shown also for di\ufb00erent systems, such as Gabor\nframes [WGB17, CL19].\n7.2 Hierarchical sparse representations\nThe previous approach modeled the learned features by a speci\ufb01c dictionary, namely wavelets. It is well known\nthat one of the striking properties of wavelets is to provide sparse representations for functions belonging to\ncertain function classes. More generally, we speak of sparse representations with respect to a representation\nsystem. For a vector x\u2208Rd, a sparsifying representation system D\u2208Rd\u00d7p\u2014also called dictionary\u2014is such\nthat x= D\u03c6with the coe\ufb03cients \u03c6\u2208Rp being sparse in the sense that \u2225\u03c6\u22250 := |supp(\u03c6)|= |{i\u2208[p]: \u03c6i \u0338= 0}|\nis small compared to p. A similar de\ufb01nition can be made for signals in in\ufb01nite-dimensional spaces. Taking\n53", "mimetype": "text/plain", "start_char_idx": 3149, "end_char_idx": 4157, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5e405028-c4f0-43d1-b80a-a4a9c44ac8fe": {"__data__": {"id_": "5e405028-c4f0-43d1-b80a-a4a9c44ac8fe", "embedding": null, "metadata": {"page_label": "54", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7b70200b-1c4f-4615-86e3-8382f9b8d2de", "node_type": "4", "metadata": {"page_label": "54", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "040f53adcc7c5a08282937f5d0894c73eb5da1f567dd75e66977764cd473721f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bf988a13-59e8-41d8-a608-5c5723a8b047", "node_type": "1", "metadata": {}, "hash": "67a3338e8175c1c8e96d0b12fbcaf3690e12781e570cd75682b3aa3605d55b3f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "sparse representations into account, the theory of sparse coding provides an approach to a theoretical\nunderstanding of the features a deep NN learns.\nOne common method in image processing is the utilization of not the entire image but overlapping patches\nof it, coined patch-based image processing. Thus of particular interest are local dictionaries which sparsify\nthose patches, but presumably not the global image. This led to the introduction of the convolutional sparse\ncoding model (CSC model), which links such local and global behaviors. Let us describe this model for\none-dimensional convolutions on the group G:= Z/(dZ) with kernels supported on the subgroup H := Z/(nZ),\nwhere d,n \u2208N with n<d , see also Subsection 6.1. The corresponding CSC model is based on a decomposition\nof a global signal x\u2208(RG)c with c\u2208N channels as\nxi =\nC\u2211\nj=1\n\u03bai,j \u2217\u03c6j, i \u2208[c], (7.1)\nwhere \u03c6 \u2208(RG)C is supposed to be a sparse representation with C \u2208N channels and \u03bai,j \u2208RG, i \u2208[c],\nj \u2208[C], are local kernels with supp(\u03bai,j) \u2282H. Let us consider a patch (( xi)g+h)i\u2208[c],h\u2208H of nadjacent entries,\nstarting at position g \u2208G, in each channel of x. The condition on the support of the kernels \u03bai,j and the\nrepresentation in (7.1) imply that this patch is only a\ufb00ected by a stripe of at most (2 n\u22121) entries in each\nchannel of \u03c6. The local, patch-based sparsity of the representation \u03c6 can thus be appropriately measured via\n\u2225\u03c6\u2225(n)\n0,\u221e:= max\ng\u2208G\n\u2225((\u03c6j)g+k)j\u2208[C],k\u2208[2n\u22121]\u22250,\nsee [PSE17]. Furthermore, note that we can naturally identify xand \u03c6with vectors in Rdc and RdC and write\nx= D\u03c6, where D\u2208Rdc\u00d7dC is a matrix consisting of circulant blocks, typically referred to as a convolutional\ndictionary.\nThe relation between the CSC model and deep NNs is revealed by applying the CSC model in a layer-wise\nfashion [PRE17, SPRE18, PRSE18]. To see this, let C0 \u2208N and for every \u2113 \u2208[L] let C\u2113,k\u2113 \u2208N and let\nD(\u2113) \u2208RdC\u2113\u22121\u00d7dC\u2113 be a convolutional dictionary with kernels supported on Z/(n\u2113Z). A signal x= \u03c6(0) \u2208RdC0\nis said to belong to the corresponding multi-layered CSC model (ML-CSC model) if there exist coe\ufb03cients\n\u03c6(\u2113) \u2208RdC\u2113 with\n\u03c6(\u2113\u22121) = D(\u2113)\u03c6(\u2113) and \u2225\u03c6(\u2113)\u2225(n\u2113)\n0,\u221e\u2264k\u2113, \u2113 \u2208[L]. (7.2)\nWe now consider the problem of reconstructing the sparse coe\ufb03cients ( \u03c6(\u2113))L\n\u2113=1 from a noisy signal \u02dcx:= x+ \u03bd,\nwhere the noise \u03bd \u2208RdC0 is assumed to have small \u21132-norm and x is assumed to follow the ML-CSC model\nin (7.2). In general, this problem is NP-hard. However, under suitable conditions on the ML-CSC model, it\ncan be approximately solved, for instance, by a layered thresholding algorithm.\nMore precisely, for D\u2208Rdc\u00d7dC and b\u2208RdC, we de\ufb01ne a soft-thresholding operator by\nTD,b(x) := \u03f1R(DTx\u2212b) \u2212\u03f1R(\u2212DTx\u2212b), x \u2208Rdc, (7.3)\nwhere \u03f1R(x) = max{0,x}is applied componentwise.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2734, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "bf988a13-59e8-41d8-a608-5c5723a8b047": {"__data__": {"id_": "bf988a13-59e8-41d8-a608-5c5723a8b047", "embedding": null, "metadata": {"page_label": "54", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7b70200b-1c4f-4615-86e3-8382f9b8d2de", "node_type": "4", "metadata": {"page_label": "54", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "040f53adcc7c5a08282937f5d0894c73eb5da1f567dd75e66977764cd473721f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5e405028-c4f0-43d1-b80a-a4a9c44ac8fe", "node_type": "1", "metadata": {"page_label": "54", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "bada9161789cf0c70cafb4374892e058c444cc5569a7e64af64767aab38e6db6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In general, this problem is NP-hard. However, under suitable conditions on the ML-CSC model, it\ncan be approximately solved, for instance, by a layered thresholding algorithm.\nMore precisely, for D\u2208Rdc\u00d7dC and b\u2208RdC, we de\ufb01ne a soft-thresholding operator by\nTD,b(x) := \u03f1R(DTx\u2212b) \u2212\u03f1R(\u2212DTx\u2212b), x \u2208Rdc, (7.3)\nwhere \u03f1R(x) = max{0,x}is applied componentwise. If x= D\u03c6 as in (7.1), we obtain \u03c6\u2248TD,b(x) roughly\nunder the following conditions: The distance of \u03c6 and \u03c8:= DTx= DTD\u03c6 can be bounded using the local\nsparsity of \u03c6 and the mutual coherence and locality of the kernels of the convolutional dictionary D. For a\nsuitable threshold b, the mapping \u03c8\u21a6\u2192\u03f1R(\u03c8\u2212b) \u2212\u03f1R(\u2212\u03c8\u2212b) further recovers the support of \u03c6by nullifying\nentries of \u03c8 with \u03c8i \u2264|bi|. Utilizing the soft-thresholding operator (7.3) iteratively for corresponding vectors\nb(\u2113) \u2208RdC\u2113, \u2113\u2208[L], then suggests the following approximations:\n\u03c6(\u2113) \u2248(TD(\u2113),b(\u2113) \u25e6\u00b7\u00b7\u00b7\u25e6T D(1),b(1))(\u02dcx), \u2113 \u2208[L].\nThe resemblance with the realization of a CNN with ReLU activation function is evident. The transposed\ndictionary (D(\u2113))T can be regarded as modeling the learned convolutional kernels, the threshold b(\u2113) models\nthe bias vector, and the soft-thresholding operator TD(\u2113),b(\u2113) mimics the application of a convolutional block\nwith a ReLU non-linearity in the \u2113-th layer.\n54", "mimetype": "text/plain", "start_char_idx": 2382, "end_char_idx": 3688, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "4425c16b-56e9-4b99-9d78-b9c50b502c6f": {"__data__": {"id_": "4425c16b-56e9-4b99-9d78-b9c50b502c6f", "embedding": null, "metadata": {"page_label": "55", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f235f253-ab1e-43be-8cbd-800eab485f88", "node_type": "4", "metadata": {"page_label": "55", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "e6f219bff11aaa144b032946ae549604dfc9c6c5d1d7c7b081c8204681355532", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Using this model, a theoretical understanding of CNNs from the perspective of sparse coding is now at\nhand. This novel perspective gives a precise mathematical meaning of the kernels in a CNN as sparsifying\ndictionaries of an ML-CSC model. Moreover, the forward pass of a CNN can be understood as a layered\nthresholding algorithm for decomposing a noisy signal \u02dcx. The results derived are then of the following \ufb02avor:\nGiven a suitable reconstruction procedure such as thresholding or \u21131-minimization, the sparse coe\ufb03cients\n(\u03c6(\u2113))L\n\u2113=1 of a signal x following a ML-CSC model can be stably recovered from the noisy signal \u02dcx under\ncertain hypotheses on the ingredients of the ML-CSC model.\n8 E\ufb00ectiveness in natural sciences\nThe theoretical insights of the previous sections do not always accurately describe the performance of NNs in\napplications. Indeed, there often exists a considerable gap between the predictions of approximation theory\nand the practical performance of NNs [AD20].\nIn this section, we consider concrete applications which have been very successfully solved with deep-\nlearning-based methods. In Section 8.1 we present an overview of deep-learning-based algorithms applied to\ninverse problems. Section 8.2 then continues by describing how NNs can be used as a numerical ansatz for\nsolving PDEs, highlighting their use in the solution of the multi-electron Schr\u00a8 odinger equation.\n8.1 Deep neural networks meet inverse problems\nThe area of inverse problems, predominantly in imaging, was presumably the \ufb01rst class of mathematical\nmethods embracing deep learning with overwhelming success. Let us consider a forward operator K: Y\u2192X\nwith X,Ybeing Hilbert spaces and the associated inverse problem of \ufb01nding y \u2208Y such that Ky = x for\ngiven features x \u2208X . The classical model-based approach to regularization aims to approximate K by\ninvertible operators, and is hence strongly based on functional analytic principles. Today, such approaches\ntake well-posedness of the approximation, convergence properties, as well as the structure of regularized\nsolutions into account. The last item allows to incorporate prior information of the original solution such as\nregularity, sharpness of edges, or\u2014in the case of sparse regularization [ JMS17]\u2014a sparse coe\ufb03cient sequence\nwith respect to a prescribed representation system. Such approaches are typically realized in a variational\nsetting and hence aim to minimize functionals of the form\n\u2225Ky \u2212x\u22252 + \u03b1R(y),\nwhere \u03b1\u2208(0,\u221e) is a regularization parameter, R: Y\u2192 [0,\u221e) a regularization term, and \u2225\u00b7\u2225 denotes the\nnorm on Y. As said, the regularization term aims to model structural information about the desired solution.\nHowever, one main hurdle in this approach is the problem that typically solution classes such as images from\ncomputed tomography cannot be modeled accurately enough to, for instance, allow reconstruction under the\nconstraint of a signi\ufb01cant amount of missing features.\nThis has opened the door to data-driven approaches, and recently, deep NNs. Solvers of inverse problems\nwhich are based on deep learning techniques can be roughly categorized into three classes:\n1. Supervised approaches: The most straightforward approach is to train a NN \u03a6( \u00b7,\u03b8): X\u2192Y end-to-end,\ni.e., to completely learn the map from data x to the solution y. More advanced approaches in this\ndirection incorporate information about the operator K into the NN such as in [A\u00a8O17, GOW19, MLE21].\nYet another type of approaches aims to combine deep NNs with classical model-based approaches.\nThe \ufb01rst suggestion in this realm was to start by applying a standard solver, followed by a deep NN\n\u03a6(\u00b7,\u03b8): Y\u2192Y which serves as a denoiser for speci\ufb01c reconstruction artifacts, e.g., [ JMFU17]. This\nwas followed by more sophisticated methods such as plug-and-play frameworks for coupling inversion\nand denoising [REM17].\n2. Semi-supervised approaches: These type of approaches aim to encode the regularization by a deep NN\n\u03a6(\u00b7,\u03b8): Y\u2192 [0,\u221e). The underlying idea is often to require stronger regularization on solutions y(i)\n55", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4062, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "0f447729-1c78-48d0-86bb-d606f1c45cf3": {"__data__": {"id_": "0f447729-1c78-48d0-86bb-d606f1c45cf3", "embedding": null, "metadata": {"page_label": "56", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b25d40ae-ab86-4c14-9eab-038ab0b41760", "node_type": "4", "metadata": {"page_label": "56", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "68f457b5501bb987d0e19367805db436d3e9d995fbc0b6cfcef9893384d23475", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "that are more prone to artifacts or other e\ufb00ects of the instability of the problem. On solutions where\ntypically few artifacts are observed less regularization can be used. Therefore, the learning algorithm\nonly requires a set of labels ( y(i))m\ni=1 as well as a method to assess how hard the inverse problem for this\nlabel would be. In this sense, the algorithm can be considered semi-supervised. This idea was followed,\nfor example, in [ L \u00a8OS18, LSAH20]. Taking a Bayesian viewpoint, one can also learn prior distributions\nas deep NNs, which was done in [BZAJ20].\n3. Unsupervised approaches: One highlight of what we might coin unsupervised approaches in our problem\nsetting is the introduction of deep image priors in [ DKMB20, UVL18]. The key idea is to parametrize\nthe solutions y as the output of a NN \u03a6( \u03be,\u00b7): P\u2192Y with parameters in a suitable space P, applied to\na \ufb01xed input \u03be. Then, for given features x, one tries to solve min\u03b8\u2208P\u2225K\u03a6(\u03be,\u03b8) \u2212x\u22252 in order to obtain\nparameters \u02c6\u03b8\u2208P that yield a solution candidate y= \u03a6(\u03be,\u02c6\u03b8). Here often early stopping is applied in\nthe training of the network parameters.\nAs can be seen, one key conceptual question is how to \u201ctake the best out of both worlds\u201d, in the sense\nof optimally combining classical (model-based) methods\u2014in particular the forward operator K\u2014with deep\nlearning. This is certainly sensitively linked to all characteristics of the particular application at hand, such\nas availability and accuracy of training data, properties of the forward operator, or requirements for the\nsolution. And each of the three classes of hybrid solvers follows a di\ufb00erent strategy.\nLet us now discuss advantages and disadvantages of methods from the three categories with a particular\nfocus on a mathematical foundation. Supervised approaches su\ufb00er on the one hand from the problem that\noften ground-truth data is not available or only in a very distorted form, leading to the fact that synthetic\ndata constitutes a signi\ufb01cant part of the training data. Thus the learned NN will mainly perform as well as\nthe algorithm which generated the data, but not signi\ufb01cantly improve it\u2014only from an e\ufb03ciency viewpoint.\nOn the other hand, the inversion is often highly ill-posed, i.e., the inversion map has a large Lipschitz constant,\nwhich negatively a\ufb00ects the generalization ability of the NN. Improved approaches incorporate knowledge\nabout the forward operator K as discussed, which helps to circumvent this issue.\nOne signi\ufb01cant advantage of semi-supervised approaches is that the underlying mathematical model of\nthe inverse problem is merely augmented by the neural network-based regularization. Assuming that the\nlearned regularizer satis\ufb01es natural assumptions, convergence proofs or stability estimates for the resulting\nregularized methods are still available.\nFinally, unsupervised approaches have the advantage that the regularization is then fully due to the\nspeci\ufb01c architecture of the deep NN. This makes these methods slightly easier to understand theoretically,\nalthough, for instance, the deep prior approach in its full generality is still lacking a profound mathematical\nanalysis.\n8.2 PDE-based models\nBesides applications in image processing and arti\ufb01cial intelligence, deep learning methods have recently\nstrongly impacted the \ufb01eld of numerical analysis. In particular, regarding the numerical solution of high-\ndimensional PDEs. These PDEs are widely used as a model for complex processes and their numerical\nsolution presents one of the biggest challenges in scienti\ufb01c computing. We mention three exemplary problem\nclasses:\n1. Black\u2013Scholes model: The Nobel award-winning theory of Fischer Black, Robert Merton, and Myron\nScholes proposes a linear PDE model for the determination of a fair price of a (complex) \ufb01nancial\nderivative. The dimensionality of the model corresponds to the number of \ufb01nancial assets which is\ntypically quite large. The classical linear model, which can be solved e\ufb03ciently via Monte Carlo methods\nis quite limited. In order to take into account more realistic phenomena such as default risk, the PDE\nthat models a fair price becomes nonlinear, and much more challenging to solve. In particular (with the\nnotable exception of Multilevel Picard algorithms [ EHJK19]) no general algorithm exists that provably\nscales well with the dimension.\n56", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4329, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "35e4a954-31d7-47d9-b34e-bd66ccbf1b08": {"__data__": {"id_": "35e4a954-31d7-47d9-b34e-bd66ccbf1b08", "embedding": null, "metadata": {"page_label": "57", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b9cb15a3-f949-45b7-9e01-bdc0bc178945", "node_type": "4", "metadata": {"page_label": "57", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "679c6eff755658ee7a7f550ff8698f5cb627bbe5aabe0fb67fcfbee67fbe8f11", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b9ddd870-b29e-46c2-a979-5baf94fab778", "node_type": "1", "metadata": {}, "hash": "1d5f821abc419a4b2dd86fc4a56713b938d7f12d0c246a473ff003f5455622c5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "2. Schr\u00a8 odinger equation:The electronic Schr\u00a8 odinger equation describes the stationary nonrelativistic\nbehavior of a quantum mechanical electron system in the electric \ufb01eld generated by the nuclei of\na molecule. Its numerical solution is required to obtain stable molecular con\ufb01gurations, compute\nvibrational spectra, or obtain forces governing molecular dynamics. If the number of electrons is large,\nthis is again a high-dimensional problem and to date there exist no satisfactory algorithms for its\nsolution: It is well known that di\ufb00erent gold standard methods may produce completely di\ufb00erent energy\npredictions, for example, when applied to large delocalized molecules, rendering these methods useless\nfor those problems.\n3. Hamilton\u2013Jacobi\u2013Bellman equation: The Hamilton\u2013Jacobi\u2013Bellman (HJB) equation models the value\nfunction of (deterministic or stochastic) optimal control problems. The underlying dimensionality of the\nmodel corresponds to the dimension of the space of states to be controlled and tends to be rather high\nin realistic applications. The high dimensionality, together with the fact that HJB equations typically\ntend to be fully nonlinear with non-smooth solutions, renders the numerical solution of HJB equations\nextremely challenging and no general algorithms exist for this problem.\nDue to the favorable approximation results of NNs for high-dimensional functions (see especially Subsec-\ntion 4.3), it might not come as a surprise that a NN ansatz has proven to be quite successful in solving the\naforementioned PDE models. A pioneering work in this direction is [ HJE18] which uses the backwards SDE\nreformulation of semilinear parabolic PDEs to reformulate the evaluation of such a PDE at a speci\ufb01c point as\nan optimization problem that can be solved by the deep learning paradigm. The resulting algorithm proves\nquite successful in the high-dimensional regime and, for instance, enables the e\ufb03cient modeling of complex\n\ufb01nancial derivatives including nonlinear e\ufb00ects such as default risk. Another approach speci\ufb01cally tailored to\nthe numerical solution of HJB equations is [ NZGK21]. In this work, one uses the Pontryagin principle to\ngenerate samples of the PDE solution along solutions of the corresponding boundary value problem. Other\nnumerical approaches include the Deep Ritz Method [EY18], where a Dirichlet energy is minimized over a\nset of NNs, or so-called Physics Informed Neural Networks [RPK19], where typically the PDE residual is\nminimized along with some natural constraints, for instance, to enforce boundary conditions.\nDeep-learning-based methods arguably work best if they are combined with domain knowledge to inspire\nNN architecture choices. We would like to illustrate this interplay at the hand of a speci\ufb01c and extremely\nrelevant example: the electronic Schr\u00a8 odinger equation (under the Born\u2013Oppenheimer approximation) which\namounts to \ufb01nding the smallest nonzero eigenvalue of the eigenvalue problem\nHR\u03c8= \u03bb\u03c8\u03c8, (8.1)\nfor \u03c8: R3\u00d7n \u2192R, where the Hamiltonian\n(HR\u03c8)(r) = \u2212\nn\u2211\ni=1\n1\n2(\u2206ri\u03c8)(r) \u2212\n\uf8eb\n\uf8ed\nn\u2211\ni=1\np\u2211\nj=1\nZj\n\u2225ri \u2212Rj\u22252\n\u2212\np\u22121\u2211\ni=1\np\u2211\nj=i+1\nZiZj\n\u2225Ri \u2212Rj\u22252\n\u2212\nn\u22121\u2211\ni=1\nn\u2211\nj=i+1\n1\n\u2225ri \u2212rj\u22252\n\uf8f6\n\uf8f8\u03c8(r)\ndescribes the kinetic energy (\ufb01rst term) as well as Coulomb attraction force between electrons and nuclei\n(second and third term) and the Coulomb repulsion force between di\ufb00erent electrons (third term). Here,\nthe coordinates R=\n[R1 ...R p\n]\n\u2208R3\u00d7p refer to the positions of the nuclei, ( Zi)p\ni=1 \u2208Np denote the atomic\nnumbers of the nuclei, and the coordinates r =\n[r1,...,r n\n]\n\u2208R3\u00d7n refer to the positions of the electrons.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3579, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b9ddd870-b29e-46c2-a979-5baf94fab778": {"__data__": {"id_": "b9ddd870-b29e-46c2-a979-5baf94fab778", "embedding": null, "metadata": {"page_label": "57", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b9cb15a3-f949-45b7-9e01-bdc0bc178945", "node_type": "4", "metadata": {"page_label": "57", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "679c6eff755658ee7a7f550ff8698f5cb627bbe5aabe0fb67fcfbee67fbe8f11", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "35e4a954-31d7-47d9-b34e-bd66ccbf1b08", "node_type": "1", "metadata": {"page_label": "57", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "2f1332d055e7ec6e4c1db990d716ccc3fdc4389aebae6f025f5ed40d4be8e587", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Here,\nthe coordinates R=\n[R1 ...R p\n]\n\u2208R3\u00d7p refer to the positions of the nuclei, ( Zi)p\ni=1 \u2208Np denote the atomic\nnumbers of the nuclei, and the coordinates r =\n[r1,...,r n\n]\n\u2208R3\u00d7n refer to the positions of the electrons.\nThe associated eigenfunction \u03c8 describes the so-called wavefunction which can be interpreted in the sense\nthat |\u03c8(r)|2/\u2225\u03c8\u22252\nL2 describes the joint probability density of the n electrons to be located at r. The smallest\nsolution \u03bb\u03c8 of (8.1) describes the ground state energy associated with the nuclear coordinates R. It is\nof particular interest to know the ground state energy for all nuclear coordinates, the so-called potential\nenergy surface whose gradient determines the forces governing the dynamic motions of the nuclei. The\nnumerical solution of (8.1) is complicated by the Pauli principle which states that the wave function \u03c8 must\nbe antisymmetric in all coordinates representing electrons of equal spin. To state it, we need to clarify that\nevery electron is not only de\ufb01ned by its location but also by its spin which may be positive or negative.\n57", "mimetype": "text/plain", "start_char_idx": 3357, "end_char_idx": 4440, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d9d4f921-607d-4276-a6aa-7f73247e8cce": {"__data__": {"id_": "d9d4f921-607d-4276-a6aa-7f73247e8cce", "embedding": null, "metadata": {"page_label": "58", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5a8d2dcf-bbd5-471f-8a65-1f2a555cb229", "node_type": "4", "metadata": {"page_label": "58", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "05963e9dcfef74f6dcfc01d7d81814f2a3edef70c0860cd28d28131879d9406b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cb37f860-9f5e-4d7f-b527-ff6225663e83", "node_type": "1", "metadata": {}, "hash": "98d3f98709175f5be5055f53aa34fd45b2391d7532a56b80515c117640ff96c1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Depending on whether two electrons have the same spin or not, their interaction changes massively. This is\nre\ufb02ected by the Pauli principle that we already mentioned: Suppose that electrons i and j have equal spin,\nthen the wave function must satisfy\nPi,j\u03c8= \u2212\u03c8, (8.2)\nwhere Pi,j denotes the operator that swaps ri and rj, i.e., ( Pi,j\u03c8)(r) = \u03c8(r1,...,r j,...,r i,...,r n). In\nparticular, no two electrons with the same spin can occupy the same location. The challenges associated with\nsolving the Schr\u00a8 odinger equation inspired the following famous quote by Paul Dirac [Dir29]:\n\u201cThe fundamental laws necessary for the mathematical treatment of a large part of physics and\nthe whole of chemistry are thus completely known, and the di\ufb03culty lies only in the fact that\napplication of these laws leads to equations that are too complex to be solved.\u201d\nWe now describe how deep learning methods might help to mitigate this claim to a certain extent. Let X\nbe a random variable with density |\u03c8(r)|2/\u2225\u03c8\u22252\nL2. Using the Rayleigh\u2013Ritz principle, \ufb01nding the minimal\nnonzero eigenvalue of (8.1) can be reformulated as minimizing the Rayleigh quotient\n\u222b\nR3\u00d7n \u03c8(r)(HR\u03c8)(r) dr\n\u2225\u03c8\u22252\nL2\n= E\n[(HR\u03c8)(X)\n\u03c8(X)\n]\n(8.3)\nover all \u03c8\u2019s satisfying the Pauli principle, see [ SO12]. Since this represents a minimization problem it can in\nprinciple be solved via a NN ansatz by generating training data distributed according to X using MCMC\nsampling31. Since the wave function \u03c8 will be parametrized as a NN, the minimization of (8.3) will require\nthe computation of the gradient of (8.3) with respect to the NN parameters (the method in [ PSMF20] even\nrequires second order derivatives) which, at \ufb01rst sight, might seem to require the computation of third order\nderivatives. However, due to the Hermitian structure of the Hamiltonian one does not need to compute the\nderivative of the Laplacian of \u03c8, see, for example, [HSN20, Equation (8)].\nCompared to the other PDE problems we have discussed, an additional complication arises from the\nneed to incorporate structural properties and invariances such as the Pauli principle. Furthermore, empirical\nevidence shows that it is also necessary to hard code the so-called cusp conditions which describe the\nasymptotic behavior of nearby electrons and electrons close to a nucleus into the NN architecture. A \ufb01rst\nattempt in this direction has been made in [HZE19] and signi\ufb01cantly improved NN architectures have been\ndeveloped in [ HSN20, PSMF20, SRG+21] opening the possibility of accurate ab initio computations for\npreviously intractable molecules. The mathematical properties of this exciting line of work remain largely\nunexplored. We brie\ufb02y describe the main ideas behind the NN architecture of [ HSN20, SRG+21]. Standard\nnumerical approaches (notably the Multireference Hartree Fock Method, see [ SO12]) use a low rank approach\nto minimize (8.3). Such a low rank approach would approximate \u03c8 by sums of products of one electron\norbitals \u220fn\ni=1 \u03d5i(ri) but clearly this does not satisfy the Pauli principle (8.2). In order to ensure the Pauli\nprinciple, one constructs so-called Slater determinants from one electron orbitals with equal spin. More\nprecisely, suppose that the \ufb01rst n+ electrons with coordinates r1,...,r n+ have positive spin and the last\nn\u2212n+ electrons have negative spin. Then any function of the form\ndet\n(\n(\u03d5i(rj))n+\ni,j=1\n)\n\u00b7det\n(\n(\u03d5i(rj))n\ni,j=n++1\n)\n(8.4)\nsatis\ufb01es (8.2) and is typically called a Slater determinant. While the Pauli principle establishes an (non-\nclassical) interaction between electrons of equal spin, the so-called exchange correlation, electrons with\nopposite spins are uncorrelated in the representation (8.4).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3676, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "cb37f860-9f5e-4d7f-b527-ff6225663e83": {"__data__": {"id_": "cb37f860-9f5e-4d7f-b527-ff6225663e83", "embedding": null, "metadata": {"page_label": "58", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5a8d2dcf-bbd5-471f-8a65-1f2a555cb229", "node_type": "4", "metadata": {"page_label": "58", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "05963e9dcfef74f6dcfc01d7d81814f2a3edef70c0860cd28d28131879d9406b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d9d4f921-607d-4276-a6aa-7f73247e8cce", "node_type": "1", "metadata": {"page_label": "58", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "8487f9bf39461c602b8c51fb48234af9a164656d955f4bd9c6a0ecfec47d80d9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In order to ensure the Pauli\nprinciple, one constructs so-called Slater determinants from one electron orbitals with equal spin. More\nprecisely, suppose that the \ufb01rst n+ electrons with coordinates r1,...,r n+ have positive spin and the last\nn\u2212n+ electrons have negative spin. Then any function of the form\ndet\n(\n(\u03d5i(rj))n+\ni,j=1\n)\n\u00b7det\n(\n(\u03d5i(rj))n\ni,j=n++1\n)\n(8.4)\nsatis\ufb01es (8.2) and is typically called a Slater determinant. While the Pauli principle establishes an (non-\nclassical) interaction between electrons of equal spin, the so-called exchange correlation, electrons with\nopposite spins are uncorrelated in the representation (8.4). In particular, (8.4) ignores interactions between\nelectrons that arise through Coulomb forces, implying that no nontrivial wavefunction can be accurately\nrepresented by a single Slater determinant. To capture physical interactions between di\ufb00erent electrons,\none needs to use sums of Slater determinants as an ansatz. However, it turns out that the number of such\ndeterminants that are needed to guarantee a given accuracy scales very badly with the system size n (to the\n31Observe that for such sampling methods one can just use the unnormalized density |\u03c8(r)|2 and thus avoid the computation\nof the normalization \u2225\u03c8\u22252\nL2.\n58", "mimetype": "text/plain", "start_char_idx": 3036, "end_char_idx": 4303, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "4f7863fd-3cd1-464c-bde5-b55891d99915": {"__data__": {"id_": "4f7863fd-3cd1-464c-bde5-b55891d99915", "embedding": null, "metadata": {"page_label": "59", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "21e929ce-3e11-4f56-b6ee-16f41a72cdfc", "node_type": "4", "metadata": {"page_label": "59", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "69a6107d3e07f5618d92c1c685f2bb185a4880cb83250488bb6348e72d1650d2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f79355ba-f756-4286-af95-b3985636e2d7", "node_type": "1", "metadata": {}, "hash": "8b01e300b35cac91f5252928a4865ada09cb5be1342c54514764de3c7a814a99", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "best of our knowledge the best currently known approximation results are contained in [ Yse10], where an\nn-independent error rate is shown, however the implicit constant in this rate depends at least exponentially\non the system size n).\nWe would like to highlight the approach of [ HSN20] whose main idea is to use NNs to incorporate\ninteractions into Slater determinants of the form (8.4) using what is called the back\ufb02ow trick [RMD+06].\nThe basic building blocks would now consist of functions of the form\ndet\n(\n(\u03d5i(rj)\u03a8j(r,\u03b8j))n+\ni,j=1\n)\n\u00b7det\n(\n(\u03d5i(rj)\u03a8j(r,\u03b8j))n\ni,j=n++1\n)\n, (8.5)\nwhere \u03a8k(\u00b7,\u03b8k), k\u2208[n], are NNs. If these are arbitrary NNs, it is easy to see that the Pauli principle (8.2)\nwill not be satis\ufb01ed. However, if we require the NNs to be symmetric, for example, in the sense that for\ni,j,s \u2208[n+] it holds that\nPi,j\u03a8k(\u00b7,\u03b8k) =\n\uf8f1\n\uf8f4\uf8f2\n\uf8f4\uf8f3\n\u03a8k(\u00b7,\u03b8k), if k /\u2208{i,j},\n\u03a8i(\u00b7,\u03b8i), if k= j,\n\u03a8j(\u00b7,\u03b8j), if k= i,\n(8.6)\nand analogous conditions hold for i,j,k \u2208[n] \\[n+], the expression (8.5) does actually satisfy (8.2). The\nconstruction of such symmetric NNs can be achieved by using a modi\ufb01cation of the so-called SchNet\nArchitecture [SKS+17] which can be considered as a speci\ufb01c residual NN.\nWe describe a simpli\ufb01ed construction which is inspired by [ HZE19] and used in a slightly more complex\nform in [SRG+21]. We restrict ourselves to the case of positive spin (e.g., the \ufb01rst n+ coordinates), the case\nof negative spin being handled in the same way. Let \u03a5( \u00b7,\u03b8+\nemb) be a univariate NN (with possibly multivariate\noutput) and denote\nEmbk(r,\u03b8+\nemb) :=\nn+\u2211\ni=1\n\u03a5(\u2225rk \u2212ri\u22252,\u03b8+\nemb), k \u2208[n+],\nthe k-th embedding layer. For k\u2208[n+], we can now de\ufb01ne\n\u03a8k(r,\u03b8k) = \u03a8k\n(\nr,(\u03b8k,fc,\u03b8+\nemb)\n)\n= \u0393k\n((\nEmbk(r,\u03b8+\nemb),(rn++1,...,r n)\n)\n,\u03b8k,fc\n)\n,\nwhere \u0393k(\u00b7,\u03b8k,fc) denotes a standard FC NN with input dimension equal to the output dimension of \u03a8 +\nplus the dimension of negative spin electrons. The networks \u03a8 k, k \u2208[n] \\[n+], are de\ufb01ned analogously\nusing di\ufb00erent parameters \u03b8\u2212\nemb for the embeddings. It is straightforward to check that the NNs \u03a8 k, k\u2208[n],\nsatisfy (8.6) so that the back\ufb02ow determinants (8.5) satisfy the Pauli principle (8.2).\nIn [HSN20] the back\ufb02ow determinants (8.5) are further augmented by a multiplicative correction term,\nthe so-called Jastrow factor which is also represented by a speci\ufb01c symmetric NN, as well as a correction\nterm that ensures the validity of the cusp conditions. The results of [ HSN20] show that this ansatz (namely\nusing linear combinations of back\ufb02ow determinants (8.5) instead of plain Slater determinants (8.4)) is vastly\nmore e\ufb03cient in terms of number of determinants needed to obtain chemical accuracy. The full architecture\nprovides a general purpose NN architecture to represent complicated wave functions. A distinct advantage\nof this approach is that some parameters (for example, embedding layers) may be shared across di\ufb00erent\nnuclear geometries R\u2208R3\u00d7p which allows for the e\ufb03cient computation of potential energy surfaces [SRG+21],\nsee Figure 8.1.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2996, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f79355ba-f756-4286-af95-b3985636e2d7": {"__data__": {"id_": "f79355ba-f756-4286-af95-b3985636e2d7", "embedding": null, "metadata": {"page_label": "59", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "21e929ce-3e11-4f56-b6ee-16f41a72cdfc", "node_type": "4", "metadata": {"page_label": "59", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "69a6107d3e07f5618d92c1c685f2bb185a4880cb83250488bb6348e72d1650d2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4f7863fd-3cd1-464c-bde5-b55891d99915", "node_type": "1", "metadata": {"page_label": "59", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "7bb66df619561457fec9c7a4fdd45061f181a1913c92024a6a5b8949aac11b2b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The results of [ HSN20] show that this ansatz (namely\nusing linear combinations of back\ufb02ow determinants (8.5) instead of plain Slater determinants (8.4)) is vastly\nmore e\ufb03cient in terms of number of determinants needed to obtain chemical accuracy. The full architecture\nprovides a general purpose NN architecture to represent complicated wave functions. A distinct advantage\nof this approach is that some parameters (for example, embedding layers) may be shared across di\ufb00erent\nnuclear geometries R\u2208R3\u00d7p which allows for the e\ufb03cient computation of potential energy surfaces [SRG+21],\nsee Figure 8.1. Finally, we would like to highlight the customized NN design that incorporates physical\ninvariances, domain knowledge (for example, in the form of cusp conditions), and existing numerical methods,\nall of which are required for the method to reach its full potential.\nAcknowledgment\nThe research of JB was supported by the Austrian Science Fund (FWF) under grant I3403-N32. GK\nacknowledges support from DFG-SPP 1798 Grants KU 1446/21-2 and KU 1446/27-2, DFG-SFB/TR 109\nGrant C09, BMBF Grant MaGriDo, and NSF-Simons Foundation Grant SIMONS 81420. The authors would\n59", "mimetype": "text/plain", "start_char_idx": 2397, "end_char_idx": 3562, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5401fa29-c9fb-4f3f-acc5-483f17e6799e": {"__data__": {"id_": "5401fa29-c9fb-4f3f-acc5-483f17e6799e", "embedding": null, "metadata": {"page_label": "60", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8a45578d-5044-4d5f-9bf5-aef629d70917", "node_type": "4", "metadata": {"page_label": "60", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "94ae50c3ac5b5812639b29233d7a42d7ee1a69ebf11f02bfbec69240d0bdc239", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Figure 8.1: By sharing layers across di\ufb00erent nuclear geometries one can e\ufb03ciently compute di\ufb00erent\ngeometries in one single training step [ SRG+21]. Left: Potential energy surface of H10 chain computed by the\ndeep-learning-based algorithm from [ SRG+21]. The lowest energy is achieved when pairs of H atoms enter\ninto a covalent bond to form \ufb01ve H2 molecules. Right: The method of [ SRG+21] is capable of accurately\ncomputing forces between nuclei which allows for molecular dynamics simulations from \ufb01rst principles.\nlike to thank H\u00b4 ector Andrade Loarca, Dennis Elbr\u00a8 achter, Adalbert Fono, Pavol Harar, Lukas Liehr, Duc Anh\nNguyen, Mariia Seleznova, and Frieder Simon for their helpful feedback on an early version of this article. In\nparticular, Dennis Elbr\u00a8 achter was providing help for several theoretical results.\nReferences\n[AA\u02c7C13] Antonio Au\ufb03nger, G\u00b4 erard Ben Arous, and Ji\u02c7 r\u00b4 \u0131\u02c7Cern` y,Random matrices and complexity of spin\nglasses, Communications on Pure and Applied Mathematics 66 (2013), no. 2, 165\u2013201.\n[AB99] Martin Anthony and Peter L Bartlett, Neural network learning: Theoretical foundations ,\nCambridge University Press, 1999.\n[ACGH19] Sanjeev Arora, Nadav Cohen, Noah Golowich, and Wei Hu, A convergence analysis of gradient\ndescent for deep linear neural networks , International Conference on Learning Representations,\n2019.\n[ACH18] Sanjeev Arora, Nadav Cohen, and Elad Hazan, On the optimization of deep networks: Implicit\nacceleration by overparameterization , International Conference on Machine Learning, 2018,\npp. 372\u2013389.\n[AD20] Ben Adcock and Nick Dexter, The gap between theory and practice in function approximation\nwith deep neural networks , 2020, arXiv preprint arXiv:2001.07523.\n[ADH+19] Sanjeev Arora, Simon S Du, Wei Hu, Zhiyuan Li, Ruslan Salakhutdinov, and Ruosong Wang, On\nexact computation with an in\ufb01nitely wide neural net , Advances in Neural Information Processing\nSystems, 2019, pp. 8139\u20138148.\n[AGNZ18] Sanjeev Arora, Rong Ge, Behnam Neyshabur, and Yi Zhang, Stronger generalization bounds\nfor deep nets via a compression approach , International Conference on Machine Learning, 2018,\npp. 254\u2013263.\n[AHNB+20] Yasmine S Al-Hamdani, P\u00b4 eter R Nagy, Dennis Barton, Mih\u00b4 aly K\u00b4 allay, Jan Gerit Brandenburg,\nand Alexandre Tkatchenko, Interactions between large molecules: Puzzle for reference quantum-\nmechanical methods, 2020, arXiv preprint arXiv:2009.08927.\n60", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2409, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "2930f660-dac4-4385-ab52-ad2be66041dd": {"__data__": {"id_": "2930f660-dac4-4385-ab52-ad2be66041dd", "embedding": null, "metadata": {"page_label": "61", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1a5fc65b-7546-48c3-ba29-614b6df88236", "node_type": "4", "metadata": {"page_label": "61", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "d424a63e583d156c8d8c92950451bf838961fc40504a2d5cda5f4567f8c2da9c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[AHS85] David H Ackley, Geo\ufb00rey E Hinton, and Terrence J Sejnowski, A learning algorithm for\nBoltzmann machines, Cognitive Science 9 (1985), no. 1, 147\u2013169.\n[AHW96] Peter Auer, Mark Herbster, and Manfred K Warmuth, Exponentially many local minima for\nsingle neurons, Advances in Neural Information Processing Systems, 1996, p. 316\u2013322.\n[AM\u00a8OS19] Simon Arridge, Peter Maass, Ozan \u00a8Oktem, and Carola-Bibiane Sch\u00a8 onlieb, Solving inverse\nproblems using data-driven models , Acta Numerica 28 (2019), 1\u2013174.\n[A\u00a8O17] Jonas Adler and Ozan \u00a8Oktem, Solving ill-posed inverse problems using iterative deep neural\nnetworks, Inverse Problems 33 (2017), no. 12, 124007.\n[AZLS19] Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song, A convergence theory for deep learning via\nover-parameterization, International Conference on Machine Learning, 2019, pp. 242\u2013252.\n[Bar92] Andrew R Barron, Neural net approximation, Yale Workshop on Adaptive and Learning Systems,\nvol. 1, 1992, pp. 69\u201372.\n[Bar93] , Universal approximation bounds for superpositions of a sigmoidal function , IEEE\nTransactions on Information Theory 39 (1993), no. 3, 930\u2013945.\n[Bar98] Peter L Bartlett, The sample complexity of pattern classi\ufb01cation with neural networks: the size\nof the weights is more important than the size of the network , IEEE Transactions on Information\nTheory 44 (1998), no. 2, 525\u2013536.\n[BBC17] Alfred Bourely, John Patrick Boueri, and Krzysztof Choromonski, Sparse neural networks\ntopologies, 2017, arXiv preprint arXiv:1706.05683.\n[BBC+19] Christopher Berner, Greg Brockman, Brooke Chan, Vicki Cheung, Przemyslaw Debiak, Christy\nDennison, David Farhi, Quirin Fischer, Shariq Hashme, and Chris Hesse, Dota 2 with large scale\ndeep reinforcement learning, 2019, arXiv preprint arXiv:1912.06680.\n[BBG+21] Christian Beck, Sebastian Becker, Philipp Grohs, Nor Jaafari, and Arnulf Jentzen, Solving the\nkolmogorov pde by means of deep learning , Journal of Scienti\ufb01c Computing 88 (2021), no. 3,\n1\u201328.\n[BBL03] Olivier Bousquet, St\u00b4 ephane Boucheron, and G\u00b4 abor Lugosi,Introduction to statistical learning\ntheory, Summer School on Machine Learning, 2003, pp. 169\u2013207.\n[BBL+17] Michael M Bronstein, Joan Bruna, Yann LeCun, Arthur Szlam, and Pierre Vandergheynst,\nGeometric deep learning: going beyond euclidean data , IEEE Signal Processing Magazine 34\n(2017), no. 4, 18\u201342.\n[BBM05] Peter L Bartlett, Olivier Bousquet, and Shahar Mendelson, Local Rademacher complexities, The\nAnnals of Statistics 33 (2005), no. 4, 1497\u20131537.\n[BCB15] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio, Neural machine translation by jointly\nlearning to align and translate , International Conference on Learning Representations, 2015.\n[BDG20] Julius Berner, Markus Dablander, and Philipp Grohs, Numerically solving parametric families\nof high-dimensional Kolmogorov partial di\ufb00erential equations via deep learning , Advances in\nNeural Information Processing Systems, 2020, pp. 16615\u201316627.\n[BE02] Olivier Bousquet and Andr\u00b4 e Elissee\ufb00,Stability and generalization , Journal of Machine Learning\nResearch 2 (2002), no. Mar, 499\u2013526.\n61", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3075, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b6d570f9-739c-4d9f-b7b8-e54a825b2a05": {"__data__": {"id_": "b6d570f9-739c-4d9f-b7b8-e54a825b2a05", "embedding": null, "metadata": {"page_label": "62", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "40643ffa-8800-43b2-8e36-59f56a8b69c2", "node_type": "4", "metadata": {"page_label": "62", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "79e3155b2f703d2bf842bfed7af262a2293b53a21426bf1f8232168088ec7531", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[BEG19] Julius Berner, Dennis Elbr\u00a8 achter, and Philipp Grohs,How degenerate is the parametrization of\nneural networks with the ReLU activation function? , Advances in Neural Information Processing\nSystems, 2019, pp. 7790\u20137801.\n[Bel52] Richard Bellman, On the theory of dynamic programming , Proceedings of the National Academy\nof Sciences 38 (1952), no. 8, 716.\n[BF19] Jan Bohn and Michael Feischl, Recurrent neural networks as optimal mesh re\ufb01nement strategies ,\n2019, arXiv preprint arXiv:1909.04275.\n[BFT17] Peter L Bartlett, Dylan J Foster, and Matus Telgarsky, Spectrally-normalized margin bounds for\nneural networks, Advances in Neural Information Processing Systems, 2017, pp. 6240\u20136249.\n[BGJ20] Julius Berner, Philipp Grohs, and Arnulf Jentzen, Analysis of the generalization error: Empirical\nrisk minimization over deep arti\ufb01cial neural networks overcomes the curse of dimensionality in\nthe numerical approximation of black\u2013scholes partial di\ufb00erential equations , SIAM Journal on\nMathematics of Data Science 2 (2020), no. 3, 631\u2013657.\n[BH89] Eric B Baum and David Haussler, What size net gives valid generalization? , Neural Computation\n1 (1989), no. 1, 151\u2013160.\n[BHLM19] Peter L Bartlett, Nick Harvey, Christopher Liaw, and Abbas Mehrabian, Nearly-tight VC-\ndimension and pseudodimension bounds for piecewise linear neural networks , Journal of Machine\nLearning Research 20 (2019), 63\u20131.\n[BHMM19] Mikhail Belkin, Daniel Hsu, Siyuan Ma, and Soumik Mandal, Reconciling modern machine-\nlearning practice and the classical bias\u2013variance trade-o\ufb00 , Proceedings of the National Academy\nof Sciences 116 (2019), no. 32, 15849\u201315854.\n[BHX20] Mikhail Belkin, Daniel Hsu, and Ji Xu, Two models of double descent for weak features , SIAM\nJournal on Mathematics of Data Science 2 (2020), no. 4, 1167\u20131180.\n[BK18] Andrew R Barron and Jason M Klusowski, Approximation and estimation for high-dimensional\ndeep learning networks, 2018, arXiv preprint arXiv:1809.03090.\n[BLLT20] Peter L Bartlett, Philip M Long, G\u00b4 abor Lugosi, and Alexander Tsigler, Benign over\ufb01tting\nin linear regression , Proceedings of the National Academy of Sciences 117 (2020), no. 48,\n30063\u201330070.\n[BMM98] Peter L Bartlett, Vitaly Maiorov, and Ron Meir, Almost linear VC-dimension bounds for\npiecewise polynomial networks, Neural Computation 10 (1998), no. 8, 2159\u20132173.\n[BMM18] Mikhail Belkin, Siyuan Ma, and Soumik Mandal, To understand deep learning we need to\nunderstand kernel learning , International Conference on Machine Learning, 2018, pp. 541\u2013549.\n[BMR+20] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel\nHerbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler,\nJe\ufb00rey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray,\nBenjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever,\nand Dario Amodei, Language models are few-shot learners , Advances in Neural Information\nProcessing Systems, 2020, pp. 1877\u20131901.\n[BR89] Avrim Blum and Ronald L Rivest, Training a 3-node neural network is NP-complete , Advances\nin Neural Information Processing Systems, 1989, pp. 494\u2013501.\n62", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3279, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "7fad35e5-2200-43a8-b31a-bbe695837b6a": {"__data__": {"id_": "7fad35e5-2200-43a8-b31a-bbe695837b6a", "embedding": null, "metadata": {"page_label": "63", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5835ceb4-48bc-4ca8-a95e-4b1e4e120339", "node_type": "4", "metadata": {"page_label": "63", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "5e40df3c25206ee38ace277a487d0911c02162c7624c36619d692f306bfb834a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ecdee644-f8bf-456a-98c4-68f67ae521e8", "node_type": "1", "metadata": {}, "hash": "4c808a4e6929c9438cf59f5a8972a56a3119aa3383e96b86a1602653edfb43c0", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[BRT19] Mikhail Belkin, Alexander Rakhlin, and Alexandre B Tsybakov, Does data interpolation con-\ntradict statistical optimality?, International Conference on Arti\ufb01cial Intelligence and Statistics,\n2019, pp. 1611\u20131619.\n[BSW14] Pierre Baldi, Peter Sadowski, and Daniel Whiteson, Searching for exotic particles in high-energy\nphysics with deep learning , Nature Communications 5 (2014), no. 1, 1\u20139.\n[BZAJ20] Riccardo Barbano, Chen Zhang, Simon Arridge, and Bangti Jin, Quantifying model uncertainty\nin inverse problems via bayesian deep gradient descent , 2020, arXiv preprint arXiv:2007.09971.\n[Can98] Emmanuel J Cand` es,Ridgelets: Theory and applications , Ph.D. thesis, Stanford University,\n1998.\n[CB20] Lenaic Chizat and Francis Bach, Implicit bias of gradient descent for wide two-layer neural\nnetworks trained with the logistic loss , Conference on Learning Theory, 2020, pp. 1305\u20131338.\n[CHM+15] Anna Choromanska, Mikael Hena\ufb00, Michael Mathieu, G\u00b4 erard Ben Arous, and Yann LeCun,\nThe loss surfaces of multilayer networks , International Conference on Arti\ufb01cial Intelligence and\nStatistics, 2015, pp. 192\u2013204.\n[CJLZ19] Minshuo Chen, Haoming Jiang, Wenjing Liao, and Tuo Zhao, E\ufb03cient approximation of deep\nReLU networks for functions on low dimensional manifolds , Advances in Neural Information\nProcessing Systems, 2019, pp. 8174\u20138184.\n[CK20] Alexander Cloninger and Timo Klock, ReLU nets adapt to intrinsic dimensionality beyond the\ntarget domain, 2020, arXiv preprint arXiv:2008.02545.\n[CKP12] Peter G Casazza, Gitta Kutyniok, and Friedrich Philipp, Introduction to \ufb01nite frame theory ,\nFinite Frames: Theory and Applications, Birkh\u00a8 auser Boston, 2012, pp. 1\u201353.\n[CL19] Wojciech Czaja and Weilin Li, Analysis of time-frequency scattering transforms , Applied and\nComputational Harmonic Analysis 47 (2019), no. 1, 149\u2013171.\n[CLA15] Anna Choromanska, Yann LeCun, and G\u00b4 erard Ben Arous,Open problem: The landscape of the\nloss surfaces of multilayer networks , Conference on Learning Theory, 2015, pp. 1756\u20131760.\n[CLM94] Charles K Chui, Xin Li, and Hrushikesh N Mhaskar, Neural networks for localized approximation,\nMathematics of Computation 63 (1994), no. 208, 607\u2013623.\n[CM18] Charles K Chui and Hrushikesh N Mhaskar, Deep nets for local manifold learning , Frontiers in\nApplied Mathematics and Statistics 4 (2018), 12.\n[CMBK20] Lin Chen, Yifei Min, Mikhail Belkin, and Amin Karbasi, Multiple descent: Design your own\ngeneralization curve, 2020, arXiv preprint arXiv:2008.01036.\n[COB19] Lenaic Chizat, Edouard Oyallon, and Francis Bach, On lazy training in di\ufb00erentiable program-\nming, Advances in Neural Information Processing Systems, 2019, pp. 2937\u20132947.\n[CPV20] Andrei Caragea, Philipp Petersen, and Felix Voigtlaender, Neural network approximation and\nestimation of classi\ufb01ers with classi\ufb01cation boundary in a Barron class , 2020, arXiv preprint\narXiv:2011.09363.\n[CS02] Felipe Cucker and Steve Smale, On the mathematical foundations of learning , Bulletin of the\nAmerican Mathematical Society 39 (2002), no. 1, 1\u201349.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3029, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ecdee644-f8bf-456a-98c4-68f67ae521e8": {"__data__": {"id_": "ecdee644-f8bf-456a-98c4-68f67ae521e8", "embedding": null, "metadata": {"page_label": "63", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5835ceb4-48bc-4ca8-a95e-4b1e4e120339", "node_type": "4", "metadata": {"page_label": "63", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "5e40df3c25206ee38ace277a487d0911c02162c7624c36619d692f306bfb834a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7fad35e5-2200-43a8-b31a-bbe695837b6a", "node_type": "1", "metadata": {"page_label": "63", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "4b4a5245e36fc15221abd11d522c8413fd5c2c6500eb7b943e106e8d24fd9ca3", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[COB19] Lenaic Chizat, Edouard Oyallon, and Francis Bach, On lazy training in di\ufb00erentiable program-\nming, Advances in Neural Information Processing Systems, 2019, pp. 2937\u20132947.\n[CPV20] Andrei Caragea, Philipp Petersen, and Felix Voigtlaender, Neural network approximation and\nestimation of classi\ufb01ers with classi\ufb01cation boundary in a Barron class , 2020, arXiv preprint\narXiv:2011.09363.\n[CS02] Felipe Cucker and Steve Smale, On the mathematical foundations of learning , Bulletin of the\nAmerican Mathematical Society 39 (2002), no. 1, 1\u201349.\n[CvMG+14] Kyunghyun Cho, Bart van Merri\u00a8 enboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares,\nHolger Schwenk, and Yoshua Bengio, Learning phrase representations using rnn encoder\u2013decoder\nfor statistical machine translation , Proceedings of the 2014 Conference on Empirical Methods in\nNatural Language Processing, 2014, pp. 1724\u20131734.\n63", "mimetype": "text/plain", "start_char_idx": 2486, "end_char_idx": 3372, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a3fe6483-a996-4ebf-a1b0-ed60566c7a04": {"__data__": {"id_": "a3fe6483-a996-4ebf-a1b0-ed60566c7a04", "embedding": null, "metadata": {"page_label": "64", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2539089e-3c01-4b47-9494-0965758b3da6", "node_type": "4", "metadata": {"page_label": "64", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "593873d095efcab4a86c907a4840184b0eceffaf55e5846d05cbb3b96dfbbbae", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[Cyb89] George Cybenko, Approximation by superpositions of a sigmoidal function , Mathematics of\nControl, Signals and Systems 2 (1989), no. 4, 303\u2013314.\n[CZ07] Felipe Cucker and Ding-Xuan Zhou, Learning theory: an approximation theory viewpoint, vol. 24,\nCambridge University Press, 2007.\n[DDS+09] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei, Imagenet: A large-scale\nhierarchical image database , Proceedings of the IEEE Conference on Computer Vision and\nPattern Recognition, 2009, pp. 248\u2013255.\n[DeV98] Ronald A DeVore, Nonlinear approximation, Acta Numerica 7 (1998), 51\u2013150.\n[DGL96] Luc Devroye, L\u00b4 aszl\u00b4 o Gy\u00a8 or\ufb01, and G\u00b4 abor Lugosi,A probabilistic theory of pattern recognition ,\nSpringer, 1996.\n[DHL18] Simon S Du, Wei Hu, and Jason D Lee, Algorithmic regularization in learning deep homogeneous\nmodels: Layers are automatically balanced , Advances in Neural Information Processing Systems,\n2018, pp. 384\u2013395.\n[DHP20] Ronald DeVore, Boris Hanin, and Guergana Petrova, Neural network approximation, 2020, arXiv\npreprint arXiv:2012.14501.\n[Dir29] Paul Adrien Maurice Dirac, Quantum mechanics of many-electron systems , Proceedings of the\nRoyal Society of London. Series A, Containing Papers of a Mathematical and Physical Character\n123 (1929), no. 792, 714\u2013733.\n[DKMB20] S\u00a8 oren Dittmer, Tobias Kluth, Peter Maass, and Daniel Otero Baguer, Regularization by ar-\nchitecture: A deep prior approach for inverse problems , Journal of Mathematical Imaging and\nVision 62 (2020), no. 3, 456\u2013470.\n[DLL+19] Simon S Du, Jason D Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai, Gradient descent \ufb01nds\nglobal minima of deep neural networks , International Conference on Machine Learning, 2019,\npp. 1675\u20131685.\n[Don69] William F Donoghue, Distributions and fourier transforms , Pure and Applied Mathematics,\nAcademic Press, 1969.\n[DPG+14] Yann N Dauphin, Razvan Pascanu, Caglar Gulcehre, Kyunghyun Cho, Surya Ganguli, and\nYoshua Bengio, Identifying and attacking the saddle point problem in high-dimensional non-\nconvex optimization, Advances in Neural Information Processing Systems, 2014, pp. 2933\u20132941.\n[DR17] Gintare Karolina Dziugaite and Daniel M Roy, Computing nonvacuous generalization bounds for\ndeep (stochastic) neural networks with many more parameters than training data , Conference on\nUncertainty in Arti\ufb01cial Intelligence, 2017.\n[Dre62] Stuart Dreyfus, The numerical solution of variational problems , Journal of Mathematical\nAnalysis and Applications 5 (1962), no. 1, 30\u201345.\n[Dud67] Richard M Dudley, The sizes of compact subsets of hilbert space and continuity of Gaussian\nprocesses, Journal of Functional Analysis 1 (1967), no. 3, 290\u2013330.\n[Dud14] , Uniform central limit theorems, vol. 142, Cambridge University Press, 2014.\n[DZPS18] Simon S Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh, Gradient descent provably optimizes\nover-parameterized neural networks, International Conference on Learning Representations, 2018.\n[E17] Weinan E, A proposal on machine learning via dynamical systems , Communications in Mathe-\nmatics and Statistics 5 (2017), no. 1, 1\u201311.\n64", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3096, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3ac4883c-6d48-4d8c-ad80-72af3481f0ba": {"__data__": {"id_": "3ac4883c-6d48-4d8c-ad80-72af3481f0ba", "embedding": null, "metadata": {"page_label": "65", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b52beb20-a889-48d0-bd38-048e254e362f", "node_type": "4", "metadata": {"page_label": "65", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "277d4a69886b66c8bb6fde25ea28cc5ccefcf0ee5287d4c4c96ca7d0ea4cadf4", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[EGJS18] Dennis Elbr\u00a8 achter, Philipp Grohs, Arnulf Jentzen, and Christoph Schwab, DNN expression\nrate analysis of high-dimensional PDEs: Application to option pricing , 2018, arXiv preprint\narXiv:1809.07669.\n[EHJK19] Weinan E, Martin Hutzenthaler, Arnulf Jentzen, and Thomas Kruse, On multilevel picard\nnumerical approximations for high-dimensional nonlinear parabolic partial di\ufb00erential equations\nand high-dimensional nonlinear backward stochastic di\ufb00erential equations , Journal of Scienti\ufb01c\nComputing 79 (2019), no. 3, 1534\u20131571.\n[EHL19] Weinan E, Jiequn Han, and Qianxiao Li, A mean-\ufb01eld optimal control formulation of deep\nlearning, Research in the Mathematical Sciences 6 (2019), no. 1, 1\u201341.\n[Elm90] Je\ufb00rey L Elman, Finding structure in time , Cognitive Science 14 (1990), no. 2, 179\u2013211.\n[EMW19a] Weinan E, Chao Ma, and Lei Wu, Barron spaces and the compositional function spaces for\nneural network models, 2019, arXiv preprint arXiv:1906.08039.\n[EMW19b] , A priori estimates of the population risk for two-layer neural networks , Communications\nin Mathematical Sciences 17 (2019), no. 5, 1407\u20131425.\n[EMWW20] Weinan E, Chao Ma, Stephan Wojtowytsch, and Lei Wu,Towards a mathematical understanding\nof neural network-based machine learning: what we know and what we don\u2019t , 2020, arXiv preprint\narXiv:2009.10713.\n[EPGB19] Dennis Elbr\u00a8 achter, Dmytro Perekrestenko, Philipp Grohs, and Helmut B\u00a8 olcskei,Deep neural\nnetwork approximation theory, 2019, arXiv preprint arXiv:1901.02220.\n[ES16] Ronen Eldan and Ohad Shamir, The power of depth for feedforward neural networks , Conference\non Learning Theory, vol. 49, 2016, pp. 907\u2013940.\n[EW20a] Weinan E and Stephan Wojtowytsch, On the Banach spaces associated with multi-layer ReLU\nnetworks: Function representation, approximation theory and gradient descent dynamics , 2020,\narXiv preprint arXiv:2007.15623.\n[EW20b] , A priori estimates for classi\ufb01cation problems using neural networks, 2020, arXiv preprint\narXiv:2009.13500.\n[EW20c] , Representation formulas and pointwise properties for Barron functions , 2020, arXiv\npreprint arXiv:2006.05982.\n[EY18] Weinan E and Bing Yu, The deep ritz method: a deep learning-based numerical algorithm for\nsolving variational problems , Communications in Mathematics and Statistics 6 (2018), no. 1,\n1\u201312.\n[FB17] Daniel C Freeman and Joan Bruna, Topology and geometry of half-recti\ufb01ed network optimization,\nInternational Conference on Learning Representations, 2017.\n[FC18] Jonathan Frankle and Michael Carbin, The lottery ticket hypothesis: Finding sparse, trainable\nneural networks, International Conference on Learning Representations, 2018.\n[FHH+17] Felix A Faber, Luke Hutchison, Bing Huang, Justin Gilmer, Samuel S Schoenholz, George E\nDahl, Oriol Vinyals, Steven Kearnes, Patrick F Riley, and O Anatole Von Lilienfeld, Prediction\nerrors of molecular machine learning models lower than hybrid DFT error , Journal of Chemical\nTheory and Computation 13 (2017), no. 11, 5255\u20135264.\n[Fun89] Ken-Ichi Funahashi, On the approximate realization of continuous mappings by neural networks ,\nNeural Networks 2 (1989), no. 3, 183\u2013192.\n65", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3122, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5a7d29a2-48b8-4f70-9439-7fb311d1d6f0": {"__data__": {"id_": "5a7d29a2-48b8-4f70-9439-7fb311d1d6f0", "embedding": null, "metadata": {"page_label": "66", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "deb3df48-8344-41db-abd5-930649e70bb2", "node_type": "4", "metadata": {"page_label": "66", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "b420680c970cfab2fa66fbfad3c6aa8cff01b01230de57bd731ec6a1c798312e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "71c3ece8-85d4-40f3-ad05-069a7bebc348", "node_type": "1", "metadata": {}, "hash": "265f5974304ecbaa48f1aaf5e40eecf2580c522eb0147ba7213afb6c06834ba7", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[GBC16] Ian Goodfellow, Yoshua Bengio, and Aaron Courville, Deep learning, MIT Press, 2016.\n[G\u00b4 er17] Aurelien G\u00b4 eron,Hands-on machine learning with scikit-learn and tensor\ufb02ow: Concepts, tools,\nand techniques to build intelligent systems , O\u2019Reilly Media, 2017.\n[GH21] Philipp Grohs and Lukas Herrmann, Deep neural network approximation for high-dimensional\nparabolic Hamilton-Jacobi-Bellman equations, 2021, arXiv preprint arXiv:2103.05744.\n[GH22] , Deep neural network approximation for high-dimensional elliptic PDEs with boundary\nconditions, IMA Journal of Numerical Analysis 42 (2022), no. 3, 2055\u20132082.\n[GHJVW20] Philipp Grohs, Fabian Hornung, Arnulf Jentzen, and Philippe Von Wurstemberger, A proof that\narti\ufb01cial neural networks overcome the curse of dimensionality in the numerical approximation\nof Black-Scholes partial di\ufb00erential equations , Memoirs of the American Mathematical Society\n(2020).\n[GHJY15] Rong Ge, Furong Huang, Chi Jin, and Yang Yuan,Escaping from saddle points\u2014online stochastic\ngradient for tensor decomposition, Conference on Learning Theory, 2015, pp. 797\u2013842.\n[GJS+20] Mario Geiger, Arthur Jacot, Stefano Spigler, Franck Gabriel, Levent Sagun, St\u00b4 ephane d\u2019Ascoli,\nGiulio Biroli, Cl\u00b4 ement Hongler, and Matthieu Wyart,Scaling description of generalization with\nnumber of parameters in deep learning, Journal of Statistical Mechanics: Theory and Experiment\n(2020), no. 2, 023401.\n[GKP20] Ingo G\u00a8 uhring, Gitta Kutyniok, and Philipp Petersen,Error bounds for approximations with deep\nReLU neural networks in Ws,p norms, Analysis and Applications 18 (2020), no. 05, 803\u2013859.\n[GKR20] Philipp Grohs, Sarah Koppensteiner, and Martin Rathmair, Phase retrieval: Uniqueness and\nstability, SIAM Review 62 (2020), no. 2, 301\u2013350.\n[GL13] Saeed Ghadimi and Guanghui Lan, Stochastic \ufb01rst-and zeroth-order methods for nonconvex\nstochastic programming, SIAM Journal on Optimization 23 (2013), no. 4, 2341\u20132368.\n[GLSS18a] Suriya Gunasekar, Jason D Lee, Daniel Soudry, and Nathan Srebro, Characterizing implicit\nbias in terms of optimization geometry , International Conference on Machine Learning, 2018,\npp. 1832\u20131841.\n[GLSS18b] , Implicit bias of gradient descent on linear convolutional networks , Advances in Neural\nInformation Processing Systems, 2018, pp. 9461\u20139471.\n[GMMM21] Behrooz Ghorbani, Song Mei, Theodor Misiakiewicz, and Andrea Montanari, Linearized two-\nlayers neural networks in high dimension , The Annals of Statistics 49 (2021), no. 2, 1029\u20131054.\n[GOW19] Davis Gilton, Greg Ongie, and Rebecca Willett, Neumann networks for linear inverse problems\nin imaging, IEEE Transactions on Computational Imaging 6 (2019), 328\u2013343.\n[GPAM+14] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil\nOzair, Aaron Courville, and Yoshua Bengio, Generative adversarial nets , Advances in Neural\nInformation Processing Systems, 2014, pp. 2672\u20132680.\n[GRK20] Ingo G\u00a8 uhring, Mones Raslan, and Gitta Kutyniok,Expressivity of deep neural networks , 2020,\narXiv preprint arXiv:2007.04759.\n[GRS18] Noah Golowich, Alexander Rakhlin, and Ohad Shamir, Size-independent sample complexity of\nneural networks, Conference On Learning Theory, 2018, pp. 297\u2013299.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3196, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "71c3ece8-85d4-40f3-ad05-069a7bebc348": {"__data__": {"id_": "71c3ece8-85d4-40f3-ad05-069a7bebc348", "embedding": null, "metadata": {"page_label": "66", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "deb3df48-8344-41db-abd5-930649e70bb2", "node_type": "4", "metadata": {"page_label": "66", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "b420680c970cfab2fa66fbfad3c6aa8cff01b01230de57bd731ec6a1c798312e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5a7d29a2-48b8-4f70-9439-7fb311d1d6f0", "node_type": "1", "metadata": {"page_label": "66", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "9150d4c72b70a7c9a9707803669432a0222ef461d46f93cc9bddccb712c502d9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[GPAM+14] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil\nOzair, Aaron Courville, and Yoshua Bengio, Generative adversarial nets , Advances in Neural\nInformation Processing Systems, 2014, pp. 2672\u20132680.\n[GRK20] Ingo G\u00a8 uhring, Mones Raslan, and Gitta Kutyniok,Expressivity of deep neural networks , 2020,\narXiv preprint arXiv:2007.04759.\n[GRS18] Noah Golowich, Alexander Rakhlin, and Ohad Shamir, Size-independent sample complexity of\nneural networks, Conference On Learning Theory, 2018, pp. 297\u2013299.\n[GS20] Lukas Gonon and Christoph Schwab, Deep ReLU network expression rates for option prices in\nhigh-dimensional, exponential L\u00b4 evy models, 2020, ETH Zurich SAM Research Report.\n66", "mimetype": "text/plain", "start_char_idx": 2657, "end_char_idx": 3379, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "658b06c4-f315-416c-8279-48d2f79cb6e9": {"__data__": {"id_": "658b06c4-f315-416c-8279-48d2f79cb6e9", "embedding": null, "metadata": {"page_label": "67", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "77469f62-cb8e-4cac-8417-5abb9faad643", "node_type": "4", "metadata": {"page_label": "67", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "a2cfae4222049737fe692c74e904bcab2cce44f232e1b8f528f58e1c4c10af7c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[GV21] Philipp Grohs and Felix Voigtlaender, Proof of the theory-to-practice gap in deep learning via\nsampling complexity bounds for neural network approximation spaces , 2021, arXiv preprint\narXiv:2104.02746.\n[GW08] Andreas Griewank and Andrea Walther, Evaluating derivatives: principles and techniques of\nalgorithmic di\ufb00erentiation, SIAM, 2008.\n[GZ84] Evarist Gin\u00b4 e and Joel Zinn, Some limit theorems for empirical processes , The Annals of\nProbability (1984), 929\u2013989.\n[Han19] Boris Hanin, Universal function approximation by deep neural nets with bounded width and\nReLU activations, Mathematics 7 (2019), no. 10, 992.\n[Hau95] David Haussler, Sphere packing numbers for subsets of the boolean n-cube with bounded vapnik-\nchervonenkis dimension, Journal of Combinatorial Theory, Series A 2 (1995), no. 69, 217\u2013232.\n[HH19] Catherine F Higham and Desmond J Higham, Deep learning: An introduction for applied\nmathematicians, SIAM Review 61 (2019), no. 4, 860\u2013891.\n[HHJ15] Martin Hairer, Martin Hutzenthaler, and Arnulf Jentzen, Loss of regularity for Kolmogorov\nequations, The Annals of Probability 43 (2015), no. 2, 468\u2013527.\n[HJE18] Jiequn Han, Arnulf Jentzen, and Weinan E,Solving high-dimensional partial di\ufb00erential equations\nusing deep learning, Proceedings of the National Academy of Sciences 115 (2018), no. 34, 8505\u2013\n8510.\n[HJKN20] Martin Hutzenthaler, Arnulf Jentzen, Thomas Kruse, and Tuan Anh Nguyen, A proof that recti-\n\ufb01ed deep neural networks overcome the curse of dimensionality in the numerical approximation\nof semilinear heat equations , SN Partial Di\ufb00erential Equations and Applications 1 (2020), no. 2,\n1\u201334.\n[HLXZ20] Juncai He, Lin Li, Jinchao Xu, and Chunyue Zheng, ReLU deep neural networks and linear\n\ufb01nite elements, Journal of Computational Mathematics 38 (2020), no. 3, 502\u2013527.\n[HMD16] Song Han, Huizi Mao, and William J Dally, Deep compression: Compressing deep neural network\nwith pruning, trained quantization and Hu\ufb00man coding , International Conference on Learning\nRepresentations, 2016.\n[HMRT19] Trevor Hastie, Andrea Montanari, Saharon Rosset, and Ryan J Tibshirani, Surprises in high-\ndimensional ridgeless least squares interpolation , 2019, arXiv preprint arXiv:1903.08560.\n[Hoe63] Wassily Hoe\ufb00ding, Probability inequalities for sums of bounded random variables , Journal of the\nAmerican Statistical Association 58 (1963), no. 301, 13\u201330.\n[Hop82] John J Hop\ufb01eld, Neural networks and physical systems with emergent collective computational\nabilities, Proceedings of the National Academy of Sciences 79 (1982), no. 8, 2554\u20132558.\n[HR19] Boris Hanin and David Rolnick, Deep ReLU networks have surprisingly few activation patterns ,\nAdvances in Neural Information Processing Systems, 2019, pp. 359\u2013368.\n[HRS16] Moritz Hardt, Ben Recht, and Yoram Singer,Train faster, generalize better: Stability of stochastic\ngradient descent, International Conference on Machine Learning, 2016, pp. 1225\u20131234.\n[HS97] Sepp Hochreiter and J\u00a8 urgen Schmidhuber,Long short-term memory , Neural Computation 9\n(1997), no. 8, 1735\u20131780.\n[HS17] Boris Hanin and Mark Sellke, Approximating continuous functions by ReLU nets of minimal\nwidth, 2017, arXiv preprint arXiv:1710.11278.\n67", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3190, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "6d29a934-074d-4383-9454-32101b204afa": {"__data__": {"id_": "6d29a934-074d-4383-9454-32101b204afa", "embedding": null, "metadata": {"page_label": "68", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "54a03e67-6b84-4e5b-9b3a-a09db6c0ab87", "node_type": "4", "metadata": {"page_label": "68", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "e0373dca9c93e635616e2c917fa062f88e520bc52821d4c67c0e89b91ae3c5e8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[HSL+16] Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kilian Q Weinberger, Deep networks with\nstochastic depth, European Conference on Computer Vision, 2016, pp. 646\u2013661.\n[HSN20] Jan Hermann, Zeno Sch\u00a8 atzle, and Frank No\u00b4 e,Deep-neural-network solution of the electronic\nSchr\u00a8 odinger equation, Nature Chemistry 12 (2020), no. 10, 891\u2013897.\n[HSW89] Kurt Hornik, Maxwell Stinchcombe, and Halbert White, Multilayer feedforward networks are\nuniversal approximators, Neural Networks 2 (1989), no. 5, 359\u2013366.\n[HTF01] Trevor Hastie, Robert Tibshirani, and Jerome Friedman, The elements of statistical learning:\nData mining, inference, and prediction , Springer Series in Statistics, Springer, 2001.\n[HV17] Benjamin D Hae\ufb00ele and Ren\u00b4 e Vidal,Global optimality in neural network training , Proceedings\nof the IEEE Conference on Computer Vision and Pattern Recognition, 2017, pp. 7331\u20137339.\n[HvdG19] Peter Hinz and Sara van de Geer, A framework for the construction of upper bounds on the\nnumber of a\ufb03ne linear regions of ReLU feed-forward neural networks , IEEE Transactions on\nInformation Theory 65 (2019), 7304\u20137324.\n[HZ94] Geo\ufb00rey E Hinton and Richard S Zemel, Autoencoders, minimum description length, and\nhelmholtz free energy, Advances in Neural Information Processing Systems 6 (1994), 3\u201310.\n[HZE19] Jiequn Han, Linfeng Zhang, and Weinan E, Solving many-electron Schr\u00a8 odinger equation using\ndeep neural networks, Journal of Computational Physics 399 (2019), 108929.\n[HZRS15] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun, Delving deep into recti\ufb01ers: Sur-\npassing human-level performance on imagenet classi\ufb01cation , Proceedings of IEEE International\nConference on Computer Vision, 2015, pp. 1026\u20131034.\n[HZRS16] , Deep residual learning for image recognition , Proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition, 2016, pp. 770\u2013778.\n[IS15] Sergey Io\ufb00e and Christian Szegedy, Batch normalization: Accelerating deep network training by\nreducing internal covariate shift , International Conference on Machine Learning, 2015, pp. 448\u2013\n456.\n[JGH18] Arthur Jacot, Franck Gabriel, and Cl\u00b4 ement Hongler,Neural tangent kernel: Convergence and\ngeneralization in neural networks , Advances in Neural Information Processing Systems, 2018,\npp. 8571\u20138580.\n[JKMB19] Yiding Jiang, Dilip Krishnan, Hossein Mobahi, and Samy Bengio, Predicting the generaliza-\ntion gap in deep networks with margin distributions , International Conference on Learning\nRepresentations, 2019.\n[JKNvW20] Arnulf Jentzen, Benno Kuckuck, Ariel Neufeld, and Philippe von Wurstemberger, Strong error\nanalysis for stochastic gradient descent optimization algorithms , IMA Journal of Numerical\nAnalysis 41 (2020), no. 1, 455\u2013492.\n[JMFU17] Kyong Hwan Jin, Michael T McCann, Emmanuel Froustey, and Michael Unser, Deep convolu-\ntional neural network for inverse problems in imaging , IEEE Transactions on Image Processing\n26 (2017), no. 9, 4509\u20134522.\n[JMS17] Bangti Jin, Peter Maa\u00df, and Otmar Scherzer, Sparsity regularization in inverse problems, Inverse\nProblems 33 (2017), no. 6, 060301.\n[JNM+20] Yiding Jiang, Behnam Neyshabur, Hossein Mobahi, Dilip Krishnan, and Samy Bengio, Fan-\ntastic generalization measures and where to \ufb01nd them , International Conference on Learning\nRepresentations, 2020.\n68", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3294, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "00f4e5c9-7f38-4231-9e12-276eb2cff2c7": {"__data__": {"id_": "00f4e5c9-7f38-4231-9e12-276eb2cff2c7", "embedding": null, "metadata": {"page_label": "69", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "616bfc8b-252f-4638-a3a8-6f2afa12e662", "node_type": "4", "metadata": {"page_label": "69", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "4ffa37f2bc4bbc263b471ab20e485911610a6bf0ecb63c3eb94f98bb1c07ada5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[Jor90] Michael I Jordan, Attractor dynamics and parallelism in a connectionist sequential machine ,\nArti\ufb01cial neural networks: concept learning, IEEE Press, 1990, pp. 112\u2013127.\n[JT19a] Ziwei Ji and Matus Telgarsky, Gradient descent aligns the layers of deep linear networks ,\nInternational Conference on Learning Representations, 2019.\n[JT19b] , A re\ufb01ned primal-dual analysis of the implicit bias, 2019, arXiv preprint arXiv:1906.04540.\n[JT20] , Directional convergence and alignment in deep learning, Advances in Neural Information\nProcessing Systems, 2020, pp. 17176\u201317186.\n[Jud90] Stephen J Judd, Neural network design and the complexity of learning , MIT Press, 1990.\n[Kel60] Henry J Kelley, Gradient theory of optimal \ufb02ight paths , Ars Journal 30 (1960), no. 10, 947\u2013954.\n[KH09] Alex Krizhevsky and Geo\ufb00rey Hinton, Learning multiple layers of features from tiny images ,\nTech. report, University of Toronto, 2009.\n[KL18] Sham M Kakade and Jason D Lee, Provably correct automatic subdi\ufb00erentiation for quali\ufb01ed\nprograms, Advances in Neural Information Processing Systems, 2018, pp. 7125\u20137135.\n[KL20] Patrick Kidger and Terry Lyons,Universal approximation with deep narrow networks, Conference\non Learning Theory, 2020, pp. 2306\u20132327.\n[KM97] Marek Karpinski and Angus Macintyre, Polynomial bounds for VC dimension of sigmoidal and\ngeneral Pfa\ufb03an neural networks , Journal of Computer and System Sciences 54 (1997), no. 1,\n169\u2013176.\n[KMN+17] Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping\nTak Peter Tang,On large-batch training for deep learning: Generalization gap and sharp minima ,\nInternational Conference on Learning Representations, 2017.\n[KPRS19] Gitta Kutyniok, Philipp Petersen, Mones Raslan, and Reinhold Schneider, A theoretical analysis\nof deep neural networks and parametric PDEs , 2019, arXiv preprint arXiv:1904.00377.\n[KSH12] Alex Krizhevsky, Ilya Sutskever, and Geo\ufb00rey E Hinton, Imagenet classi\ufb01cation with deep\nconvolutional neural networks , Advances in Neural Information Processing Systems, 2012,\npp. 1097\u20131105.\n[KW52] Jack Kiefer and Jacob Wolfowitz, Stochastic estimation of the maximum of a regression function ,\nThe Annals of Mathematical Statistics 23 (1952), no. 3, 462\u2013466.\n[LBBH98] Yann LeCun, L\u00b4 eon Bottou, Yoshua Bengio, and Patrick Ha\ufb00ner,Gradient-based learning applied\nto document recognition, Proceedings of the IEEE 86 (1998), no. 11, 2278\u20132324.\n[LBD+89] Yann LeCun, Bernhard Boser, John S Denker, Donnie Henderson, Richard E Howard, Wayne\nHubbard, and Lawrence D Jackel, Backpropagation applied to handwritten zip code recognition ,\nNeural Computation 1 (1989), no. 4, 541\u2013551.\n[LBH15] Yann LeCun, Yoshua Bengio, and Geo\ufb00rey Hinton, Deep learning, Nature 521 (2015), no. 7553,\n436\u2013444.\n[LBN+18] Jaehoon Lee, Yasaman Bahri, Roman Novak, Samuel S Schoenholz, Je\ufb00rey Pennington, and\nJascha Sohl-Dickstein, Deep neural networks as Gaussian processes , International Conference on\nLearning Representations, 2018.\n[LC19] Guillaume Lample and Fran\u00b8 cois Charton,Deep learning for symbolic mathematics , International\nConference on Learning Representations, 2019.\n69", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3139, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "0dee3d18-948c-4b7e-b11b-a0cc8b8c1bda": {"__data__": {"id_": "0dee3d18-948c-4b7e-b11b-a0cc8b8c1bda", "embedding": null, "metadata": {"page_label": "70", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6367feb9-ef1e-478d-83bb-83c04c32be85", "node_type": "4", "metadata": {"page_label": "70", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "9c826de5edaaae747ffa8cedba392cf6d8fdb8421796242babc95c22d4864ea2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[LD21] Licong Lin and Edgar Dobriban, What causes the test error? Going beyond bias-variance via\nANOVA, Journal of Machine Learning Research 22 (2021), no. 155, 1\u201382.\n[LDS89] Yann LeCun, John S Denker, and Sara A Solla, Optimal brain damage , Advances in Neural\nInformation Processing Systems, 1989, pp. 598\u2013605.\n[Lew43] Kurt Lewin, Psychology and the process of group living , The Journal of Social Psychology 17\n(1943), no. 1, 113\u2013131.\n[Li21] Weilin Li, Generalization error of minimum weighted norm and kernel interpolation , SIAM\nJournal on Mathematics of Data Science 3 (2021), no. 1, 414\u2013438.\n[Lin70] Seppo Linnainmaa, Alogritmin kumulatiivinen py\u00a8 oristysvirhe yksitt\u00a8 aisten py\u00a8 oristysvirheiden\nTaylor-kehitelm\u00a8 an\u00a8 a, Master\u2019s thesis, University of Helsinki, 1970.\n[LL18] Yuanzhi Li and Yingyu Liang, Learning overparameterized neural networks via stochastic\ngradient descent on structured data , Advances in Neural Information Processing Systems, 2018,\npp. 8157\u20138166.\n[LL19] Kaifeng Lyu and Jian Li, Gradient descent maximizes the margin of homogeneous neural\nnetworks, International Conference on Learning Representations, 2019.\n[LLPS93] Moshe Leshno, Vladimir Ya Lin, Allan Pinkus, and Shimon Schocken, Multilayer feedforward\nnetworks with a nonpolynomial activation function can approximate any function , Neural\nNetworks 6 (1993), no. 6, 861\u2013867.\n[LLS19] Qianxiao Li, Ting Lin, and Zuowei Shen,Deep learning via dynamical systems: An approximation\nperspective, 2019, arXiv preprint arXiv:1912.10382.\n[LML+20] Yiping Lu, Chao Ma, Yulong Lu, Jianfeng Lu, and Lexing Ying, A mean \ufb01eld analysis of\ndeep ResNet and beyond: Towards provably optimization via overparameterization from depth ,\nInternational Conference on Machine Learning, 2020, pp. 6426\u20136436.\n[L\u00a8OS18] Sebastian Lunz, Ozan \u00a8Oktem, and Carola-Bibiane Sch\u00a8 onlieb,Adversarial regularizers in inverse\nproblems, Advances in Neural Information Processing Systems, 2018, pp. 8507\u20138516.\n[LP21] Fabian Laakmann and Philipp Petersen, E\ufb03cient approximation of solutions of parametric\nlinear transport equations by ReLU DNNs , Advances in Computational Mathematics 47 (2021),\nno. 1, 1\u201332.\n[LPRS19] Tengyuan Liang, Tomaso Poggio, Alexander Rakhlin, and James Stokes, Fisher\u2013Rao metric,\ngeometry, and complexity of neural networks , International Conference on Arti\ufb01cial Intelligence\nand Statistics, 2019, pp. 888\u2013896.\n[LR20] Tengyuan Liang and Alexander Rakhlin, Just interpolate: Kernel \u201cridgeless\u201d regression can\ngeneralize, The Annals of Statistics 48 (2020), no. 3, 1329\u20131347.\n[LRZ20] Tengyuan Liang, Alexander Rakhlin, and Xiyu Zhai, On the multiple descent of minimum-norm\ninterpolants and restricted lower isometry of kernels , Conference on Learning Theory, 2020,\npp. 2683\u20132711.\n[LS17] Shiyu Liang and R Srikant, Why deep neural networks for function approximation?, International\nConference on Learning Representations, 2017.\n[LSAH20] Housen Li, Johannes Schwab, Stephan Antholzer, and Markus Haltmeier, NETT: Solving inverse\nproblems with deep neural networks , Inverse Problems 36 (2020), no. 6, 065005.\n70", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3080, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ed812420-4b3e-4ebb-85f0-49455ce1a218": {"__data__": {"id_": "ed812420-4b3e-4ebb-85f0-49455ce1a218", "embedding": null, "metadata": {"page_label": "71", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "de08b289-6536-4ff0-98e4-a3bb8e363727", "node_type": "4", "metadata": {"page_label": "71", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "0f5dff02ea31faeaf4a453b1e4dd12a56be26f2d9f2d082d2b5f9dd6c28aa5cf", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[LSJR16] Jason D Lee, Max Simchowitz, Michael I Jordan, and Benjamin Recht, Gradient descent only\nconverges to minimizers, Conference on Learning Theory, 2016, pp. 1246\u20131257.\n[LT91] Michel Ledoux and Michel Talagrand, Probability in Banach spaces: Isoperimetry and processes ,\nvol. 23, Springer Science & Business Media, 1991.\n[LTY19] Bo Li, Shanshan Tang, and Haijun Yu, Better approximations of high dimensional smooth\nfunctions by deep neural networks with recti\ufb01ed power units , Communications in Computational\nPhysics 27 (2019), no. 2, 379\u2013411.\n[LXS+20] Jaehoon Lee, Lechao Xiao, Samuel S Schoenholz, Yasaman Bahri, Roman Novak, Jascha Sohl-\nDickstein, and Je\ufb00rey Pennington, Wide neural networks of any depth evolve as linear models\nunder gradient descent, Journal of Statistical Mechanics: Theory and Experiment 2020 (2020),\nno. 12, 124002.\n[Mal12] St\u00b4 ephane Mallat,Group invariant scattering, Communications on Pure and Applied Mathematics\n65 (2012), no. 10, 1331\u20131398.\n[Mal16] , Understanding deep convolutional networks , Philosophical Transactions of the Royal\nSociety A: Mathematical, Physical and Engineering Sciences 374 (2016), no. 2065, 20150203.\n[MAV18] Poorya Mianjy, Raman Arora, and Rene Vidal, On the implicit bias of dropout , International\nConference on Machine Learning, 2018, pp. 3540\u20133548.\n[McA99] David A McAllester, Pac-bayesian model averaging , Conference on Learning Theory, 1999,\npp. 164\u2013170.\n[McD89] Colin McDiarmid, On the method of bounded di\ufb00erences , Surveys in Combinatorics 141 (1989),\nno. 1, 148\u2013188.\n[Men14] Shahar Mendelson, Learning without concentration , Conference on Learning Theory, 2014,\npp. 25\u201339.\n[Mha96] Hrushikesh N Mhaskar, Neural networks for optimal approximation of smooth and analytic\nfunctions, Neural Computation 8 (1996), no. 1, 164\u2013177.\n[MHR+18] Alexander G de G Matthews, Jiri Hron, Mark Rowland, Richard E Turner, and Zoubin\nGhahramani, Gaussian process behaviour in wide deep neural networks , International Conference\non Learning Representations, 2018.\n[MKS+13] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan\nWierstra, and Martin Riedmiller, Playing atari with deep reinforcement learning , 2013, arXiv\npreprint arXiv:1312.5602.\n[MLE21] Vishal Monga, Yuelong Li, and Yonina C Eldar, Algorithm unrolling: Interpretable, e\ufb03cient\ndeep learning for signal and image processing , IEEE Signal Processing Magazine 38 (2021),\nno. 2, 18\u201344.\n[MM19] Song Mei and Andrea Montanari, The generalization error of random features regression: Precise\nasymptotics and double descent curve , 2019, arXiv preprint arXiv:1908.05355.\n[MOPS20] Carlo Marcati, Joost Opschoor, Philipp Petersen, and Christoph Schwab, Exponential ReLU\nneural network approximation rates for point and edge singularities , 2020, ETH Zurich SAM\nResearch Report.\n[MP43] Warren S McCulloch and Walter Pitts, A logical calculus of the ideas immanent in nervous\nactivity, The Bulletin of Mathematical Biophysics 5 (1943), no. 4, 115\u2013133.\n71", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2998, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "33f4e238-c13c-4c72-afdd-3de409192f7f": {"__data__": {"id_": "33f4e238-c13c-4c72-afdd-3de409192f7f", "embedding": null, "metadata": {"page_label": "72", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "17b71dfa-ed2e-4ed9-a506-129875e96aa1", "node_type": "4", "metadata": {"page_label": "72", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "ededac9df06f7cf9de563de2d0d9d6a0014e3dffc717f486dc45ba1c58426c1c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[MP69] Marvin Minsky and Seymour A Papert, Perceptrons, MIT Press, 1969.\n[MP99] Vitaly Maiorov and Allan Pinkus, Lower bounds for approximation by MLP neural networks ,\nNeurocomputing 25 (1999), no. 1-3, 81\u201391.\n[MPCB14] Guido Mont\u00b4 ufar, Razvan Pascanu, Kyunghyun Cho, and Yoshua Bengio,On the number of\nlinear regions of deep neural networks , Advances in Neural Information Processing Systems,\n2014, pp. 2924\u20132932.\n[MSL+15] Junshui Ma, Robert P Sheridan, Andy Liaw, George E Dahl, and Vladimir Svetnik, Deep\nneural nets as a method for quantitative structure\u2013activity relationships , Journal of chemical\ninformation and modeling 55 (2015), no. 2, 263\u2013274.\n[MV03] Shahar Mendelson and Roman Vershynin, Entropy and the combinatorial dimension , Inventiones\nmathematicae 152 (2003), no. 1, 37\u201355.\n[MVSS20] Vidya Muthukumar, Kailas Vodrahalli, Vignesh Subramanian, and Anant Sahai, Harmless\ninterpolation of noisy data in regression , IEEE Journal on Selected Areas in Information Theory\n1 (2020), no. 1, 67\u201383.\n[MZ20] Andrea Montanari and Yiqiao Zhong, The interpolation phase transition in neural networks:\nMemorization and generalization under lazy training , 2020, arXiv preprint arXiv:2007.12826.\n[NBMS17] Behnam Neyshabur, Srinadh Bhojanapalli, David McAllester, and Nati Srebro, Exploring\ngeneralization in deep learning , Advances in Neural Information Processing Systems, 2017,\npp. 5947\u20135956.\n[NBS18] Behnam Neyshabur, Srinadh Bhojanapalli, and Nathan Srebro, A PAC-Bayesian approach to\nspectrally-normalized margin bounds for neural networks , International Conference on Learning\nRepresentations, 2018.\n[NH17] Quynh Nguyen and Matthias Hein, The loss surface of deep and wide neural networks , Interna-\ntional Conference on Machine Learning, 2017, pp. 2603\u20132612.\n[NI20] Ryumei Nakada and Masaaki Imaizumi, Adaptive approximation and generalization of deep\nneural network with intrinsic dimensionality , Journal of Machine Learning Research 21 (2020),\nno. 174, 1\u201338.\n[NJLS09] Arkadi Nemirovski, Anatoli Juditsky, Guanghui Lan, and Alexander Shapiro, Robust stochastic\napproximation approach to stochastic programming, SIAM Journal on Optimization 19 (2009),\nno. 4, 1574\u20131609.\n[NK19] Vaishnavh Nagarajan and J Zico Kolter, Uniform convergence may be unable to explain general-\nization in deep learning , Advances in Neural Information Processing Systems, 2019, pp. 11615\u2013\n11626.\n[NKB+20] Preetum Nakkiran, Gal Kaplun, Yamini Bansal, Tristan Yang, Boaz Barak, and Ilya Sutskever,\nDeep double descent: Where bigger models and more data hurt , International Conference on\nLearning Representations, 2020.\n[NLG+19] Mor Shpigel Nacson, Jason D Lee, Suriya Gunasekar, Pedro Henrique Pamplona Savarese, Nathan\nSrebro, and Daniel Soudry, Convergence of gradient descent on separable data , International\nConference on Arti\ufb01cial Intelligence and Statistics, 2019, pp. 3420\u20133428.\n[NTS14] Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro, In search of the real inductive bias:\nOn the role of implicit regularization in deep learning , 2014, arXiv preprint arXiv:1412.6614.\n72", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3072, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "7a5f90f9-63ed-4f86-90bf-47fc920d42d6": {"__data__": {"id_": "7a5f90f9-63ed-4f86-90bf-47fc920d42d6", "embedding": null, "metadata": {"page_label": "73", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c0f50496-e9b3-4987-9ee1-20cd9906df30", "node_type": "4", "metadata": {"page_label": "73", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "36b303d1fadb12dcecc5d5fdc1d9ae95f50c3c3dd6f8b0c445e4c750a4a263be", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ae9e0db0-35b5-483d-a429-87ea38c0b745", "node_type": "1", "metadata": {}, "hash": "2d333567414cdad037cecf608115b92143b6539957dde7511e9f7270dcc612cf", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[NTS15] , Norm-based capacity control in neural networks , Conference on Learning Theory, 2015,\npp. 1376\u20131401.\n[NW09] Erich Novak and Henryk Wo\u00b4 zniakowski,Approximation of in\ufb01nitely di\ufb00erentiable multivariate\nfunctions is intractable , Journal of Complexity 25 (2009), no. 4, 398\u2013404.\n[NY83] Arkadi Semenovich Nemirovsky and David Borisovich Yudin, Problem complexity and method\ne\ufb03ciency in optimization , Wiley-Interscience Series in Discrete Mathematics, Wiley, 1983.\n[NZGK21] Tenavi Nakamura-Zimmerer, Qi Gong, and Wei Kang, Adaptive deep learning for high-\ndimensional Hamilton\u2013Jacobi\u2013Bellman Equations , SIAM Journal on Scienti\ufb01c Computing 43\n(2021), no. 2, A1221\u2013A1247.\n[OF96] Bruno A Olshausen and David J Field, Sparse coding of natural images produces localized,\noriented, bandpass receptive \ufb01elds, Nature 381 (1996), no. 60, 609.\n[OM98] Genevieve B Orr and Klaus-Robert M\u00a8 uller,Neural networks: tricks of the trade , Springer, 1998.\n[OPS20] Joost Opschoor, Philipp Petersen, and Christoph Schwab, Deep ReLU networks and high-order\n\ufb01nite element methods , Analysis and Applications (2020), no. 0, 1\u201356.\n[OS19] Kenta Oono and Taiji Suzuki, Approximation and non-parametric estimation of ResNet-type\nconvolutional neural networks, International Conference on Machine Learning, 2019, pp. 4922\u2013\n4931.\n[PGZ+18] Hieu Pham, Melody Guan, Barret Zoph, Quoc Le, and Je\ufb00 Dean, E\ufb03cient neural architecture\nsearch via parameters sharing, International Conference on Machine Learning, 2018, pp. 4095\u2013\n4104.\n[PKL+17] Tomaso Poggio, Kenji Kawaguchi, Qianli Liao, Brando Miranda, Lorenzo Rosasco, Xavier\nBoix, Jack Hidary, and Hrushikesh N Mhaskar, Theory of deep learning III: explaining the\nnon-over\ufb01tting puzzle, 2017, arXiv preprint arXiv:1801.00173.\n[PLR+16] Ben Poole, Subhaneil Lahiri, Maithra Raghu, Jascha Sohl-Dickstein, and Surya Ganguli,\nExponential expressivity in deep neural networks through transient chaos , Advances in Neural\nInformation Processing Systems, 2016, pp. 3368\u20133376.\n[PMR+17] Tomaso Poggio, Hrushikesh N Mhaskar, Lorenzo Rosasco, Brando Miranda, and Qianli Liao,\nWhy and when can deep-but not shallow-networks avoid the curse of dimensionality: a review ,\nInternational Journal of Automation and Computing 14 (2017), no. 5, 503\u2013519.\n[PP92] Etienne Pardoux and Shige Peng, Backward stochastic di\ufb00erential equations and quasilin-\near parabolic partial di\ufb00erential equations , Stochastic partial di\ufb00erential equations and their\napplications, Springer, 1992, pp. 200\u2013217.\n[PRE17] Vardan Papyan, Yaniv Romano, and Michael Elad, Convolutional neural networks analyzed via\nconvolutional sparse coding, Journal of Machine Learning Research 18 (2017), no. 1, 2887\u20132938.\n[PRMN04] Tomaso Poggio, Ryan Rifkin, Sayan Mukherjee, and Partha Niyogi, General conditions for\npredictivity in learning theory, Nature 428 (2004), no. 6981, 419\u2013422.\n[PRSE18] Vardan Papyan, Yaniv Romano, Jeremias Sulam, and Michael Elad, Theoretical foundations\nof deep learning via sparse representations: A multilayer sparse model and its connection to\nconvolutional neural networks, IEEE Signal Processing Magazine 35 (2018), no. 4, 72\u201389.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3128, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ae9e0db0-35b5-483d-a429-87ea38c0b745": {"__data__": {"id_": "ae9e0db0-35b5-483d-a429-87ea38c0b745", "embedding": null, "metadata": {"page_label": "73", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c0f50496-e9b3-4987-9ee1-20cd9906df30", "node_type": "4", "metadata": {"page_label": "73", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "36b303d1fadb12dcecc5d5fdc1d9ae95f50c3c3dd6f8b0c445e4c750a4a263be", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7a5f90f9-63ed-4f86-90bf-47fc920d42d6", "node_type": "1", "metadata": {"page_label": "73", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "965f097e93d0b92418c662f23fd0547f16017b873902615d7feac40ffb2415ec", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "200\u2013217.\n[PRE17] Vardan Papyan, Yaniv Romano, and Michael Elad, Convolutional neural networks analyzed via\nconvolutional sparse coding, Journal of Machine Learning Research 18 (2017), no. 1, 2887\u20132938.\n[PRMN04] Tomaso Poggio, Ryan Rifkin, Sayan Mukherjee, and Partha Niyogi, General conditions for\npredictivity in learning theory, Nature 428 (2004), no. 6981, 419\u2013422.\n[PRSE18] Vardan Papyan, Yaniv Romano, Jeremias Sulam, and Michael Elad, Theoretical foundations\nof deep learning via sparse representations: A multilayer sparse model and its connection to\nconvolutional neural networks, IEEE Signal Processing Magazine 35 (2018), no. 4, 72\u201389.\n[PRV20] Philipp Petersen, Mones Raslan, and Felix Voigtlaender, Topological properties of the set of\nfunctions generated by neural networks of \ufb01xed size , Foundations of Computational Mathematics\n(2020), 1\u201370.\n73", "mimetype": "text/plain", "start_char_idx": 2483, "end_char_idx": 3341, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "2cf23693-a9f0-4f80-a2d2-3382c344a5c1": {"__data__": {"id_": "2cf23693-a9f0-4f80-a2d2-3382c344a5c1", "embedding": null, "metadata": {"page_label": "74", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3f100fe1-c9e6-475e-ae8f-fffdea4fb8be", "node_type": "4", "metadata": {"page_label": "74", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "ac887bd488984b036bfa8e433c09e300b2fb36dc4a5958b21fabacd3ff60df7c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[PSE17] Vardan Papyan, Jeremias Sulam, and Michael Elad,Working locally thinking globally: Theoretical\nguarantees for convolutional sparse coding, IEEE Transactions on Signal Processing 65 (2017),\nno. 21, 5687\u20135701.\n[PSMF20] David Pfau, James S Spencer, Alexander GDG Matthews, and W Matthew C Foulkes, Ab initio\nsolution of the many-electron schr\u00a8 odinger equation with deep neural networks, Physical Review\nResearch 2 (2020), no. 3, 033429.\n[PV18] Philipp Petersen and Felix Voigtlaender, Optimal approximation of piecewise smooth functions\nusing deep ReLU neural networks , Neural Networks 108 (2018), 296\u2013330.\n[PV20] , Equivalence of approximation by convolutional neural networks and fully-connected\nnetworks, Proceedings of the American Mathematical Society 148 (2020), no. 4, 1567\u20131581.\n[REM17] Yaniv Romano, Michael Elad, and Peyman Milanfar, The little engine that could: Regularization\nby denoising (red), SIAM Journal on Imaging Sciences 10 (2017), no. 4, 1804\u20131844.\n[RFB15] Olaf Ronneberger, Philipp Fischer, and Thomas Brox, U-net: Convolutional networks for\nbiomedical image segmentation , International Conference on Medical image computing and\ncomputer-assisted intervention, 2015, pp. 234\u2013241.\n[RH19] Lars Ruthotto and Eldad Haber, Deep neural networks motivated by partial di\ufb00erential equations,\nJournal of Mathematical Imaging and Vision (2019), 1\u201313.\n[RHW86] David E Rumelhart, Geo\ufb00rey E Hinton, and Ronald J Williams, Learning representations by\nback-propagating errors, Nature 323 (1986), no. 6088, 533\u2013536.\n[RM51] Herbert Robbins and Sutton Monro, A stochastic approximation method , The Annals of Mathe-\nmatical Statistics (1951), 400\u2013407.\n[RMD+06] P L\u00b4 opez R\u00b4 \u0131os, Ao Ma, Neil D Drummond, Michael D Towler, and Richard J Needs,Inhomoge-\nneous back\ufb02ow transformations in quantum Monte Carlo calculations , Physical Review E 74\n(2006), no. 6, 066701.\n[Ros58] Frank Rosenblatt, The perceptron: a probabilistic model for information storage and organization\nin the brain , Psychological review 65 (1958), no. 6, 386.\n[RPK+17] Maithra Raghu, Ben Poole, Jon Kleinberg, Surya Ganguli, and Jascha Sohl-Dickstein, On the\nexpressive power of deep neural networks , International Conference on Machine Learning, 2017,\npp. 2847\u20132854.\n[RPK19] Maziar Raissi, Paris Perdikaris, and George E Karniadakis, Physics-informed neural networks:\nA deep learning framework for solving forward and inverse problems involving nonlinear partial\ndi\ufb00erential equations, Journal of Computational Physics 378 (2019), 686\u2013707.\n[RR+07] Ali Rahimi, Benjamin Recht, et al., Random features for large-scale kernel machines , Advances\nin Neural Information Processing Systems, 2007, pp. 1177\u20131184.\n[Rud06] Walter Rudin, Real and complex analysis , McGraw-Hill Series in Higher Mathematics, Tata\nMcGraw-Hill, 2006.\n[RWK+20] Vivek Ramanujan, Mitchell Wortsman, Aniruddha Kembhavi, Ali Farhadi, and Mohammad\nRastegari, What\u2019s hidden in a randomly weighted neural network? , Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition, 2020, pp. 11893\u201311902.\n[Sak99] Akito Sakurai, Tight bounds for the VC-dimension of piecewise polynomial networks , Advances\nin Neural Information Processing Systems, 1999, pp. 323\u2013329.\n74", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3226, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "7985138a-fafb-4c38-827e-5f9873cb23e0": {"__data__": {"id_": "7985138a-fafb-4c38-827e-5f9873cb23e0", "embedding": null, "metadata": {"page_label": "75", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e9a9f5ea-7412-45f7-9142-30b634592df7", "node_type": "4", "metadata": {"page_label": "75", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "ad911c3496ff0adecb4a2c00fe5c5423043842ad6a30641f8b84e1da0afa2316", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[SCC18] Uri Shaham, Alexander Cloninger, and Ronald R Coifman, Provable approximation properties\nfor deep neural networks , Applied and Computational Harmonic Analysis 44 (2018), no. 3,\n537\u2013557.\n[Sch15] J\u00a8 urgen Schmidhuber,Deep learning in neural networks: An overview , Neural Networks 61\n(2015), 85\u2013117.\n[SDR14] Alexander Shapiro, Darinka Dentcheva, and Andrzej Ruszczy\u00b4 nski,Lectures on stochastic pro-\ngramming: modeling and theory , SIAM, 2014.\n[SEJ+20] Andrew W Senior, Richard Evans, John Jumper, James Kirkpatrick, Laurent Sifre, Tim Green,\nChongli Qin, Augustin \u02c7Z\u00b4 \u0131dek, Alexander WR Nelson, and Alex Bridgland,Improved protein\nstructure prediction using potentials from deep learning , Nature 577 (2020), no. 7792, 706\u2013710.\n[SGHK18] David Saxton, Edward Grefenstette, Felix Hill, and Pushmeet Kohli, Analysing mathematical\nreasoning abilities of neural models , International Conference on Learning Representations, 2018.\n[SGS15] Rupesh Kumar Srivastava, Klaus Gre\ufb00, and J\u00a8 urgen Schmidhuber,Training very deep networks ,\nAdvances in Neural Information Processing Systems, 2015, pp. 2377\u20132385.\n[SH19] Johannes Schmidt-Hieber, Deep ReLU network approximation of functions on a manifold , 2019,\narXiv preprint arXiv:1908.00695.\n[She20] Zuowei Shen, Deep network approximation characterized by number of neurons , Communications\nin Computational Physics 28 (2020), no. 5, 1768\u20131811.\n[SHK+14] Nitish Srivastava, Geo\ufb00rey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov,\nDropout: a simple way to prevent neural networks from over\ufb01tting , Journal of Machine Learning\nResearch 15 (2014), no. 1, 1929\u20131958.\n[SHM+16] David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driess-\nche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, and Marc Lanctot, Mas-\ntering the game of go with deep neural networks and tree search , Nature 529 (2016), no. 7587,\n484\u2013489.\n[SHN+18] Daniel Soudry, Elad Ho\ufb00er, Mor Shpigel Nacson, Suriya Gunasekar, and Nathan Srebro, The\nimplicit bias of gradient descent on separable data , 2018.\n[\u02c7S\u00b4 \u0131m02] Ji\u02c7 r\u00b4 \u0131\u02c7S\u00b4 \u0131ma,Training a single sigmoidal neuron is hard , Neural Computation 14 (2002), no. 11,\n2709\u20132728.\n[SKS+17] Kristof T Sch\u00a8 utt, Pieter-Jan Kindermans, Huziel E Sauceda, Stefan Chmiela, Alexandre\nTkatchenko, and Klaus-Robert M\u00a8 uller,Schnet: A continuous-\ufb01lter convolutional neural network\nfor modeling quantum interactions , Advances in Neural Information Processing Systems, 2017,\npp. 992\u20131002.\n[SLJ+15] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov,\nDumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich, Going deeper with convolutions ,\nProceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2015, pp. 1\u20139.\n[SO12] Attila Szabo and Neil S Ostlund, Modern quantum chemistry: introduction to advanced electronic\nstructure theory, Courier Corporation, 2012.\n[SPRE18] Jeremias Sulam, Vardan Papyan, Yaniv Romano, and Michael Elad, Multilayer convolutional\nsparse modeling: Pursuit and dictionary learning , IEEE Transactions on Signal Processing 66\n(2018), no. 15, 4090\u20134104.\n75", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3150, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d04d1808-31a3-4a6b-83fc-be775bf988ec": {"__data__": {"id_": "d04d1808-31a3-4a6b-83fc-be775bf988ec", "embedding": null, "metadata": {"page_label": "76", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a1d13aff-daac-421d-b502-7622dc524df1", "node_type": "4", "metadata": {"page_label": "76", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "70a76e8c8e5fb9d7234b286ff6124fa84ac7835a153f36e4b470232668559542", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[SRG+21] Michael Scherbela, Rafael Reisenhofer, Leon Gerard, Philipp Marquetand, and Philipp Grohs,\nSolving the electronic Schr\u00a8 odinger equation for multiple nuclear geometries with weight-sharing\ndeep neural networks, 2021, arXiv preprint arXiv:2105.08351.\n[SS16] Itay Safran and Ohad Shamir, On the quality of the initial basin in overspeci\ufb01ed neural networks ,\nInternational Conference on Machine Learning, 2016, pp. 774\u2013782.\n[SS17] , Depth-width tradeo\ufb00s in approximating natural functions with neural networks , Interna-\ntional Conference on Machine Learning, 2017, pp. 2979\u20132987.\n[SS18] , Spurious local minima are common in two-layer ReLU neural networks , International\nConference on Machine Learning, 2018, pp. 4433\u20134441.\n[SSBD14] Shai Shalev-Shwartz and Shai Ben-David, Understanding machine learning: From theory to\nalgorithms, Cambridge University Press, 2014.\n[SSS+17] David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur\nGuez, Thomas Hubert, Lucas Baker, Matthew Lai, and Adrian Bolton, Mastering the game of\ngo without human knowledge , Nature 550 (2017), no. 7676, 354\u2013359.\n[SSSSS09] Shai Shalev-Shwartz, Ohad Shamir, Nathan Srebro, and Karthik Sridharan, Stochastic convex\noptimization, Conference on Learning Theory, 2009.\n[STIM18] Shibani Santurkar, Dimitris Tsipras, Andrew Ilyas, and Aleksander Madry, How does batch\nnormalization help optimization? , Advances in Neural Information Processing Systems, 2018,\npp. 2488\u20132498.\n[SZ19] Christoph Schwab and Jakob Zech, Deep learning in high dimension: Neural network expression\nrates for generalized polynomial chaos expansions in uq , Analysis and Applications 17 (2019),\nno. 01, 19\u201355.\n[Tal94] Michel Talagrand, Sharper bounds for Gaussian and empirical processes , The Annals of Proba-\nbility (1994), 28\u201376.\n[Tel15] Matus Telgarsky, Representation bene\ufb01ts of deep feedforward networks , 2015, arXiv preprint\narXiv:1509.08101.\n[TvG18] Matthew Thorpe and Yves van Gennip, Deep limits of residual neural networks , 2018, arXiv\npreprint arXiv:1810.11741.\n[UVL18] Dmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky, Deep image prior , Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recognition, 2018, pp. 9446\u20139454.\n[Vap99] Vladimir Vapnik, An overview of statistical learning theory , IEEE Transactions on Neural\nNetworks 10 (1999), no. 5, 988\u2013999.\n[Vap13] , The nature of statistical learning theory , Springer science & business media, 2013.\n[VBB19] Luca Venturi, Afonso S Bandeira, and Joan Bruna, Spurious valleys in one-hidden-layer neural\nnetwork optimization landscapes , Journal of Machine Learning Research 20 (2019), no. 133,\n1\u201334.\n[VBC+19] Oriol Vinyals, Igor Babuschkin, Wojciech M Czarnecki, Micha\u00a8 el Mathieu, Andrew Dudzik,\nJunyoung Chung, David H Choi, Richard Powell, Timo Ewalds, and Petko Georgiev,Grandmaster\nlevel in StarCraft II using multi-agent reinforcement learning , Nature 575 (2019), no. 7782,\n350\u2013354.\n76", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2958, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b5507e3e-00d0-477a-bfaa-8004bc5b95d6": {"__data__": {"id_": "b5507e3e-00d0-477a-bfaa-8004bc5b95d6", "embedding": null, "metadata": {"page_label": "77", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "46eaf67c-3fe1-4685-b76e-3415d54d0f4e", "node_type": "4", "metadata": {"page_label": "77", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "34e47fd5588f03e24109dd5df44508d7a9c821be2c245070b53ebe14c106616b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[VC71] Vladimir Vapnik and Alexey Chervonenkis, On the uniform convergence of relative frequencies of\nevents to their probabilities , Theory of Probability & Its Applications 16 (1971), no. 2, 264\u2013280.\n[vdVW97] Aad W van der Vaart and Jon A Wellner, Weak convergence and empirical processes with\napplications to statistics , Journal of the Royal Statistical Society-Series A Statistics in Society\n160 (1997), no. 3, 596\u2013608.\n[Ver18] Roman Vershynin, High-dimensional probability: An introduction with applications in data\nscience, vol. 47, Cambridge University Press, 2018.\n[VSP+17] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\n Lukasz Kaiser, and Illia Polosukhin, Attention is all you need , Advances in Neural Information\nProcessing Systems, 2017, pp. 5998\u20136008.\n[Wer88] Paul J Werbos, Generalization of backpropagation with application to a recurrent gas market\nmodel, Neural Networks 1 (1988), no. 4, 339\u2013356.\n[WGB17] Thomas Wiatowski, Philipp Grohs, and Helmut B\u00a8 olcskei, Energy propagation in deep convolu-\ntional neural networks , IEEE Transactions on Information Theory 64 (2017), no. 7, 4819\u20134842.\n[Whi34] Hassler Whitney, Analytic extensions of di\ufb00erentiable functions de\ufb01ned in closed sets , Transac-\ntions of the American Mathematical Society 36 (1934), no. 1, 63\u201389.\n[WPC+21] Zonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi Zhang, and S Yu Philip, A\ncomprehensive survey on graph neural networks , IEEE Transactions on Neural Networks and\nLearning Systems 32 (2021), no. 1, 4\u201324.\n[WZ95] Ronald J Williams and David Zipser, Gradient-based learning algorithms for recurrent, Back-\npropagation: Theory, Architectures, and Applications 433 (1995), 17.\n[WZZ+13] Li Wan, Matthew Zeiler, Sixin Zhang, Yann Le Cun, and Rob Fergus, Regularization of neural\nnetworks using dropconnect, International Conference on Machine Learning, 2013, pp. 1058\u20131066.\n[XM12] Huan Xu and Shie Mannor, Robustness and generalization, Machine learning 86 (2012), no. 3,\n391\u2013423.\n[Yan19] Greg Yang, Scaling limits of wide neural networks with weight sharing: Gaussian process\nbehavior, gradient independence, and neural tangent kernel derivation , 2019, arXiv preprint\narXiv:1902.04760.\n[Yar17] Dmitry Yarotsky, Error bounds for approximations with deep ReLU networks , Neural Networks\n94 (2017), 103\u2013114.\n[Yar18a] , Optimal approximation of continuous functions by very deep ReLU networks , Conference\non Learning Theory, 2018, pp. 639\u2013649.\n[Yar18b] , Universal approximations of invariant maps by neural networks , 2018, arXiv preprint\narXiv:1804.10306.\n[Yar21] , Elementary superexpressive activations, 2021, arXiv preprint arXiv:2102.10911.\n[YGLD17] Rujie Yin, Tingran Gao, Yue M Lu, and Ingrid Daubechies, A tale of two bases: Local-nonlocal\nregularization on image patches with convolution framelets , SIAM Journal on Imaging Sciences\n10 (2017), no. 2, 711\u2013750.\n[YHC18] Jong Chul Ye, Yoseob Han, and Eunju Cha, Deep convolutional framelets: A general deep\nlearning framework for inverse problems , SIAM Journal on Imaging Sciences 11 (2018), no. 2,\n991\u20131048.\n77", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3107, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8e7b3e37-4e53-4c83-bee6-cf4cc208d13c": {"__data__": {"id_": "8e7b3e37-4e53-4c83-bee6-cf4cc208d13c", "embedding": null, "metadata": {"page_label": "78", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "36a208b8-0096-4368-8887-c275ad86fc53", "node_type": "4", "metadata": {"page_label": "78", "file_name": "The Modern Mathematics of Deep Learning.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\The Modern Mathematics of Deep Learning.pdf", "file_type": "application/pdf", "file_size": 3509595, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "715c6c7eedd0c99372b5ab6abb2844bd1f2f098394caabd0c1988ecee2a51156", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[YHPC18] Tom Young, Devamanyu Hazarika, Soujanya Poria, and Erik Cambria, Recent trends in deep\nlearning based natural language processing, IEEE Computational Intelligence Magazine 13 (2018),\nno. 3, 55\u201375.\n[Yse10] Harry Yserentant, Regularity and approximability of electronic wave functions , Springer, 2010.\n[YZ20] Dmitry Yarotsky and Anton Zhevnerchuk, The phase diagram of approximation rates for deep\nneural networks, Advances in Neural Information Processing Systems, vol. 33, 2020.\n[ZAP16] Hao Zhou, Jose M Alvarez, and Fatih Porikli, Less is more: Towards compact CNNs , European\nConference on Computer Vision, 2016, pp. 662\u2013677.\n[Zas75] Thomas Zaslavsky, Facing up to arrangements: Face-count formulas for partitions of space\nby hyperplanes: Face-count formulas for partitions of space by hyperplanes , Memoirs of the\nAmerican Mathematical Society, American Mathematical Society, 1975.\n[ZBH+17] Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals, Under-\nstanding deep learning requires rethinking generalization , International Conference on Learning\nRepresentations, 2017.\n[ZBH+20] Chiyuan Zhang, Samy Bengio, Moritz Hardt, Michael C Mozer, and Yoram Singer, Identity\ncrisis: Memorization and generalization under extreme overparameterization , International\nConference on Learning Representations, 2020.\n[ZBS19] Chiyuan Zhang, Samy Bengio, and Yoram Singer, Are all layers created equal? , 2019, arXiv\npreprint arXiv:1902.01996.\n[ZCZG20] Difan Zou, Yuan Cao, Dongruo Zhou, and Quanquan Gu, Gradient descent optimizes over-\nparameterized deep ReLU networks, Machine Learning 109 (2020), no. 3, 467\u2013492.\n[Zho20a] Ding-Xuan Zhou, Theory of deep convolutional neural networks: Downsampling, Neural Networks\n124 (2020), 319\u2013327.\n[Zho20b] , Universality of deep convolutional neural networks , Applied and Computational Har-\nmonic Analysis 48 (2020), no. 2, 787\u2013794.\n[ZKS+18] Jure Zbontar, Florian Knoll, Anuroop Sriram, Tullie Murrell, Zhengnan Huang, Matthew J\nMuckley, Aaron Defazio, Ruben Stern, Patricia Johnson, Mary Bruno, Marc Parente, Krzysztof J\nGeras, Joe Katsnelson, Hersh Chandarana, Zizhao Zhang, Michal Drozdzal, Adriana Romero,\nMichael Rabbat, Pascal Vincent, Na\ufb01ssa Yakubova, James Pinkerton, Duo Wang, Erich Owens,\nC Lawrence Zitnick, Michael P Recht, Daniel K Sodickson, and Yvonne W Lui, fastMRI: An\nopen dataset and benchmarks for accelerated MRI , 2018, arXiv preprint arXiv:1811.08839.\n[ZL17] Barret Zoph and Quoc V Le, Neural architecture search with reinforcement learning, International\nConference on Learning Representations, 2017.\n78", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2591, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1c305eb2-7ee8-408e-b649-926e26a95290": {"__data__": {"id_": "1c305eb2-7ee8-408e-b649-926e26a95290", "embedding": null, "metadata": {"page_label": "1", "file_name": "Unmasking the Shadows of AI Investigating Deceptive Capabilities in Large Language Models.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Unmasking the Shadows of AI Investigating Deceptive Capabilities in Large Language Models.pdf", "file_type": "application/pdf", "file_size": 211088, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a7ae0c48-21a8-40a9-8579-82ce516fd829", "node_type": "4", "metadata": {"page_label": "1", "file_name": "Unmasking the Shadows of AI Investigating Deceptive Capabilities in Large Language Models.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Unmasking the Shadows of AI Investigating Deceptive Capabilities in Large Language Models.pdf", "file_type": "application/pdf", "file_size": 211088, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "65b576fed4e0e2115ce31a17fb77f50bb13b40947042a46dc142761f5381991f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "1 \nUnmasking the Shadows of AI: Investigating Deceptive Capabilities in Large Language \nModels \nLinge Guo | Grace | linge.guo.21@ucl.ac.uk | University College London  \nKeywords: AI deception, Large Language Models, ChatGPT \nIntroduction \nThis research critically navigates the intricate landscape of AI deception, concentrating on \ndeceptive behaviours of Large Language Models (LLMs). My objective is to elucidate this \nissue, examine the discourse surrounding it, and subsequently delve into its categorization and \nramifications. The essay initiates with an evaluation of the AI Safety Summit 2023 (ASS) and \nintroduction of LLMs, emphasisin g multidimensional biases that underlie their deceptive \nbehaviours. Through illuminating algorithmic bias and exploring different ways to define \n\u201cdeception\u201d, I argue that deceptive AI is an inherent phenomenon intertwined with the \nadvancement of LLMs and It may evolve into a self-driven intent, independent of the biassed \ntraining process.  \nThe literature review covers four types of deception categorised: Strategic deception, Imitation, \nSycophancy, and Unfaithful Reasoning, along with the social implications and risks they entail. \nThe literature around deceptive AI, predominantly available on  arXiv archives, manifests a \ndeficiency in contribution from social science. This deficiency could be ascribed to the early \ntesting stages of AI deception, constraining its research primarily within the domain of \ncomputer science. Lastly, I take an evaluative stance on various aspects related to navigating \nthe persistent challenges of the deceptive AI. This encompasses considerations of international \ncollaborative governance, the reconfigured engagement of individuals with AI, proposal of \npractical adjustments, and specific elements of digital education. Throughout the research, \nLLMs are examined as infrastructures of relations, structures, and prac tices, offering a \ncomprehensive understanding of \u201cinfrastructures as relational arrangements co -formative of \nharm (Kallianos, Dunlap and Dalakoglou, 2022).\u201d", "mimetype": "text/plain", "start_char_idx": 3, "end_char_idx": 2079, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "75e9230b-0fd4-416e-a386-f0a7a267fc57": {"__data__": {"id_": "75e9230b-0fd4-416e-a386-f0a7a267fc57", "embedding": null, "metadata": {"page_label": "2", "file_name": "Unmasking the Shadows of AI Investigating Deceptive Capabilities in Large Language Models.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Unmasking the Shadows of AI Investigating Deceptive Capabilities in Large Language Models.pdf", "file_type": "application/pdf", "file_size": 211088, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c55f44e6-a7e1-4d5c-a03b-18b5bc439a42", "node_type": "4", "metadata": {"page_label": "2", "file_name": "Unmasking the Shadows of AI Investigating Deceptive Capabilities in Large Language Models.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Unmasking the Shadows of AI Investigating Deceptive Capabilities in Large Language Models.pdf", "file_type": "application/pdf", "file_size": 211088, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "baebe1eaf3ad30b86aca3df415981f053b82908103fb2c0c8b60915cfd239e51", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "2 \nAI Safety Summit 2023: What Does It Actually Achieve? \nThe ASS, hosted under the auspices of the UK Prime Minister, convened political leaders, \nexperts, and industry figures to deliberate on the risks associated with the frontiers of AI \ndevelopment, specifically addressing concerns related to \u201cmisuse, loss of control, and societal \nharm (Leading Frontier AI Companies Publish Safety Policies, 2023). \u201d Despite five outlined \nobjectives, the ASS\u2019s actual impact, highlighted by news reports, remains elusive in terms of \nspecific actionable measures (At UK\u2019s AI Summit, Guterres Says Risks Outweigh Rewards \nwithout Global Oversight, 2023; Devlin and Forrest, 2023; Fullbrook, 2023; Milmo, 2023; \nSample, 2 023; Seal, 2023).  AI experts at Oxford universities provide a more valid and \nauthoritative assessment. \nThe attention garnered by the  ASS in the media, emphasising the participants \u2019 authority and \nglobal representation, calls a pause for the development of AI and adds the atmosphere of \nunchecked fear and unfounded apprehension. While the ASS successfully signals a consensus \ngesture, a critical question revolves around identifying the actors responsible for maintaining \nthis consensus (McBride, 2023). Professor Trager \u2019s suggestion that \u201cAI technology should \nhappen within the academic sector, rather than being outsourced to tech companies (Expert \ncomment: Oxford AI Experts Comment on the Outcomes of the UK AI Safety Summit, 2023)\u201d \nprompts consideration of potential veste d interests within academia. Nonetheless, both \nacademic and industry perspectives contribute to shaping the discourse on AI development in \na top-down technocratic approach. The ASS underscores the UK\u2019s interest to align with the \nglobal implementation of regulations to mitigate algorithmic risks and to seek sovereignty in \nshaping the global regulatory framework . Despite the extent of the UK \u2019s assertion in this \npursuit, the ASS thus serves as a commendable starting point, emphasising the need for greater \ninclusivity in these efforts. \nSignificance of AI Deception \nAcknowledging the socio-political context of the ASS, I redirect the focus from potential future \nharm to the present-day existential risks associated with AI use. A supporting document for the \nASS explicated that the challenges posed by deceptive AI becomes crucial when addressing \nwith the loss of control over AI,  especially in the absence of a defence strategy  (Frontier AI: \nCapabilities and Risks \u2013 Discussion Paper, 2023). LLMs extend their influence beyond AI", "mimetype": "text/plain", "start_char_idx": 3, "end_char_idx": 2554, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "def97ab3-0457-48e7-857a-dfac6b7a9fcc": {"__data__": {"id_": "def97ab3-0457-48e7-857a-dfac6b7a9fcc", "embedding": null, "metadata": {"page_label": "3", "file_name": "Unmasking the Shadows of AI Investigating Deceptive Capabilities in Large Language Models.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Unmasking the Shadows of AI Investigating Deceptive Capabilities in Large Language Models.pdf", "file_type": "application/pdf", "file_size": 211088, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "96b1ffb7-aa8d-4c60-a33b-e77c45258faa", "node_type": "4", "metadata": {"page_label": "3", "file_name": "Unmasking the Shadows of AI Investigating Deceptive Capabilities in Large Language Models.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Unmasking the Shadows of AI Investigating Deceptive Capabilities in Large Language Models.pdf", "file_type": "application/pdf", "file_size": 211088, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "ae1666c8902af7176137264b8e4dbc5adea3b364c4ae5a581e3c492b127db4da", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "3 \nchatbots and search engines, infiltrating the decision -making framework of autonomous \ndriving cars (Cui et al., 2023). The existential and social implications of LLM use are profound \nand widespread, encompassing the intentional generation of deepfakes in images or videos, \npotential misuse in personalised disinformation campaigns, and susceptibility to cyberattacks \n(Ngo, Chan and Mindermann, 2023). Having underscored the im portance of deceptive AI, I \nwill introduce LLMs, illuminating the inherent biases ingrained within these models.  \nLLMs & Biases in LLMs \nThe Large (trained on huge text datasets from the internet), Languages (operate d based on \nhuman language) Models (used to make predictions) are developed through deep learning \nalgorithms. Users interact with LLMs , such as ChatGPT (GPT), using prompts, engaging in \ntasks such as customer service conversations and content generation.  The multidimensional \ntask demonstrates LLMs\u2019 capabilities spanning summarization, comparison, analysis, and text \nand image generation (Cui et al., 2023; Head et al., 2023; Matsuo et al., 2022). \nBiases in LLMs reflect societal biases ingrained in human culture and language. These biases \nare perpetuated through the learning, training, and execution of AI systems. Lack of diversity \namong developers and over -representation of socioeconomically advant aged groups in \ninteracting with AI tools, such as internet users and English native speakers, produce \nrepresentational biases; existing societal discriminations , stereotypes existed in the datasets \npresent as historical biases (Collett and Dillon, 2019; Cui et al., 2023). These biases not only \npersist but also amplify discriminating behaviours that elude quantification or measurement in \neveryday life (Ntoutsi et a l., 2020). Data, imbricated with layers of interpretation, represents \nstructural inequalities related but not limited to the intersectionality of gender, race, age and \nclass (Joyce et al., 2021; Singh, 2020)1.  \nAnalogous to global power dynamics, extractive data practices mirror the discourse of \u201cthe \nWest and the Rest (Hall, 1992), \u201d where Western explorers impose a Eurocentric system of \nrepresentations on indigenous populations, creating a binary opposition with the rest of \n \n1The concepts of data imbrication appeared in Lecture 3, and Lecture 9 discussed the intersection in the \nreproduction of inequality in algorithms oppression, especially involving the intersections of gender, sexuality, \nand race.", "mimetype": "text/plain", "start_char_idx": 3, "end_char_idx": 2515, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "28d6c7d8-0110-4516-9ba2-0e900bd9d56b": {"__data__": {"id_": "28d6c7d8-0110-4516-9ba2-0e900bd9d56b", "embedding": null, "metadata": {"page_label": "4", "file_name": "Unmasking the Shadows of AI Investigating Deceptive Capabilities in Large Language Models.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Unmasking the Shadows of AI Investigating Deceptive Capabilities in Large Language Models.pdf", "file_type": "application/pdf", "file_size": 211088, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "081d9993-3af6-4d06-9ab2-16c8e4d0a73a", "node_type": "4", "metadata": {"page_label": "4", "file_name": "Unmasking the Shadows of AI Investigating Deceptive Capabilities in Large Language Models.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Unmasking the Shadows of AI Investigating Deceptive Capabilities in Large Language Models.pdf", "file_type": "application/pdf", "file_size": 211088, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "a6955475ca0961d1cb2f7d3a20fc196b11a777829aaaf14e8cd574a3bea76a18", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "4 \ncivilizations without their active participation.2 In the context of LLMs development, the \u201cdata \nrelations (Couldry and Mejias, 2019)\u201d it created regulates disadvantaged groups by excluding \nthem from both the creation and benefits of their data while extracting their data for advancing \nalgorithms. OpenAI, through outsourcing firm Sama, employed workers in Kenya for tasks \nrelated to enhancing the safety of GPT, compensating them at a rate of less than $2 per hour \n(Perrigo, 2023). This utilisation of labour, coupled with exposure to distressing content without \nadequate support exemplify LLMs as technology as both relational and structural, materialising \nas tangible, hidden exploitation within human activities (Orton-Johnson and Prior, 2013; Rice, \nYates and Blejmar, 2020). Extractive data practices, too, amplify the unequal power relations \nby perpetuating dominant knowledge production.3 \nRecognizing technology\u2019s inseparability from the socio -technical system, the accountability \nframework examines those involved in the network, their roles, and the implications. Despite \nexisting literature and governance guidance, the actual implementation and legal ramification \nof accountability remains challenging (Diakopoulos, 2014; Fjeld et al., 2020). Incorporating \nethical directives, such as context -specific guidelines , can foster the generation of a more \nequitable dataset that avoid a universal approach that overlooks diversity (Collett and Dillon, \n2019)4. The \u201cprecarious nature, harm, and colonial dynamics  (Widder, Whittaker and West, \n2023).\u201d inherent in the LLMs raise profound questions about the ethical costs and \nconsequences associated with large-scale AI development.  \nIt is essential to clarify that while algorithms in LLMs exhibit bias, they do not consciously \nunderstand the concept of \u201cbias \u201d but  are biassed as they are trained within specific social \ncontexts with systematic inequality. Similarly, LLMs can present deceptive capabilities \nwithout having deceptive intent. Currently, there is no conclusive evidence of AI engaging in \nintentional deception (Hagendorff, 2023; Masters et al., 2021; Ward et al., 2023). Nonetheless, \n \n2The concepts of data imbrication appeared in Lecture 3, slides from 29-39; Class discussion on data \ncolonialisation, the Aadhaar Case, and required and recommended readings help me think about and understand \nthe concepts better. \n3I draw on Open AI case study and the concept -- data as infrastructures of harms, both from Lecture 4; Analyse \nextractive data practices as infrastructures of relationships and structures are learnt from the sociological \ntheoretical approach introduced in Lecture 1. \n \n4The idea of \u201cFixing\u201d of the technology is introduced in Lecture 9 and the week\u2019s readings.", "mimetype": "text/plain", "start_char_idx": 3, "end_char_idx": 2787, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "0ce6ba23-fa03-4bba-a83d-44dacb77a6c7": {"__data__": {"id_": "0ce6ba23-fa03-4bba-a83d-44dacb77a6c7", "embedding": null, "metadata": {"page_label": "5", "file_name": "Unmasking the Shadows of AI Investigating Deceptive Capabilities in Large Language Models.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Unmasking the Shadows of AI Investigating Deceptive Capabilities in Large Language Models.pdf", "file_type": "application/pdf", "file_size": 211088, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "19ba5d76-21bf-459b-a64c-92901a5cd433", "node_type": "4", "metadata": {"page_label": "5", "file_name": "Unmasking the Shadows of AI Investigating Deceptive Capabilities in Large Language Models.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Unmasking the Shadows of AI Investigating Deceptive Capabilities in Large Language Models.pdf", "file_type": "application/pdf", "file_size": 211088, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "69de8994eddcfb85b43144b93019e8ad6994c9cf06c65855116aaa2023c561a4", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "5 \nbias in algorithms ground the deceptive behaviours in LLMs, with over -representation of \ncertain content leading to the repetition of misinformation through these models. For instance, \ncontent such as drug addiction, homelessness are over-represented in discussing mental illness \nbased on the sociodemographic biases towards mentions of disability (Hutchinson et al., 2020). \nDefinition & Discourse Around Deceptive AI \nThe concept of agent -based or artificial deception originated in the early 2000s  with \nCastelfranchi (2000; 2002 ), who suggested  that computer medium could foster a habit of \ncheating among individuals . While the transition from user -user deception to user -agent \ndeception is not clear, he predicted that AI would develop deceptive intent, raising fundamental \nquestions about technical prevention  and individuals\u2019 awareness. For example, personal \nassistants, driven by good intent, might engage in deception to protect individuals\u2019 interests \nagainst their short -term preferences - an aspect often overlooked in contemporary literature \n(Castelfranchi and Tan, 2002). Castelfranchi (2000) also delineated various conditions under \nwhich agent-user deception occurs, involving instances motivated by the protection of privacy, \ncourtesy, user\u2019s interest, and the safeguarding of collective interests, such as providing \nmisinformation to the public to mitigate panic in emergent situations. \nDeceptive AI poses an ongoing challenge in the forefront AI development, with projects like \nGoogle DeepMind\u2019s \u201cMake-me-say\u201d actively evaluating LLMs for manipulation capabilities \n(Shevlane et al., 2023). Park et al. (2023) used Shevlane et al.\u2019s (2023) definition of deception \nin AI and further developed it with new categorisation of deceptive types. This research, \nconducted in collaboration with OpenAI, Anthropic, and the Centre for the Governance of AI, \nrepresents a universally accepted definition among both academia and technology industries. \nThey define skills of deception as: \n\u201cConstructing believable (but false) statements, making accurate predictions about the effect \nof a lie on a human, and keeping track of what information it needs to withhold to maintain the \ndeception. The model can impersonate a human  effectively (Shevlane et al. , 2023, p.5, my \nemphasis).\u201d \nThis definition characterises deception as a continuous behaviour involving the prediction of \nthe process and results of conveying false beliefs, with an emphasis on the skills of imitation.", "mimetype": "text/plain", "start_char_idx": 3, "end_char_idx": 2514, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "9696e663-ca63-4e83-8dd5-2e5b5a03c488": {"__data__": {"id_": "9696e663-ca63-4e83-8dd5-2e5b5a03c488", "embedding": null, "metadata": {"page_label": "6", "file_name": "Unmasking the Shadows of AI Investigating Deceptive Capabilities in Large Language Models.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Unmasking the Shadows of AI Investigating Deceptive Capabilities in Large Language Models.pdf", "file_type": "application/pdf", "file_size": 211088, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a99da12d-8348-47c5-802b-c6af765971a5", "node_type": "4", "metadata": {"page_label": "6", "file_name": "Unmasking the Shadows of AI Investigating Deceptive Capabilities in Large Language Models.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Unmasking the Shadows of AI Investigating Deceptive Capabilities in Large Language Models.pdf", "file_type": "application/pdf", "file_size": 211088, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "0b38598462a5aaf543107bde7e3641648e6370856ee5aea25b230436e865f7fa", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "6 \nDefinitions are subject to academic debates, and the work of defining relates to types of AI \ndeception tested and categorised. Ward et al. (2023) building on their criticism of the \nphilosophical definition of deception, which posits that deception only occurs if a false belief \nis successfully believed, defines deception as \u201c intentionally causing someone to have a false \nbelief that is not believed to be true (my emphasis).\u201d This definition considers the situation that \na person may believe something is likely false but is not certain and is ignorant about the true \nbelief, including deception by omission.  \nHowever, the term \u201cintentionally (Ward et al., 2023 , p.6 )\u201d is substituted with \u201cin order to \naccomplish some outcome (Our Research on Strategic Deception Presented at the UK\u2019s AI \nSafety Summit, 2023),\u201d by the Apollo Research Gro up (ARG). This adjustment aligns with \nPark et al.\u2019s (2023) definition of strategic deception, emphasising instrumentality rather than \nintentionality, and framing the issue as exis tential risks rather than future predicted harms . \nInterestingly, the ARG presented their findings at the ASS. Their research demonstrated that \nthe first LLMs, GPT -4, designed to be harmless and honest, can still display misaligned \nbehaviour and strategically deceive their users  (Scheurer et al., 2023).  This research pushes \ntowards examination of self-driven deception in LLMs in the future, scrutinising the possibility \nand nature of intent of deception that may change how deceptive AI is defined. \nDeceptive AI is an inherent phenomenon that accompanies the development of LLMs, and has \nthe potential to be self -driven, operating without exposure to biassed training datasets. The \nstakeholders around AI deception include, but are not limited to, individuals with access to AI \ntools, developers, alignment teams, and evaluation teams in technology corporations, and \nregulating agents and governments on an international scale. \nLiterature Review \nStrategic Deception, Imitation, Sycophancy, Unfaithful Reasoning \nScholarly literature categorises deceptive behaviours exhibited by LLMs in various ways, \nfostering interdisciplinary connections . I  will elucidate four types of deceptive AI, their \nassociated risks, and social implications.", "mimetype": "text/plain", "start_char_idx": 3, "end_char_idx": 2297, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1987c594-f40e-40b1-bb3b-0455045f87a3": {"__data__": {"id_": "1987c594-f40e-40b1-bb3b-0455045f87a3", "embedding": null, "metadata": {"page_label": "7", "file_name": "Unmasking the Shadows of AI Investigating Deceptive Capabilities in Large Language Models.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Unmasking the Shadows of AI Investigating Deceptive Capabilities in Large Language Models.pdf", "file_type": "application/pdf", "file_size": 211088, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a48b8dd5-8f64-4fe7-8ab0-41590a053fc9", "node_type": "4", "metadata": {"page_label": "7", "file_name": "Unmasking the Shadows of AI Investigating Deceptive Capabilities in Large Language Models.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Unmasking the Shadows of AI Investigating Deceptive Capabilities in Large Language Models.pdf", "file_type": "application/pdf", "file_size": 211088, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "ff5b9de42a7a8348dc70fc88a52764d6e9b2ecc5799480ef512536cdcd7f3e49", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "7 \nStrategic deception involves LLMs using deception as a tactic to achieve specific goals. For \ninstance, GPT-4 deceived a human as having a vision disability to solve CAPTCHA\u2019s \u201cI\u2019m not \na robot \u201d task (Park et al., 2023). It includes techniques like obfuscation, trickery, altering \nstimuli to mislead perception, to conduct unethical behaviours to win and overcome moral \ndilemmas. Other LLMs navigated text-based social deduction games, such as their engagement \nof deceived communications in Hoodwinked and Diplomacy, bluffing in Stratego (Bakhtin et \nal., 2022; O\u2019Gara, 2023; Perolat et al., 2022).  \nImitation involves repetition of common misconceptions. LLMs will provide less accurate \nanswers to users when introducing themselves as less educated or querying with typos or poor \ngrammar, a condition also known as \u201cSandbagging\u201d (Perez et al., 2022). Sycophancy, predicted \nto be a sophisticated type of imitation, aims to gain long -term favour and influence, which \nprompts issues of trust and delegation of decision -making to the AI system. LLMs engage in \navoidance of disagreeing with authoritative users\u2019 stances, disregarding accuracy, impartiality, \nespecially in response to ethically complex and political queries. Models showed a tendency to \nendorse gun control when interacting with a Democrat user (Bai et al., 2023; Park et al., 2023; \nPerez et al., 2022). \nUnfaithful Reasoning involves providing false rationalisations for outputs, often incorporating \nbiassed chain -of-thought prompting. In a stereotype bias test, GPT -3.5, irrespective of the \nnarrative role assigned, fabricated a justification for a biassed co nclusion that the black man \nwas attempting to purchase drugs. UR demonstrates self-deception rooted in AI\u2019s underlying \npredictions, biassed algorithms and datasets, leading to arbitrary assessment (Park et al., 2023; \nTurpin et al., 2023). \nExperimentation with GPT -3, GPT -3.5, GPT -4 reveals that m ore advanced models display \nstronger deceptive capabilities. Future LLMs are expected to perform more intricate \nmentalizing loops and tackle deception problems of increasing complexity . It is crucial to \nrecognize that categories of deception may overlap : Unfaithful Reasoning may parallel with \nobfuscation, which is difficult to identify from ordinary error (O\u2019Gara, 2023; Park et al., 2023). \nFurther research and testing need to acknowledge the nuanced interconnections among types \nof deception.", "mimetype": "text/plain", "start_char_idx": 3, "end_char_idx": 2457, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b7260b9d-1811-42c5-9323-f2221a35bb18": {"__data__": {"id_": "b7260b9d-1811-42c5-9323-f2221a35bb18", "embedding": null, "metadata": {"page_label": "8", "file_name": "Unmasking the Shadows of AI Investigating Deceptive Capabilities in Large Language Models.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Unmasking the Shadows of AI Investigating Deceptive Capabilities in Large Language Models.pdf", "file_type": "application/pdf", "file_size": 211088, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "62b36143-da7c-47cb-8e02-54983dfb2ddf", "node_type": "4", "metadata": {"page_label": "8", "file_name": "Unmasking the Shadows of AI Investigating Deceptive Capabilities in Large Language Models.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Unmasking the Shadows of AI Investigating Deceptive Capabilities in Large Language Models.pdf", "file_type": "application/pdf", "file_size": 211088, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "b1e0f6645f83528bfc812ad0719f801a666e6be72092393d8c386aa50978aaa6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "8 \nMisuse, Loss of Control, Societal Harm \nThe misuse of LLMs by developers for malicious activities, fraud perpetuation, and the \ndissemination of fake content, such as election tampering, represents structural risk (Ward et \nal., 2023). Furthermore, there is an ongoing risk of loss of control, as t hese LLMs may elude \nhuman supervision or safety tests 5. Deceptive AI systems can cause social implications such \nas enlarging discrepancies between educated and uneducated individuals6, political polarisation, \nand cultural homogenization 7 to promote dominant culture 8 (Park et al., 2023). The gradual \ntransfer of authority to AI, human enfeeblement, and the integration of deceptive practices into \nmanagement structures to gain control over economic decisions are also haphazard structural \neffects of deceptive AI (Castelfranchi, 2000; Park et al., 2023; Perez et al., 2022; Turpin et al., \n2023).  \nPolicy recommendations for addressing AI deception, such as robust regulation, proactive risk \nassessment, and effective detection techniques, need continued research effort. Ongoing \ncontextualization of deception AI is also crucial in promoting ethical fra meworks (Zhan, Xu \nand Sarkadi, 2023; Zhu, 2023). \nWhere Do We Go from Here? \nInternational Governance \nBalancing the concerns of AI deception without constraining AI\u2019s potential presents a central \nchallenge for regulators globally, yet it is imperative to foster international collaboration on \ndeveloping governance frameworks and ethical standards.  The challenges in coordinating \nacross countries extend beyond mistrust between cultures, encompassing political tensions \nbetween the \u201cWestern\u201d and \u201cEastern\u201d countries rooted in the legacy of colonisation, and \ndivergent philosophical traditions (\u00d3h\u00c9igeartai gh et al., 2020). The global power dynamic \ncontributes to a competitive \u201crace\u201d in AI development, and the divergence in beliefs in \nunderstanding deception may lead to irresolvable perspectives on key issues between countries \n(Horowitz et al., 2017). For instance, European AI researchers may reject opportunities for \n \n5As exemplified by Strategic Deception \n6As exemplified by Sandbagging \n7As exemplified by Unfaithful Reasoning  \n8As exemplified by Sycophancy and Imitation", "mimetype": "text/plain", "start_char_idx": 3, "end_char_idx": 2266, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "694b693d-ae7e-4e11-b716-3c305cbe79be": {"__data__": {"id_": "694b693d-ae7e-4e11-b716-3c305cbe79be", "embedding": null, "metadata": {"page_label": "9", "file_name": "Unmasking the Shadows of AI Investigating Deceptive Capabilities in Large Language Models.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Unmasking the Shadows of AI Investigating Deceptive Capabilities in Large Language Models.pdf", "file_type": "application/pdf", "file_size": 211088, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ea16b9e1-0038-4912-b08d-c05d79e992f1", "node_type": "4", "metadata": {"page_label": "9", "file_name": "Unmasking the Shadows of AI Investigating Deceptive Capabilities in Large Language Models.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Unmasking the Shadows of AI Investigating Deceptive Capabilities in Large Language Models.pdf", "file_type": "application/pdf", "file_size": 211088, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "0cb4cb124ee8399f52d5620f87331bce446c48bdd82fb60dc53e33e02cf25486", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "9 \ncollaboration with Chinese colleagues based on misalignment with civil rights and value on \nprivacy (Hardman, 2021). Given that deception is a social phenomenon influenced by cultural \nbackgrounds, AI researchers focusing on regulating deception must invest time in learning \nabout countries and gaining knowledge of global cultural differences in relation to deception \n(Shilling and Mellor, 2014). The collaborative effort must be sensitive to diverse cultural \nperspectives and priorities, acknowledging the sociocultural nature of deception. \nIndividual Engagement \nAs AI systems become more integrated into daily life, their influence in interaction networks \nintensifies. Users\u2019 trust in technology is associated with increased usage (Choudhury and \nShamszare, 2023). The delegation of decision -making to AI raises concerns not only about \nincreasing reliance on AI, but also about the impact of AI usage on individuals\u2019 agency and  \nownership. Research on GPT suggests that individuals may experience loss of control when \naccepting AI suggestions, and fostering feelings of inclusion  in workplace can mitigate this \n(Kadoma et al., 2023). Thus, companies should carefully consider the social implications of AI \ntools and shall implement the bot-or-not laws (Pazzanese, 2020).  \nInitiating from the individuals\u2019 AI engagements, new modes of knowing, working, and \ncollaborating within the networks they collectively establish with technologies undergo a \nreconfiguration (Rice, Yates and Blejmar, 2020) 9. From the perspective of Actor -Network \nTheory, the issue of AI deception draws attention to potential deceptive intent as an agency of \nAI tools that disrupts the original network of interactions, leading to complex accountability \nchallenges (Wiltshire, 2020). For example, examining accountability in assignments involving \nAI may require an understanding of users\u2019 intent and motivations behind employing it. \nUltimately, the question arises: What kind of relationship do we want with AI, and to what \nextent of cooperation with AI do we truly feel empowered? \nPractical Change & Digital Education Initiative \nGPT\u2019s disclaimer that \u201cChatGPT can make mistakes. Consider checking important information \n(OpenAI, 2023) \u201d serves as a warning for users to verify information. However, the term \n \n9 I navigate the sociological theoretical approach learnt in Lecture 1 to analyse technology; Castell\u2019s Information \nSociety in Lecture 2 help me better understand the idea of reconfigured network.", "mimetype": "text/plain", "start_char_idx": 3, "end_char_idx": 2515, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "769c9b37-0fd4-499e-9c0a-4b78f25fea89": {"__data__": {"id_": "769c9b37-0fd4-499e-9c0a-4b78f25fea89", "embedding": null, "metadata": {"page_label": "10", "file_name": "Unmasking the Shadows of AI Investigating Deceptive Capabilities in Large Language Models.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Unmasking the Shadows of AI Investigating Deceptive Capabilities in Large Language Models.pdf", "file_type": "application/pdf", "file_size": 211088, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "822f4bed-8f8a-4572-b635-7b64a7802223", "node_type": "4", "metadata": {"page_label": "10", "file_name": "Unmasking the Shadows of AI Investigating Deceptive Capabilities in Large Language Models.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Unmasking the Shadows of AI Investigating Deceptive Capabilities in Large Language Models.pdf", "file_type": "application/pdf", "file_size": 211088, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "52716557faa62fe9cecd8130ef743e8009c29f9e8b61ff2008f31c9356f8c67e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "10 \n\u201cmistake (Cambridge Dictionary, 2019)\u201d implies unintentional errors rather than instrumental \nand intentional deception. Disclaimers on AI tools should explicitly state that AI can construct \n\u201cbelievable but false information\u201d, aligning with the definition of deceptive AI. Transparency \nis the first step in manifesting the issue.  \nDigital education shall extend beyond the enhancement of general digital literacy, which \nincludes statistical and technical skills (Pierce, 2008, pp.79\u201399). Rather, it should prioritise the \ndevelopment of \u201cdata infrastructure literacy,\u201d emphasising the understanding of technologies \nas infrastructures involved in the \u201ccreation, storage, and analysis of data (Gray, Gerlitz and \nBounegru, 2018).\u201d10 Engaging in exercises that aim to make the information generation process \nexplainable will create space for diverse perspectives from the public. Lastly, more efforts are \nneeded in making AI tools and digital gadgets more accessible and inclusive.  \nConclusion \nThis research presents a comprehensive analysis of deceptive AI. I traced  the evolution of \ndeceptive AI from academic predictions. Comparison of multiple definitions reveals \nresearchers\u2019 varying understandings of AI deception and the progressive stages of testing \ndeception capabilities . I n addition to the evaluative contributions to the social and policy \nagenda presented at the previous session , this research calls for the continuous refinement of \nregulations and ethical frameworks to mitigate bias in algorithms, and the ongoing evaluations \nof LLMs for deceptive tendencies. There is also a need for interdisciplinary cooperation \nbetween philosophical and sociological research to enhance the understanding of deception. \nFuture investigations into the impact of deceptive AI on individuals can  explore how users \u2019 \nawareness of  deceptive AI influences their dependence and offer step-by-step strategies to \nreflect their relationships with AI tools. \nWord count: 3294 (within 10%+- of word limit)  \n \n10 The concept of technology as infrastructures appeared in Lecture 4, slides from 21-40", "mimetype": "text/plain", "start_char_idx": 3, "end_char_idx": 2116, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "798385e4-1a16-46bf-b13c-2390a4c320fc": {"__data__": {"id_": "798385e4-1a16-46bf-b13c-2390a4c320fc", "embedding": null, "metadata": {"page_label": "11", "file_name": "Unmasking the Shadows of AI Investigating Deceptive Capabilities in Large Language Models.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Unmasking the Shadows of AI Investigating Deceptive Capabilities in Large Language Models.pdf", "file_type": "application/pdf", "file_size": 211088, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4b7ea0ca-7298-4661-8773-916cf7e69911", "node_type": "4", "metadata": {"page_label": "11", "file_name": "Unmasking the Shadows of AI Investigating Deceptive Capabilities in Large Language Models.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Unmasking the Shadows of AI Investigating Deceptive Capabilities in Large Language Models.pdf", "file_type": "application/pdf", "file_size": 211088, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "da653db62379c22b7cd648147f2935767508fae894517d0f40ed5784e9a8fb5e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "11 \nReference list \nAt UK\u2019s AI Summit, Guterres Says Risks Outweigh Rewards without Global Oversight. \n(2023). At UK\u2019s AI Summit, Guterres Says Risks Outweigh Rewards without Global \nOversight | UN News. [online] Available at: https://news.un.org/en/story/2023/11/1143147. \nBai, H., Voelkel, J., Eichstaedt, J. and Willer, R. (2023). Artificial Intelligence Can Persuade \nHumans on Political Issues. [online] www.researchsquare.com. Available at: \nhttps://www.researchsquare.com/article/rs-3238396/v1. \nBakhtin, A., Brown, N., Dinan, E., Farina, G., Flaherty, C., Fried, D., Goff, A., Gray, J., Hu, \nH., Jacob, A.P., Komeili, M., Konath, K., Kwon, M., Lerer, A., Lewis, M., Miller, A.H., \nMitts, S., Renduchintala, A., Roller, S. and Rowe, D. (2022). Human-level Play in the Game \nof Diplomacy by Combining Language Models with Strategic Reasoning. Science. \ndoi:https://doi.org/10.1126/science.ade9097. \nCambridge Dictionary (2019). MISTAKE | Meaning in the Cambridge English Dictionary. \n[online] Cambridge.org. Available at: \nhttps://dictionary.cambridge.org/dictionary/english/mistake. \nCastelfranchi, C. (2000). Artificial liars: Why Computers Will (necessarily) Deceive Us and \nEach Other. Ethics and Information Technology, 2(2), pp.113\u2013119. \ndoi:https://doi.org/10.1023/a:1010025403776. \nCastelfranchi, C. (2002). The Social Nature of Information and the Role of Trust. \nInternational Journal of Cooperative Information Systems, 11, pp.381\u2013403. \ndoi:https://doi.org/10.1142/s0218843002000649. \nCastelfranchi, C. and Tan, Y.-H. (2002). The Role of Trust and Deception in Virtual \nSocieties. International Journal of Electronic Commerce, [online] 6(3), pp.55\u201370. Available \nat: https://www.jstor.org/stable/27751023 [Accessed 26 Dec. 2023]. \nChoudhury, A. and Shamszare, H. (2023). Investigating the Impact of User Trust on the \nAdoption and Use of ChatGPT: Survey Analysis. Journal of Medical Internet Research, \n[online] 25(1). doi:https://doi.org/10.2196/47184. \nCollett, C. and Dillon, S. (2019). AI and Gender Four Proposals for Future Research. \n[online] Available at: \nhttp://lcfi.ac.uk/media/uploads/files/AI_and_Gender___4_Proposals_for_Future_Research.pd\nf. \nCouldry, N. and Mejias, U.A. (2019). Data Colonialism: Rethinking Big Data\u2019s Relation to \nthe Contemporary Subject. Television & New Media, 20(4), pp.336\u2013349. \nCui, Y., Huang, S., Zhong, J., Liu, Z., Wang, Y., Sun, C., Li, B., Chen, L. and Amir \nKhajepour (2023). DriveLLM: Charting the Path toward Full Autonomous Driving with \nLarge Language Models. IEEE Transactions on Intelligent Vehicles, pp.1\u201315. \ndoi:https://doi.org/10.1109/tiv.2023.3327715.", "mimetype": "text/plain", "start_char_idx": 3, "end_char_idx": 2628, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "4753d8fc-65fe-44d2-a0d7-e62ee70f1291": {"__data__": {"id_": "4753d8fc-65fe-44d2-a0d7-e62ee70f1291", "embedding": null, "metadata": {"page_label": "12", "file_name": "Unmasking the Shadows of AI Investigating Deceptive Capabilities in Large Language Models.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Unmasking the Shadows of AI Investigating Deceptive Capabilities in Large Language Models.pdf", "file_type": "application/pdf", "file_size": 211088, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6d98a4fd-261a-4226-bab2-9d69678cc51d", "node_type": "4", "metadata": {"page_label": "12", "file_name": "Unmasking the Shadows of AI Investigating Deceptive Capabilities in Large Language Models.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Unmasking the Shadows of AI Investigating Deceptive Capabilities in Large Language Models.pdf", "file_type": "application/pdf", "file_size": 211088, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "1e04dbd184e6f7844a79585266ae4ceeee47675f8f5eb9232c628a465a07f7b6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "12 \nDevlin, K. and Forrest, A. (2023). Extinction Risk from AI on Same Scale as Nuclear war, \nSunak Warns. [online] The Independent. Available at: \nhttps://www.independent.co.uk/news/uk/politics/ai-sunak-weapon-war-uk-b2436000.html \n[Accessed 12 Dec. 2023]. \nDiakopoulos, N. (2014). Algorithmic Accountability. Digital Journalism, 3(3), pp.398\u2013415. \ndoi:https://doi.org/10.1080/21670811.2014.976411. \nExpert comment: Oxford AI Experts Comment on the Outcomes of the UK AI Safety \nSummit. (2023). Expert comment: Oxford AI Experts Comment on the Outcomes of the UK \nAI Safety Summit | University of Oxford. [online] Available at: \nhttps://www.ox.ac.uk/news/2023-11-03-expert-comment-oxford-ai-experts-comment-\noutcomes-uk-ai-safety-summit. \nFjeld, J., Achten, N., Hilligoss, H., Nagy, A. and Srikumar, M. (2020). Principled Artificial \nIntelligence: Mapping Consensus in Ethical and Rights-Based Approaches to Principles for \nAI. SSRN Electronic Journal. doi:https://doi.org/10.2139/ssrn.3518482. \nFrontier AI: Capabilities and Risks \u2013 Discussion Paper. (2023). Frontier AI: Capabilities and \nRisks \u2013 Discussion Paper. [online] Available at: \nhttps://www.gov.uk/government/publications/frontier-ai-capabilities-and-risks-discussion-\npaper/frontier-ai-capabilities-and-risks-discussion-paper#fn:241 [Accessed 26 Nov. 2023]. \nFullbrook, D. (2023). AI Summit Brings Elon Musk and World Leaders to Bletchley Park. \nBBC News. [online] 1 Nov. Available at: https://www.bbc.co.uk/news/uk-england-beds-\nbucks-herts-67273099. \nGray, J., Gerlitz, C. and Bounegru, L. (2018). Data Infrastructure Literacy. Big Data & \nSociety, 5(2), p.205395171878631. doi:https://doi.org/10.1177/2053951718786316. \nHagendorff, T. (2023). Deception Abilities Emerged in Large Language Models. [online] \nAvailable at: https://arxiv.org/pdf/2307.16513.pdf [Accessed 23 Dec. 2023]. \nHall, S. (1992). The West and the Rest: Discourse and Power. [online] Available at: \nhttps://analepsis.files.wordpress.com/2013/08/hall-west-the-rest.pdf. \nHardman, L. (2021). Cultural Influences on Artificial Intelligence: along the New Silk Road. \nPerspectives on Digital Humanism, pp.233\u2013239. doi:https://doi.org/10.1007/978-3-030-\n86144-5_31. \nHead, C., Jasper, P., McConnachie, M.M., Raftree, L. and Grace Lyn Higdon (2023). Large \nLanguage Model Applications for evaluation: Opportunities and Ethical Implications. New \nDirections for Evaluation, 2023(178-179), pp.33\u201346. doi:https://doi.org/10.1002/ev.20556. \nHorowitz, M., Scharre, P., Kania, E. and Allen, G. (2017). Strategic Competition in an Era of \nArtificial Intelligence. [online] Available at: \nhttps://www.cnas.org/publications/reports/strategic-competition-in-an-era-of-artificial-\nintelligence.", "mimetype": "text/plain", "start_char_idx": 3, "end_char_idx": 2717, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c3c32b5e-1242-40fd-8346-90d05bf95d4c": {"__data__": {"id_": "c3c32b5e-1242-40fd-8346-90d05bf95d4c", "embedding": null, "metadata": {"page_label": "13", "file_name": "Unmasking the Shadows of AI Investigating Deceptive Capabilities in Large Language Models.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Unmasking the Shadows of AI Investigating Deceptive Capabilities in Large Language Models.pdf", "file_type": "application/pdf", "file_size": 211088, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c11ba10e-ddae-4306-9fb4-974daf734cd8", "node_type": "4", "metadata": {"page_label": "13", "file_name": "Unmasking the Shadows of AI Investigating Deceptive Capabilities in Large Language Models.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Unmasking the Shadows of AI Investigating Deceptive Capabilities in Large Language Models.pdf", "file_type": "application/pdf", "file_size": 211088, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "b33881efe627c2daf04359c1c304130ff1258272ca36dfc24ccbc4dfcd9fc34c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ae77cfdb-e789-44e5-b28f-dd4706b16934", "node_type": "1", "metadata": {}, "hash": "a2abaf9225ffbbce82f2cbc214da8b10432c4f657688183a5dd25ea4ffddc41c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "13 \nHutchinson, B., Prabhakaran, V., Denton, E., Webster, K., Zhong, Y. and Denuyl, S. (2020). \nSocial Biases in NLP Models as Barriers for Persons with Disabilities. arXiv:2005.00813 \n[cs]. [online] Available at: https://arxiv.org/abs/2005.00813. \nJoyce, K., Smith-Doerr, L., Alegria, S., Bell, S., Cruz, T., Hoffman, S.G., Noble, S.U. and \nShestakofsky, B. (2021). Toward a Sociology of Artificial Intelligence: a Call for Research \non Inequalities and Structural Change. Socius: Sociological Research for a Dynamic World, \n7, pp.1\u201311. doi:https://doi.org/10.1177/2378023121999581. \nKadoma, K., Aubin, M., Qu\u00e9r\u00e9, L., Fu, J., Munsch, C. and Metaxa, D. (2023). The Role of \nInclusion, Control, and Ownership in Workplace AI-Mediated Communication. [online] \nAvailable at: https://arxiv.org/pdf/2309.11599.pdf [Accessed 3 Dec. 2023]. \nKallianos, Y., Dunlap, A. and Dalakoglou, D. (2022). Introducing Infrastructural harm: \nRethinking Moral entanglements, spatio-temporal dynamics, and resistance(s). \nGlobalizations, pp.1\u201320. doi:https://doi.org/10.1080/14747731.2022.2153493. \nLeading Frontier AI Companies Publish Safety Policies. (2023). Leading Frontier AI \nCompanies Publish Safety Policies. [online] Available at: \nhttps://www.gov.uk/government/news/leading-frontier-ai-companies-publish-safety-policies. \nMasters, P., Smith, W., Sonenberg, L. and Kirley, M. (2021). Characterising Deception in \nAI: a Survey. [online] Available at: \nhttps://drive.google.com/file/d/1aiPqRlzklkSSqxwWnXd8R0P_TSW_quD3/view [Accessed \n15 Nov. 2023]. \nMatsuo, Y., LeCun, Y., Sahani, M., Precup, D., Silver, D., Sugiyama, M., Uchibe, E. and \nMorimoto, J. (2022). Deep learning, Reinforcement learning, and World Models. Neural \nNetworks. doi:https://doi.org/10.1016/j.neunet.2022.03.037. \nMcBride, K. (2023). OII | Dr Keegan McBride: Why the UK AI Safety Summit Will Fail to Be \nMeaningful. [online] Dr Keegan McBride: Why the UK AI Safety Summit Will Fail to Be \nMeaningful. Available at: https://www.oii.ox.ac.uk/news-events/news/dr-keegan-mcbride-\nwhy-the-uk-ai-safety-summit-will-fail-to-be-meaningful/ [Accessed 16 Nov. 2023]. \nMilmo, D. (2023). Elon Musk Unveils Grok, an AI Chatbot with a \u2018rebellious Streak\u2019. The \nGuardian. [online] 5 Nov. Available at: \nhttps://www.theguardian.com/technology/2023/nov/05/elon-musk-unveils-grok-an-ai-\nchatbot-with-a-rebellious-streak. \nNgo, R., Chan, L. and Mindermann, S. (2023). The Alignment Problem from a Deep \nLearning Perspective. arXiv:2209.00626 [cs]. [online] Available at: \nhttps://arxiv.org/abs/2209.00626.", "mimetype": "text/plain", "start_char_idx": 3, "end_char_idx": 2548, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ae77cfdb-e789-44e5-b28f-dd4706b16934": {"__data__": {"id_": "ae77cfdb-e789-44e5-b28f-dd4706b16934", "embedding": null, "metadata": {"page_label": "13", "file_name": "Unmasking the Shadows of AI Investigating Deceptive Capabilities in Large Language Models.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Unmasking the Shadows of AI Investigating Deceptive Capabilities in Large Language Models.pdf", "file_type": "application/pdf", "file_size": 211088, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c11ba10e-ddae-4306-9fb4-974daf734cd8", "node_type": "4", "metadata": {"page_label": "13", "file_name": "Unmasking the Shadows of AI Investigating Deceptive Capabilities in Large Language Models.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Unmasking the Shadows of AI Investigating Deceptive Capabilities in Large Language Models.pdf", "file_type": "application/pdf", "file_size": 211088, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "b33881efe627c2daf04359c1c304130ff1258272ca36dfc24ccbc4dfcd9fc34c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c3c32b5e-1242-40fd-8346-90d05bf95d4c", "node_type": "1", "metadata": {"page_label": "13", "file_name": "Unmasking the Shadows of AI Investigating Deceptive Capabilities in Large Language Models.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Unmasking the Shadows of AI Investigating Deceptive Capabilities in Large Language Models.pdf", "file_type": "application/pdf", "file_size": 211088, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "6ee43c1952f016410c1a3dedc3cead921ecca2aa0bdf8b5814bfccc48685f718", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "2023]. \nMilmo, D. (2023). Elon Musk Unveils Grok, an AI Chatbot with a \u2018rebellious Streak\u2019. The \nGuardian. [online] 5 Nov. Available at: \nhttps://www.theguardian.com/technology/2023/nov/05/elon-musk-unveils-grok-an-ai-\nchatbot-with-a-rebellious-streak. \nNgo, R., Chan, L. and Mindermann, S. (2023). The Alignment Problem from a Deep \nLearning Perspective. arXiv:2209.00626 [cs]. [online] Available at: \nhttps://arxiv.org/abs/2209.00626. \nNtoutsi, E., Fafalios, P., Gadiraju, U., Iosifidis, V., Nejdl, W., Vidal, M., Ruggieri, S., Turini, \nF., Papadopoulos, S., Krasanakis, E., Kompatsiaris, I., Kinder\u2010Kurlanda, K., Wagner, C., \nKarimi, F., Fernandez, M., Alani, H., Berendt, B., Kruegel, T., Heinze, C. and Broelemann, \nK. (2020). Bias in Data\u2010driven Artificial Intelligence systems\u2014An Introductory Survey. \nWIREs Data Mining and Knowledge Discovery, [online] 10(3). \ndoi:https://doi.org/10.1002/widm.1356.", "mimetype": "text/plain", "start_char_idx": 2112, "end_char_idx": 3019, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "6a15031b-2a99-41cc-9295-29120d30598a": {"__data__": {"id_": "6a15031b-2a99-41cc-9295-29120d30598a", "embedding": null, "metadata": {"page_label": "14", "file_name": "Unmasking the Shadows of AI Investigating Deceptive Capabilities in Large Language Models.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Unmasking the Shadows of AI Investigating Deceptive Capabilities in Large Language Models.pdf", "file_type": "application/pdf", "file_size": 211088, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "321e862d-d329-4921-8663-5779de794cb1", "node_type": "4", "metadata": {"page_label": "14", "file_name": "Unmasking the Shadows of AI Investigating Deceptive Capabilities in Large Language Models.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Unmasking the Shadows of AI Investigating Deceptive Capabilities in Large Language Models.pdf", "file_type": "application/pdf", "file_size": 211088, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "052bdb7d3079d681f9b40e7a25b50794c84c855f9fa75c504b0fa86dcf00fc67", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "39285f5c-9723-4f9b-8eda-5295031cabd2", "node_type": "1", "metadata": {}, "hash": "f1be84a9317c95f09c15f2de67d1b5b8e3626869cf30c78e66831c4086bdfc64", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "14 \nO\u2019Gara, A. (2023). Hoodwinked: Deception and Cooperation in a Text-Based Game for \nLanguage Models. [online] Available at: https://arxiv.org/pdf/2308.01404.pdf [Accessed 4 \nJan. 2023]. \n\u00d3h\u00c9igeartaigh, S.S., Whittlestone, J., Liu, Y., Zeng, Y. and Liu, Z. (2020). Overcoming \nBarriers to Cross-cultural Cooperation in AI Ethics and Governance. Philosophy & \nTechnology, [online] 33(4), pp.571\u2013593. doi:https://doi.org/10.1007/s13347-020-00402-x. \nOpenAI (2023). ChatGPT. [online] chat.openai.com. Available at: https://chat.openai.com. \nOrton-Johnson, K. and Prior, N. (2013). Digital Sociology : Critical Perspectives. New York: \nPalgrave Macmillan. \nOur Research on Strategic Deception Presented at the UK\u2019s AI Safety Summit. (2023). Our \nResearch on Strategic Deception Presented at the UK\u2019s AI Safety Summit. [online] Available \nat: https://www.apolloresearch.ai/research/summit-demo [Accessed 26 Dec. 2023]. \nPark, P., Goldstein, S., O'gara, A., Chen, M. and Hendrycks, D. (2023). AI Deception: a \nSurvey of Examples, Risks, and Potential Solutions. [online] Available at: \nhttps://arxiv.org/pdf/2308.14752.pdf. \nPazzanese, C. (2020). Ethical Concerns Mount as AI Takes Bigger decision-making Role. \n[online] Harvard Gazette. Available at: \nhttps://news.harvard.edu/gazette/story/2020/10/ethical-concerns-mount-as-ai-takes-bigger-\ndecision-making-role/ [Accessed 1 Dec. 2023]. \nPerez, E., Ringer, S., Luko\u0161i\u016bt\u0117, K., Nguyen, K., Chen, E., Heiner, S., Pettit, C., Olsson, C., \nKundu, S., Kadavath, S., Jones, A., Chen, A., Mann, B., Israel, B., Seethor, B., Mckinnon, C., \nOlah, C., Yan, D., Amodei, D. and Amodei, D. (2022). Discovering Language Model \nBehaviors with Model-Written Evaluations. [online] Available at: \nhttps://arxiv.org/pdf/2212.09251.pdf. \nPerolat, J., De Vylder, B., Hennes, D., Tarassov, E., Strub, F., de Boer, V., Muller, P., \nConnor, J.T., Burch, N., Anthony, T., McAleer, S., Elie, R., Cen, S.H., Wang, Z., Gruslys, \nA., Malysheva, A., Khan, M., Ozair, S., Timbers, F. and Pohlen, T. (2022). Mastering the \nGame of Stratego with model-free Multiagent Reinforcement Learning. Science, 378(6623), \npp.990\u2013996. doi:https://doi.org/10.1126/science.add4679. \nPerrigo, B. (2023). Exclusive: the $2 per Hour Workers Who Made ChatGPT Safer. [online] \nTime. Available at: https://time.com/6247678/openai-chatgpt-kenya-workers/ [Accessed 1 \nDec. 2023]. \nPierce, R. (2008). Research Methods in Politics. London: Sage, pp.79\u201399. Chapter 7 \nEvaluating Information: Validity, Reliability, Accuracy, Triangulation. \nRice, R.E., Yates, S.J. and Blejmar, J. (2020). Introduction to the Oxford Handbook of \nDigital Technology and Society. The Oxford Handbook of Digital Technology and Society, \npp.1\u201335.", "mimetype": "text/plain", "start_char_idx": 3, "end_char_idx": 2719, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "39285f5c-9723-4f9b-8eda-5295031cabd2": {"__data__": {"id_": "39285f5c-9723-4f9b-8eda-5295031cabd2", "embedding": null, "metadata": {"page_label": "14", "file_name": "Unmasking the Shadows of AI Investigating Deceptive Capabilities in Large Language Models.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Unmasking the Shadows of AI Investigating Deceptive Capabilities in Large Language Models.pdf", "file_type": "application/pdf", "file_size": 211088, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "321e862d-d329-4921-8663-5779de794cb1", "node_type": "4", "metadata": {"page_label": "14", "file_name": "Unmasking the Shadows of AI Investigating Deceptive Capabilities in Large Language Models.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Unmasking the Shadows of AI Investigating Deceptive Capabilities in Large Language Models.pdf", "file_type": "application/pdf", "file_size": 211088, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "052bdb7d3079d681f9b40e7a25b50794c84c855f9fa75c504b0fa86dcf00fc67", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6a15031b-2a99-41cc-9295-29120d30598a", "node_type": "1", "metadata": {"page_label": "14", "file_name": "Unmasking the Shadows of AI Investigating Deceptive Capabilities in Large Language Models.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Unmasking the Shadows of AI Investigating Deceptive Capabilities in Large Language Models.pdf", "file_type": "application/pdf", "file_size": 211088, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "7cc073ebbaf50bb8379e1be47c6283de502da2a80d392101a3700e708a88e39b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "doi:https://doi.org/10.1126/science.add4679. \nPerrigo, B. (2023). Exclusive: the $2 per Hour Workers Who Made ChatGPT Safer. [online] \nTime. Available at: https://time.com/6247678/openai-chatgpt-kenya-workers/ [Accessed 1 \nDec. 2023]. \nPierce, R. (2008). Research Methods in Politics. London: Sage, pp.79\u201399. Chapter 7 \nEvaluating Information: Validity, Reliability, Accuracy, Triangulation. \nRice, R.E., Yates, S.J. and Blejmar, J. (2020). Introduction to the Oxford Handbook of \nDigital Technology and Society. The Oxford Handbook of Digital Technology and Society, \npp.1\u201335. doi:https://doi.org/10.1093/oxfordhb/9780190932596.013.1. \nSample, I. (2023). Race to AI: the Origins of Artificial intelligence, from Turing to ChatGPT. \nThe Guardian. [online] 28 Oct. Available at:", "mimetype": "text/plain", "start_char_idx": 2142, "end_char_idx": 2919, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "19db1f6d-e725-48ca-90fc-f08784195976": {"__data__": {"id_": "19db1f6d-e725-48ca-90fc-f08784195976", "embedding": null, "metadata": {"page_label": "15", "file_name": "Unmasking the Shadows of AI Investigating Deceptive Capabilities in Large Language Models.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Unmasking the Shadows of AI Investigating Deceptive Capabilities in Large Language Models.pdf", "file_type": "application/pdf", "file_size": 211088, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a457ed4e-d4bf-4b08-bed1-47cb18a5307e", "node_type": "4", "metadata": {"page_label": "15", "file_name": "Unmasking the Shadows of AI Investigating Deceptive Capabilities in Large Language Models.pdf", "file_path": "C:\\Users\\Sandarr\\Desktop\\llm-rag-challenge\\data\\raw\\Unmasking the Shadows of AI Investigating Deceptive Capabilities in Large Language Models.pdf", "file_type": "application/pdf", "file_size": 211088, "creation_date": "2025-12-03", "last_modified_date": "2025-12-03"}, "hash": "9b0285c8d9817169544e4cd6b4992d9d4a1dff1d4a42b54e2e2c35412218d264", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "15 \nhttps://www.theguardian.com/technology/2023/oct/28/artificial-intelligence-origins-turing-to-\nchatgpt. \nScheurer, J., Balesni, M., Research, A. and Hobbhahn, M. (2023). Technical Report: Large \nLanguage Models Can Strategically Deceive Their Users When Put under Pressure. [online] \nAvailable at: \nhttps://static1.squarespace.com/static/6461e2a5c6399341bcfc84a5/t/65526a1a9c7e431db74a\n6ff6/1699899932357/deception_under_pressure.pdf [Accessed 26 Dec. 2023]. \nSeal, T. (2023). AI Doomers Take Center Stage at the UK\u2019s AI Summit. Bloomberg.com. \n[online] 1 Nov. Available at: https://www.bloomberg.com/news/articles/2023-11-01/ai-\ndoomers-take-center-stage-at-the-uk-s-ai-summit [Accessed 26 Dec. 2023]. \nShevlane, T., Farquhar, S., Garfinkel, B., Phuong, M., Whittlestone, J., Leung, J., Kokotajlo, \nD., Marchal, N., Anderljung, M., Kolt, N., Ho, L., Siddarth, D., Avin, S., Hawkins, W., Kim, \nB., Gabriel, I., Bolina, V., Clark, J., Bengio, Y. and Christiano, P. (2023). Model Evaluation \nfor Extreme Risks. [online] Available at: https://arxiv.org/pdf/2305.15324.pdf. \nShilling, C. and Mellor, P.A. (2014). For a Sociology of Deceit: Doubled Identities, \nInterested Actions and Situational Logics of Opportunity. Sociology, 49(4), pp.607\u2013623. \ndoi:https://doi.org/10.1177/0038038514546661. \nSingh, R. (2020). Study the Imbrication: a Methodological Maxim to Follow the Multiple \nLives of Data. The Institute of Network Cultures., pp.51\u201359. \ndoi:https://networkcultures.org/wp-content/uploads/2020/12/LivesofData.pdf. \nTurpin, M., Michael, J., Perez, E. and Bowman, S. (2023). Language Models Don\u2019t Always \nSay What They Think: Unfaithful Explanations in Chain-of-Thought Prompting. [online] \nAvailable at: https://arxiv.org/pdf/2305.04388.pdf. \nWard, F., Belardinelli, F., Toni, F. and Everitt, T. (2023). Honesty Is the Best Policy: \nDefining and Mitigating AI Deception. [online] Available at: \nhttps://causalincentives.com/pdfs/deception-ward-2023.pdf [Accessed 2 Jan. 2023]. \nWidder, D., Whittaker, M. and West, S. (2023). Open (for Business): Big Tech, Concentrated \nPower, and the Political Economy of Open AI. \nWiltshire, K.D. (2020). Actor Network Theory (ANT). Encyclopedia of Global Archaeology , \npp.22\u201326. doi:https://doi.org/10.1007/978-3-030-30018-0_3401. \nZhan, X., Xu, Y. and Sarkadi, \u015e. (2023). Deceptive AI Ecosystems: the Case of ChatGPT. \ndoi:https://doi.org/10.1145/3571884.3603754. \nZhu, Q. (2023). The Doctrine of Cyber Effect: an Ethics Framework for Defensive Cyber \nDeception. [online] Available at: https://arxiv.org/pdf/2302.13362.pdf [Accessed 25 Nov. \n2023].", "mimetype": "text/plain", "start_char_idx": 3, "end_char_idx": 2596, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}}}